{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Chicago%2C+IL&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for a Data Engineer to join our Data Engineering team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!\\n\\nAs a Data Engineer you will build and expand upon the testing framework and testing infrastructure of IAS' core ad verification, analytics and anti ad fraud software products.\\n\\nThe ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment. You should be able to not take yourself too seriously as well.\\n\\nWhat you'll do:\\n\\nWorking on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases\\nArchitect, design, code and maintain components for aggregating tens of billions of daily transactions\\nLead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's\\nMentor junior team members\\n\\nWho you are and what you have:\\n\\nUp to 5 years of recent hands-on experience with object oriented languages (Java, Scala, Python)\\nStrong knowledge of collections, multi-threading, JVM memory model, etc.\\nGreat understanding of designing for performance, scalability, and reliability\\nSuperb understanding of algorithms, data structures, scalability and various tradeoffs in a Big Data setting\\nIn-depth understanding of object oriented programming concepts\\nExcellent interpersonal and communication skills\\nUnderstanding of full software development life cycle, agile development and continuous integration\\nGood knowledge of Linux command line tools\\nExperience with Hadoop MapReduce, Spark, Pig\\nGood understanding of database fundamentals, good knowledge of SQL\\n\\nWhat puts you over the top:\\n\\nExposure to messaging frameworks like Kafka or RabbitMQ\\nSome exposure to functional programming languages like Scala\\nExperience with Spark\\n\\nAbout Integral Ad Science\\n\\nIntegral Ad Science (IAS) is the global market leader in digital ad verification, offering technologies that drive high-quality advertising media. IAS equips advertisers and publishers with both the insight and technology to protect their advertising investments from fraud and unsafe environments as well as to capture consumer attention, and drive business outcomes. Founded in 2009, IAS is headquartered in New York with global operations in 17 offices across 13 countries. IAS is part of the Vista Equity Partners portfolio of software companies. For more on how IAS is powering great impressions for top publishers and advertisers around the world, visit integralads.com ( http://integralads.com/ ).\\n\\nEqual Opportunity Employer:\\nIAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.\\n\\nTo learn more about us, please visit http://integralads.com/ ( http://integralads.com/ ) and https://muse.cm/2t8eGlN ( https://muse.cm/2t8eGlN )\\n\\nAttention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Why VillageMD?\\n\\nVillageMD is changing the trajectory of healthcare by empowering primary care physicians to make informed decisions and engage patients in meaningful ways. We work with thousands of clinicians and healthcare disruptors across the country to build and contribute to our platform to improve patient health while driving down the cost to deliver it.\\n\\nWe are a mission-oriented organization and are thrilled about the work that we do every day. We're transparent, collaborative, and relentless in pursuit of our mission, all while doing so with humility and a low ego. We believe that diverse backgrounds and experiences create the best opportunity for innovation and the community that we are creating is greater than any individual.\\n\\nWe've built our technology using the best of cloud and open-source technologies to create an open, data-first platform that is enriched with analytical models and modernly connected to internal and external apps. These apps drive clinical decision support, patient engagement, and other facilitators of innovative, information-enriched health experiences.\\n\\nData Engineers at VillageMD build distributed components, pipelines, and tools that enable our organization to make analytical, data-driven decisions. We're in a unique position to impact everyone in primary care from independent, family-owned practices to world-class health systems. We aggregate, process, and deliver rich datasets to improve the effectiveness of primary care for our doctors and patients.\\n\\nWhat are examples of work that Data Engineers have done at VillageMD?\\n\\n\\nBuilt and implemented a data profiling tool to reverse engineer data schemas from new data sources facilitating normalization of the data into our data model\\nCreated a summary data platform supporting our presentation layer that allows clinicians and operators in our practices to pinpoint interventions on-demand to patients most in need\\nAnalyzed and designed the best ways to expand our data model to incorporate more data that's mission critical\\n\\nWhat will make you successful here?\\n\\n\\nStrong analytical and technical skills\\nA real passion for problem solving and learning new technology\\nVision to balance speed and maintainability in solution design\\nThe ability to handle multiple, concurrent projects\\nCrafting and implementing requirements, keeping projects on track, and engaging partners\\nChallenging the status quo to improve our processes and tools\\nCommunicating complex technical details in meaningful business context\\nA low ego and humility; an ability to gain trust by doing what you say you will do\\n\\nWhat you might do in your first year:\\n\\nOwn ten projects to design and implement best-in-class data processing enabling clean data flow directly to our data model and on to our presentation layer\\nWork with analytics, engineering and operations to design and implement a new analytics product that supports improving patient health\\nDesign a new concept within our data model to meet a new operational or analytical need\\n\\nThe following experience is relevant to us:\\n\\n5+ years of full-time experience including extensive experience with healthcare data\\nAbility to understand and design relational data structures required\\nVery strong capabilities manipulating data using SQL\\nKnowledge of, and/or willingness to learn, non-relational data structures and other technologies (e.g. Postgres, Redshift, Cassandra, MongoDB, Neo4j, S3, etc.)\\nExperience or willingness to learn building information pipelines utilizing Python or Java a plus\\nBS/MS in computer science, math, engineering, or other related fields is required.\\nTrack record of successfully executing projects with multiple partners\\n\\nWhat can we offer you?\\n\\n\\nCompetitive salary, bonus, and health benefits\\nPaid gym membership\\nFun, fast-paced, startup environment (with snacks)\\nPre-tax savings on commute expenses\\nRemote flexibility\\nA highly-collaborative, conscientious, forward-thinking environment that welcomes the impact you can make from Day 1.\\nA clear link between our daily work on products and services and the improved quality of healthcare that this work facilitates for patients.\\n\\nAt VillageMD, we see diversity and inclusion as a source of strength in transforming healthcare. We believe building trust and innovation are best achieved through diverse perspectives. To us, acceptance and respect are rooted in an understanding that people do not experience things in the same way, including our healthcare system. Individuals seeking employment at VillageMD are considered without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cloud Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who We Are!\\nAt Maven Wave, we are relentless in hiring the industry’s top talent. Each employee is hand-picked not only for their skills, but for their personality and broad expertise. We are looking for this rare combination of talent that sets us apart in the industry.\\nMaven Wave helps leading companies make the shift to digital and shorten the fuse to innovation. We combine the expertise of top-tier consulting with the agility of a cutting-edge technology firm. This multidisciplinary blend of skills allows us to create unique digital advantages for our clients. Maven Wave’s digital solutions are agile, mobile, rooted in analytics, and built in the cloud.\\n\\nMaven Wave, Google, and YOU: Help us build data driven cloud solutions.\\nWe are looking for a skilled Cloud Data Engineer to design, build, and test data ingestion and ETL programs with a strong focus on performance and data quality management.\\n\\nYour Life As a Maven:\\nBuild and implement complex data solutions in the cloud (AWS, GCP and/or Microsoft).\\nUncover and recommend remediations for data quality anomalies.\\nInvestigate, recommend and implement data ingestion and ETL performance improvements.\\nDocument data ingestion and ETL program designs, present findings, conduct peer code reviews.\\nDevelop and execute test plans to validate code.\\n Your Expertise:\\n4+ years experience building complex ETL programs with Informatica, DataStage, Spark, Dataflow, etc.\\n3+ years experience in Python and/or Java, developing complex SQL queries, and working with relational database technologies.\\nExperience configuring big data solutions in a cloud environment (AWS, Azure or GCP).\\nExperience using cloud storage and computing technologies such as BigQuery, RedShift, or Snowflake.\\nExperience developing complex technical and ETL programs within a Hadoop ecosystem.\\n Your X-Factor:\\nAptitude - You have an innate capacity to transition from project to project without skipping a beat.\\nCommunication - You have excellent written and verbal communication skills for coordination across projects and teams.\\nImpact - You are a critical thinker with an emphasis on creativity and innovation.\\nPassion - You have the drive to succeed paired with a continuous hunger to learn.\\nLeadership - You are trusted, empathetic, accountable, and empower others around you.\\nWhy We’re Proud To Be Mavens!\\nGoogle Cloud North America Services Partner of the Year 2019, 2018\\n#21 Best Workplaces in Chicago, FORTUNE, 2018\\nGreat Place To Work Certification, Great Place to Work, 2017 &amp; 2018\\nFast Fifty, Crain's Chicago Business\\n101 Best and Brightest Companies to Work For, National Association for Business Resources (NABR)\\nTop Google Cloud Partner, Clutch\\nFastest Growing Consulting Firms in North America (#11, #37), Consulting Magazine\\nTop IT Services Companies, Clutch\\nGoogle Global Rising Star Partner of the Year\\nReady to Learn More?\\nLife as a Maven\\nCheck out the Apps and Data Team\\nSee what Glassdoor has to say\\nReal Customer Stories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Azure Data Architect</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of consulting or client service delivery experience on Azure\\n</td>\n",
       "      <td>DevOps on an Azure platform</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\n</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS &amp; NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nQualifications\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 10 years of experience in big data, database and data warehouse architecture and delivery\\nMinimum of 5 years of professional experience in 2 of the following areas:\\n§ Solution/technical architecture in the cloud\\n§ Big Data/analytics/information analysis/database management in the cloud\\n§ IoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training &amp; Certification\\nMCSE Cloud Platform &amp; Infratsructiure Training &amp; Certification\\nMCSD Azure Solutions Architect Training &amp; Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nStrong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big Data Engineer - Senior Consultant</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.\\n\\nWe are...\\n\\n • The Industry-recognized data and analytics leaders\\n • Passionate problem solvers across a broad spectrum of technologies and industries\\n • Value seekers for measurable business outcomes\\n • Continuous learners through training and education\\n • Focused on a work-life balance with an unlimited paid time off policy\\n\\nBig Data engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.\\nBig Data Engineers deliver value through...\\nHands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools\\nDemonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights\\nCollaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights\\nInternal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO\\nA Big Data Engineer's skills include, but are not limited to...\\nBachelors Degree and 5+ years of work experience\\n5+ years of professional IT work experience\\nSQL, SQL, SQL!\\n2+ years of Spark\\nProgramming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)\\nLinux / Windows (Command line)\\nBig Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop)\\nCloud Platforms (AWS, Azure, Google Cloud Platform)\\nData Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)\\nData Integration Tools (Talend, DataStage, Informatica, SSIS)\\nDatabases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)\\nMarkup Languages (JSON, XML, YAML)\\nCode Management Tools (Git/GitHub, SVN, TFS)\\nDevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)\\nTesting / Data Quality (TDD, unit, regression, automation)\\nSolving complex data and technology problems\\nLeading technical teams of 2+ consultants\\nAbility to design components of a larger implementation\\nExcellent communication to narrate data driven insights and technical approach\\n\\nIf this sounds like you, let’s talk!\\n\\nCandidates must be comfortable with a national travel model to client locations weekly (M-TH is typical).\\n\\nClarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.\\n \\n#LI-NT1\\nGLDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consultant - Data Engineer</td>\n",
       "      <td>Rolling Meadows, IL</td>\n",
       "      <td>Rolling Meadows</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Development knowledge for integrating components and contributing to core code base - Java preferred\\nSolid understanding of database and data warehousing technologies\\nKnowledge of SQL as well as NoSQL queries, syntax, and technologies\\nKnowledge of big data requirements, applications, and technologies such as Hadoop\\nKnowledge of ETL methods and approaches including triggers, named views, temporary tables, etc.\\nLinux expertise\\n</td>\n",
       "      <td>Specific responsibilities include:\\nHelp design an architecture for federated data stores and data fusion\\nHelp design methods for storing data in a way that facilitates extremely fast data parsing and management\\nImplement \"glue code\" that connects middle tier components with backend components\\nImplement data management and analytics code utilizing data architecture (e.g. map reduce)\\nCollaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores\\nCollaborate with data architects to understand the applications we integrate with and the data they produce\\nReview requirements for new approaches to big data storage and analytics\\nDesign methods for caching, paging, and integrating real-time data with historical data stores\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Development knowledge for integrating components and contributing to core code base - Java preferred\\nSolid understanding of database and data warehousing technologies\\nKnowledge of SQL as well as NoSQL queries, syntax, and technologies\\nKnowledge of big data requirements, applications, and technologies such as Hadoop\\nKnowledge of ETL methods and approaches including triggers, named views, temporary tables, etc.\\nLinux expertise\\n</td>\n",
       "      <td>Insygnum needs a Consultant - Data Engineer to help our clients for data analysis, data integration and data quality. Our Chicago-based team is small but growing fast and we need to complement our in-house experts who knows how to tame challenging data. This is a unique opportunity to not only work with cool technology, but also to create a new methodologies and techniques. You'll get in on the ground floor of a new company, help shape its future, and benefit directly from your work.\\nWhy work here\\nJoining insygnum now offers several unique opportunities\\nYou will receive competitive salary, benefits, and stock options\\nYou will be working on hard, interesting problems\\nYou will help shape the culture of the company as we grow\\nYou will have the opportunity to apply your skills in a meaningful way and have a real-world impact\\nResponsibilities\\nYou'll help design a new system for capturing, storing, analyzing, and acting on performance, security, and network data. The ideal candidate will have a solid grasp of several different database and data warehousing technologies to help architect ETL for. Technologies to be used may include some combination of relational databases (PostgreSQL, Teradata, Aster, HANA), NoSQL, Hadoop, Object-based stores, and OLAP.\\nSpecific responsibilities include:\\nHelp design an architecture for federated data stores and data fusion\\nHelp design methods for storing data in a way that facilitates extremely fast data parsing and management\\nImplement \"glue code\" that connects middle tier components with backend components\\nImplement data management and analytics code utilizing data architecture (e.g. map reduce)\\nCollaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores\\nCollaborate with data architects to understand the applications we integrate with and the data they produce\\nReview requirements for new approaches to big data storage and analytics\\nDesign methods for caching, paging, and integrating real-time data with historical data stores\\nDesired Skills and Experience\\nRequirements\\nDevelopment knowledge for integrating components and contributing to core code base - Java preferred\\nSolid understanding of database and data warehousing technologies\\nKnowledge of SQL as well as NoSQL queries, syntax, and technologies\\nKnowledge of big data requirements, applications, and technologies such as Hadoop\\nKnowledge of ETL methods and approaches including triggers, named views, temporary tables, etc.\\nLinux expertise\\nBonus Points\\nJava is strongly preferred (e.g. for working with map reduce) but not ultimately a requirement if you excel in other areas\\nStrong SQL skills are highly desirable\\nOLAP experience\\nExperience with ETL tools like Informatica, Boomi, Pentaho, AbIntio, Datastage, etc.,\\nContact: HR Manager\\nEmail: hr@insygnum.com\\nPhone: (224)-800-1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.\\n\\nAs a Lead Data Engineer, here's what we'll be looking for you to bring:\\n\\n\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others\\nDeep understanding of streaming data architectures and technologies for real-time and low-latency data processing\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nUnderstanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists\\nUnderstanding of agile development methods including: core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftsmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different levels\\n\\nThere's no typical day or engagement for our Senior Data Engineers. Here's what you'll do:\\n\\n\\nBe the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\n\\nA few important things to know:\\n-------------------------------\\n\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn't the right role for you? That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&amp;utm_medium=jd&amp;utm_campaign=access-thoughtworks ), our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.\\n\\n#LI-NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Google Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum of 3 years previous Consulting or client service delivery experience on Google GCP\\n</td>\n",
       "      <td>DevOps on an GCP platform. Multi-cloud experience a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.\\n\\nBasic Qualifications\\nMinimum of 3 years previous Consulting or client service delivery experience on Google GCP\\nMinimum of 3 years of RDBMS experience\\nMinimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using GCP services etc:\\nData Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core\\nData Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore\\nStreaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam\\nData Warehousing &amp; Data Lake : BigQuery, Cloud Storage\\nAdvanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow &amp; Sheets\\nBachelors or higher degree in Computer Science or a related discipline.\\nAble to trval 100% M-TH\\n\\nCandidate Must Have Completed The Following Certifications\\nCertified GCP Developer - Associate\\nCertified GCP DevOps – Professional (Nice to have)\\nCertified GCP Big Data Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an GCP platform. Multi-cloud experience a plus.\\nExperience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion\\nIoT, event-driven, microservices, containers/Kubernetes in the cloud\\n\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Breadth - Substantial experience as a high-performance Engineer across multiple environments on high-performance data systems that process data across various sources at a near real-time pace\\nAt least 1 project where you personally designed, implemented, and operated large scale, high throughput data pipelines with a focus on high data quality\\nAt least 1 other project where you built a secure, reliable, scalable system on AWS that saw significant traffic\\nDepth - You can go up and down the stack from deep in the infrastructure up to the data, application, and client layers\\nSpeed - Experience with small teams that move fast - all members are expected to be able to achieve maximum results with minimal direction\\nOwnership and accountability - You own the things that you build\\nDrive - When you see a need, you fill a need. You can step in where you see a need and push us all forward\\nModern Infra - Hands-on experience with Kubernetes, Airflow, and Kafka\\nModern CI/CD - Hands-on experience with Codefresh, Spinnaker, Jenkins or similar\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At Otus, a fast-growing EdTech company based at 1K Fulton in the West Loop of Chicago, you will work with people passionate about improving the lives of teachers and students. We are a group of talented designers, developers, coaches, and leaders. We love our work and strive to do our best each day.\\n\\nOn our team, you will find musicians, beer enthusiasts, designer toy collectors, table tennis fanatics, and more.\\n\\n\\nYour Role\\nOtus is a building a next generation Platform for EdTech in order to support our transformative vision for K-12 education across the US\\nModel and architect our data in a way that will scale with the increasingly complex ways we’re analyzing it\\nBuild robust pipelines that make sure data is where it needs to be, when it needs to be there\\nBuild frameworks and tools to help others design and build their own data pipelines in a self-service manner\\nPerformance testing and engineering to ensure that our systems always scale to meet our needs\\nKey member of the team focused on pure hands-on contribution to the implementation and operation of our data platform\\n\\n\\nQualifications\\nBreadth - Substantial experience as a high-performance Engineer across multiple environments on high-performance data systems that process data across various sources at a near real-time pace\\nAt least 1 project where you personally designed, implemented, and operated large scale, high throughput data pipelines with a focus on high data quality\\nAt least 1 other project where you built a secure, reliable, scalable system on AWS that saw significant traffic\\nDepth - You can go up and down the stack from deep in the infrastructure up to the data, application, and client layers\\nSpeed - Experience with small teams that move fast - all members are expected to be able to achieve maximum results with minimal direction\\nOwnership and accountability - You own the things that you build\\nDrive - When you see a need, you fill a need. You can step in where you see a need and push us all forward\\nModern Infra - Hands-on experience with Kubernetes, Airflow, and Kafka\\nModern CI/CD - Hands-on experience with Codefresh, Spinnaker, Jenkins or similar\\n\\n\\nBenefits and Perks\\nCompetitive salary and stock options\\n20 PTO days a year, plus a day off for your birthday (or a loved one), an interest day (to do something you love) and paid holidays\\nUp to 12 weeks of parental leave, and great work/life balance\\nFlexible hours and work from home policy so you can be at your most productive\\nExcellent medical, dental, and vision insurance\\nLife Insurance and disability benefits\\n401K with an employer match (up to 4%)\\nWorking in arguably the best neighborhood in the city with the excellent food, drinks, coffee and donuts all within walking distance\\nWorking in an amazing office building with gym, rooftop deck, frequent social events, shuttle to and from train stations\\nOtus is an Equal Opportunity Employer and embraces diversity of every kind. You must be legally authorized to work in the US. Unfortunately, the company is unable to support sponsorships at this time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nIngestion of data from multiple, unstructured sources using multiple analytics tools\\nImplementing ETL process\\nMonitoring performance and advising any necessary infrastructure changes\\nDefining data retention policies</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nVoted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.</td>\n",
       "      <td>Job Title: Data Engineer\\n\\nLocation: San Francisco, Chicago, San Jose, Palo Alto, Austin, TX\\n\\nTerms: Full-time, Contract, Contract-2-Hire\\n\\nAbout Trianz\\nTrianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.\\n\\nWhat We Stand For\\nOur clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.\\n\\nAs a result, Trianz is focusing on three important themes in our engagement model with clients.\\nCrystallize business impact from a top management point of view\\nHelp Clients achieve results from strategy-by making execution predictable through innovative execution techniques\\nCreate a positive, enriching partnership experience in everything we do\\n\\nIndustries, Clients &amp; Practices\\nTrianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:\\n\\nCloud\\nAnalytics\\nDigitization\\nInfrastructure\\nSecurity\\n\\nSr. Data Engineer\\nJob Description\\nResponsibilities\\nIngestion of data from multiple, unstructured sources using multiple analytics tools\\nImplementing ETL process\\nMonitoring performance and advising any necessary infrastructure changes\\nDefining data retention policies\\n\\nRequirements\\n3+ years of relevant professional experience\\nExperience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)\\nProficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)\\nGood understanding of SQL Engine and able to conduct query performance tuning\\nStrong skills in one of the scripting language (Python, Ruby, Bash)\\n1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)\\n\\nWe are Growing Rapidly: 2019 Highlights\\n\\nTrianz is growing rapidly. Here are some highlights.\\n\\nVoted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.\\n\\nWon the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.\\n\\nWon UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.\\n\\nFeatured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.\\n\\nAchieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.\\n\\nTalk to us, Join us &amp; Develop into Leaders\\nCome join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.\\n We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!\\n Equal Opportunity Employer\\nTrianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Software Development Engineer I</td>\n",
       "      <td>Chicago, IL 60661</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60661</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Expedia\\nDo you have a passion for travel and e-Commerce? Do you have a passion for operating globally in a high growth, energizing environment? Expedia’s eCommerce Platform (eCP) team has an opening for a Software Development Engineer I (Data Engineer) as part of our Financial Systems and Core Transactional Services (FCTS) team. The online travel market is a $2 trillion industry that never stands still. This 400-person organization operating between Expedia’s Global Brands, Supply Offerings, and Finance Teams, is the transactional heartbeat of Expedia Inc. FCTS is composed of two key platform components – Financial Systems, primarily based on the Oracle E-Business Suite (EBS); and the Core Transaction Platform that comprises Order and Fulfillment Orchestration, Trip Management, Incentive Services, and Booking Data Engineering. Through our technology and operations infrastructure, we handle billions of dollars of bookings, collections, and payments every month. This role will primarily focus on the Core Transaction Platform, one of the platforms operated by FCTS.\\nWhat you’ll do\\nDesign and Architect scalable systems with a strong bias for BI &amp; analytics approach\\nCreate ETLs/ELTs to take data from various operational systems and provide data for analytics and reporting\\nWork with multi-terabyte data sets using relational databases (RDBMS) and SQL\\nAbility to handle ambiguity\\nSupport business decisions with ad hoc analysis as needed.\\nExcellent problem-solving and debugging prowess with an understanding of testing practices\\nCommunication and cross group partnership skills\\nWho you are\\nYou will have at least 2+ years of experience with detailed knowledge of data warehouse technical architectures, ETL/ELT and reporting/analytic tools\\n2+ years of experience with any scripting language (Ruby, python, etc.)\\n2+ years of experience using Informatica, Teradata and similar technologies\\nExperience working with multi-terabyte data sets using relational databases (RDBMS) and SQL\\n1+ year of experience with BI implementation in the Cloud\\nGood familiarity with Linux/Unix\\nExpedia Group recognizes our success is dependent on the success of our people. We are the world's travel platform, made up of the most knowledgeable, passionate, and creative people in our business. Our brands recognize the power of travel to break down barriers and make people's lives better – that responsibility inspires us to be the place where exceptional people want to do their best work, and to provide them the tools to do so.\\nWhether you're applying to work in engineering or customer support, marketing or lodging supply, at Expedia Group we act as one team, working towards a common goal; to bring the world within reach. We relentlessly strive for better, but not at the cost of the customer. We act with humility and optimism, respecting ideas big and small. We value diversity and voices of all volumes. We are a global organization but keep our feet on the ground, so we can act fast and stay simple. Our teams also have the chance to give back on a local level and make a difference through our corporate social responsibility program, Expedia Cares.\\nIf you have a hunger to make a difference with one of the most loved consumer brands in the world and to work in the dynamic travel industry, this is the job for you.\\nOur family of travel brands includes: Brand Expedia®, Hotels.com®, Expedia® Partner Solutions, Egencia®, trivago®, HomeAway®, Orbitz®, Travelocity®, Wotif®, lastminute.com.au®, ebookers®, CheapTickets®, Hotwire®, Classic Vacations®, Expedia® Media Solutions, CarRentals.com™, Expedia Local Expert®, Expedia® CruiseShipCenters®, SilverRail Technologies, Inc., ALICE and Traveldoo®.LI-AN1\\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>McDonalds is looking to hire a Data Engineer. The\\nrole will work closely with data architects and includes designing,\\narchitecting, and building data pipeline to support business use cases.\\nResponsibilities also include collaborating with business leaders to translate\\nbusiness requirements into technical, scalable solution.\\n\\nResponsibilities\\n\\n·\\nBachelor’s\\nor Master’s Degree in Mathematics, Computer Science, or Information technology\\npreferred\\n\\n·\\nAbility\\nto present to senior leadership and partners\\n\\n·\\nExperience\\nmanaging applications in AWS and familiarity with core services including EC2,\\nS3, RDS, Redshift, EMR.\\n\\n·\\nExperience\\nin ETL and data warehouse technologies (Oracle, SQL Server, etc.)\\n\\n·\\nSkilled\\nmanipulating Big Data using HDFS/Hadoop eco system tools\\n\\n·\\nFamiliarity\\nwith modern Machine Learning techniques\\n\\n·\\nExperience\\nand desire to work in a Global delivery environment is a plus\\n\\n·\\nStrong\\nknowledge of relational and multi-dimensional database architecture\\n\\n·\\nStrong\\nverbal and written communication skills, and ability to synthesize technical\\ninformation for a business audience\\n\\nMinimum Requirements\\n\\nWho\\nare we?\\n\\nWe are proud to be one of the most recognized brands\\nin the world, with restaurants in over 100 countries and billions of customers\\nserved each year. McDonald's is people business just as much as we are a\\nrestaurant business. We strive to be the most inclusive brand on the globe by\\nbuilding a workforce with different strengths who make delicious, feel good\\nmoments that are easy for everyone to enjoy.\\n\\nAt McDonald's, we are dedicated to using our scale\\nfor good: good for people, our industry and the planet. We see every single day\\nas a chance to have a lasting impact on our customers, our people and our\\npartners. We will continue to pursue big, global initiatives while remaining\\nkind neighbors and supporters of our local communities.\\n\\nWe are moving fast and are building a passionate\\nteam to help us. This means the company is looking for innovators, leaders,\\nsprinters who are focused on crafting memorable experiences for our customers,\\nemployees and partners. Joining McDonald's means thinking big daily and\\npreparing for a career that can have impact around the world.\\n\\nWe are an\\nequal opportunity employer and value diversity at our company. We do not\\ndiscriminate on the basis of race, religion, color, national origin, gender,\\nsexual orientation, age, marital status, veteran status, or disability status.\\n\\nCountryUnited States\\nRequisition Number\\n\\n7033BR\\n\\nEOE Statement\\n\\nMcDonald’s Corporation is an equal opportunity employer committed to a diverse and inclusive workforce.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL 60654</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60654</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMinimum 3-5 years of experience is required\\nB.S. in Computer Science or closely related field preferred, but not required. Real-world experience and proven track records count as much, if not more.\\nProgramming Languages: Python and C# (C++ and R are a plus).\\nExperience with databricks is a plus.\\nEstablished expertise developing ETL pipelines on serverless cloud solutions. (AWS Lambda, Azure App Services, etc..)\\nLinux/Unix\\nDocker\\nDatabases: SQL, JSON, object storage, etc. Experience with graph databases like Neo4j and TigerGraph is a plus.\\nStrong technical writing skills will be heavily stressed\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBuild automated ETL pipelines for cloud environments including, AWS, GCP, Azure, and Heroku.\\nDevelop and support data integrity reporting and alerting for ETL pipelines.\\nAbility to work closely with Engineering, Product and Customer Success Teams.\\nBuild robust and deployable software in Python, C++ or C#.\\nBuild, deploy, and maintain RESTful APIs to access datasets.\\nParse and extract data from common formats including, XML, JSON, CSV, and Pipe delimited.\\nIntegrate with customer provided APIs.\\nBuild, organize, and maintain datamarts using any of SQL, JSON, Blob, or other databases as needed.\\nWrite and maintain excellent documentation of all work.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Chicago, IL\\n\\nData Engineering Full-Time\\nCompany Description\\n\\nrMark Bio helps life science companies solve for the complexities that come with digital transformation by developing end-to-end AI solutions that deliver personalized business intelligence through integrated applications and API accessible services.\\n\\nHealthcare innovation is best served when individuals with diverse backgrounds come together with a common purpose and clear objectives to improve patient lives.\\nWe are product strategists, engineers, data scientists and designers who are experts in our domain and passionate about our mission to accelerate innovation, collaboration and scientific discovery for life sciences\\nJob Description\\n\\nrMark Bio, is searching for an experienced Data Engineer to work within an agile team of data scientists, software architects and developers to implement and maintain secure, scalable, cloud-based software solutions, build customer integration solutions, and streamline the team’s software delivery tools and processes. Technical aptitude is a must as well as the right team-minded attitude and the ability to work interchangeably with others. The core team is as a small SWAT-style team with everyone pulling their own weight, playing a variety of roles, and covering each others’ responsibilities when needed. Applicants should be qualified in collecting, cleaning, and maintaining large datasets that are critical to customer and product success.\\nJob Responsibilities\\n\\nBuild automated ETL pipelines for cloud environments including, AWS, GCP, Azure, and Heroku.\\nDevelop and support data integrity reporting and alerting for ETL pipelines.\\nAbility to work closely with Engineering, Product and Customer Success Teams.\\nBuild robust and deployable software in Python, C++ or C#.\\nBuild, deploy, and maintain RESTful APIs to access datasets.\\nParse and extract data from common formats including, XML, JSON, CSV, and Pipe delimited.\\nIntegrate with customer provided APIs.\\nBuild, organize, and maintain datamarts using any of SQL, JSON, Blob, or other databases as needed.\\nWrite and maintain excellent documentation of all work.\\nExperience and Qualifications\\n\\nMinimum 3-5 years of experience is required\\nB.S. in Computer Science or closely related field preferred, but not required. Real-world experience and proven track records count as much, if not more.\\nProgramming Languages: Python and C# (C++ and R are a plus).\\nExperience with databricks is a plus.\\nEstablished expertise developing ETL pipelines on serverless cloud solutions. (AWS Lambda, Azure App Services, etc..)\\nLinux/Unix\\nDocker\\nDatabases: SQL, JSON, object storage, etc. Experience with graph databases like Neo4j and TigerGraph is a plus.\\nStrong technical writing skills will be heavily stressed\\nIf you are a recruiter or placement agency, please do not submit resumes to any person or email address at rMark Bio prior to having a signed agreement from rMark Bio’s HR department. rMark Bio is not liable for and will not pay placement fees for candidates submitted by any agency other than its prior-approved recruitment partners. Furthermore, any resumes sent to us without a written signed agreement in place will be considered your company’s gift to rMark Bio. and may be forwarded to our recruiters for their attention. Thank you.\\n\\nrMark Bio is an equal opportunity employer. All qualified applicants for employment will be considered without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, status as an individual with a disability, veteran status, or any other basis protected by federal, state, or local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or customer-facing role\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Sr. Data Engineer!\\n\\nYour Mission\\n\\nAs a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.\\n\\nYou will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.\\n\\nPathway to Success\\n\\n#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.\\n\\nYour success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.\\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Data Engineer Certified\\n\\n[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment\\n\\nRequired Qualifications:\\n\\nMastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or customer-facing role\\n\\nUseful Qualifications:\\n\\nExperience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)\\nExperience with IoT architectures and building real-time data streaming pipelines\\nExperience operationalizing machine learning models on large datasets\\nHihg\\nDemonstrated leadership and self-direction -- a willingness to teach others and learn new techniques\\nDemonstrated skills in selecting the right statistical tools given a data analysis problem\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Engineer - GAMMA</td>\n",
       "      <td>Chicago, IL 60654</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60654</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer - GAMMA\\n9553BR\\nGAMMA Data Engineering\\nWHAT YOU'LL DO\\nBCG's Advanced Analytics Group, GAMMA Solutions &amp; Services (S) delivers powerful analytics-based insights designed to help our clients tackle their most pressing business problems. We partner with BCG case teams and practices across the full analytics value-chain: framing new business challenges, building fact-bases, designing innovative analytics workflows, training colleagues in new methodologies, and interpreting findings for our clients. The GAMMA S team is a global resource, working with clients in every BCG region and in every industry area. It is a core member of a rapidly growing Analytics enterprise at BCG – a constellation of teams focused on driving practical results for BCG clients by applying leading edge analytics approaches.\\n\\n As Senior Analyst on our Data Engineering team you will have the chance to roll up your sleeves and apply cutting edge engineering and data pipelining techniques to real-world business situations across a variety of industries. Successful candidates are intellectually curious builders who are biased toward action, scrappy, and communicative.\\nYOU'RE GOOD AT\\nAre comfortable in a client-facing role when necessary\\nAre able to explain technical data pipelining topics to a non-technical audience\\nLove to build elegant data flows with cutting edge, modern tools\\nEnjoy finding solutions across disparate technologies\\nHave a software development or DevOps experience within an agile based environment\\nYOU BRING (EXPERIENCE &amp; QUALIFICATIONS)\\nBachelor’s Degree in a related field required. Master’s Degree in computer science, machine learning, or other data centric disciplines with 1-2 years of relevant industry work experience solutions strongly preferred.\\n5+ years of experience working in a software, DevOps or consulting environment preferred\\nHands-on experience working with Spark, Hadoop, Docker and CI. Some clients like Airflow as well\\nExperience with AWS, Azure or similar cloud infrastructures\\nFluency in SQL, Python a must. Other programming languages (Java, Scala) a plus\\nCITY\\nChicago\\nCOUNTRY\\nUnited States\\nYOU'LL BE TRAVELLING\\nYes (40%)\\nYOUR EMPLOYEE TYPE IS\\nRegular\\nYOUR JOB TYPE IS\\nFull time\\nWHO WE ARE\\nBCG pioneered strategy consulting more than 50 years ago, and we continue to innovate and redefine the industry. We offer multiple career paths for the world’s best talent to have a real impact on business and society. As part of our team, you will benefit from the breadth and diversity of what we are doing today and where we are headed next. We count on your authenticity, exceptional work, and strong integrity. In return we are committed to supporting you in discovering the most fulfilling career journey possible—and unlocking your potential to advance the world.\\n\\nBCG GAMMA combines innovative skills in computer science, artificial intelligence, statistics, and machine learning with deep industry expertise. The BCG GAMMA team is comprised of world-class data engineers, data scientists and business consultants who specialize in the use of advanced analytics to get breakthrough business results. Our teams own the full analytics value-chain end to end: framing new business challenges, building fact-bases, designing innovative algorithms, creating scale through designing tools and apps, and training colleagues and clients in new solutions. Here at BCG GAMMA, you’ll have the chance to work with clients in every BCG region and every industry area. We are also a core member of a rapidly growing analytics enterprise at BCG – a constellation of teams focused on driving practical results for BCG clients by applying leading edge analytics approaches, data, and technology.\\nEQUAL OPPORTUNITY\\nThe Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws. In addition, as a federal government contractor, BCG maintains an affirmative action program which furthers its commitment and complies with recordkeeping and reporting requirements under certain federal civil rights laws and regulations. BCG is an E-Verify Employer. Click here for more information on E-Verify. VEVRAA Federal Contractor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BCG Omnia - ETL Data Engineer</td>\n",
       "      <td>Chicago, IL 60654</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60654</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BCG Omnia - ETL Data Engineer\\n9958BR\\nTechnology &amp; Engineering\\nWHAT YOU'LL DO\\nWe are seeking a motivated and innovative candidate with a “start-up” mentality to join a fast-moving team of engineers that augments traditional consulting engagements with software. As a ETL Data Engineer joining our Chicago-based BCG Omnia team, you will be supporting the Retail Catalyst platform, a cloud-based analytics hub &amp; suite of analytical apps for Retailers. In this role, you will come up with better ways to wrangle, structure, and Hadoop-query massive amounts of (client-) data, all while improving BCG’s ways of working and ability to scale our IP. The ideal candidate has 2-3 years of ETL experience (SQL is a must) with heavy focus on building scalable and repeatable data schemas. Ideally he or she also brings a deep understanding of Hadoop / Spark cluster configuration, cloud platforms (GCP and Azure), Knowledge of and hands-on experience with visualized ETL (e.g., Alteryx; Dataiku), Python / Flask, and Retail industry (data) is a big plus.\\nYOU'RE GOOD AT\\nETL and wrangling huge data sets is in your blood\\nYou are a master of SQL and Hadoop/Spark-based data querying\\nBuilding fast, scalable, and clean data schemas which can be re-used over and over\\nBeing a self-learner and highly proactive / ambitious in your work\\nConstantly connecting the dots – technical and business context + anticipating issues before they arise\\nYou enjoy working in a small startup-like team and don’t mind working across disciplines (e.g., digging into the frontend apps to troubleshoot data issues)\\nYou thrive in a fairly complex multi-cloud, multi-tenant environment and building containerized web applications\\nUtilizing an 80/20 mind-set of value vs. time to deliver\\nThoroughly testing schemas and code; proactively identifying bugs and issues\\nYOU BRING (EXPERIENCE &amp; QUALIFICATIONS)\\nDegree in computer science or related field\\n2-3 years ETL / Data Engineering experience\\nDeep expertise in SQL (ideally PostgreSQL) and standardized data schemas / architecture\\nExperience with visualized ETL (e.g., Alteryx or Dataiku)\\nExperience in setting up and optimizing Hadoop / Spark cluster configuration\\nExperience working in collaborative, fast-moving environments\\nExperience in cloud technology (Google Cloud / Microsoft Azure; Docker; Kubernetes)\\nStrong knowledge of security and networking protocols\\nSelf-driven, entrepreneurial, and strong team player\\nYOU'LL WORK WITH\\nOur technology consultants and specialists partner with our clients and colleagues to build and implement digital solutions through a broad spectrum of activities. Technology jobs and engineering jobs include design of IT architectures, large-scale transformation, agile development, software engineering, cybersecurity consulting, and risk management.\\nCITY\\nChicago\\nCOUNTRY\\nUnited States\\nYOU'LL BE TRAVELLING\\nYes (10%)\\nYOUR EMPLOYEE TYPE IS\\nRegular\\nYOUR JOB TYPE IS\\nFull time\\nWHO WE ARE\\nBCG pioneered strategy consulting more than 50 years ago, and we continue to innovate and redefine the industry. We offer multiple career paths for the world’s best talent to have a real impact on business and society. As part of our team, you will benefit from the breadth and diversity of what we are doing today and where we are headed next. We count on your authenticity, exceptional work, and strong integrity. In return we are committed to supporting you in discovering the most fulfilling career journey possible—and unlocking your potential to advance the world. BCG Omnia works with Boston Consulting Group’s practice areas—specific industries and capabilities—in order to transform the firm’s unique intellectual property into professional, scalable products to support our client service efforts. BCG Omnia employs leading-edge technologies and specialized experts in product design and development, visualization, benchmarking, and custom analysis to provide world-class solutions.\\nEQUAL OPPORTUNITY\\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Chicago, IL 60601</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60601</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experience with cloud-based systems like AWS, AZURE or Google Cloud.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We have a wide variety of career opportunities around the world — come find yours.\\n\\nInformation Technology\\nUnited Airlines is seeking talented people to join the Data Engineering team. Data Engineering organization is responsible for driving data driven insights &amp; innovation to support the data needs for commercial and operational projects with a digital focus.\\n You will partner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives that create sustainable revenue and share growth\\n Design, develop, and implement streaming and near-real time data pipelines that feed systems that are the operational backbone of our business.\\n Execute unit tests and validating expected results to ensure accuracy &amp; integrity of data and applications through analysis, coding, writing clear documentation and problem resolution.\\n This role will also drive the adoption of data processing and analysis within the Hadoop environment and help cross train other members of the team.\\n Leverage strategic and analytical skills to understand and solve customer and business centric questions.\\n Coordinate and guide cross-functional projects that involve team members across all areas of the enterprise, vendors, external agencies and partners.\\n Leverage data from a variety of sources to develop data marts and insights that provide a comprehensive understanding of the business.\\n Develop and implement innovative solutions leading to automation.\\n Use of Agile methodologies to manage projects.\\n Mentor and train junior engineers.\\nRequired\\n BS/BA, in computer science or related STEM field\\n We are seeking creative, driven, detail-oriented individuals who enjoy tackling tough problems with data and insights. Individuals who have a natural curiosity and desire to solve problems are encouraged to apply\\n 10+ years of IT experience in software development\\n 5+ years of development experience using Java, Python, Scala\\n 5+ years of experience with Big Data technologies like Spark, Hadoop, Hive, HBASE, Kafka, Nifi\\n 4+ years of experience with relational database systems like MS SQL Server, Oracle, Teradata\\n Must be legally authorized to work in US for any employer without sponsorship - Successful completion of interview required to meet job qualification\\n Reliable, punctual attendance is an essential function of the position\\nPreferred Skills:\\n Master’s in computer science or related STEM field.\\n Experience with cloud-based systems like AWS, AZURE or Google Cloud.\\n Certified Developer / Architect on AWS.\\n Strong experience with continuous integration &amp; delivery using Agile methodologies - Data engineering experience with transportation/airline industry\\n Strong problem-solving skills.\\n Strong knowledge in Big Data\\n\\n\\nEqual Opportunity Employer – Minorities/Women/Veterans/Disabled/LGBT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Please make sure to read the job posting in its entirety as it reflects both the University roles and responsibilities, followed by the specific description.\\nDepartment\\n13760 Urban Crime Labs\\nAbout the Unit\\nBy 2050, the global urban population will nearly double to 6.4 billion. This unprecedented growth in the global urban population creates incredible opportunities but also intensifies the most difficult problems cities face, such as concentrated poverty, crime, poor-quality schooling, and pollution. The University of Chicago founded Urban Labs to help address these challenges. Urban Labs is a set of highly synergistic labs focused on undertaking inquiry and having impact on five essential dimensions of urban life: crime, education, health, poverty, and energy &amp; environment. Urban Labs partners with cities to identify and rigorously evaluate the policies and programs with the greatest potential to improve human lives at scale. Urban Labs’ evidence-based approach gives policymakers and practitioners the knowledge they need to effectively achieve the greatest social good per dollar spent. In sum, UChicago Urban Labs: • Identifies promising solutions to urban challenges• Tests the most promising urban policies and programs • Scales-up the most effective and cost-efficient policies and programs. For more information about the UChicago Urban Labs, go to http://urbanlabs.uchicago.edu/\\nJob Family\\nResearch\\nResponsible for all aspects of research projects and research facilities. Plans and conducts clinical and non-clinical research; facilitates and monitors daily activities of clinical trials or research projects. Directs engineering and technical support activities to develop and maintain tools and computational methods needed to gather and analyze data.\\nCareer Track and Job Level\\nData Science\\nConducts data investigation, including data wrangling, cleaning, sampling, management, exploratory analysis, regression and classification, prediction, and data communication. Implements foundational concepts of data computation, such as data structure, algorithms, parallel computing, simulation, and analysis. Utilizes knowledge in game theory, statistical quality control, exponential smoothing, seasonally adjusted trend analysis, or data visualization to gain insights, develop new strategies, and cultivate actionable business intelligence in diverse career tracks across the University.\\nP3: Requires in-depth knowledge and experience. Uses best practices and knowledge of internal or external University issues to improve products or services. Solves complex problems; takes a new perspective using existing solutions. Works independently, receives minimal guidance. Acts as a resource for colleagues with less experience.\\nRole Impact\\nIndividual Contributor\\nResponsibilities\\nThe job uses best practices and knowledge of data manipulation, statistical applications, programming, analysis and modeling in order to implement projects related to the University's various internal data systems as well as from external sources. The job is responsible for managing operational protocols.\\n1) Has a deep understanding of methods to analyze complex data sets for the purpose of extracting and purposefully using applicable information. May develop and maintain infrastructure that connects data sets., 2) Guides staff or faculty members in defining the project and applies principals of data science in manipulation, statistical applications, programming, analysis and modeling., 3) Calibrates data between large and complex research and administrative datasets. Guides and may set the operational protocols for collecting and analyzing information from the University's various internal data systems as well as from external sources., 4) Designs and evaluates statistical models and reproducible data processing pipelines using expertise of best practices in machine learning and statistical inference. Provides expertise for high level or complex data-related requests and engages other IT resources as needed. Partners with other campus teams to assist faculty with data science related needs., 5) Performs other related work as needed.\\nUnit-specific Responsibilities\\n1) Leads the design and implementation of standardized ETL processes for our most widely used datasets.\\n2) Builds tools and APIs that enable easy loading of clean data. These tools and APIs must work for users of R, Python, and Stata.\\n3) Creates design requirements by talking to our analysts and designing to fit their needs.\\n4) Mentors analysts and runs internal workshops on best practices of software engineering.\\n5) Creates user-friendly documentation that encourages tool adoption and promotes the maintainability of tools and infrastructure.\\nUnit-preferred Competencies\\n1) Proficiency in either Python or R, with a working knowledge of the other.\\n2) Proficiency in SQL.\\n3) Ability to create robust, high-quality, and tested data pipeline.\\n4) Strong interpersonal skills.\\n5) Strong initiative and a resourceful approach to problem solving and learning.\\n6) Ability to mentor junior staff.\\nEducation, Experience, and Certifications\\nMinimum requirements include a college or university degree in related field.\\nMinimum requirements include knowledge and skills developed through 5-7 years of work experience in a related job discipline.\\nPreferred Qualifications\\nEducation\\n1) Bachelor’s or master’s degree in computer science, statistics, data science, economics or a closely related field.\\nExperience\\n1) Five years of related experience\\nRequired Documents\\n1) Resume\\n2) Cover letter\\n3) A sample of data work, such as a final report for a class project, link to a GitHub repo, or blog post\\nNOTE: When applying, all required documents MUST be uploaded under the Resume/CV section of the application.\\nFLSA Status\\nExempt\\nPay Frequency\\nMonthly\\nPay Grade\\nDepends on Qualifications\\nScheduled Weekly Hours\\n37.5\\nBenefits Eligible\\nYes\\nDrug Test Required\\nNo\\nHealth Screen Required\\nNo\\nMotor Vehicle Record Inquiry Required\\nNo\\nPosting Date\\n2019-07-24-07:00\\nRemove from Posting On or Before\\n2020-01-24-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sr Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>PREFERRED SKILLS:\\nBachelor’s degree in quantitative field, MS preferred\\nSeveral years of experience with data warehouse, analytics, ETL – exceptional command of sql in different flavors\\nVery strong knowledge of data modeling in relational DBs\\nFew years of experience with big data technologies - Hadoop, spark\\nStrong programming skills in a high-level, general purpose programming language – java, scala, or python\\nStrong communication skills, ability to work effectively with remote members of the team and collaborate over long-distance\\nExperience in a cloud infrastructure is a plus – AWS, GCP, Azure\\nFamiliarity with functional programming is a plus\\nFamiliarity with CI/CD concepts is a plus – git, Jenkins, etc.\\n\\nGogo is the inflight internet company. Our worldwide inflight Wi-Fi services have made internet and video entertainment a regular part of flying. We are a diverse and mission-minded group of professionals all working together in extraordinary harmony. And that’s just the beginning. We connect the aviation industry and air travelers with innovative technology and applications, and we do it all in a high-energy environment that welcomes the next challenge. Be prepared to join a performance-obsessed team that is passionate about bringing the internet to every device, every flight, everywhere.\\nEqual Opportunity Employer/Vets/Disabled\\nGogo participates in E-Verify. Details in English and Spanish. Right to Work Statement in English and Spanish.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Application Software Engineer</td>\n",
       "      <td>Chicago, IL 60290</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60290</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Where good people build rewarding careers.\\nThink that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.\\nJob Description\\nAllstate is seeking to hire a Application Software Engineer to join our Innovation team at our downtown Chicago location. Allstate’s Innovation Team is at the forefront of change and growth within the company, driving the creation of standalone businesses or reinvention of existing ones. Innovation projects typically include cross-functional team members within Business Strategy, Product, Technology, Design, and Go to Market functions, who use data-driven prototypes and testing to achieve desired business outcomes. The team is seeking to augment its analytics capabilities further by growing a small team of predictive analytics researchers and data engineers with a senior data engineer position. Past projects in the team have included the development of a new consumer “digital footprint” technology, announced at CES 2019, and new approaches to risk prediction for traditional insurance products.\\nThe Application Software Engineer will develop novel techniques for on-boarding, transforming, and persisting structured and semi-structured data to create solutions for advanced risk prediction, reporting, and other types of analytics. This individual will also assist in efforts to evaluate, extract, scrub/prep, analyze and visualize a host of internal and external datasets and communicate the results to various stakeholders across and outside the company.\\nResponsibilities include:\\nUsing Sqoop, SQL, Hadoop stack, Spark, and other Big Data technologies to create an end-to-end ETL and Data Mining pipelines gathering inputs from disparate sources within the enterprise and transforming the data to a format suitable for Data Science work\\nWorking closely with our Data Science team as well as the business team to understand complex requirements around data acquisition, transformation, streaming, and other data pipeline requirements\\nWearing multiple hats including Data Engineering, Web Development, DevOps, Reporting and other disciplines as necessary in a challenging highly complex environment\\nRepresenting Allstate in customer meetings, external functions, projects and consortia, as necessary\\nHaving readiness to challenge the current process and proposing new solutions based on extensive previous experience\\nJob Qualifications\\nBachelor’s degree in computer science or a similar technical field with 5 or more years of experience in highly data driven projects. OR\\nA Master’s or Ph.D. in computer science or a similar technical field with 3 or more years of experience in highly data driven projects.\\nStrong proficiency in Python, Scala, Java or equivalent.\\nStrong proficiency in SQL.\\nStrong proficiency in Linux.\\nSignificant experience working in a distributed environment including Hadoop (HDFS, Hive, etc.) and Spark (ETL) ecosystems\\nSignificant experience in a technically complex product development environment (e.g. database or software applications).\\nBackground in information systems, big data and analytics experience with familiarity of contemporary tools and agile methodologies.\\nAbility to execute in a cross-functional team environment spread across many departments.\\nAbility to operate in a dynamic, fast moving environment, and communicate complex ideas in a clear, concise manner both verbally and in writing.\\nResults oriented as demonstrated by proven ability to meet short deadlines and execute against multiple competing priorities with minimum direct supervision.\\nPreferred Qualifications\\nStrong proficiency in TDD, CI/CD, Git and one of Maven, SBT, Gradle.\\nDeep knowledge of a web framework like Spring Boot or equivalent (Play, Rails, Django).\\nExperience with Dimensional Modeling and Star Schemas.\\nExperience with reporting tools like Tableau or Business Objects.\\nExperience with Cloudera or other Hadoop distributions.\\nThe candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.\\n\\n\\nGood Work. Good Life. Good Hands®.\\n\\nAs a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.\\n\\nLearn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.\\n\\nAllstate generally does not sponsor individuals for employment-based visas for this position.\\nEffective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.\\n\\nFor jobs in San Francisco, please click \"here\" for information regarding the San Francisco Fair Chance Ordinance.\\nFor jobs in Los Angeles, please click \"here\" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.\\n\\nIt is the policy of Allstate to employ the best qualified individuals available for all jobs without regard to race, color, religion, sex, age, national origin, sexual orientation, gender identity/gender expression, disability, and citizenship status as a veteran with a disability or veteran of the Vietnam Era.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL 60654</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60654</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>1-3 years of experience in quantitative analysis experience.\\nBachelor's degree in Computer Science, Statistics, Math or other technical field required. Graduate degrees preferred.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Data Engineer is responsible for applying your expertise in quantitative analysis, database and data warehousing, partnered with operation and product teams, to solve problems and identify trends and opportunities. The Data Engineer role has to work across the following areas:\\nDatabase maintenance\\nBuilding and analyzing dashboards and reports\\nEvaluating and defining metrics and perform exploratory analysis\\nMonitoring key product metrics and understanding root causes of changes in metrics\\nEmpower and assist operation and product teams through building key data sets and data-based recommendations\\nAutomating analyses and authoring pipelines via SQL/python based ETL framework\\nKey Competencies\\nSuperb SQL programming skill.\\nUnderstanding of ETL tools and database architecture.\\nAdvanced knowledge of data warehousing.\\nDemonstrable familiarity with code and programming concepts. Experience with Python is preferred but not required.\\nA product mindset - you ask and address the most important analytical questions with a view on enhancing product impact.\\nPassionate and attentive self-starters, great communicators.\\nEducation and Experience\\n1-3 years of experience in quantitative analysis experience.\\nBachelor's degree in Computer Science, Statistics, Math or other technical field required. Graduate degrees preferred.\\nJob Classification\\nThis is a full time exempt position\\nSMS Assist is an Equal Opportunity Employer (EOE) that welcomes and encourages all applicants to apply regardless of age, race, color, religion, sex, sexual orientation, gender identify and/or expression, national origin, disability, veteran status, marital or parental status, ancestry, citizenship status, pregnancy or other reasons prohibited by law.\\n#ZP\\n#Indeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Cloud Solutions Architect</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExpertise in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.\\nApplication Development: building custom web and mobile applications on top of the GCP stack.\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExcellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.\\nHands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.\\nExperience working with engineering and sales teams.\\nExperience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.\\nTime management with the ability to manage multiple streams.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Cloud Solutions Architect!\\n\\nYour Mission\\n\\nAs a Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.\\n\\nYou will be an established contributor within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures successfully. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.\\n\\nPathway to Success\\n\\n#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.\\n\\nYou will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the solutions architecture or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events.\\nCustomer Facing - This is very customer-facing role. You will usually interact with customers on a daily basis. You will participate on calls and onsite customer meetings to qualify consultative engagements with the engineering teams. You will present architecture proposals and code samples to build trust, confidence, and consensus both externally and internally.\\nTraining - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Cloud Architect Certified\\n\\n[https://cloud.google.com/certification/cloud-architect] and/or Google\\nProfessional Data Engineer Certified\\n[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.\\n\\nRequired Qualifications:\\n\\nExpertise in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.\\nApplication Development: building custom web and mobile applications on top of the GCP stack.\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExcellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.\\nHands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.\\nExperience working with engineering and sales teams.\\nExperience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.\\nTime management with the ability to manage multiple streams.\\n\\nUseful Qualifications:\\n\\nDirect experience working with a variety of cloud technologies as well as designing and recommending elegant solutions that drive business outcomes.\\nUnderstanding of infrastructure automation, continuous integration/deployment, relational/NoSQL data stores, security, networking, and cloud-based delivery models.\\nAbility to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance and security.\\nThought leadership with the ability to recommend cloud-native approaches to solve customer business and technical challenges.\\nUnderstanding of best practices, design patterns and reference architectures with an uncanny ability to recommend these as needed.\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Rosemont, IL</td>\n",
       "      <td>Rosemont</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Computer Science, Information Systems, Business Administration, or other related field required\\nMinimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources\\nMinimum 2 years of experience with PowerBI\\nMinimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications\\nMinimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired\\nExperience working with both structured and unstructured data\\nDemonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types (star, snowflake, etc.)\\nData modeling experience in building logical and physical data models\\nApplied knowledge of Microsoft Security/Authentication Concepts (Active Directory, IIS, Windows OS)\\nStrong technical planning skills with the ability to prioritize and multitask across a number of work streams\\nMust have a passion for continued improvement, learning, and mentoring\\nPolished presentation skills; experience creating and presenting findings to executive level staff\\nStrong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively\\nMust be highly collaborative with the ability to manage and motivate project teams and meet deliverables\\nAbility to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders\\nExperience with Data Warehouse is a plus\\nKnowledge of SSRS is a plus\\nHealthcare industry experience a plus</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>You are known for your development and deployment of innovative big data platforms for advanced analytics and data processing. You lead innovation through exploration, benchmarking, making recommendations, and implementing big data technologies for platforms. You are excited by defining and building data pipelines that will enable faster, better, data-informed decision-making within the business. You have a passion for continued improvement, learning and mentoring, and leverage this enthusiasm when building stakeholder relationships. You are collaborative, insightful and want to work for an organization making a difference in the field of orthopaedics on behalf our members and their patients.\\nIf this sounds like you, read on!\\nThe Data Engineer is primarily responsible for maintaining and enhancing the Registry’s data acquisition, integration, and ETL pipelines in support of both operational and business intelligence data stores. The incumbent is responsible for applying diverse data cleansing and transformation techniques as well as the ongoing management and monitoring of all Registry databases. This includes addressing issues pertaining to the ongoing operations and optimization of the data environment including performance, reliability, logging, scalability, etc. This position will also provide support for the Academy’s database systems, warehouse, marts, and supporting applications.\\nLead the effort to develop a unified enterprise data model for the Academy. Design, develop, and maintain high-performance data platforms on premise and in Microsoft Azure cloud-based environments including leading the development of a data warehouse environment to support the Registry’s business intelligence roadmap. Champion efforts that will ensure that the Academy’s business intelligence applications remain relevant for use by internal business groups by actively participating in strategy and project planning discussions. Work collaboratively with Registry participants and internal support teams, identify and implement changes that improve system performance and the user experience.\\nDesign, develop, and maintain the ETL pipelines using SSIS that standardize raw data from multiple data sources and optimize both the operational and dimensional/star schema data model necessary for transactional systems and business intelligence applications. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions aligning to an overall data architecture. Extract, transform, and load data to and from various data sources including relational databases, NoSQL databases, web services, and flat files. Produce various technical documents such as ER diagrams, table schemas, data lineage, API documents, etc.\\nProvide leadership and oversight on database architectural design for existing Academy systems including database administration, performance monitoring, and troubleshooting. Provide complex analysis, conceptualize, design, implement, and develop solutions for critical data-centric projects. Perform dataflow, system and data analysis, and develop meaningful and useful presentation of data in downstream applications. Plan and implement standards, define/code conformed global and reusable objects, perform complex database design and data repository modelling.\\nMonitor ETL processes, system audits, dashboard reporting, and presentation layer functioning and performance. Proactively identify and implement procedures that resolve performance and/or data reporting issues. Support the optimal performance of the Academy’s data and BI systems. Monitor database performance, provide optimization recommendations, and implement recommendations. Follow the release cycles and implement on-time delivery of task assignments, defect correction, change requests, and enhancements. Troubleshoot and solve technical problems. Perform other responsibilities as assignment by management.\\nRequired Qualifications:\\nBachelor’s degree in Computer Science, Information Systems, Business Administration, or other related field required\\nMinimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources\\nMinimum 2 years of experience with PowerBI\\nMinimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications\\nMinimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired\\nExperience working with both structured and unstructured data\\nDemonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types (star, snowflake, etc.)\\nData modeling experience in building logical and physical data models\\nApplied knowledge of Microsoft Security/Authentication Concepts (Active Directory, IIS, Windows OS)\\nStrong technical planning skills with the ability to prioritize and multitask across a number of work streams\\nMust have a passion for continued improvement, learning, and mentoring\\nPolished presentation skills; experience creating and presenting findings to executive level staff\\nStrong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively\\nMust be highly collaborative with the ability to manage and motivate project teams and meet deliverables\\nAbility to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders\\nExperience with Data Warehouse is a plus\\nKnowledge of SSRS is a plus\\nHealthcare industry experience a plus\\nIf this describes YOU, please apply by sharing the following:\\nClearly communicate why you are the ideal candidate for this role, providing specific examples and experiences as proof points.\\nAttach your resume, cover letter and any additional materials that support your application.\\nSalary expectations must be included to be considered for this opportunity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL 60601</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60601</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Avanade\\n\\nAvanade leads in providing innovative digital services, business solutions and design-led experiences for its clients, delivered through the power of people and the Microsoft ecosystem. Our professionals combine technology, business and industry expertise to build and deploy solutions to realize results for clients and their customers. Avanade has 37,000 digitally connected people across 24 countries, bringing clients the best thinking through a collaborative culture that honors diversity and reflects the communities in which we operate. Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com\\n\\nWhy Avanade?\\n\\n14-time winner of Microsoft Partner of the Year\\n24,000+ certifications in Microsoft technology\\n90+ Microsoft partner awards\\n17 Gold Competencies\\n3,500 analytics professionals worldwide\\n1,000 data engineers\\nImplemented analytics systems for more than 550 clients\\n400 AI practitioners\\n300 cognitive service experts\\n\\nHow We Support You:\\nWe believe in gender equity and an inclusive community. We offer a comprehensive benefits package: generous vacation allowance disability coverage, retirement plans, paid maternity and paternity leave, life insurance, hotel and travel discounts, extended benefits to cover items that support your well-being, health, dental and vision insurance, professional development and paid Microsoft certification opportunities.\\n\\nRole Overview:\\nAs an Data Engineer you will collect, aggregate, store, and reconcile data in support of Client business decisions. You will help design and build data pipelines, data streams, reporting tools, information dashboards, data service APIs, data generators and other end-user information portals and insight tools. You will be a critical part of the data supply chain, ensuring that business partners can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics.\\n\\nKey Role Responsibilities:\\nDay-to-day, you will:\\nTranslate business requirements to technical solutions using strong business insight.\\nAnalyzes current business practices, processes, and procedures as well as identifying future business opportunities for demonstrating Microsoft Azure Data &amp; Analytics PaaS Services.\\nSupport the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments.\\nDelivery of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.\\nDesign and Build Modern Data Pipelines and Data Streams.\\nDesign and Build Data Service APIs.\\nDevelop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.\\nExpose data to end-users using Power BI, Azure API Apps or other modern visualization platform or experience.\\nImplement effective metrics and monitoring processes.\\n\\nKey Role Skill &amp; Capability Requirements:\\nYour technical/non-technical skills include:\\nDemonstrable experience of turning business use cases and requirements to technical solutions.\\nExperience in business processing mapping of data and analytics solutions.\\nAbility to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.\\nThe ability to apply such methods to take on business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration.\\nT-SQL is required.\\nKnowledge of Azure Data Factory, Azure Data Lake, Azure SQL DW, and Azure SQL, Azure App Service, Databricks, and Azure Data Warehouse.\\nExperience preparing data for Data Science and Machine Learning.\\nKnowledge of Lambda and Kappa architecture patterns.\\nKnowledge of Master Data Management (MDM) and Data Quality tools and processes.\\nStrong collaboration ethic and experience working with remote teams.\\nKnowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals.\\nWorking experience with Visual Studio, PowerShell Scripting, and ARM templates.\\nExperience with Git/TFS/VSTS is a requirement.\\n\\nPreferred Education Background:\\nYou likely possess a Bachelor's degree in Computer Science, Information Technology, Business, or another relevant field. An equivalent combination of education and experience will also suffice.\\n\\nPreferred Years of Work Experience:\\nYou likely have about 5+ years of relevant professional experience.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Architect</td>\n",
       "      <td>Chicago, IL 60606</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60606</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Facilitate collaboration for development of long-term strategic plans of production databases in conjunction with data owners, business units, IT product owners, and Enterprise Architecture\\nAssist business units and project management in developing the budget projections based on short- and long-term goals and objectives\\nDevelop and maintain data models and data flows that represent essential data consumed and produced\\nDevelop and maintain documentation that maps data models to information systems and business applications\\nBe resourceful in the use of enterprise data, including guidance in understanding and exploitation of the underlying data and business models, and identifying optimal data sourcing and mapping\\nAnalyze the logical and physical database models to ensure models are in accordance with standards\\nAssist in building of enterprise architecture information blueprints that illustrate how information is stored, processed, and accessed.\\nParticipate in enterprise information architecture discussions around needs and alignment to business goals\\nCollaborate with Database Administrators and other staff to resolve data issues, performance issues and to ensure the highest possible degree of data integrity\\nValidate and fix information management issues related to data quality framework (completeness, accuracy, availability, timeliness, consistency, etc.)\\nEnforce enterprise information standards, guidelines, and principles\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of experience as a Data Architect/Data Engineer\\n10 years of strong working knowledge of Microsoft SQL server, SSIS and SSAS (multidimensional model)\\nIn-depth understanding of database structure principles\\nAbility to write complex SQL queries for ETL or reporting\\nMust be project oriented with an ability to meet deadlines.\\nExcellent written and oral communications\\nFamiliarity with data visualization tools (e.g. Tableau, XLCubed, ESRI, SSRS etc.)\\nFamiliarity with Python for data analysis is a plus\\nExperience with Yardi or any other Real Estate Property/Accounting/Asset Management system is a plus\\nBachelor's degree in Computer Science, preferred\\n</td>\n",
       "      <td>Job Description:\\nThe Data Architect leads and supports all components of a database design environment, including the most complex database designs for both transactional and data warehouse systems (test through production). This role will also facilitate and collaborate with IT and business units to develop, maintain, and support system data models. The Data Architect will define and deliver database design solutions based on requirements from both business and technology disciplines.\\nResponsibilities:\\nFacilitate collaboration for development of long-term strategic plans of production databases in conjunction with data owners, business units, IT product owners, and Enterprise Architecture\\nAssist business units and project management in developing the budget projections based on short- and long-term goals and objectives\\nDevelop and maintain data models and data flows that represent essential data consumed and produced\\nDevelop and maintain documentation that maps data models to information systems and business applications\\nBe resourceful in the use of enterprise data, including guidance in understanding and exploitation of the underlying data and business models, and identifying optimal data sourcing and mapping\\nAnalyze the logical and physical database models to ensure models are in accordance with standards\\nAssist in building of enterprise architecture information blueprints that illustrate how information is stored, processed, and accessed.\\nParticipate in enterprise information architecture discussions around needs and alignment to business goals\\nCollaborate with Database Administrators and other staff to resolve data issues, performance issues and to ensure the highest possible degree of data integrity\\nValidate and fix information management issues related to data quality framework (completeness, accuracy, availability, timeliness, consistency, etc.)\\nEnforce enterprise information standards, guidelines, and principles\\nRequirements:\\n5+ years of experience as a Data Architect/Data Engineer\\n10 years of strong working knowledge of Microsoft SQL server, SSIS and SSAS (multidimensional model)\\nIn-depth understanding of database structure principles\\nAbility to write complex SQL queries for ETL or reporting\\nMust be project oriented with an ability to meet deadlines.\\nExcellent written and oral communications\\nFamiliarity with data visualization tools (e.g. Tableau, XLCubed, ESRI, SSRS etc.)\\nFamiliarity with Python for data analysis is a plus\\nExperience with Yardi or any other Real Estate Property/Accounting/Asset Management system is a plus\\nBachelor's degree in Computer Science, preferred\\nVentas, Inc. offers a competitive compensation and benefits package to the successful candidate.\\nVentas, Inc. is an Equal Opportunity Employer.\\nVentas, Inc. does not accept unsolicited resumes from staffing agencies, search firms or any third parties.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>OSP Fiber Design Engineer</td>\n",
       "      <td>Chicago, IL 60601</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60601</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Outside Plant Fiber Design Engineer is responsible for Fiber Optic detailed network design for fiber-based networks. This position requires an in depth understanding of current fiber optic design standards and industry processes. Typical OSP design responsibilities range from the initial determination of the pathway routing and construction alternatives based on field/site walk outs to final detailed design generation.\\nThe OSP fiber design engineer is fully responsible and accountable for the final design content and document generation to support the placement of the physical fiber infrastructure supporting Outside and Inside Plant communication facilities.\\nResponsibilities include:\\nCreate high level preliminary route design and facility specifics to manage fielding request instructions.\\nSelf-managed with the ability to multi-task and meet deadlines through strong organization and communication skills.\\nHave a complete understanding of outside plant design principles, work order procedures, and telephone industry standards for aerial and underground designs.\\nKnowledge of methods and interpretation of boring, trenching, aerial work and ROW with fiber optics.\\nBe knowledgeable in local, state and railroad permitting regulations regarding ROW, Property Rights and Easements.\\nCreate and perform quality checks of redlines and designed prints with specifications, standards, practices and guidelines.\\nCreate drawings and records of fiber routes, splice locations, construction notes, bills of materials and project overviews.\\nProject Execution Responsibilities Include:\\nDriving to various locations across the Chicago Metro Area to collect field data.\\nEngineer preliminary and final fiber layout designs for aerial, buried and underground construction.\\nFinding creative solutions to the challenges of route selection, network design, and right of way issues.\\nUnderstand project milestones. Monitor status of milestones and provide regular updates.\\nSupport QC and as built efforts with design changes/updates.\\nPreferred Job Qualifications:\\nÂ· Bachelor's degree in engineering or construction or equivalent in work experience, 7+ years telecommunications experience involving OSP telecom design.\\nÂ· Related work experience in OSP and/or ISP fiber detailed design &amp; fiber splicing.\\nÂ· Possess a deep understanding of inside and/or outside plant fiber optic network infrastructure alternatives available.\\nÂ· Familiar with construction focused on fiber builds.\\nÂ· Strong interpersonal, analytical and communication skills.\\nÂ· Flexible and able to think quickly working to resolve design issues and makes decisions.\\nÂ· Experience in working on multiple projects simultaneously.\\nÂ· Functions as a technical specialist with minimal supervision.\\nPreferred Technical Competencies:\\nÂ· Fiber Network Planning.\\nÂ· Fiber routing for aerial and underground installations.\\nÂ· Proficient in Microsoft Office â€“ 5+ years.\\nÂ· Manhole design including splicing and racking.\\nÂ· Design of fiber terminals, switches, routers and security.\\nÂ· Customer equipment room design.\\nÂ· Knowledge of construction &amp; constructability practices &amp; principles.\\nÂ· Knowledge of Aramis, AutoCad, and Micro Station is a plus.\\nAbout KDM Engineering:\\nKDM Engineering is a WBE/MBE engineering consulting firm specializing in primary distribution design in the Chicagoland area. Our core staff has a wealth of design and project management experience. Our aim is to provide our clients with excellent project support, a high attention to detail, and great customer service.\\nBenefits &amp; Top Reasons to Work for Us:\\nCompetitive base salary\\nComprehensive medical, dental and vision plans\\n401(k) plans with substantial company match\\nPTO, company paid holidays and great work/life balance\\nFun and Flexible work environment that puts people first\\nDynamic leadership team that are experts in their field\\nEqual Employment Opportunity:\\nKDM Engineering strongly supports equal employment opportunity for all applicants regardless of race, color, religion, sex, gender identity, pregnancy, national origin, ancestry, citizenship, age, marital status, physical disability, mental disability, medical condition, sexual orientation, genetic information, or any other characteristic protected by state or federal law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Data Architect</td>\n",
       "      <td>Chicago, IL 60654</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60654</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Facilitate collaboration for development of long-term strategic plans of production databases in conjunction with data owners, business units, IT product owners, and Enterprise Architecture\\nAssist business units and project management in developing the budget projections based on short- and long-term goals and objectives\\nDevelop and maintain data models and data flows that represent essential data consumed and produced\\nDevelop and maintain documentation that maps data models to information systems and business applications\\nBe resourceful in the use of enterprise data, including guidance in understanding and exploitation of the underlying data and business models, and identifying optimal data sourcing and mapping\\nAnalyze the logical and physical database models to ensure models are in accordance with standards\\nAssist in building of enterprise architecture information blueprints that illustrate how information is stored, processed, and accessed.\\nParticipate in enterprise information architecture discussions around needs and alignment to business goals\\nCollaborate with Database Administrators and other staff to resolve data issues, performance issues and to ensure the highest possible degree of data integrity\\nValidate and fix information management issues related to data quality framework (completeness, accuracy, availability, timeliness, consistency, etc.)\\nEnforce enterprise information standards, guidelines, and principles\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of experience as a Data Architect/Data Engineer\\n10 years of strong working knowledge of Microsoft SQL server, SSIS and SSAS (multidimensional model)\\nIn-depth understanding of database structure principles\\nAbility to write complex SQL queries for ETL or reporting\\nMust be project oriented with an ability to meet deadlines.\\nExcellent written and oral communications\\nFamiliarity with data visualization tools (e.g. Tableau, XLCubed, ESRI, SSRS etc.)\\nFamiliarity with Python for data analysis is a plus\\nExperience with Yardi or any other Real Estate Property/Accounting/Asset Management system is a plus\\nBachelor's degree in Computer Science, preferred\\n</td>\n",
       "      <td>Job Description:\\nThe Data Architect leads and supports all components of a database design environment, including the most complex database designs for both transactional and data warehouse systems (test through production). This role will also facilitate and collaborate with IT and business units to develop, maintain, and support system data models. The Data Architect will define and deliver database design solutions based on requirements from both business and technology disciplines.\\nResponsibilities:\\nFacilitate collaboration for development of long-term strategic plans of production databases in conjunction with data owners, business units, IT product owners, and Enterprise Architecture\\nAssist business units and project management in developing the budget projections based on short- and long-term goals and objectives\\nDevelop and maintain data models and data flows that represent essential data consumed and produced\\nDevelop and maintain documentation that maps data models to information systems and business applications\\nBe resourceful in the use of enterprise data, including guidance in understanding and exploitation of the underlying data and business models, and identifying optimal data sourcing and mapping\\nAnalyze the logical and physical database models to ensure models are in accordance with standards\\nAssist in building of enterprise architecture information blueprints that illustrate how information is stored, processed, and accessed.\\nParticipate in enterprise information architecture discussions around needs and alignment to business goals\\nCollaborate with Database Administrators and other staff to resolve data issues, performance issues and to ensure the highest possible degree of data integrity\\nValidate and fix information management issues related to data quality framework (completeness, accuracy, availability, timeliness, consistency, etc.)\\nEnforce enterprise information standards, guidelines, and principles\\nRequirements:\\n5+ years of experience as a Data Architect/Data Engineer\\n10 years of strong working knowledge of Microsoft SQL server, SSIS and SSAS (multidimensional model)\\nIn-depth understanding of database structure principles\\nAbility to write complex SQL queries for ETL or reporting\\nMust be project oriented with an ability to meet deadlines.\\nExcellent written and oral communications\\nFamiliarity with data visualization tools (e.g. Tableau, XLCubed, ESRI, SSRS etc.)\\nFamiliarity with Python for data analysis is a plus\\nExperience with Yardi or any other Real Estate Property/Accounting/Asset Management system is a plus\\nBachelor's degree in Computer Science, preferred\\nLillibridge offers a competitive compensation and benefits package to the successful candidate.\\nLillibridge is an Equal Opportunity Employer.\\nLillibridge does not accept unsolicited resumes from staffing agencies, search firms or any third parties.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sr. Consultant - Data Engineer</td>\n",
       "      <td>Rolling Meadows, IL</td>\n",
       "      <td>Rolling Meadows</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies\\nMinimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation\\nMinimum five years experience using one or more data integration tools including Data Quality and ETL\\nDevelopment knowledge for integrating components and contributing to core code base - Java preferred\\nSolid understanding of database and data warehousing technologies\\nExperienced in advanced SQL as well as NoSQL queries, syntax, and technologies\\nExperienced in big data requirements, applications, and technologies such as Hadoop\\nProficient in ETL methods and approaches including triggers, named views, temporary tables, etc.\\nExperienced in Linux environments\\n</td>\n",
       "      <td>Identify data warehouse needs and develop strategy for implementing a warehousing solution including investigating data sources, rationalizing information sources, identifying technology components, building roadmaps and reference architecture stacks\\nWork with Business Analysts and other information management professionals through all phases of project development, from envisioning to architecture definition and end solution realization\\nProvide direction and collaborate with Data Engineers to implement enterprise solutions that will support organizational business intelligence and analytics requirements\\nWork with the business to identify opportunities where technology can be leveraged to solve existing problems and can assist with new market opportunities\\nMeet with technology vendors, convey technical requirements and business use cases, develop scorecards, install vendor products in a lab environment, and summarize findings and results of testing\\nTechnologies to be used may include some combination of relational databases (PostgreSQL, Teradata, Aster, HANA), NoSQL, Hadoop, Object-based stores, and OLAP.\\nSpecific responsibilities include:\\nHelp design an architecture for federated data stores and data fusion\\nHelp design methods for storing data in a way that facilitates extremely fast data parsing and management\\nImplement \"glue code\" that connects middle tier components with backend components\\nImplement data management and analytics code utilizing data architecture (e.g. map reduce)\\nCollaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores\\nCollaborate with enterprise architects to understand the applications we integrate with and the data they produce\\nReview requirements for new approaches to big data storage and analytics\\nDesign methods for caching, paging, and integrating real-time data with historical data stores\\nDesired Skills and Experience\\nRequirements\\nMinimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies\\nMinimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation\\nMinimum five years experience using one or more data integration tools including Data Quality and ETL\\nDevelopment knowledge for integrating components and contributing to core code base - Java preferred\\nSolid understanding of database and data warehousing technologies\\nExperienced in advanced SQL as well as NoSQL queries, syntax, and technologies\\nExperienced in big data requirements, applications, and technologies such as Hadoop\\nProficient in ETL methods and approaches including triggers, named views, temporary tables, etc.\\nExperienced in Linux environments\\nBonus Points\\nExperience working with IT strategy teams, business teams and business analysts to define information systems, services and management\\nExperience with RDBMS including SQL Server, Oracle 11g, MySQL; Big Data including SQL Data Warehouse Appliance, Oracle Exadata, Netezza, Greenplum, Vertica, Teradata, Aster Data, SAP HANA, Hadoop a plus; Analytics including SAS, SPSS, Spotfire, Tableau, Qlikview, R, Oracle Endeca; BI Tools including Oracle OBIEE, SAP Business Objects, SAS and other Analytics Vendors with BI components; ETL &amp; MDM including Informatica, SAS Dataflux, IBM, Siperion, Rochade, Map/Reduce for ETL is a plus\\nJava is strongly preferred (e.g. for working with map reduce) but not ultimately a requirement if you excel in other areas\\nStrong SQL skills are highly desirable\\nOLAP experience</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies\\nMinimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation\\nMinimum five years experience using one or more data integration tools including Data Quality and ETL\\nDevelopment knowledge for integrating components and contributing to core code base - Java preferred\\nSolid understanding of database and data warehousing technologies\\nExperienced in advanced SQL as well as NoSQL queries, syntax, and technologies\\nExperienced in big data requirements, applications, and technologies such as Hadoop\\nProficient in ETL methods and approaches including triggers, named views, temporary tables, etc.\\nExperienced in Linux environments\\n</td>\n",
       "      <td>Insygnum needs a Consultant - Data Engineer to help our clients for data analysis, data integration and data quality. Our Chicago-based team is small but growing fast and we need to complement our in-house experts who knows how to tame challenging data. This is a unique opportunity to not only work with cool technology, but also to create a new methodologies and techniques. You'll get in on the ground floor of a new company, help shape its future, and benefit directly from your work.\\nWhy work here\\nJoining insygnum now offers several unique opportunities\\nYou will receive competitive salary, benefits, and stock options\\nYou will be working on hard, interesting problems\\nYou will help shape the culture of the company as we grow\\nYou will have the opportunity to apply your skills in a meaningful way and have a real-world impact\\nResponsibilities\\nIdentify data warehouse needs and develop strategy for implementing a warehousing solution including investigating data sources, rationalizing information sources, identifying technology components, building roadmaps and reference architecture stacks\\nWork with Business Analysts and other information management professionals through all phases of project development, from envisioning to architecture definition and end solution realization\\nProvide direction and collaborate with Data Engineers to implement enterprise solutions that will support organizational business intelligence and analytics requirements\\nWork with the business to identify opportunities where technology can be leveraged to solve existing problems and can assist with new market opportunities\\nMeet with technology vendors, convey technical requirements and business use cases, develop scorecards, install vendor products in a lab environment, and summarize findings and results of testing\\nTechnologies to be used may include some combination of relational databases (PostgreSQL, Teradata, Aster, HANA), NoSQL, Hadoop, Object-based stores, and OLAP.\\nSpecific responsibilities include:\\nHelp design an architecture for federated data stores and data fusion\\nHelp design methods for storing data in a way that facilitates extremely fast data parsing and management\\nImplement \"glue code\" that connects middle tier components with backend components\\nImplement data management and analytics code utilizing data architecture (e.g. map reduce)\\nCollaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores\\nCollaborate with enterprise architects to understand the applications we integrate with and the data they produce\\nReview requirements for new approaches to big data storage and analytics\\nDesign methods for caching, paging, and integrating real-time data with historical data stores\\nDesired Skills and Experience\\nRequirements\\nMinimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies\\nMinimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation\\nMinimum five years experience using one or more data integration tools including Data Quality and ETL\\nDevelopment knowledge for integrating components and contributing to core code base - Java preferred\\nSolid understanding of database and data warehousing technologies\\nExperienced in advanced SQL as well as NoSQL queries, syntax, and technologies\\nExperienced in big data requirements, applications, and technologies such as Hadoop\\nProficient in ETL methods and approaches including triggers, named views, temporary tables, etc.\\nExperienced in Linux environments\\nBonus Points\\nExperience working with IT strategy teams, business teams and business analysts to define information systems, services and management\\nExperience with RDBMS including SQL Server, Oracle 11g, MySQL; Big Data including SQL Data Warehouse Appliance, Oracle Exadata, Netezza, Greenplum, Vertica, Teradata, Aster Data, SAP HANA, Hadoop a plus; Analytics including SAS, SPSS, Spotfire, Tableau, Qlikview, R, Oracle Endeca; BI Tools including Oracle OBIEE, SAP Business Objects, SAS and other Analytics Vendors with BI components; ETL &amp; MDM including Informatica, SAS Dataflux, IBM, Siperion, Rochade, Map/Reduce for ETL is a plus\\nJava is strongly preferred (e.g. for working with map reduce) but not ultimately a requirement if you excel in other areas\\nStrong SQL skills are highly desirable\\nOLAP experience\\nContact: HR Manager\\nEmail: hr@insygnum.com\\nPhone: (630)-799-1556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL 60601</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60601</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s Degree\\nAt least 3 years of SDLC experience using Java technologies\\nAt least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper\\nAt least 1 years experience in one of the following Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>77 West Wacker Dr (35012), United States of America, Chicago, Illinois\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nData Engineer\\n\\nDo you want to work for a tech company that writes its own code, develops its own software, and builds its own products? We experiment and innovate leveraging the latest technologies, engineer breakthrough customer experiences, and bring simplicity and humanity to banking. We make a difference for 65 million customers. We're changing banking for good. At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs. We want you to be curious and ask “what if?” Capital One started as an information strategy company that specialized in credit cards, and we have become one of the most impactful and disruptive players in the industry. We have grown to see ourselves as a technology company in consumer finance, with great opportunities for software engineers who want to build innovative applications to give users smarter ways to save, transact, borrow and invest their money, as we seek to disrupt the industry again. As a Capital One Software Engineer, you'll work on everything from customer-facing web and mobile applications using cutting-edge open source frameworks, to highly-available RESTful services, to back-end Java based systems using the hottest techniques in Big Data.\\n\\nYou'll bring solid experience in emerging and traditional technologies such as: node.js, Java, AngularJS, React, Python, REST, JSON, XML, Ruby, HTML / HTML5, CSS, NoSQL databases, relational databases, Hadoop, Chef, Maven, iOS, Android, and AWS/Cloud Infrastructure to name a few.\\n\\nYou will:\\n\\nWork with product owners to understand desired application capabilities and testing scenarios\\nContinuously improve software engineering practices\\nWork within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies\\nLead the craftsmanship, availability, resilience, and scalability of your solutions\\nBring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal &amp; external technology communities, and mentor other members of the engineering community\\nEncourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity\\n\\nBasic Qualifications:\\n\\nBachelor’s Degree\\nAt least 3 years of SDLC experience using Java technologies\\nAt least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper\\nAt least 1 years experience in one of the following Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform\\n\\nPreferred Qualifications:\\n\\nMaster's Degree\\n2+ year experience with Spark\\n3+ years experience developing software solutions to solve complex business problems\\n\\nAt this time, Capital One will NOT sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>master Solid understanding of workflow systems and their application to customer business process improvement Experience working with HL7 interface engines such as Mirth Connect, Cloverleaf, Corepoint, or Ensemble General understanding of healthcare revenue cycle management (RCM), including patient accounting, claims processing, follow-up processes, and reimbursement practices Familiarity with Salesforce.com\\nIn compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL 60601</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60601</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer\\n\\n US-IL-Chicago\\n\\n\\nJob Description\\n\\nChicago, IL\\n\\nFull Time Perm\\n\\n\\nDo you love working in a collaborative environment with free breakfast, coffee/tea/soda, social events and much more? Then read on because Omnitracs is looking for the best and brightest to help us disrupt the freight and logistics industry! What sets us apart from other logistics technology companies is our rich history in data! In 1988, Omnitracs (then Qualcomm) fundamentally changed the way fleets operate and we’re doing it again today. With over a million assets in over 70 countries, Omnitracs has a lot of data. Omnitracs’ newly formed Innovation Lab is innovating on this data to create new products - helping our customers not just survive, but thrive, in today’s complex transportation ecosystem. We are looking for you, Data Engineer, to join our fast-paced Agile team in Chicago. Who You Are As a Data Engineer, you will be responsible for data management tasks including design, development, and technical administration in an AWS environment. You will also provide technical leadership to the team and be responsible for maintaining technical specifications. As a Data Engineer, you will have a heavy focus on designing the solutions to deliver data products. To be successful in this role you will need a solid understanding of data management concepts and how cloud technology can solve data issues.\\n\\n\\nResponsibilities and Duties As the Cloud Data Architect, your responsibilities will include, but are not limited to:\\n• Oversight of the design and standards of AWS (S3, Redshift/Snowflake/SQL Server, Glue,) data applications • Train and coach data team developers • Oversee and implement security features, access and standards around data management • Assist in capacity and budgeting for data systems • Provide estimates and oversight within a Scrum environment • Strong SQL skills in data warehouse environment • Spark, Python and/or Scala experience • Lead code reviews Qualifications and Skills • At least 3 years IT experience in AWS data services • Hands on experience working with complex Data Warehouses and or customer linking systems • Data Lake experience using Spark, Scala, EMR and/or Glue • Data Modeling experience • Proficient with SQL • Solid S3 understanding • Experience with AWS Aurora, Oracle and/or SQL Server developer experience • Hands on experience using AWS RDS Nice to Have • Experience with Redshift or Snowflake using Matillion or other ETL tools • Identity &amp; Access Management (Security Provisioning) understanding and knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.\\n\\nAs a Senior Data Engineer, here's what we'll be looking for you to bring:\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Kafka, Hadoop, ecosystem technologies, and others\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nDeep understanding of relational database technologies and database development techniques\\nUnderstanding of how to architect solutions for data science and analytics\\nData management for reporting and BI experience is a plus\\nUnderstanding of “Agility”, including core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different levels\\nAny experience building and leading an offshore/outsourcing function would be highly beneficial.\\nThere's no typical day or engagement for our Senior Engineers. Here’s what you’ll do:\\n\\nBe the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\nRegardless of what you do at ThoughtWorks, you’ll always have the opportunity to:\\n\\nThink through hard problems, and work with a team to make them reality.\\nLearn something new every day.\\nWork in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title\\nTravel the world.\\nSpeak at conferences.\\nWrite blogs and books.\\nDevelop your career outside of the confinements of a traditional career path by focusing on what you’re passionate about rather than a predetermined one-size-fits-all plan\\nBe part of a company with Social and Economic Justice at the heart of its mission.\\nA few important things to know:\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years experience programming with Python is required\\n3+ years in an ETL or Data Engineering role, building and implementing data pipelines\\nStrong skills with PySpark and SQL with the ability to write efficient queries\\nFamiliarity with AWS big data services: S3, Redshift, Glue, EC2, Lambda, SageMaker, Dynamo\\nExperience working in a highly collaborative environment - we do Agile using sprints, planning, retro, etc.\\nExperience with Airflow and other open source technologies\\nExcited and willing to learn new things\\nBackground in computer science, engineering, mathematics, related field, or equivalent work experience</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Collaborate with our reporting, analytics, and data science teams to understand data sources and business requirements\\nWork within a collaborative team, adhering to Agile best practices, documentation, and knowledge sharing\\nGather, clean, enrich, and transform data to feed internal and external client needs\\nDefine, build, test, and implement data pipelines, batch and streaming\\nMonitor pipeline performance and document infrastructure changes\\nMake code decisions and adhere to best practices for ETL and programming\\nContribute to our overall architecture and pipeline design and make contributions to the product road map</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Location:\\nChicago, Illinois\\nWe're looking for a hands-on, collaborative Senior Data Engineer to join our Data Engineering team. When you get out of bed in the morning, you look forward to building solutions, you love working with data, and you enjoy working as a team.\\nIn this role, you'll have the opportunity to make code decisions and build cloud infrastructure and pipelines that will deliver data solutions to our Food and Beverage and Sports industry clients. You will report to our Director of Data Engineering and help bring best practices to the team while working with a team of Data Engineers.\\nIf you have a git repository, we'd be excited to see it!\\nThe Company is not able to sponsor employment visas for this position.\\nCore Responsibilities\\nCollaborate with our reporting, analytics, and data science teams to understand data sources and business requirements\\nWork within a collaborative team, adhering to Agile best practices, documentation, and knowledge sharing\\nGather, clean, enrich, and transform data to feed internal and external client needs\\nDefine, build, test, and implement data pipelines, batch and streaming\\nMonitor pipeline performance and document infrastructure changes\\nMake code decisions and adhere to best practices for ETL and programming\\nContribute to our overall architecture and pipeline design and make contributions to the product road map\\nMinimum Qualifications\\n3+ years experience programming with Python is required\\n3+ years in an ETL or Data Engineering role, building and implementing data pipelines\\nStrong skills with PySpark and SQL with the ability to write efficient queries\\nFamiliarity with AWS big data services: S3, Redshift, Glue, EC2, Lambda, SageMaker, Dynamo\\nExperience working in a highly collaborative environment - we do Agile using sprints, planning, retro, etc.\\nExperience with Airflow and other open source technologies\\nExcited and willing to learn new things\\nBackground in computer science, engineering, mathematics, related field, or equivalent work experience\\nPreferred Qualifications\\nInterest or experience in DevOps and CI/CD\\nExperience with building data lake solutions\\nExperience with JavaScript\\nExperience with Netezza and Data Stage\\n\\nAbout E15\\nAt E15, we are the spark that ignites. Our team delivers next-generation insights based on data, not hunches, to drive business in MLB, NHL, NBA, NFL, College Sports, and beyond. E15 brings unmatched industry intelligence and cutting edge analytics to sports, entertainment, hospitality, and retail industries to help companies make forward-looking decisions to benefit their business, fans and customers. www.e15group.com\\n\\nAbout Levy\\nLevy is the leader in Sports and Entertainment dining, catering such renowned sports venues at Wrigley Field in Chicago, STAPLES Center and Dodger Stadium in Los Angeles, Ford Field in Detroit and Churchill Downs in Louisville. Levy also caters events including Super Bowls, World Series, NASCAR Racing, the Kentucky Derby, the U.S. Open Tennis Tournament and the Grammy Awards. www.levyrestaurants.com\\n\\nE15 is an equal opportunity employer. At E15 we are committed to treating all Applicants and Team Members fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Chicago / Data / Full-time\\n\\n\\nData Engineering is the foundational layer of our Data &amp; Analytics stack at Arrive. Data Engineers not only build and optimize data pipelines that transform and store data in a way that allows the rest of the organization (analysts, data scientists, other stakeholders) analyze and consume data. They are also in charge of building the systems and establishing the processes to enable the rest of the data team to develop, test, and deploy analyses and code in an efficient and scalable way.\\n\\n\\nWe’re looking for an experienced Data Engineer to help us grow our data infrastructure and platform. If you're seeking a role that is high impact and full of ownership....please read on.\\nWhat you'll tackle:\\n\\nDesign new enterprise data models and ETL processes to populate them\\nExtract and transform data from production databases and 3rd party services to provide consumable data and support functions across the organization\\nDetect quality issues, track them to their root source, and implement fixes and preventative audits\\nManage and optimize Redshift clusters/data lake to ensure current health and performance and future scaling needs\\nHelp maintain the process we use to develop, test, and deploy good code\\nBecome the “go to” expert of our data. Work closely with staff to understand all data from our core systems, partner services, and any other platforms we rely on\\nWhat you brIng to the table:\\n\\nExperience with AWS; expertise in Redshift, Postgres or other RDBSs (preferably column-oriented)\\nExpertise in SQL and ability to write and optimize complex queries\\nExperience with Docker, Elastic Container Service, Lambda a plus\\nAbility to write customized software in Python, Bash, Go or other common open source languages. Experience with Airflow or similar scheduling service a plus\\nExperience with CI/CD tools like Jenkins or Drone\\nCreativity in approaching data organization challenges with an understanding of the end goal\\nA collaborative nature and entrepreneurial spirit. Prior startup experience a huge plus\\nMore about the team:\\n\\nWe are a tight knit team that is fun, inclusive, and hard working. We have a firm, non-negotiable no jerk policy. We accept you for who you are and consider everyone on an equal opportunity basis without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Cameo:\\nOur mission is to create the most personalized and authentic fan experiences in the world. We're a marketplace where fans can book personalized video shoutouts from their favorite people. We've helped create over 300,000 moments for our customers and built a marketplace for over 20,000 talent to connect with their biggest fans. We're breaking down the exclusivity myth of celebrity by building personal relationships between fans and talent.\\nCameo is one of LinkedIn's Top 50 Startups to Work For (https://www.google.com/url?q=https%3A%2F%2Fwww.linkedin.com%2Fpulse%2Flinkedin-top-startups-2019-50-hottest-us-companies-work-jessi-hempel%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFQz4KGF3FEAqrh6xYc_kUjnYGLgA) and also recognized on TIME Magazine's 50 Most Genius Companies (https://www.google.com/url?q=https%3A%2F%2Ftime.com%2Fcollection%2Fgenius-companies-2018%2F5412492%2Fcameo%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNE9Df7YI_B-XbUZVC-4pYCcxbjFSw) list. We are a global company, headquartered in Chicago, IL in the Fulton Market neighborhood and HQ2 in Venice, CA.\\nWe recently closed our Series B round (https://www.google.com/url?q=https%3A%2F%2Ftechcrunch.com%2F2019%2F06%2F25%2Fcameo%2Famp%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFY84hBjAlCkm3dKa11-ir5rctW5A) led by Kleiner Perkins (https://www.google.com/url?q=https%3A%2F%2Fwww.kleinerperkins.com&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHCRd_6JnbpDjifoex74E-_YO49_A) who has backed other tech giants including Google, Spotify, and Amazon. Join our team and be able to experience a rocketship from its early days. We want you to be excited about coming to work every day, knowing that the work you dedicate yourself to will have a material impact and help shape the direction of the next great tech company.\\n\\nAbout the Role:\\nYou will be helping us improve &amp; scale the platform that connects fans and talent to create the perfect video shoutout. In particular, you will help us make sure we can support our growing data team's needs securely and performantly.\\nAs a data engineer, you will be working across multiple technologies, primarily our data infrastructure (AWS redshift, AWS Lambda, Mixpanel, Google Analytics, Tableau), backend API (node.js + express), database (mongoDB), and infrastructure (Heroku + Redis + AWS + Atlas).\\nDesign and implement data schemas, real-time pipelines, and batch processing jobs to support analytical modeling and reporting needs\\nWork with stakeholders to develop data expertise and resolve upstream issues relating to data quality\\nDefine best practices and design for the management of data\\nPartner with internal stakeholders to build and maintain internal data processing and visualization tools\\nTranslate requests into replicable analytic reports using varying applications\\nCreate tools to serve data such as APIs and packages\\nYou will help drive a Data mindset and culture across the company\\n\\nEngineering Culture and Values:\\nCheck out our Key Values (https://www.google.com/url?q=https%3A%2F%2Fwww.keyvalues.com%2Fcameo&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEsr_WDoiQuRejuZZZRBpEp3Fccsg) profile\\nA few things about yourself:\\n2+ years experience working as a data engineer, in particular data pipelines and SQL dbs\\n1+ years experience with NoSQL databases (MongoDB or Elasticsearch preferred)\\n3+ years experience with programming languages (Python, Java, R, and/or Scala preferred)\\nFamiliarity with a variety of data processing technologies (e.g. Spark, Kafka, Hadoop)\\nExcellent communication skills, including a knack for clear documentation\\nExperience managing data transformation processes and making data available through service applications and databases.\\nWhat we hope you'll bring to the table:\\nExperience working with Redshift, Postgres, Airflow, Docker\\nExperience supporting product analytics, with an emphasis on web/mobile applications\\n1+ years experience with Javascript\\nSome experience with frontend web-development\\nExperience defining and implementing APIs\\nCameo is an equal opportunity employer. We are committed to creating an inclusive and welcoming environment for every person who walks through our doors. All employment is decided on the basis of qualifications, merit, and business need. Cameo celebrates and embraces diversity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Entry Level Data Engineer</td>\n",
       "      <td>Chicago, IL 60601</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60601</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Do you know how excellent data design can support critical business decision-making? So do we.\\n\\nData is more than just information: its valuable insight that gives businesses the knowledge to transform.\\n\\nAbout you\\n\\nYou help to design data solutions that enable clients to see the whole picture and provide insightful and accurate analysis that helps to build successful businesses.\\n\\nYou have a passion for learning teamed with good business sense and time management and can manage your clients’ expectations effectively. You have earned a bachelor’s degree or equivalent in a relevant Analytics field and have one to two years of academic experience in your field.\\n\\nAbout the job\\n\\nIn Data Engineering, you’ll use modern data engineering techniques and advanced analytics methods to give your clients the information they need. You collect, aggregate, store and reconcile data from various sources, helping to design and build data pipelines, streams, reporting tools, data generators and a whole range of tools to provide information and insight. Your work gives people the tools they need to find and use data for routine and non-routine analysis.\\n\\nDay to day, you will:\\nSupport the planning and implementation of data design services, providing sizing and configured assistance and performing needs assessments, as well as delivering data warehouse and storage architectures\\n\\nProvide leadership and vision for designing and implementing data analytics and modelling strategies, you identify new analytics tools and techniques and build environments that generate and study information that supports creative, solutions based on the data, and using advanced statistical, data mining and machine-learning techniques.\\n\\nDeliver data using Microsoft SQL Server Reporting Services (SRSS), Microsoft Excel, PowerPivot and Microsoft SharePoint Performance Point\\n\\nCustomize data storage and extraction, data mining, database architecture, metadata and repository creation\\n\\nImplement effective metrics and monitoring processes\\n\\nTravel as required - at times could be 80% of the work week.\\n\\nYour skills\\n\\nYou really know your way around database technology and data reporting tools. And you also understand the point of it all—to produce confirmed insights to drive business decisions. You have an analytical mind on top of good technical skills.\\n\\nYours skills and experience include:\\nKnowledge of database storage, collection and aggregation models, techniques and technologies and the ability to apply such methods to address business problems\\n\\nKnowledge of structured problem-solving assignments\\n\\nExcellent project management and people management\\n\\nKnowledge of Microsoft SharePoint, PowerPivot, SRSS, Excel (with embedded Pivot Tables and macros)\\n\\nSome SQL experiences.\\n\\nAvanade® Is An Equal Opportunity Employer. Avanade prohibits discrimination and harassment against any employee or applicant for employment because of race, color, age, religion, sex, national origin, gender identity or expression, sexual orientation, disability, veteran, military or marital status, genetic information or any other protected status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Oakbrook Terrace, IL</td>\n",
       "      <td>Oakbrook Terrace</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>SIRVA is a leading partner for corporations to outsource their mobility needs, relocating and moving their executives and staff globally. SIRVA offers an extensive portfolio of mobility services across approximately 170 countries providing an end-to-end solution to deliver an enhanced mobility experience and program control and security for customers. SIRVA has a portfolio of well-known and recognizable brands including Allied Van Lines, northAmerican Van Lines, SMARTBOX, and Allied Pickfords. For more information please visit www.sirva.com.\\n\\nSIRVA brings together strong, collaborative people in a dynamic culture of mutual respect, support, and passion for the brand and product. We believe innovation drives winning performance, and we constantly challenge ourselves to be the very best we can in every aspect of our business. You will be surrounded by some of the brightest and most driven people in the industry. At SIRVA, you will be in great company!\\n\\nThe Data Engineer, which is an emerging role in the SIRVA data and analytics team, will play a pivotal role in operationalizing the most-urgent data and analytics initiatives for SIRVA digital business initiatives. The bulk of the data engineer’s work would be in building, managing and optimizing data pipelines and then moving these data pipelines effectively into production for key data and analytics consumers (like business/data analysts, data scientists or any personal that needs curated data for data and analytics use cases). Data engineers also be responsible to ensure compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines.\\nFUNCTIONS AND RESPONSIBILITIES\\n10% -Create functional &amp; technical documentation - e.g. Data pipelines, source to target mappings, ETL specification documents, run books\\n80% -Build data pipelines, apply automation in data integration and management, tracking data consumption patterns, performing intelligent sampling and caching\\n10% -Tests, debugs, and documents Pipelines and data integration processes, SQL and/or no-SQL queries QUALIFICATIONS AND PREFERRED SKILLS\\n\\nStrong experience with advanced analytics tools for Object-oriented/object function scripting using languages such as [R, Python, Java, Scala, others]. • Strong ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. The ability to work with both IT and business in integrating analytics and data science output into business processes and workflows. • Strong experience with popular database programming languages including [SQL, PL/SQL, others] for relational databases and certifications on upcoming [NoSQL/Hadoop oriented databases like HBase, Cassandra, others] for nonrelational databases. • Strong experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include [ETL/ELT, data replication/CDC, message-oriented data movement, API design and access] and upcoming data ingestion and integration technologies such as [stream data integration, CEP and data virtualization]. • Strong experience in working with SQL on Hadoop tools and technologies including [HIVE, Impala, Presto, others] from an open source perspective and [Hortonworks Data Flow (HDF), Talend, others] from a commercial vendor perspective. • Strong experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production. • Basic experience working with popular data discovery, analytics and BI software tools like [Sisense, Tableau, Qlik, PowerBI and others] for semantic-layer-based data discovery. • Strong experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms. • Basic understanding of popular open-source and commercial data science platforms such as [Python, R, KNIME, Alteryx, others] is a strong plus but not required/compulsory. • Demonstrated ability to work across multiple deployment environments including [cloud, on-premises and hybrid], multiple operating systems. • Adept in agile methodologies and capable of applying DevOps and increasingly DataOps principles to data pipelines to improve the communication, integration, reuse and automation of data flows between data managers and consumers across an organization • Has good judgment, a sense of urgency and has demonstrated commitment to high standards of ethics, regulatory compliance, customer service and business integrity. • Strong experience supporting and working with cross-functional teams in a dynamic business environment.\\n\\nEDUCATION AND CERTIFICATION REQUIREMENTS\\n\\nA bachelor's or master's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is required. • Advanced degree in computer science, statistics, applied mathematics is preferred.\\n\\nSIRVA is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, military status, genetic information or any other consideration made unlawful by applicable federal, state, or local laws. SIRVA also prohibits harassment of applicants and employees based on any of these protected categories. It is also SIRVA's policy to comply with all applicable state and federal laws respecting consideration of unemployment status in making hiring decisions. The Federal EEO Law Poster may be found at http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf If you need a reasonable accommodation because of a disability of any part of the employment process, please send an email to Human Resources at HRSIRVA@SIRVA.com and let us know the nature of your request and your contact information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Data Engineer Intern</td>\n",
       "      <td>Naperville, IL 60563</td>\n",
       "      <td>Naperville</td>\n",
       "      <td>IL</td>\n",
       "      <td>60563</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.\\nInterest in Machine Learning algorithms, techniques and methodologies and growing in that space\\nFamiliarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies\\nFamiliarity with one or more general purpose programming language including but not limited to Python and C++.\\nDemonstrated ability to learn quickly and has a passion for emerging digital technologies\\nComfortable with communicating clearly and concisely across a variety of audiences including technology and business.\\nAbility to think strategically as well as tactically in order to drive ideas into action.\\nAbility to collaborate in a team-oriented, dynamic environment</td>\n",
       "      <td>Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.\\nInterest in Machine Learning algorithms, techniques and methodologies and growing in that space\\nFamiliarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies\\nFamiliarity with one or more general purpose programming language including but not limited to Python and C++.\\nDemonstrated ability to learn quickly and has a passion for emerging digital technologies\\nComfortable with communicating clearly and concisely across a variety of audiences including technology and business.\\nAbility to think strategically as well as tactically in order to drive ideas into action.\\nAbility to collaborate in a team-oriented, dynamic environment</td>\n",
       "      <td>Conceptual, analytical thinker, with a real passion for data exploration and design\\nExposure to business intelligence which includes Databases, ETL, Machine Learning, and Data Mining\\nParticipate in hands-on technical projects and enthusiastic to tackle problems supporting our team.\\nShows great attention to detail</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.\\nInterest in Machine Learning algorithms, techniques and methodologies and growing in that space\\nFamiliarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies\\nFamiliarity with one or more general purpose programming language including but not limited to Python and C++.\\nDemonstrated ability to learn quickly and has a passion for emerging digital technologies\\nComfortable with communicating clearly and concisely across a variety of audiences including technology and business.\\nAbility to think strategically as well as tactically in order to drive ideas into action.\\nAbility to collaborate in a team-oriented, dynamic environment</td>\n",
       "      <td>Overview\\nKeHE-a natural, organic, specialty and fresh food distributor-is all about \"good\" and is growing, so there's never been a more exciting time to join our team. If you're enthusiastic about working in an environment with a people-first culture and an organization committed to good living, good food and good service, we'd love to talk to you!\\n\\nWhere KeHE goes, goodness follows. KeHE has grown tremendously over the years by exceeding expectations and bringing goodness in all we do. We’re now bringing that same goodness to our workforce by creating friction-free processes.\\nWe’re focused on simple and intuitive ways to work. We’re building systems where our employees come together each day and do what truly makes us human; collaborate, share, and connect.\\nYou’ll have the opportunity to expand and apply your skills in new and interesting ways. And you’ll have fun doing it. Join a team where our collective intelligence, collective effort, and collective passion is brought together to do something worth doing!\\nPrimary Responsibilities\\nKeHE Distributors is looking for ambitious and energetic people to join our Summer 2020 Technology Internship Program. A technology internship at KeHE will help you develop the skills you need to help you thrive throughout your career. Working alongside the best technologists in the industry, you will perform relevant work with direct impact on our business and customers. The Internship Program also provides opportunities for interns to tour a warehouse, participate in a trade show, and opportunities for networking.\\n\\nOpportunities available in the corporate office in Naperville, IL, lasting between 10-12 weeks long. Our technology organization focuses on new, visionary ideas. You will learn about KeHE's continuous research, effective deployment, and rapid adoption of groundbreaking technology platforms. KeHE prides itself on its culture and values. During your time as KeHE technical intern, you will begin to build a foundation around understanding our culture and values as well as receive an introduction to the food distribution industry and the technologies that support it.\\nEssential Functions\\nConceptual, analytical thinker, with a real passion for data exploration and design\\nExposure to business intelligence which includes Databases, ETL, Machine Learning, and Data Mining\\nParticipate in hands-on technical projects and enthusiastic to tackle problems supporting our team.\\nShows great attention to detail\\nMinimum Requirements, Qualifications, Additional Skills, Aptitude\\nPursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.\\nInterest in Machine Learning algorithms, techniques and methodologies and growing in that space\\nFamiliarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies\\nFamiliarity with one or more general purpose programming language including but not limited to Python and C++.\\nDemonstrated ability to learn quickly and has a passion for emerging digital technologies\\nComfortable with communicating clearly and concisely across a variety of audiences including technology and business.\\nAbility to think strategically as well as tactically in order to drive ideas into action.\\nAbility to collaborate in a team-oriented, dynamic environment\\nRequisition ID\\n2019-5992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL 60604</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60604</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Prioritizes and executes rapid raw data collection from source systems, targets and implements efficient storage of, employs fast and reliable access patterns.\\nUnderstands system protocols, how systems operate and data flows. Aware of current and emerging technology tools and their benefits. Expected to independently develop a full software stack. Understands the building blocks, interactions, dependencies, and tools required to complete software and automation work. Independent study of evolving technology is expected.\\nDrives engineering projects by developing software solutions; conducting tests and inspections; building reports and calculations.\\nStrong focus on innovation and enablement, contributes to designs to implement new ideas which improve an existing and new system/process/service. Understands and can apply new industry perspectives to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.\\nMaintains knowledge of existing technology documents. Writes basic documentation on how technology works using collaboration tools like Confluence. Creates clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption and consideration at the manager level.\\nCollaborates with technical teams and utilizes system expertise to deliver technical solutions. Continuously learns and teaches others existing and new technologies. Contributes to the development of others through mentoring or in-house workshops and learning sessions.\\nDrives team practices and procedures to achieve repeatable success and defined expectation of services\\nProvides a significant collaborative role in long-term department planning, with focus on initiatives achieving data empowerment, operational efficiency and sustainability\\nMonitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.\\nPrior experience in Capital Markets strongly preferred.\\n5+ years of experience developing software in a professional environment (preferably financial services but not required)\\n3 years of hands on Data Driven Enterprise Application development, preferable in financial industry\\nStrong understanding of Enterprise architecture patterns, Object Oriented &amp; Service Oriented principles, design patterns, industry best practices\\nFoundational knowledge of data structures, algorithms, and designing for performance.\\nProficiency in programming in Java, C# or Python and willingness to learn and adopt new languages as necessary\\nExperience in database technology like MSSQL and one of key value and document databases like MongoDb, Dynamo Db, Casandra.\\nExposure to containers, microservices, distributed systems architecture, orchestrators and cloud computing.\\nComfortable with core programming concepts and techniques (e.g. concurrency, memory management)\\nEnjoys working with algorithms and data structures (e.g. trees, hash maps, queues)\\nData Analytics and Data Science experience will be a plus.\\nGood sense of user interaction and usability design to provide an intuitive, seamless end user experience.\\nExcellent communications skills and the ability to work with subject matter expert to extract critical business concepts.\\nAbility to work and potentially lead in an Agile methodology environment.\\n</td>\n",
       "      <td>Location: Chicago, IL Job Code: HCR# 2534\\nDescription\\nPosition Purpose:\\nThe Data Engineer is responsible for empowering the Data team to achieve its primary objectives: ingesting, mastering and exposing real-time, event-driven data streams pertaining to the firm’s data assets. The ideal candidate will exhibit passion for continuous improvement and a dedicated focus on enabling consumers to achieve their goals by making data driven decisions.\\nPrimary Accountabilities/Responsibilities:\\nPrioritizes and executes rapid raw data collection from source systems, targets and implements efficient storage of, employs fast and reliable access patterns.\\nUnderstands system protocols, how systems operate and data flows. Aware of current and emerging technology tools and their benefits. Expected to independently develop a full software stack. Understands the building blocks, interactions, dependencies, and tools required to complete software and automation work. Independent study of evolving technology is expected.\\nDrives engineering projects by developing software solutions; conducting tests and inspections; building reports and calculations.\\nStrong focus on innovation and enablement, contributes to designs to implement new ideas which improve an existing and new system/process/service. Understands and can apply new industry perspectives to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.\\nMaintains knowledge of existing technology documents. Writes basic documentation on how technology works using collaboration tools like Confluence. Creates clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption and consideration at the manager level.\\nCollaborates with technical teams and utilizes system expertise to deliver technical solutions. Continuously learns and teaches others existing and new technologies. Contributes to the development of others through mentoring or in-house workshops and learning sessions.\\nDrives team practices and procedures to achieve repeatable success and defined expectation of services\\nProvides a significant collaborative role in long-term department planning, with focus on initiatives achieving data empowerment, operational efficiency and sustainability\\nMonitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities.\\n\\nJob Requirements:\\nBachelor’s degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.\\nPrior experience in Capital Markets strongly preferred.\\n5+ years of experience developing software in a professional environment (preferably financial services but not required)\\n3 years of hands on Data Driven Enterprise Application development, preferable in financial industry\\nStrong understanding of Enterprise architecture patterns, Object Oriented &amp; Service Oriented principles, design patterns, industry best practices\\nFoundational knowledge of data structures, algorithms, and designing for performance.\\nProficiency in programming in Java, C# or Python and willingness to learn and adopt new languages as necessary\\nExperience in database technology like MSSQL and one of key value and document databases like MongoDb, Dynamo Db, Casandra.\\nExposure to containers, microservices, distributed systems architecture, orchestrators and cloud computing.\\nComfortable with core programming concepts and techniques (e.g. concurrency, memory management)\\nEnjoys working with algorithms and data structures (e.g. trees, hash maps, queues)\\nData Analytics and Data Science experience will be a plus.\\nGood sense of user interaction and usability design to provide an intuitive, seamless end user experience.\\nExcellent communications skills and the ability to work with subject matter expert to extract critical business concepts.\\nAbility to work and potentially lead in an Agile methodology environment.\\n\\nPhysical requirements/Working conditions:\\nClimate controlled office environment\\nMinimal physical requirements other than occasional light lifting of boxed materials • Dynamic, time-sensitive, trade room environment\\nTravel as needed\\n\\nWe encourage applicants of all ages and experience, as we do not discriminate on the basis of the applicant's age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL 60601</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60601</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>C-K (Cramer-Krasselt) is one of the largest independent, totally integrated agencies in the country with over $700 million in billings, almost $400 million in media assets under management and 61% of revenue from digital.\\n\\nWith a mission to Make Friends, Not Ads®, C-K has built a reputation for changing perception and behaviors that lead to purchasing action for brands. It’s how we helped Porsche achieve seven years of consecutive record-breaking growth, how Corona became the #1 import and Pacifico grew 19% in a single year.\\n\\nWe’ve done it by interconnecting an ever-expanding range of disciplines from strategic branding to digital, social, analytics, media/programmatic, SEM, PR, UX and more.\\nMajor brands include Benihana, Cedar Fair Entertainment Company (Knott’s Berry Farm, Cedar Point and nine others), Cintas, Corona Extra, Edward Jones, Kroger Divisions, Pacifico Beer and Porsche.\\nAbout C-K Chicago:\\n\\nChicago’s independent spirit makes it the perfect headquarters for C-K, one of the nation’s oldest and largest independent agencies. C-K’s Chicago office, located just off the Magnificent Mile, is full of native Midwesterners and transplants alike that are united in the values of hard work and collaboration.\\nAt C-K in Chicago, we work with major global and national brands like Porsche, Corona Extra, Cedar Fair, BIC, Edward Jones and many others. We’re proud of these partnerships and the integrated approach we take to every client and every element of a campaign, be it large or small.\\nOur experts here have backgrounds spanning every marketing, advertising and communications discipline across all categories imaginable. C-K Chicago is a hub of innovation and the nerve center of the agency.\\nWe also apply our work ethic to the community. Our flagship charity in the Chicago office is Off the Street Club (OTSC), the oldest boys and girls club in the city. Throughout the year we participate in the planning and execution of several key OTSC fundraising events such as Swing for the Kids, The Firefly Ball, Summer Bake Sale and Holiday Luncheon. We also participate in the Battle for Hope, the Chicago-area advertising agency battle of the bands. In 2015, our house band, the Angry Pickles, won the Battle for Hope and helped raise thousands for OTSC in the process.\\n\\n\\nThe Data Engineer will partner with data scientists and analysts to develop data pipelines and data services that address agency needs. The candidate should be comfortable exploring, recommending and building solutions for extracting, transforming, and loading data from various disparate sources (APIs, databases, flat files, etc.).\\n\\nAbout the role:\\nCollaborate with analysts and data scientists to understand data needs and arrive at implementation requirements.\\nDesign and develop data pipeline architecture.\\nMaintain and enhance existing data services and database infrastructure.\\nDefine and manage standards for performance, reliability, and monitoring data pipelines, and databases.\\nMay require overnight travel up to 5%.\\nAbout you:\\n2-3 years of experience working with a data lake/data warehouse.\\nExperience building and maintaining data pipelines.\\nFamiliar working with RDBMS, normalized data modeling.\\nSkilled at query writing and database optimization skills.\\nStrong knowledge with at least one of: Python, Java, Scala, Ruby or Go.\\nComfortable working within Linux command-line and Linux servers.\\nAbility to adapt to a quickly changing environment and work on a broad project/pipeline base with diverse needs and functional responsibilities.\\nAbility to communicate technical ideas, solutions, and issues with technical and non-technical team members.\\nYou enjoy getting to the root of client business problems, thinking critically about solutions to them, and developing those solutions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Google Technical Architect</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum 5 years of Consulting or client service delivery experience on Google GCP\\n</td>\n",
       "      <td>DevOps on an GCP platform. Multi-cloud experience a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS &amp; NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nMinimum 5 years of Consulting or client service delivery experience on Google GCP\\nMinimum 10 years of experience in big data, database and data warehouse architecture and delivery\\nBachelors degree or 12 years previous professional experience\\nAble to travel 100% (M-TH)\\nMinimum of 5 years of professional experience in 2 of the following areas:\\nSolution/technical architecture in the cloud\\nBig Data/analytics/information analysis/database management in the cloud\\nIoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using GCP services etc.:\\nData Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core\\nStreaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam\\nData Warehousing &amp; Data Lake : BigQuery, Cloud Storage\\nAdvanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow &amp; Sheets\\n\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nCertified GCP Solutions Architect - Associate\\nCertified GCP Solutions Architect – Professional (Nice to have)\\nCertified GCP Big Data Specialty (Nice to have)\\nCertified GCP AI/ML Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an GCP platform. Multi-cloud experience a plus.\\nExperience developing and deploying ETL solutions on GCP\\nStrong in Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience beyond GCP a plus - AWS and Azure\\n\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Technical Data Engineer Lead</td>\n",
       "      <td>Chicago, IL 60606</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60606</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMust have a Bachelor’s degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Master’s degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.\\nUnderstand Hadoop cluster administration concepts.\\n3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.\\nMust have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.\\nMust have experience with batch and real-time data pipelines.\\nMust have experience as a Hadoop Technical Lead / Architect\\nMust have experience with design, development and deployment in the specified technologies.\\nMust have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.\\nWriting complex SQL queries, extracting and importing large amounts of data.\\nMust be willing to work in a fast-paced environment with an on shore – off shore distributed Agile teams.\\nMust have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.\\nMust have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.\\nMust have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.\\nExcellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nLead a development team of data engineers\\nImplement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases (Hbase) and EMR (Hadoop)\\nResponsible for design, development, testing oversight and implementation\\nWorks closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks\\nCoordinate with data engineers and API developers to drive program delivery.\\nDrive technical development and application standards across enterprise data lake\\nBenchmark and debug critical issues with algorithms and software as they arise.\\nLead and assist with the technical design and implementation of the Big Data cluster in various environments.\\nGuide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.\\nPerform other duties and/or special projects as assigned</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMust have a Bachelor’s degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Master’s degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.\\nUnderstand Hadoop cluster administration concepts.\\n3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.\\nMust have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.\\nMust have experience with batch and real-time data pipelines.\\nMust have experience as a Hadoop Technical Lead / Architect\\nMust have experience with design, development and deployment in the specified technologies.\\nMust have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.\\nWriting complex SQL queries, extracting and importing large amounts of data.\\nMust be willing to work in a fast-paced environment with an on shore – off shore distributed Agile teams.\\nMust have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.\\nMust have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.\\nMust have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.\\nExcellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through</td>\n",
       "      <td>Job Description:\\nRole Summary/Purpose:\\nWe are looking for a Technical Data Engineer Lead to lead the development of consumer-centric low latency analytic environment leveraging Big Data technologies and transform the legacy systems.\\nEssential Responsibilities:\\nLead a development team of data engineers\\nImplement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases (Hbase) and EMR (Hadoop)\\nResponsible for design, development, testing oversight and implementation\\nWorks closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks\\nCoordinate with data engineers and API developers to drive program delivery.\\nDrive technical development and application standards across enterprise data lake\\nBenchmark and debug critical issues with algorithms and software as they arise.\\nLead and assist with the technical design and implementation of the Big Data cluster in various environments.\\nGuide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.\\nPerform other duties and/or special projects as assigned\\nQualifications/Requirements:\\nMust have a Bachelor’s degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Master’s degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.\\nUnderstand Hadoop cluster administration concepts.\\n3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.\\nMust have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.\\nMust have experience with batch and real-time data pipelines.\\nMust have experience as a Hadoop Technical Lead / Architect\\nMust have experience with design, development and deployment in the specified technologies.\\nMust have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.\\nWriting complex SQL queries, extracting and importing large amounts of data.\\nMust be willing to work in a fast-paced environment with an on shore – off shore distributed Agile teams.\\nMust have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.\\nMust have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.\\nMust have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.\\nExcellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through\\nDesired Characteristics:\\nExtensive experience working with data warehouses and big data platforms\\nDemonstrated experience building strong relationships with senior leaders\\nStrong leadership and influencing skills\\nOutstanding written and verbal skills and the ability to influence and motivate teams\\nEligibility Requirements:\\nYou must be 18 years or older\\nYou must have a high school diploma or equivalent\\nYou must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the onboarding process\\nYou must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.\\nNew hires (Level 4-7) must have 9 months of continuous service with the company before they are eligible to post on other roles. Once this new hire time in position requirement is met, the associate will have a minimum 6 months’ time in position before they can post for future non-exempt roles. Employees, level 8 or greater, must have at least 24 months’ time in position before they can post. All internal employees must have at least a “consistently meets expectations” performance rating and have approval from your manager to post (or the approval of your manager and HR if you don’t meet the time in position or performance requirement).\\nLegal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening.\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.\\nReasonable Accommodation Notice:\\nFederal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.\\nIf you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am – 5pm Monday to Friday, Central Standard Time.\\nThe salary range for this position is 85,000.00 - 170,000.00 USD Annual\\nSalaries are adjusted according to market in CA and Metro NY and some positions are bonus eligible.\\nGrade/Level: 12\\nJob Family Group:\\nInformation Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Senior Big Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for a Senior Big Data Engineer to join our Data Engineering team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!\\n\\nOur platform is the engine that powers the verification, optimization, and analytics solutions we provide. It has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day and evaluate thousands of data-points in real-time all while responding in just a few milliseconds.\\n\\nWhat you'll do:\\n\\nWorking on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases\\nArchitect, design, code and maintain components for aggregating tens of billions of daily transactions\\nLead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's\\nMentor junior team members\\n\\nWho you are and what you have:\\n\\n5+ years of recent hands-on Java experience\\nStrong knowledge of collections, multi-threading, JVM memory model, etc.\\nGreat understanding of designing for performance, scalability, and reliability\\nSuperb understanding of algorithms, scalability and various tradeoffs in a Big Data setting\\nIn-depth understanding of object oriented programming concepts\\nExcellent interpersonal and communication skills\\nUnderstanding of full software development life cycle, agile development and continuous integration\\nGood knowledge of Linux command line tools\\nExperience with Hadoop MapReduce, Spark, Pig\\nSolid understanding of database fundamentals, good knowledge of SQL\\n\\nWhat puts you over the top:\\n\\nExposure to messaging frameworks like Kafka or RabbitMQ\\nSome exposure to functional programming languages like Scala\\nExperience with Spark\\n\\nAbout Integral Ad Science\\n\\nIntegral Ad Science (IAS) is the global market leader in digital ad verification, offering technologies that drive high-quality advertising media. IAS equips advertisers and publishers with both the insight and technology to protect their advertising investments from fraud and unsafe environments as well as to capture consumer attention, and drive business outcomes. Founded in 2009, IAS is headquartered in New York with global operations in 17 offices across 13 countries. IAS is part of the Vista Equity Partners portfolio of software companies. For more on how IAS is powering great impressions for top publishers and advertisers around the world, visit integralads.com ( http://integralads.com/ ).\\n\\nEqual Opportunity Employer:\\nIAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.\\n\\nTo learn more about us, please visit http://integralads.com/ ( http://integralads.com/ ) and https://muse.cm/2t8eGlN ( https://muse.cm/2t8eGlN )\\n\\nAttention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Your Future Evolves Here\\nEvolent Health has a bold mission to change the health of the nation by changing the way health care is delivered. Our pursuit of this mission is the driving force that brings us to work each day. We believe in embracing new ideas, challenging ourselves and failing forward. We respect and celebrate individual talents and team wins. We have fun while working hard and Evolenteers often make a difference in everything from scrubs to jeans.\\nAre we growing? Absolutely—56.7% in year-over-year revenue growth in 2016. Are we recognized? Definitely. We have been named one of “Becker’s 150 Great Places to Work in Healthcare” in 2016 and 2017, and one of the “50 Great Places to Work” in 2017 by Washingtonian, and our CEO was number one on Glassdoor’s 2015 Highest-Rated CEOs for Small and Medium Companies. If you’re looking for a place where your work can be personally and professionally rewarding, don’t just join a company with a mission. Join a mission with a company behind it.\\n\\nWe are looking for a Data Engineer in our Payor Data Services department. This position involves the programming and analysis of healthcare data with an emphasis on payer data, coding and data analytics.\\nWhat you'll be doing:\\nUtilize SQL programs to build metadata for various data feeds\\nDevelop SAS programs (once trained) to integrate and analyze payer data from multiple sources\\nLoad and synthesize healthcare data from multiple sources\\nReview data requirements/design and implement logic to achieve data needs\\nImplement and develop data quality control protocols and monitor their impact\\nAssist in designing, programming and standardizing processes and reports\\nThe Experience you'll need (must haves):\\nCompetency in use of SQL language and scripting to load SQL Server environments\\nFamiliarity with SAS and a desire to expand SAS programming knowledge is a plus\\nCompetency in data manipulation and analysis: accessing raw data in varied formats with different methods and analyzing and processing data\\nMust be analytical, detail oriented, and possess desire to advance and grow personally and professionally\\nAbility to multi-task and manage multiple projects with varying timelines\\nMust have a passion for data and healthcare\\nBS, BA, or Masters in Computer Science, Data Analytics, Informatics, or a comparable program with a quantitative emphasis.\\n4+ years of SAS and SQL programming experience, including SAS macros, PROC SQL, and/or Enterprise Guide\\nFinishing Touches (preferred)\\nExperience in healthcare\\n\\nEvolent Health is an equal opportunity employer and considers all qualified applicants equally without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About JLL\\n\\nWe’re JLL. We’re a professional services and investment management firm specializing in real estate. As a Fortune 500 company, we help real estate owners, occupiers and investors achieve their business ambitions. We have nearly 300 corporate offices across 80 countries, with a team of more than 86,000 individuals. If you’re looking to step up your career, JLL is the perfect professional home. With us, you’ll have a chance to innovate with the world’s leading businesses, put that expertise into action on landmark projects, and work on game-changing real estate initiatives. You’ll also make long-lasting professional connections and be inspired by the best. We’re focused on opportunity and want to help you make the most of yours. Achieve your ambitions—join us at JLL!\\n\\nWhat this job involves\\n\\nWe are looking for a Data Engineer who is self-starter to work in a diverse and fast-paced environment that can join our Enterprise Data team. This is an individual contributor role that is responsible for architecting, designing and developing of data solutions that are strategic for the business and built on the latest technologies and patterns. This a global role that requires partnering with the broader JLL Technology, Data, and Information Management (TDIM) team at the country, regional and global level by utilizing in-depth knowledge of data, infrastructure, technologies and data engineering experience.\\n\\n\\nContributes to the design of information infrastructure, and data management processes to move the organization to a more sophisticated, agile and robust target state data architecture\\nDevelop systems that ingest, cleanse and normalize diverse datasets, develop data pipelines from various internal and external sources and build structure for previously unstructured data\\nInterfaces with internal colleagues and external professionals to determine requirements, anticipate future needs, and identify areas of opportunity to drive data development\\nDevelop good understanding of how data will flow &amp; stored through an organization across multiple applications such as CRM, Broker &amp; Sales, Finance, HR, MDM, ODS, Data Lake, &amp; EDW\\nDevelop data solutions that enable non-technical staff to make data-driven decisions\\nDesign &amp; develop data management and data persistence solutions for application use cases leveraging relational, non-relational databases and enhancing our data processing capabilities\\nDevelop POCs to influence platform architects, product managers and software engineers to validate solution proposals and migrate\\nArchitect and develop data lake solution to store structured and unstructured data from internal and external sources and provide technical guidance to help migrate colleagues to modern technology platform\\n\\nSound like you? To apply you need to be:\\n\\nBachelor’s degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science.\\nHands-on engineering lead who is curious about technology, should be able to quickly adopt to change and one who understands the technologies supporting areas such as Cloud Computing (AWS, Azure(preferred), etc.), Micro Services, Streaming Technologies, Network, Security, etc.\\n3 to 5 years of experience as a data developer using Python-spark, Kafka, Spark Streaming, Azure SQL Server, Cosmos DB/Mongo DB, Azure Event Hubs, Azure Data Lake Storage, Azure Search etc.\\nHands-on Experience for building Data Pipelines in Cloud and well versed with CICD and DevOps process.\\nDesign &amp; develop data management and data persistence solutions for application use cases leveraging relational, non-relational databases and enhancing our data processing capabilities.\\nExperience handling un-structured data, working in a data lake environment, leveraging data streaming and developing data pipelines driven by events/queues\\nExperience building and maintaining a data warehouse/ data lake in a production environment with efficient ETL design, implementation, and maintenance\\nTeam player, Reliable, self-motivated, and self-disciplined individual capable of executing on multiple projects simultaneously within a fast-paced environment working with cross functional teams\\n\\nWhat you can expect from us\\n\\nWe’re an entrepreneurial, inclusive culture. We succeed together—across the desk and around the globe. We believe the best inspire the best, so we invest in supporting each other, learning together and celebrating our success.\\nOur Total Rewards program reflects our commitment to helping you achieve your ambitions in career, recognition, well-being, benefits and pay. We’ll offer you a competitive salary and benefits package.\\nWith us, you’ll develop your strengths and enjoy a career full of varied experiences. We can’t wait to see where your ambitions take you at JLL.\\n\\nApply today!\\n\\nApply quoting reference [REQ74412] at jll.com/careers.\\n\\n#LI #dibot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Cloud Data Engineer, Google Professional Services</td>\n",
       "      <td>Chicago, IL 60607</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60607</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nAct as a trusted technical advisor to customers and solve complex Big Data challenges.\\nCreate and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.\\nTravel up to 30% of the time.\\nCommunicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Note: By applying to this position your application is automatically submitted to the following locations: Chicago, IL, USA; Atlanta, GA, USA\\nMinimum qualifications:\\n\\nBA/BS degree in Computer Science, Mathematics or related technical field, or equivalent practical experience.\\nExperience with data processing software (such as Hadoop, Spark, Pig, Hive) and with data processing algorithms (MapReduce, Flume).\\nExperience in writing software in one or more languages such as Java, C++, Python, Go and/or JavaScript.\\nExperience managing internal or client-facing projects to completion; experience troubleshooting clients' technical issues; experience working with engineering teams, sales, services, and customers.\\n\\nPreferred qualifications:\\nExperience working data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments.\\nExperience in technical consulting.\\nExperience working with big data, information retrieval, data mining or machine learning as well as experience in building multi-tier high availability applications with modern web technologies (such as NoSQL, MongoDB, SparkML, Tensorflow).\\nExperience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments.\\nAbout the job\\nAs a Cloud Data Engineer, you'll guide customers on how to ingest, store, process, analyze and explore/visualize data on the Google Cloud Platform. You will work on data migrations and transformational projects, and with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform challenges.\\nIn this role you are the Google Engineer working with Google's most strategic Cloud customers. Together with the team you will support customer implementation of Google Cloud products through: architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more.\\nThe Google Cloud Platform team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of businesses of all sizes use technology to connect with customers, employees and partners.\\nResponsibilities\\nAct as a trusted technical advisor to customers and solve complex Big Data challenges.\\nCreate and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.\\nTravel up to 30% of the time.\\nCommunicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.\\nAt Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL 60611</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60611</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Strong Analytics is seeking a data engineer to collaborate with our team building and managing ETL pipelines, embedding statistical algorithms in robust applications, and deploying machine learning applications to the cloud.\\n\\nThis role requires high-level Python programming, high-level SQL, experience deploying applications to AWS, and experience in distributed data processing (e.g., Hadoop, Spark, Hive).\\n\\nWe offer a comprehensive compensation package, including:\\nCompetitive salary\\nProfit sharing or equity, based on experience\\nHealth insurance\\nFour weeks paid vacation\\nWork-from-Home Wednesdays\\n\\nCandidates will be evaluated based on their skills with the following technologies/workflows:\\nPython\\nSQL\\nRelational Databases (e.g., Postgres, MySQL)\\nDistributed Computation (e.g., Spark, Hive, Hadoop)\\nDeploying and Managing Cloud Services (we use AWS/Azure)\\nInteracting with and building RESTful APIs\\nGit\\nAll applicants will be considered based on their experience and demonstrated skill/aptitude, not formal education.\\n\\nApplicants should have the ability to travel infrequently (&lt;5% of your time) for team meetings, conferences, and occasional client site visits.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL 60626</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60626</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks.\\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\\nCraft analytics tools that utilize the data pipeline to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\\nWork with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\\nOrchestrate large, complex data sets that meet functional / non-functional business requirements.\\nSeek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.\\nPartner with data and analytics talent to strive for greater functionality in our data systems.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Interested candidates must submit an application and resume/CV online to be considered\\nMust be 18 years of age or older\\nMust be willing to submit to a background investigation; any offer of employment is conditioned upon the successful completion of a background investigation\\nMust have unrestricted work authorization to work in the United States. For U.S. employment opportunities, Gallagher hires U.S. citizens, permanent residents, asylees, refugees, and temporary residents. Temporary residence does not include those with non-immigrant work authorization (F, J, H or L visas), such as students in practical training status. Exceptions to these requirements will be determined based on shortage of qualified candidates with a particular skill. Gallagher will require proof of work authorization\\nMust be willing to execute Gallagher's Employee Agreement or Confidentiality and Non-Disclosure Agreement, which require, among other things, post-employment obligations relating to non-solicitation, confidentiality and non-disclosure</td>\n",
       "      <td>Gallagher is a global leader in insurance, risk management and consulting services. We help businesses grow, communities thrive and people prosper. We live a culture defined by The Gallagher Way, our set of shared values and guiding tenets. A culture driven by our people, over 30,000 strong, serving our clients with customized solutions that will protect them and fuel their futures.\\n\\nPosition Summary:\\nConsider joining our growing team of data and analytics experts responsible for expanding our data and pipeline architecture, as well as optimizing data flow and collection for multiple functional teams. We support our data analysts and data scientists on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. You will engage in supporting the data needs of multiple teams, systems and products. Do you find the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives most exciting? We really should explore together.\\n\\nEssential Duties and Responsibilities:\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks.\\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\\nCraft analytics tools that utilize the data pipeline to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\\nWork with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\\nOrchestrate large, complex data sets that meet functional / non-functional business requirements.\\nSeek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.\\nPartner with data and analytics talent to strive for greater functionality in our data systems.\\n\\n\\nRequired:\\nBachelor's degree in Computer Science, Engineering or Information Systems Management or related field.\\nAt least 5 years of data engineering experience/ETL experience leveraging tools such as Integration Services (SSIS), Azure Data Factory, Informatica and Ab Initio.\\nU.S. Eligibility Requirements:\\nInterested candidates must submit an application and resume/CV online to be considered\\nMust be 18 years of age or older\\nMust be willing to submit to a background investigation; any offer of employment is conditioned upon the successful completion of a background investigation\\nMust have unrestricted work authorization to work in the United States. For U.S. employment opportunities, Gallagher hires U.S. citizens, permanent residents, asylees, refugees, and temporary residents. Temporary residence does not include those with non-immigrant work authorization (F, J, H or L visas), such as students in practical training status. Exceptions to these requirements will be determined based on shortage of qualified candidates with a particular skill. Gallagher will require proof of work authorization\\nMust be willing to execute Gallagher's Employee Agreement or Confidentiality and Non-Disclosure Agreement, which require, among other things, post-employment obligations relating to non-solicitation, confidentiality and non-disclosure\\nGallagher offers competitive salaries and benefits, including: medical/dental/vision plans, life and accident insurance, 401(K), employee stock purchase plan, educational expense reimbursement, employee assistance program, flexible work hours (availability varies by office and job function) training programs, matching gift program, and more.\\n\\n\\nGallagher believes that all persons are entitled to equal employment opportunity and does not discriminate against nor favor any applicant because of race, sex, color, disability, national origin, religion, creed, age, marital status, citizenship, veteran status, gender, gender identity / expression, actual or perceived sexual orientation, or any other protected characteristic. Equal employment opportunity will be extended in all aspects of the employer-employee relationship, including, but not limited to, recruitment, hiring, training, promotion, transfer, demotion, compensation, benefits, layoff, and termination. In addition, Gallagher will make reasonable accommodations to known physical or mental limitations of an otherwise qualified applicant with a disability, unless the accommodation would impose an undue hardship on the operation of our business.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Data Engineer, Python</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor's Degree in computer science or equivalent experience required.\\n2+ years of experience in the design and development of data pipelines and tasks.\\nGood understanding of data warehousing concepts and dimensional data modeling.\\nHands-on experience with troubleshooting performance issues and fine tuning SQL queries.\\nExperience in Python including in modules/libraries such as pandas, numpy, Flask, scikit-learn, and sci-py.\\nProven experience extracting data from structured data sources (SQL, Excel, CSV files, Couchbase) and unstructured data sources\\n(Splunk, log files) both on-premise and in the cloud.\\nExperience consuming data from web services, REST and SOAP, HTML, XML and JSON.\\nKnowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.\\nExperience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.\\nFamiliar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.\\n</td>\n",
       "      <td>\\nDesign, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.\\nMonitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.\\nPerform unit testing, system integration testing and assist with user acceptance testing.\\nAdapt data components to accommodate changes in source data and new business requirements.\\nCreate and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.\\nEnsure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.\\nCollaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>GoHealth is looking for Data Engineers who will be responsible for the design, development, and delivery of data transformation tasks used in transforming data into a format that can be easily analyzed. We are seeking candidates who have experience in data analysis, collection, and optimization for the purpose of informing business decisions. The Data Engineer will work with other team members in owning data pipelines including execution, documentation, maintenance, and metadata management. In this role, you will also support the development of the data infrastructure necessary for full scale data science, predictive analytics and machine learning.\\n\\nResponsibilities:\\n\\nDesign, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.\\nMonitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.\\nPerform unit testing, system integration testing and assist with user acceptance testing.\\nAdapt data components to accommodate changes in source data and new business requirements.\\nCreate and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.\\nEnsure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.\\nCollaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.\\n\\nSkills and Experience:\\n\\nBachelor's Degree in computer science or equivalent experience required.\\n2+ years of experience in the design and development of data pipelines and tasks.\\nGood understanding of data warehousing concepts and dimensional data modeling.\\nHands-on experience with troubleshooting performance issues and fine tuning SQL queries.\\nExperience in Python including in modules/libraries such as pandas, numpy, Flask, scikit-learn, and sci-py.\\nProven experience extracting data from structured data sources (SQL, Excel, CSV files, Couchbase) and unstructured data sources\\n(Splunk, log files) both on-premise and in the cloud.\\nExperience consuming data from web services, REST and SOAP, HTML, XML and JSON.\\nKnowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.\\nExperience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.\\nFamiliar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.\\n\\nBenefits and Perks:\\n\\nOpen vacation policy\\n401k program with company match\\nMedical, dental, vision, and life insurance benefits\\nFlexible spending accounts\\nCommuter and transit benefits\\nProfessional growth opportunities\\nCasual dress code\\nGenerous employee referral bonuses\\nHappy hours, ping-pong tournaments, and more company-sponsored events\\nSubsidized gym memberships\\nGoHealth is an Equal Opportunity Employer\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Data Engineer - Senior Consultant (Spark) - Chicago, IL</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.\\n\\nWe are...\\n\\n • The Industry-recognized data and analytics leaders\\n • Passionate problem solvers across a broad spectrum of technologies and industries\\n • Value seekers for measurable business outcomes\\n • Continuous learners through training and education\\n • Focused on a work-life balance with an unlimited paid time off policy\\n\\nData engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.\\nData engineers deliver value through...\\nHands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools\\nDemonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights\\nCollaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights\\nInternal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO\\nA data engineer's skills include, but are not limited to...\\nBachelors Degree and 5+ years of work experience\\n5+ years of professional IT work experience\\nSQL, SQL, SQL!\\n2+ years of Spark\\nProgramming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)\\nLinux / Windows (Command line)\\nBig Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop)\\nCloud Platforms (AWS, Azure, Google Cloud Platform)\\nData Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)\\nData Integration Tools (Talend, DataStage, Informatica, SSIS)\\nDatabases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)\\nMarkup Languages (JSON, XML, YAML)\\nCode Management Tools (Git/GitHub, SVN, TFS)\\nDevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)\\nTesting / Data Quality (TDD, unit, regression, automation)\\nSolving complex data and technology problems\\nLeading technical teams of 2+ consultants\\nAbility to design components of a larger implementation\\nExcellent communication to narrate data driven insights and technical approach\\n\\nIf this sounds like you, let’s talk!\\n\\nCandidates must be comfortable with a national travel model to client locations weekly (M-TH is typical).\\n\\nClarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.\\n\\n #LI-RC1\\nGLDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>JOB RESPONSIBILITIES:\\nDevelops data pipelines for traditional Data Warehousing and Big Data solutions\\nAssists Data Architecture with rapid prototyping, technologies research, performance testing\\nParticipates in designs discussion with data architecture and system architecture to support decisions and design choices with data and experiment designs\\nCollaborates with remove team members in an agile development framework – scrum, sprints\\nPREFERRED SKILLS:\\nBachelor’s degree in quantitative field, MS preferred\\nAbility to learn new technologies and experiment quickly\\nGood knowledge of CI/CD concepts – git, Jenkins, etc.\\nSeveral years of experience with data warehouse, analytics, ETL – exceptional command of sql in different flavors\\nVery strong knowledge of data modeling in relational DBs\\nGood programming skills in a high-level, general purpose programming language – java, scala, or python\\nGood communication skills, ability to work effectively with remote members of the team and collaborate over long-distance\\nFamiliarity with big data technologies is a plus - Hadoop, spark\\nExperience in a cloud infrastructure is a plus – AWS, GCP, Azure\\nGenerally proficient in performing data transformations via scripting, stored procedures, or an ETL framework\\nGood understanding of the 4Vs of data and development strategies for accommodating them in integration\\nSome experience developing and supporting all aspects of a big data cluster: Ingestion (rsync), Processing (Apache Nifi), Parsing (Java), integration (Python, Spark, Scale and PIG), data movement (SQOOP), workflow management (OOZIE and ActiveBatch), and querying (HIVE and Impala)\\nSome proficiency in at least one of the following programming languages: Java, JavaScript, Python, and Scala\\nProficiency in Unix and Linux operating systems\\nCapable of navigating and working effectively in a DevOps model including leveraging related technologies: Jenkins, GitLab, etc….\\nStrong foundational experience with SQL\\nExperience and Education\\nA Bachelor’s Degree in Computer Science or related field required\\nGogo’s worldwide inflight Wi-Fi services have made internet and video entertainment a regular part of flying. We are a diverse group of technologists, marketers, strategists, and any other function you can think of- all working together in extraordinary harmony. And that’s just the beginning.\\nWe connect the aviation industry and its travelers with innovative technology and applications, and we do it all in a high-energy environment that welcomes the next challenge. Be prepared for a dynamic ride with people who are passionate about what they’re building.\\n\\nGogo is an equal opportunity employer and works in compliance with both federal and state laws. We are committed to the concept regarding Equal Employment opportunity. Qualified candidates will be considered for employment regardless of race, color, religion, age, sex, national origin, marital status, medical condition or disability. The EEO is the law and is available here.\\n\\nGogo participates in E-Verify. Details in English and Spanish. Right to Work Statement in English and Spanish.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Kenna Security is revolutionizing cyber risk with a SaaS-based platform that uses data science to combine vulnerability data with exploit intelligence to measure risk, predict attacks and prioritize remediation. We are leading the way in helping enterprises reduce their risk while increasing their efficiency and preventing attacks. Kenna Security was recently named one of the top 10 hottest start-ups and named to the Inc. 500 fastest growing companies list.\\n\\n\\nWe are looking for a Sr Engineer to join our growing Research and Data Science team. This person will be responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross-functional teams. The Engineer will support our data scientists, researchers and software engineers on data initiatives and will ensure optimal data delivery is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited to support our next generation of products and data driven initiatives.\\nRESPONSIBILITIES\\nCreate and maintain optimal data pipeline.\\nBuilding functional prototypes to support Research team initiatives.\\nAssemble large, complex data sets that meet functional / non-functional business requirements.\\nWork with data scientists and researchers to design, implement, extend, tune and scale Data Science and Machine Learning libraries, frameworks, algorithms, pipelines, and tooling\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other ‘big data’ technologies.\\nCreate tools for Research team members that assist them in building and optimizing our product.\\nWork with Engineering and Product to build interfaces that allow our platform to consume insights from Research and Data Science teams.\\nResearch, Prototype, Test, &amp; Implement new technologies\\nQUALIFICATIONS\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.\\nExperience building and optimizing ‘big data’ data pipelines, architectures and data sets.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nStrong analytic skills related to working with unstructured datasets.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nA successful history of manipulating, processing and extracting value from large disconnected datasets.\\nConceptual familiarity with common machine learning techniques: classification, regression, decision trees, etc.\\nExperience supporting and working with cross-functional teams in a dynamic environment.\\nExperience with machine learning libraries and frameworks such as Tensorflow, Keras, and SciKit Learn.\\nExperience with relational SQL and NoSQL databases, including Postgres and MySQL.\\nExperience with storage/search and related logging/presentation techniques (e.g. Elasticsearch/Kibana,S3, fluentd, Solr/Lucene)\\nExperience with cloud services: EC2, EMR, RDS, Redshift, BigQuery.\\nExperience with object-oriented/object function scripting languages: Python, Java, Scala, etc.\\nExperience deploying and supporting software in a production SaaS environment.\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex or national origin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Senior Data Engineer – DBA (Data Base Admin)</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBS degree in a computer discipline or relevant certification\\n3+ years of database management with expertise in backup, recovery and replication\\n1+ years of managing an AWS Ecosystem with knowledge in areas of RDS, Dynamo DB, MySQL, Maria DB, and/or AWS Aurora\\nStrong practical knowledge of SQL query performance, resolution and overall tuning\\nProven knowledge of identifying and setting up system and performance monitoring\\nUnderstanding of the development lifecycle and service management processes including Code Promotion, Change Control and Incident/Problem Management\\nHands on Security and Compliance (PCI / SOX) on data infrastructure\\nBackup/recovery, replication, cluster failover and disaster recovery\\nWillingness to participate in On-call rotation and off hour releases\\nWorked on DB Migrations with zero downtime\\nExcellent oral and written communication</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Manage data users to enable appropriate data distribution to the user in a timely manner\\nManage transaction recovery and backup data\\nMinimize database downtime and manage parameters to provide fast query responses\\nDetermine, enforce, and document database policies, procedures, and standards\\nPerform regular tests and evaluations to ensure data security, privacy and integrity\\nMonitor database performance, implement changes and apply new patches and versions when required\\nAutomate SOX security compliance group checks\\nRequired Education and Experience\\nBS degree in a computer discipline or relevant certification\\n3+ years of database management with expertise in backup, recovery and replication\\n1+ years of managing an AWS Ecosystem with knowledge in areas of RDS, Dynamo DB, MySQL, Maria DB, and/or AWS Aurora\\nStrong practical knowledge of SQL query performance, resolution and overall tuning\\nProven knowledge of identifying and setting up system and performance monitoring\\nUnderstanding of the development lifecycle and service management processes including Code Promotion, Change Control and Incident/Problem Management\\nHands on Security and Compliance (PCI / SOX) on data infrastructure\\nBackup/recovery, replication, cluster failover and disaster recovery\\nWillingness to participate in On-call rotation and off hour releases\\nWorked on DB Migrations with zero downtime\\nExcellent oral and written communication\\nNice to have\\nDemonstration of use of Source Code Management Tools E.g. Git\\nExperience in cross-region replication with RDS\\nWorked in an agile environment and participated in Daily Scrum activities\\nKnowledge of programming such as Python\\nGogo is the inflight internet company. Our worldwide inflight Wi-Fi services have made internet and video entertainment a regular part of flying. We are a diverse and mission-minded group of professionals all working together in extraordinary harmony. And that’s just the beginning. We connect the aviation industry and air travelers with innovative technology and applications, and we do it all in a high-energy environment that welcomes the next challenge. Be prepared to join a performance-obsessed team that is passionate about bringing the internet to every device, every flight, everywhere.\\nEqual Opportunity Employer/Vets/Disabled\\nGogo participates in E-Verify. Details in English and Spanish. Right to Work Statement in English and Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Senior Cloud Data Engineer</td>\n",
       "      <td>Downers Grove, IL 60515</td>\n",
       "      <td>Downers Grove</td>\n",
       "      <td>IL</td>\n",
       "      <td>60515</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nCloud Data Engineer - Downers Grove, Illinois\\n\\nFTD Companies, we continually evolve the business by promoting a culture where solutions from the technology group help transform our business through customer-meaningful solutions, innovative technologies, and business-impacting projects.\\n\\nThis highly crucial Cloud Data Engineer position will take a leading role in helping analyze the data platform within our multi-brand $1B+ E-Commerce company. We're actively searching for top-level engineers to join newly created teams at FTD Companies who will be tasked with building an entirely new state-of-the-art platform and architecture from scratch.\\n\\nWhat You Will Contribute:\\n\\nWe participate with a multi-functional team (technical &amp; non-technical members) on highly visible strategic projects and work independently as needed. You are going to have the opportunity to tackle relevant Big Data engineering problems and interact with top-caliber software engineers, web developers, and QA automation resources contributing to our growing ecosystem of cutting-edge SOA based applications.\\n\\nYou'll manage a large-scale, high-availability, rapidly-growing BIG DATA infrastructure on Google Cloud Services that supports multi data sources. You will also be working closely with DevOps and other teams within FTD-IT area to provide quick Data Solutions.\\n\\nWe need your help to design, architect and build our data platform while using a variety of BIG DATA technologies\\nWork closely in the team to analyze and develop data architecture; ETL processes, ERD modeling and physical database implementation with GCP Data Services: Big Query, Big Table, Data Flow\\nWork with us to design, develop and roll out new application features that impact databases\\nYou can develop and maintain an in-depth understanding of the data/ETL architecture and the general application functionality used to maintain data integrity\\nYou will develop Data Flow jobs to answer complex analytical and real-time operational questions.\\nYou'll innovate by exploring, recommending, benchmarking, and implementing data-centric platform technologies.\\nProvide hardware architectural guidance, estimate cluster capacity, and create roadmaps for Hadoop or BIG DATA Cloud services.\\nYou provide support for both analytics and operational platforms.\\nWork closely with team-members including IT managers to deliver defect-free solutions in a timely manner. Update work status on a frequent (as often as daily) basis\\nYou follow and improve upon processes and policies for database application development methodologies and lifecycles\\nWork on multiple projects at a time either independently or as a team member\\nWork with developers and business owners to provide database needs for the entire FTD platform\\nOversee the development and release of solutions to non-production environments\\nAre you'll willing to collaborate with some of the best Java architects to establish platform standards when new technologies are introduced in the FTD platform?\\nAre you curious and want to continually investigate new technologies and their possible application to the company's business requirements?\\nDo you wish to assist in the development of application development processes, policies, and standards?\\nWhat we seek:\\n\\nWe seek for you to have a BS in computer science, engineering or similar technical discipline. Alternatively, you can have an equivalent combination of education and experience. Ideally, you have 3 years of experience developing solutions following \"standard methodologies\" within a large/complex environment. Agile process experience preferred while implementing solutions to complex business and technical problems\\n\\nYou are capable of logical/physical database design, development, analysis, architecture, and modeling\\nYou possess experience in architecting multi-tier, distributed database applications\\nYou have 1 year of experience with Apache Beam and 5 or more in Java or Python and with Data Transformation\\nYou've worked for 3 years with the Hadoop stack and 1 or more working with GCP services like Data Flow, Big Table, Big Query, and GCP Data Storage Buckets\\nYou are passionate about, Data, BIG DATA, and ML-learning new technologies\\nYou're experienced in designing and developing large scale applications utilizing BIG DATA tech\\nYou will have a good sense of engineering trade-offs, with an ability to understand the impact of software changes on extendibility, scalability, performance, and maintainability\\nYou have experience with Kafka/ Pub-Sub, SQL programming, and performance tuning skills\\nDo you want the ability to oversee a wide range of assignments by managing dependencies, expectations, and effectively communicating with partners? Apply now.\\nWhat we offer:\\n\\nA stupendous Senior Cloud Data Engineer opportunity to be a part of a team that builds and evolves high performance, scalable order processing systems handling large transactional volumes. We encourage and welcome out of the box ideas in all areas like artificial intelligence, data processing, and information retrieval, voice capabilities, automation, computer vision, and other ideas welcome. We much enjoy all the benefits of working for a digital E-Commerce Company (flexible hours, a healthy vacation plan, summer hours) in a relaxed work environment.\\n\\nWe firmly embrace the definition of a \"flat\" organization where you will spend time with business leaders, managers, developers, product management and executive leadership. There are job opportunities, and then there are career opportunities... Let us provide you with a career where you look forward to Mondays. Careers that have a positive impact and offer you a future as we change, grow, expand and explore new and exciting experiences for our global customer base\\n\\nCompany Description\\nFTD has been a leader in the floral industry for over a century. We are a private equity-backed company with one of the largest florist networks in the world, supported by the iconic Mercury Man® logo displayed in over 30,000 floral shops in more than 125 countries. We partner with local florists to hand-craft floral arrangements available for same-day delivery on FTD.com and ProFlowers.com. In addition to delivering flowers, we support locally-owned retail florists by providing technology, marketing, and digital services to members of our florist network. For all of life's occasions and everyday moments, visit FTD.com, ProFlowers.com, and ProPlants.com, and follow us on Facebook and Instagram at @ftdflowers. We love helping our customers #SayMorewithFlowers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Tax Staff – National Tax – Tax Technology and Transformation (TTT) – Data Scientist – Advanced Technologies - Chicago</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nStrong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.\\nExperience with common data science programming languages, such as Python R\\nStrong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential\\nExperience with data visualization tools, such as PowerBI, D3.js, etc.\\nExperience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB\\nStrong scripting and programming skills\\nOwnership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.\\nAbility to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies\\nAbility to consolidate tax data to make analysis and planning more efficient\\nFocus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities\\nWillingness to support project team members in any way needed to help ensure timely completion of deliverables</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Tax Technology and Transformation offers services to companies in response to the impact of existing and emerging technology, including the growing data burden that many businesses face, driving efficiencies to create a cost-effective tax function and the need to understand how to make data an asset. The underlying objective of the combined offerings is to help businesses navigate the digital age of tax transparency alongside new trends in tax compliance and tax audit methods, as well as helping to solve the most pressing challenges that businesses face. Tax Technology and Transformation is composed of the following services:\\nDigital tax transformation\\nTax applications-as-a-service\\nTax data and improvement\\nTax analytics and reporting enhancement\\nEmerging tax technology, including robotic process automation (RPA), artificial intelligence (AI), blockchain, cloud solutions, data lake development and business intelligence innovation\\nTax technology program mobilization\\nCustom tax technology application development and deployment\\nTax technology strategy and road mapping\\nDirect and indirect tax systems implementation and configuration\\nPost-transaction (M&amp;A) tax function operational services\\nTax operating model transformation, including process improvement, risk and controls\\nTax function assessments\\nThe opportunity\\nTax Technology and Transformation is an area that has seen significant growth and investment, and you will see that reflected in your experience. It is no exaggeration to say that you will be working on highly publicized projects. The field of taxation is constantly changing as new laws, regulations, and technologies are created, and this is your opportunity to be part of that development.\\n\\nKey responsibilities\\nWe are looking for an ambitious, self-motivated data scientist or data engineer who will help us discover the information hidden in vast amounts of data, and help us deliver even better products to our clients. Your primary focus will be in applying data mining techniques, doing statistical analysis and building high quantity prediction systems integrated with our products. You will be expected to team on a national and even global scale, so strong communication skills, attention to detail, and ability to effectively drive results are essential.\\nSelecting features and, building and optimizing classifiers using machine learning techniques\\nData mining using state-of-the-art methods\\nEnhancing data collection procedures to include information that is relevant for building analytic systems\\nProcessing, cleansing and verifying the integrity of data used for analysis\\nDoing ad-hoc analysis and presenting results in a clear manner\\nDepending on your unique skills and ambitions, you could be supporting various client projects, ranging from assisting in the production of leading edge machine models, to designing and implementing robust data pipelines that can handle data at a multinational, enterprise scale. Whatever you find yourself doing, you will contribute and help toward developing a highly trained team, all the while handling activities with a focus on quality and commercial value. This is a highly regulated industry, so it is all about maintaining our reputation as trusted advisors by taking on bold initiatives and owning new challenges.\\n\\nSkills and attributes for success\\nStrong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.\\nExperience with common data science programming languages, such as Python R\\nStrong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential\\nExperience with data visualization tools, such as PowerBI, D3.js, etc.\\nExperience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB\\nStrong scripting and programming skills\\nOwnership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.\\nAbility to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies\\nAbility to consolidate tax data to make analysis and planning more efficient\\nFocus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities\\nWillingness to support project team members in any way needed to help ensure timely completion of deliverables\\nTo qualify for the role, you must have\\nA bachelor’s degree in information system, tax technology, management information systems or computer science or related field and a minimum of one year of related work experience\\nA passionate interest in data science and its role in the organization\\nExcellent communication and business writing skills\\nA natural flair for problem solving and an entrepreneurial approach to work\\nStrong organizational and time management skills, with exceptional client-serving consulting skills\\nDemonstrated ability to capture and synthesize business requirements\\nDesire and demonstrated ability to provide leadership within a team\\nIdeally, you’ll also have\\nExperience with Apache Spark\\nExperience with Hadoop and/or distributed database systems\\nExperience working in the Microsoft Azure Cloud environment\\nExperience developing ETL solutions using SSIS or other tools\\nERP experience, including SAP and/or Oracle-preferred but not required\\nPractical experience or strong theoretical understanding of neural networks\\nWhat we look for\\nWe are looking for knowledgeable data science professionals with a passion for turning data into actionable insight. You will need strong business acumen and a firm strategic vision, so if you are ready to use those skills to develop your team, this role is for you.\\n\\nWhat working at EY offers\\nWe offer a competitive compensation package where you will be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, a minimum of three weeks of vacation plus ten observed holidays and three paid personal days; and a range of programs and benefits designed to support your physical, financial and social well-being. We also offer:\\nSupport and coaching from some of the most engaging colleagues in the industry\\nOpportunities to develop new skills and progress your career\\nThe freedom and flexibility to handle your role in a way that is right for you\\nAbout EY\\nAs a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.\\n\\nEY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\\n\\nIf you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible.\\n\\nMake your mark. Apply today.\\n\\n.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Enova:\\nHard working people need access to fast, trust-worthy credit and Enova uses advanced technology and analytics to provide that to them. With a focus on non-prime customers and small businesses, we've served over 5 millions customers through our six businesses in the US and abroad. We're born and raised in Chicago and we pride ourselves on hiring smart, driven people who like solving challenging business problems. Our philosophy is simple, \"Life's short. Work some place awesome.\"\\n\\nMany of us consider our people to be Enova's best perk. We have 1,500+ employees and your teammates are as passionate about their work as you are. Your manager and team will encourage you to think outside the box and will celebrate your wins with you along the way. We're big on career growth and make sure you have the tools you need to succeed. On top of that, we also offer competitive salaries, health care benefits, a 401k matching plan, summer hours, tuition reimbursement and a sabbatical program. Our Chicago headquarters even offers over 100 different kinds of snacks, a game room, onsite massages/barbers/nail technicians, and a variety of different social events.\\n\\nWhat you'll be doing:\\nIn this role, you will build technical solutions to help improve the scalability and performance of our data stores and our overall systems. As a Senior Engineer, you will focus on efforts that will provide increased flexibility and accessibility to our data, such as our cloud based data warehouse and data pipeline initiatives. In addition, you will have the opportunity to take ownership of driving Enova forward with technologies and architecture patterns that will best serve the business.\\n\\nYour core priorities will be to:\\n\\nCollaborate with other teams in Software Engineering, Analytics and Infrastructure to implement new solutions.\\nServe as an advocate for best practices and standards for database related efforts, and work toward optimizing the interfaces between our applications and our data stores.\\nProvide increased flexibility and accessibility to our data.\\nMentor other engineers on best practices and patterns.\\n\\nWhat you should have:\\n\\n10+ years of experience in software engineering with a focus on database related technologies\\nDeep technical knowledge of SQL and database related technologies (particularly PostgreSQL)\\nStrong knowledge of relational database modeling principles and techniques\\nExposure to various non-relational / NoSQL database management systems\\nExperience with various service providers such as AWS\\nComfort with orchestration and scheduling tooling such as Jenkins/Airflow/Rundeck\\nWorking knowledge of one or more programming languages\\nBachelor's degree in CS, IT, or related study\\n\\nAbout Engineering:\\nThe Software Engineering Team, one of the largest groups in the company, is responsible and accountable for meeting the demands of our current and future businesses. We help create the \"Tech\" in FinTech.\\n\\nWe are structured into small full-stack teams, each aligned to specific business lines or core services. Each team is responsible for defining and delivering solutions through smart interactive development. We code in Ruby, Go, Java, and Swift. We use Vue and other JS frameworks for front-end development. However, we welcome engineers from different technical backgrounds and have created a training program to get you up to speed on our tech stack. Baseline is our self-paced training program, which provides a suite of exercises for all new engineering hires to work through during their first few weeks, ensuring they have the knowledge needed to be successful in their role.\\n\\nAlthough we are divided into unique teams, our culture of collaboration promotes and encourages engagement across every team and department within the company - no team is a silo. This enables us to align our core values and create strong, best practices.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Senior Big Data Engineer - Procurement Team</td>\n",
       "      <td>Chicago, IL 60642</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60642</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Mars Commercial (Procurement) manages a significant part of Mars’ P&amp;L, ensuring that our organization continuously improves our cost base while driving improvements in quality and innovation. This team is embarking on an ambitious and exciting journey to transform our analytics capabilities. This will involve building core foundations to push information to our Commercial associates while delivering fast results to our category teams by developing decision-making tools that solve complex problems.\\n\\nWhat you’ll do\\n\\nKeeping the data flowing and readily available to solve problems that need immediate solutions is core to what you’ll do. Your passion for big data challenges around innovation, emerging technologies and legacy environments drives your desire to design and implement next-gen solutions to traditional business challenges. You’re ready to step up and take charge of developing data-driven processes that improve the functionality of your stakeholders and result in leaner operations, processes and profitability.\\nSome of your day-to-day work will include, but won’t be limited to:\\n\\nLeading the effort to develop well-designed modular code that’s usable and easily maintained\\nTaking ownership of establishing proprietary internal libraries of shareable functions to establish a standardization for engineering work\\nEngineering data-centric solutions from scratch through the incorporation of ongoing emerging technologies\\nTaking ownership of an untamed data lake and turning into highly functional, pristine data sources\\nSupporting the big data strategies of your stakeholders through self-service analytics, real-time decision engines and AI/ML advances\\nStaying abreast of emerging technologies, architecture patterns, programming languages and ML algorithms and assuming the mentorship and training of other team members\\nCollecting functional and non-functional client requirements while monitoring technical environments, business constraints and enterprise requirements\\n\\nWhat you’ll need\\n\\nMinimum 7 years of experience preferred within a Big Data Azure and/or AWS environment\\nPrior experience within a Procurement environment is preferred; knowledge of SAP Procurement-related modules will be helpful\\nExperience with or exposure to the FMCG/CPG industry or Procurement\\nPossess the ability to bring together divergent data sets that meet the requirements of the Data Science and Data Analytics teams\\nProficiency with data modelling, query techniques and complexity analysis\\nExperience with cloud, container and micro service infrastructures\\nTechnical expertise with emerging Big Data technologies, such as: Python, Scala, Spark, Hadoop, Clojure, Git, Flink, Elasticsearch, SQL, scikit-learn, TensorFlow, and DNB (databricks); visualization tools: Tableau and PowerBI\\nA good understanding of and adherence to data security standards\\nA Bachelor’s in computer science or similar disciplines\\n\\nInspiring the whole Mars business to adopt data driven decision-making by developing advanced analytics methods using Machine Learning/AI is huge in your role. How? By mining vast amounts of data from company databases for insights that will help solve business problems and ultimately make Mars more profitable.\\n\\n#LI-SA1\\n\\nThis is an exciting time at Mars. We’re using digital, data and user insights to transform our business by finding answers to problems that we’ve often never asked ourselves before. From joining the dots to improve our data ecosystem, to streamlining the efficiency and automation of our supply chain and quality operations, we’re already seeing some brilliant results. In fact, we’ve built so much momentum that we’re now looking for industry leaders in Business Translation, Data Science and Data Engineering with different and complementary skills to influence how we operate and grow beyond anything we’ve achieved before. Join us, and discover a company set up to develop your capabilities and ambitions and a group of colleagues ready to support and inspire you. Working together, we’ll create a better world for our planet, our communities and our pets.\\n\\nMars is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Spark with Big Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>7 years development experience in one of the Big Data Technologies Hadoop eco system Pig Hive HBASE Spark Scala KafkaCross trained in Talend is added advantageDW basic conceptsUnix scripting is added advantageTo be proficient in designing efficient and robust ETL workflowsTo be able to work with cloud computing environmentsGather and process raw data at scale including writing scripts web scraping calling APIs write SQL queries etcSecondary skillset Knowledge on JIRA Github Jenkins Zena Healthcare Claims KnowledgeExperience in Agile Development methodologiesExcellent communication skills and team dynamics. Should have good hands on experience in Hadoop ecosystem like Spark, Scala, Hive, Pig, Sqoop and HDFS.Job Types: Full-time, ContractContract Length:More than 1 yearContract Renewal:LikelyFull Time Opportunity:YesWork Location:One location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Data Engineer Midwest</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are:\\n\\nApplied Intelligence, the people who love using data to tell a story. We’re also the world’s largest team of data scientists, data engineers, and experts in machine learning and AI. A great day for us? Solving big problems using the latest tech, serious brain power, and deep knowledge of just about every industry. We believe a mix of data, analytics, automation, and responsible AI can do almost anything—spark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services. Want to join our crew of sharp analytical minds? Visit us here to find out more about Applied Intelligence.\\n\\n\\nYou are:\\n\\nAn expert engineer with an eye for AI. You want to change how the world works and lives by taking AI out of the lab and into everyday life.\\n\\n\\nThe work:\\n\\nYou’ll be part of a team with incredible end-to-end digital transformation capabilities that shares your passion for digital technology and takes pride in making a tangible difference. If you want to contribute on an incredible array of the biggest and most complex projects in the digital space, consider a career with Accenture Digital.\\n\\nHere’s what you need:\\nMinimum 2+ years of experience in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments using Spark, pySpark, SparkSQL, with Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)\\nMinimum 1 year of designing and building performant data tiers (or refactoring existing ones), that supports scaled AI and Analytics, using different Cloud native data stores on AWS (Kinesis, S3. GLUE, DynamoDB etc.) or Azure (HDInsights, AzureData Factory) or GCP (DataProc, PubSub, BigQuery) as well as using NoSQL and Graph Stores.\\nMinimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies\\nMinimum 1-year performance engineering, profiling, debugging very large big data and ML production solutions on Spark and native Cloud technologies\\nBonus points if:\\n\\nMinimum 6 months of experience in implementation with Databricks.\\nMinimum 1 year of designing and building secured and governed Big Data ETL pipelines, using Talend or Informatica technologies; for data curation and analysis of large le production deployed solutions.\\nExperience implementing smart data preparation tools such as Palate, Trifacta, Tamr for enhancing analytics solutions.\\nMinimum 1 year of building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions\\n\\n\\nImportant information\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.\\n\\nAccenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Solstice is an innovation and emerging technology firm that helps Fortune 500 companies seize new opportunities through world-changing digital solutions. As strategists and consultants, we help organizations evolve their digital strategy to solve mission-critical problems. As designers and developers, we build incredible hardware and software solutions that transcend a standalone product and transform an organization's relationship with its customers.\\n\\nSolstice is looking to hire a Data Engineer to join our growing capabilities team. If you are innovative, passionate about data and AI technologies, and looks to continually learn and enjoys sharing expertise, read on!\\n\\nAbout you\\n\\n\\nYou have a strong passion for building innovative and intelligent solutions around data\\nExperience with designing data models and ETL.\\nExperience in working with message queuing, stream processing, and highly scalable big data stores.\\nExperience with big data tools like Google BigQuery, Hadoop, Spark, Kafka, Elasticsearch etc.\\nExperience with relational SQL and NoSQL databases such as Postgres and Cassandra.\\nExperience with data pipeline and workflow management tools (any in particular?)\\nExperience with stream-processing systems such as Storm and Spark-Streaming\\nStrong background in Python, Java and/or .NET, knowledge with Kotlin and Scala is a huge plus\\nFamiliar with Microservice design patterns including Serverless and BFF\\nExperience designing, building, integrating and testing with RESTful APIs\\nExperience in developing and implementing scripts for database maintenance, monitoring, and performance tuning to be applied across the business\\nYou have the ability to effectively communicate technical topics to product owners, stakeholders and other business team members\\nExperience with data visualization and reporting tools like Looker, Tableau or PowerBi\\nExperience with cloud technologies such as Google Cloud Platform (GCP) or Amazon Web Services (AWS)\\nStrong verbal and written communication is a must\\nExperience working in an Agile Scrum development environment, or in a consulting capacity is a plus\\nExperience in Machine Learning is a plus\\n\\nWhat You Will be Doing\\n\\n\\nDesigning, Migrating, Building, and Testing large scale data processing architectures\\nBuilding enterprise applications on the cloud and technologies such as Google Cloud, BigQuery, AutoML, Google Data Studio\\nHelping clients implement ways to improve data reliability, efficiency, and quality, and build intelligent solutions leveraging data\\nWorking with designers to help visualize data to provide insights to end-users\\nPerforming ad-hoc analyses of data stored in the business's MySQL/MS SQL databases and writing SQL scripts, stored procedures, functions, and views\\nInterfacing with our clients and providing technical recommendations\\nHelping evaluate emerging cross-platform frameworks and enterprise application platforms\\nBridging the gap between elegant front end design and existing enterprise back end architectures\\nWorking with experienced data engineers, data scientists, and data architects to foster your experience and growth\\n\\nWe welcome Solsties to show up as their full selves everyday. Because this is so important to us, Solstice is proud to be an equal opportunity employer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Title           Location     City  \\\n",
       "0    Big Data Engineer                            Chicago, IL        Chicago   \n",
       "1    Data Engineer                                Chicago, IL        Chicago   \n",
       "2    Cloud Data Engineer                          Chicago, IL        Chicago   \n",
       "3    Azure Data Architect                         Chicago, IL        Chicago   \n",
       "4    Big Data Engineer - Senior Consultant        Chicago, IL        Chicago   \n",
       "..                                     ...                ...            ...   \n",
       "124  Senior Data Engineer                         Chicago, IL        Chicago   \n",
       "125  Senior Big Data Engineer - Procurement Team  Chicago, IL 60642  Chicago   \n",
       "126  Spark with Big Data Engineer                 Chicago, IL        Chicago   \n",
       "127  Data Engineer Midwest                        Chicago, IL        Chicago   \n",
       "128  Data Engineer                                Chicago, IL        Chicago   \n",
       "\n",
       "    State         Zip     Country  \\\n",
       "0    IL    None Found  None Found   \n",
       "1    IL    None Found  None Found   \n",
       "2    IL    None Found  None Found   \n",
       "3    IL    None Found  None Found   \n",
       "4    IL    None Found  None Found   \n",
       "..   ..           ...         ...   \n",
       "124  IL    None Found  None Found   \n",
       "125  IL    60642       None Found   \n",
       "126  IL    None Found  None Found   \n",
       "127  IL    None Found  None Found   \n",
       "128  IL    None Found  None Found   \n",
       "\n",
       "                                                                      Qualifications  \\\n",
       "0    None Found                                                                        \n",
       "1    None Found                                                                        \n",
       "2    None Found                                                                        \n",
       "3    At least 5 years of consulting or client service delivery experience on Azure\\n   \n",
       "4    None Found                                                                        \n",
       "..          ...                                                                        \n",
       "124  None Found                                                                        \n",
       "125  None Found                                                                        \n",
       "126  None Found                                                                        \n",
       "127  None Found                                                                        \n",
       "128  None Found                                                                        \n",
       "\n",
       "                          Skills Responsibilities   Education  \\\n",
       "0    None Found                   None Found       None Found   \n",
       "1    None Found                   None Found       None Found   \n",
       "2    None Found                   None Found       None Found   \n",
       "3    DevOps on an Azure platform  None Found       None Found   \n",
       "4    None Found                   None Found       None Found   \n",
       "..          ...                          ...              ...   \n",
       "124  None Found                   None Found       None Found   \n",
       "125  None Found                   None Found       None Found   \n",
       "126  None Found                   None Found       None Found   \n",
       "127  None Found                   None Found       None Found   \n",
       "128  None Found                   None Found       None Found   \n",
       "\n",
       "                                                                   Requirement  \\\n",
       "0    None Found                                                                  \n",
       "1    None Found                                                                  \n",
       "2    None Found                                                                  \n",
       "3     Proven ability to build, manage and foster a team-oriented environment\\n   \n",
       "4    None Found                                                                  \n",
       "..          ...                                                                  \n",
       "124  None Found                                                                  \n",
       "125  None Found                                                                  \n",
       "126  None Found                                                                  \n",
       "127  None Found                                                                  \n",
       "128  None Found                                                                  \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           FullDescriptions  \n",
       "0    Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for a Data Engineer to join our Data Engineering team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!\\n\\nAs a Data Engineer you will build and expand upon the testing framework and testing infrastructure of IAS' core ad verification, analytics and anti ad fraud software products.\\n\\nThe ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment. You should be able to not take yourself too seriously as well.\\n\\nWhat you'll do:\\n\\nWorking on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases\\nArchitect, design, code and maintain components for aggregating tens of billions of daily transactions\\nLead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's\\nMentor junior team members\\n\\nWho you are and what you have:\\n\\nUp to 5 years of recent hands-on experience with object oriented languages (Java, Scala, Python)\\nStrong knowledge of collections, multi-threading, JVM memory model, etc.\\nGreat understanding of designing for performance, scalability, and reliability\\nSuperb understanding of algorithms, data structures, scalability and various tradeoffs in a Big Data setting\\nIn-depth understanding of object oriented programming concepts\\nExcellent interpersonal and communication skills\\nUnderstanding of full software development life cycle, agile development and continuous integration\\nGood knowledge of Linux command line tools\\nExperience with Hadoop MapReduce, Spark, Pig\\nGood understanding of database fundamentals, good knowledge of SQL\\n\\nWhat puts you over the top:\\n\\nExposure to messaging frameworks like Kafka or RabbitMQ\\nSome exposure to functional programming languages like Scala\\nExperience with Spark\\n\\nAbout Integral Ad Science\\n\\nIntegral Ad Science (IAS) is the global market leader in digital ad verification, offering technologies that drive high-quality advertising media. IAS equips advertisers and publishers with both the insight and technology to protect their advertising investments from fraud and unsafe environments as well as to capture consumer attention, and drive business outcomes. Founded in 2009, IAS is headquartered in New York with global operations in 17 offices across 13 countries. IAS is part of the Vista Equity Partners portfolio of software companies. For more on how IAS is powering great impressions for top publishers and advertisers around the world, visit integralads.com ( http://integralads.com/ ).\\n\\nEqual Opportunity Employer:\\nIAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.\\n\\nTo learn more about us, please visit http://integralads.com/ ( http://integralads.com/ ) and https://muse.cm/2t8eGlN ( https://muse.cm/2t8eGlN )\\n\\nAttention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1    Why VillageMD?\\n\\nVillageMD is changing the trajectory of healthcare by empowering primary care physicians to make informed decisions and engage patients in meaningful ways. We work with thousands of clinicians and healthcare disruptors across the country to build and contribute to our platform to improve patient health while driving down the cost to deliver it.\\n\\nWe are a mission-oriented organization and are thrilled about the work that we do every day. We're transparent, collaborative, and relentless in pursuit of our mission, all while doing so with humility and a low ego. We believe that diverse backgrounds and experiences create the best opportunity for innovation and the community that we are creating is greater than any individual.\\n\\nWe've built our technology using the best of cloud and open-source technologies to create an open, data-first platform that is enriched with analytical models and modernly connected to internal and external apps. These apps drive clinical decision support, patient engagement, and other facilitators of innovative, information-enriched health experiences.\\n\\nData Engineers at VillageMD build distributed components, pipelines, and tools that enable our organization to make analytical, data-driven decisions. We're in a unique position to impact everyone in primary care from independent, family-owned practices to world-class health systems. We aggregate, process, and deliver rich datasets to improve the effectiveness of primary care for our doctors and patients.\\n\\nWhat are examples of work that Data Engineers have done at VillageMD?\\n\\n\\nBuilt and implemented a data profiling tool to reverse engineer data schemas from new data sources facilitating normalization of the data into our data model\\nCreated a summary data platform supporting our presentation layer that allows clinicians and operators in our practices to pinpoint interventions on-demand to patients most in need\\nAnalyzed and designed the best ways to expand our data model to incorporate more data that's mission critical\\n\\nWhat will make you successful here?\\n\\n\\nStrong analytical and technical skills\\nA real passion for problem solving and learning new technology\\nVision to balance speed and maintainability in solution design\\nThe ability to handle multiple, concurrent projects\\nCrafting and implementing requirements, keeping projects on track, and engaging partners\\nChallenging the status quo to improve our processes and tools\\nCommunicating complex technical details in meaningful business context\\nA low ego and humility; an ability to gain trust by doing what you say you will do\\n\\nWhat you might do in your first year:\\n\\nOwn ten projects to design and implement best-in-class data processing enabling clean data flow directly to our data model and on to our presentation layer\\nWork with analytics, engineering and operations to design and implement a new analytics product that supports improving patient health\\nDesign a new concept within our data model to meet a new operational or analytical need\\n\\nThe following experience is relevant to us:\\n\\n5+ years of full-time experience including extensive experience with healthcare data\\nAbility to understand and design relational data structures required\\nVery strong capabilities manipulating data using SQL\\nKnowledge of, and/or willingness to learn, non-relational data structures and other technologies (e.g. Postgres, Redshift, Cassandra, MongoDB, Neo4j, S3, etc.)\\nExperience or willingness to learn building information pipelines utilizing Python or Java a plus\\nBS/MS in computer science, math, engineering, or other related fields is required.\\nTrack record of successfully executing projects with multiple partners\\n\\nWhat can we offer you?\\n\\n\\nCompetitive salary, bonus, and health benefits\\nPaid gym membership\\nFun, fast-paced, startup environment (with snacks)\\nPre-tax savings on commute expenses\\nRemote flexibility\\nA highly-collaborative, conscientious, forward-thinking environment that welcomes the impact you can make from Day 1.\\nA clear link between our daily work on products and services and the improved quality of healthcare that this work facilitates for patients.\\n\\nAt VillageMD, we see diversity and inclusion as a source of strength in transforming healthcare. We believe building trust and innovation are best achieved through diverse perspectives. To us, acceptance and respect are rooted in an understanding that people do not experience things in the same way, including our healthcare system. Individuals seeking employment at VillageMD are considered without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2    Who We Are!\\nAt Maven Wave, we are relentless in hiring the industry’s top talent. Each employee is hand-picked not only for their skills, but for their personality and broad expertise. We are looking for this rare combination of talent that sets us apart in the industry.\\nMaven Wave helps leading companies make the shift to digital and shorten the fuse to innovation. We combine the expertise of top-tier consulting with the agility of a cutting-edge technology firm. This multidisciplinary blend of skills allows us to create unique digital advantages for our clients. Maven Wave’s digital solutions are agile, mobile, rooted in analytics, and built in the cloud.\\n\\nMaven Wave, Google, and YOU: Help us build data driven cloud solutions.\\nWe are looking for a skilled Cloud Data Engineer to design, build, and test data ingestion and ETL programs with a strong focus on performance and data quality management.\\n\\nYour Life As a Maven:\\nBuild and implement complex data solutions in the cloud (AWS, GCP and/or Microsoft).\\nUncover and recommend remediations for data quality anomalies.\\nInvestigate, recommend and implement data ingestion and ETL performance improvements.\\nDocument data ingestion and ETL program designs, present findings, conduct peer code reviews.\\nDevelop and execute test plans to validate code.\\n Your Expertise:\\n4+ years experience building complex ETL programs with Informatica, DataStage, Spark, Dataflow, etc.\\n3+ years experience in Python and/or Java, developing complex SQL queries, and working with relational database technologies.\\nExperience configuring big data solutions in a cloud environment (AWS, Azure or GCP).\\nExperience using cloud storage and computing technologies such as BigQuery, RedShift, or Snowflake.\\nExperience developing complex technical and ETL programs within a Hadoop ecosystem.\\n Your X-Factor:\\nAptitude - You have an innate capacity to transition from project to project without skipping a beat.\\nCommunication - You have excellent written and verbal communication skills for coordination across projects and teams.\\nImpact - You are a critical thinker with an emphasis on creativity and innovation.\\nPassion - You have the drive to succeed paired with a continuous hunger to learn.\\nLeadership - You are trusted, empathetic, accountable, and empower others around you.\\nWhy We’re Proud To Be Mavens!\\nGoogle Cloud North America Services Partner of the Year 2019, 2018\\n#21 Best Workplaces in Chicago, FORTUNE, 2018\\nGreat Place To Work Certification, Great Place to Work, 2017 & 2018\\nFast Fifty, Crain's Chicago Business\\n101 Best and Brightest Companies to Work For, National Association for Business Resources (NABR)\\nTop Google Cloud Partner, Clutch\\nFastest Growing Consulting Firms in North America (#11, #37), Consulting Magazine\\nTop IT Services Companies, Clutch\\nGoogle Global Rising Star Partner of the Year\\nReady to Learn More?\\nLife as a Maven\\nCheck out the Apps and Data Team\\nSee what Glassdoor has to say\\nReal Customer Stories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3    Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.\\n\\nRole & Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nQualifications\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 10 years of experience in big data, database and data warehouse architecture and delivery\\nMinimum of 5 years of professional experience in 2 of the following areas:\\n§ Solution/technical architecture in the cloud\\n§ Big Data/analytics/information analysis/database management in the cloud\\n§ IoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training & Certification\\nMCSE Cloud Platform & Infratsructiure Training & Certification\\nMCSD Azure Solutions Architect Training & Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nStrong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.  \n",
       "4    Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.\\n\\nWe are...\\n\\n • The Industry-recognized data and analytics leaders\\n • Passionate problem solvers across a broad spectrum of technologies and industries\\n • Value seekers for measurable business outcomes\\n • Continuous learners through training and education\\n • Focused on a work-life balance with an unlimited paid time off policy\\n\\nBig Data engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.\\nBig Data Engineers deliver value through...\\nHands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools\\nDemonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights\\nCollaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights\\nInternal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO\\nA Big Data Engineer's skills include, but are not limited to...\\nBachelors Degree and 5+ years of work experience\\n5+ years of professional IT work experience\\nSQL, SQL, SQL!\\n2+ years of Spark\\nProgramming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)\\nLinux / Windows (Command line)\\nBig Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop)\\nCloud Platforms (AWS, Azure, Google Cloud Platform)\\nData Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)\\nData Integration Tools (Talend, DataStage, Informatica, SSIS)\\nDatabases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)\\nMarkup Languages (JSON, XML, YAML)\\nCode Management Tools (Git/GitHub, SVN, TFS)\\nDevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)\\nTesting / Data Quality (TDD, unit, regression, automation)\\nSolving complex data and technology problems\\nLeading technical teams of 2+ consultants\\nAbility to design components of a larger implementation\\nExcellent communication to narrate data driven insights and technical approach\\n\\nIf this sounds like you, let’s talk!\\n\\nCandidates must be comfortable with a national travel model to client locations weekly (M-TH is typical).\\n\\nClarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.\\n \\n#LI-NT1\\nGLDR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "124  About Enova:\\nHard working people need access to fast, trust-worthy credit and Enova uses advanced technology and analytics to provide that to them. With a focus on non-prime customers and small businesses, we've served over 5 millions customers through our six businesses in the US and abroad. We're born and raised in Chicago and we pride ourselves on hiring smart, driven people who like solving challenging business problems. Our philosophy is simple, \"Life's short. Work some place awesome.\"\\n\\nMany of us consider our people to be Enova's best perk. We have 1,500+ employees and your teammates are as passionate about their work as you are. Your manager and team will encourage you to think outside the box and will celebrate your wins with you along the way. We're big on career growth and make sure you have the tools you need to succeed. On top of that, we also offer competitive salaries, health care benefits, a 401k matching plan, summer hours, tuition reimbursement and a sabbatical program. Our Chicago headquarters even offers over 100 different kinds of snacks, a game room, onsite massages/barbers/nail technicians, and a variety of different social events.\\n\\nWhat you'll be doing:\\nIn this role, you will build technical solutions to help improve the scalability and performance of our data stores and our overall systems. As a Senior Engineer, you will focus on efforts that will provide increased flexibility and accessibility to our data, such as our cloud based data warehouse and data pipeline initiatives. In addition, you will have the opportunity to take ownership of driving Enova forward with technologies and architecture patterns that will best serve the business.\\n\\nYour core priorities will be to:\\n\\nCollaborate with other teams in Software Engineering, Analytics and Infrastructure to implement new solutions.\\nServe as an advocate for best practices and standards for database related efforts, and work toward optimizing the interfaces between our applications and our data stores.\\nProvide increased flexibility and accessibility to our data.\\nMentor other engineers on best practices and patterns.\\n\\nWhat you should have:\\n\\n10+ years of experience in software engineering with a focus on database related technologies\\nDeep technical knowledge of SQL and database related technologies (particularly PostgreSQL)\\nStrong knowledge of relational database modeling principles and techniques\\nExposure to various non-relational / NoSQL database management systems\\nExperience with various service providers such as AWS\\nComfort with orchestration and scheduling tooling such as Jenkins/Airflow/Rundeck\\nWorking knowledge of one or more programming languages\\nBachelor's degree in CS, IT, or related study\\n\\nAbout Engineering:\\nThe Software Engineering Team, one of the largest groups in the company, is responsible and accountable for meeting the demands of our current and future businesses. We help create the \"Tech\" in FinTech.\\n\\nWe are structured into small full-stack teams, each aligned to specific business lines or core services. Each team is responsible for defining and delivering solutions through smart interactive development. We code in Ruby, Go, Java, and Swift. We use Vue and other JS frameworks for front-end development. However, we welcome engineers from different technical backgrounds and have created a training program to get you up to speed on our tech stack. Baseline is our self-paced training program, which provides a suite of exercises for all new engineering hires to work through during their first few weeks, ensuring they have the knowledge needed to be successful in their role.\\n\\nAlthough we are divided into unique teams, our culture of collaboration promotes and encourages engagement across every team and department within the company - no team is a silo. This enables us to align our core values and create strong, best practices.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "125  Mars Commercial (Procurement) manages a significant part of Mars’ P&L, ensuring that our organization continuously improves our cost base while driving improvements in quality and innovation. This team is embarking on an ambitious and exciting journey to transform our analytics capabilities. This will involve building core foundations to push information to our Commercial associates while delivering fast results to our category teams by developing decision-making tools that solve complex problems.\\n\\nWhat you’ll do\\n\\nKeeping the data flowing and readily available to solve problems that need immediate solutions is core to what you’ll do. Your passion for big data challenges around innovation, emerging technologies and legacy environments drives your desire to design and implement next-gen solutions to traditional business challenges. You’re ready to step up and take charge of developing data-driven processes that improve the functionality of your stakeholders and result in leaner operations, processes and profitability.\\nSome of your day-to-day work will include, but won’t be limited to:\\n\\nLeading the effort to develop well-designed modular code that’s usable and easily maintained\\nTaking ownership of establishing proprietary internal libraries of shareable functions to establish a standardization for engineering work\\nEngineering data-centric solutions from scratch through the incorporation of ongoing emerging technologies\\nTaking ownership of an untamed data lake and turning into highly functional, pristine data sources\\nSupporting the big data strategies of your stakeholders through self-service analytics, real-time decision engines and AI/ML advances\\nStaying abreast of emerging technologies, architecture patterns, programming languages and ML algorithms and assuming the mentorship and training of other team members\\nCollecting functional and non-functional client requirements while monitoring technical environments, business constraints and enterprise requirements\\n\\nWhat you’ll need\\n\\nMinimum 7 years of experience preferred within a Big Data Azure and/or AWS environment\\nPrior experience within a Procurement environment is preferred; knowledge of SAP Procurement-related modules will be helpful\\nExperience with or exposure to the FMCG/CPG industry or Procurement\\nPossess the ability to bring together divergent data sets that meet the requirements of the Data Science and Data Analytics teams\\nProficiency with data modelling, query techniques and complexity analysis\\nExperience with cloud, container and micro service infrastructures\\nTechnical expertise with emerging Big Data technologies, such as: Python, Scala, Spark, Hadoop, Clojure, Git, Flink, Elasticsearch, SQL, scikit-learn, TensorFlow, and DNB (databricks); visualization tools: Tableau and PowerBI\\nA good understanding of and adherence to data security standards\\nA Bachelor’s in computer science or similar disciplines\\n\\nInspiring the whole Mars business to adopt data driven decision-making by developing advanced analytics methods using Machine Learning/AI is huge in your role. How? By mining vast amounts of data from company databases for insights that will help solve business problems and ultimately make Mars more profitable.\\n\\n#LI-SA1\\n\\nThis is an exciting time at Mars. We’re using digital, data and user insights to transform our business by finding answers to problems that we’ve often never asked ourselves before. From joining the dots to improve our data ecosystem, to streamlining the efficiency and automation of our supply chain and quality operations, we’re already seeing some brilliant results. In fact, we’ve built so much momentum that we’re now looking for industry leaders in Business Translation, Data Science and Data Engineering with different and complementary skills to influence how we operate and grow beyond anything we’ve achieved before. Join us, and discover a company set up to develop your capabilities and ambitions and a group of colleagues ready to support and inspire you. Working together, we’ll create a better world for our planet, our communities and our pets.\\n\\nMars is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "126  7 years development experience in one of the Big Data Technologies Hadoop eco system Pig Hive HBASE Spark Scala KafkaCross trained in Talend is added advantageDW basic conceptsUnix scripting is added advantageTo be proficient in designing efficient and robust ETL workflowsTo be able to work with cloud computing environmentsGather and process raw data at scale including writing scripts web scraping calling APIs write SQL queries etcSecondary skillset Knowledge on JIRA Github Jenkins Zena Healthcare Claims KnowledgeExperience in Agile Development methodologiesExcellent communication skills and team dynamics. Should have good hands on experience in Hadoop ecosystem like Spark, Scala, Hive, Pig, Sqoop and HDFS.Job Types: Full-time, ContractContract Length:More than 1 yearContract Renewal:LikelyFull Time Opportunity:YesWork Location:One location                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "127  We are:\\n\\nApplied Intelligence, the people who love using data to tell a story. We’re also the world’s largest team of data scientists, data engineers, and experts in machine learning and AI. A great day for us? Solving big problems using the latest tech, serious brain power, and deep knowledge of just about every industry. We believe a mix of data, analytics, automation, and responsible AI can do almost anything—spark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services. Want to join our crew of sharp analytical minds? Visit us here to find out more about Applied Intelligence.\\n\\n\\nYou are:\\n\\nAn expert engineer with an eye for AI. You want to change how the world works and lives by taking AI out of the lab and into everyday life.\\n\\n\\nThe work:\\n\\nYou’ll be part of a team with incredible end-to-end digital transformation capabilities that shares your passion for digital technology and takes pride in making a tangible difference. If you want to contribute on an incredible array of the biggest and most complex projects in the digital space, consider a career with Accenture Digital.\\n\\nHere’s what you need:\\nMinimum 2+ years of experience in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments using Spark, pySpark, SparkSQL, with Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)\\nMinimum 1 year of designing and building performant data tiers (or refactoring existing ones), that supports scaled AI and Analytics, using different Cloud native data stores on AWS (Kinesis, S3. GLUE, DynamoDB etc.) or Azure (HDInsights, AzureData Factory) or GCP (DataProc, PubSub, BigQuery) as well as using NoSQL and Graph Stores.\\nMinimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies\\nMinimum 1-year performance engineering, profiling, debugging very large big data and ML production solutions on Spark and native Cloud technologies\\nBonus points if:\\n\\nMinimum 6 months of experience in implementation with Databricks.\\nMinimum 1 year of designing and building secured and governed Big Data ETL pipelines, using Talend or Informatica technologies; for data curation and analysis of large le production deployed solutions.\\nExperience implementing smart data preparation tools such as Palate, Trifacta, Tamr for enhancing analytics solutions.\\nMinimum 1 year of building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions\\n\\n\\nImportant information\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.\\n\\nAccenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "128  Solstice is an innovation and emerging technology firm that helps Fortune 500 companies seize new opportunities through world-changing digital solutions. As strategists and consultants, we help organizations evolve their digital strategy to solve mission-critical problems. As designers and developers, we build incredible hardware and software solutions that transcend a standalone product and transform an organization's relationship with its customers.\\n\\nSolstice is looking to hire a Data Engineer to join our growing capabilities team. If you are innovative, passionate about data and AI technologies, and looks to continually learn and enjoys sharing expertise, read on!\\n\\nAbout you\\n\\n\\nYou have a strong passion for building innovative and intelligent solutions around data\\nExperience with designing data models and ETL.\\nExperience in working with message queuing, stream processing, and highly scalable big data stores.\\nExperience with big data tools like Google BigQuery, Hadoop, Spark, Kafka, Elasticsearch etc.\\nExperience with relational SQL and NoSQL databases such as Postgres and Cassandra.\\nExperience with data pipeline and workflow management tools (any in particular?)\\nExperience with stream-processing systems such as Storm and Spark-Streaming\\nStrong background in Python, Java and/or .NET, knowledge with Kotlin and Scala is a huge plus\\nFamiliar with Microservice design patterns including Serverless and BFF\\nExperience designing, building, integrating and testing with RESTful APIs\\nExperience in developing and implementing scripts for database maintenance, monitoring, and performance tuning to be applied across the business\\nYou have the ability to effectively communicate technical topics to product owners, stakeholders and other business team members\\nExperience with data visualization and reporting tools like Looker, Tableau or PowerBi\\nExperience with cloud technologies such as Google Cloud Platform (GCP) or Amazon Web Services (AWS)\\nStrong verbal and written communication is a must\\nExperience working in an Agile Scrum development environment, or in a consulting capacity is a plus\\nExperience in Machine Learning is a plus\\n\\nWhat You Will be Doing\\n\\n\\nDesigning, Migrating, Building, and Testing large scale data processing architectures\\nBuilding enterprise applications on the cloud and technologies such as Google Cloud, BigQuery, AutoML, Google Data Studio\\nHelping clients implement ways to improve data reliability, efficiency, and quality, and build intelligent solutions leveraging data\\nWorking with designers to help visualize data to provide insights to end-users\\nPerforming ad-hoc analyses of data stored in the business's MySQL/MS SQL databases and writing SQL scripts, stored procedures, functions, and views\\nInterfacing with our clients and providing technical recommendations\\nHelping evaluate emerging cross-platform frameworks and enterprise application platforms\\nBridging the gap between elegant front end design and existing enterprise back end architectures\\nWorking with experienced data engineers, data scientists, and data architects to foster your experience and growth\\n\\nWe welcome Solsties to show up as their full selves everyday. Because this is so important to us, Solstice is proud to be an equal opportunity employer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "[129 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_Chicago.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
