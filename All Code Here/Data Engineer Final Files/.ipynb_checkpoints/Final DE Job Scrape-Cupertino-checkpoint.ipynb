{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Cupertino%2C+CA&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "      <td>Redwood City</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.\\n\\nSenior Data Engineer\\n\\nPosition Summary\\nAt Auction.com, we are embarking on a journey to transform the real estate market with technological innovations. A critical prerequisite for this transformation is a robust data infrastructure. As a senior data engineer, you will help us design and implement our big data environment that is real-time, stable and scalable. You will work with a talented data engineering team to improve our data processing pipeline and developing new capabilities to support mission-critical initiatives. You will mentor junior engineers and help evaluating new technology along the way. You impact will be felt across the team as well as the entire Auction.com organization.\\n\\nResponsibilities/Duties\\n\\nMake major contribution to the implementation of our real time big data initiative\\nBuild and automate productized data processing pipelines in AWS big data platform\\nHelp improve our development process and standards through mentoring and leading-by-example\\nHelp define and implement data ingestion contracts\\nCreate data environment to support our data analytics, reporting and data science teams\\n\\nKnowledge, Skills and Abilities\\n\\nIn-depth understanding of modern big data technology, including Hadoop and Spark\\nKnowledge of real time data streaming and aggregation architectural patterns and practice\\nProficient in programming languages such as Python, Scala and Java\\nFamiliarity with NoSQL as well as SQL databases\\nData modeling and machine learning skill is a plus\\n\\nEducation/Experience\\n\\nBachelor's Degree in computer science, data science or related fields\\nFamiliarity with agile developmental process\\nPrevious experience developing data product required\\nHands-on experience with real time data streaming, aggregation and presentation strongly preferred\\nPrevious experience with production ETL pipeline development required\\nAt least a years’ experience with AWS cloud or another cloud platform\\n\\nTo all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelors or Masters in Computer Science, Engineering, or a related quantitative field.\\n3+ years of experience with building scalable and reliable date pipelines using technologies like Spark, AWS EMR, Kafka, etc.\\n3+ years of production coding experience with at least one general software development programming language (Java, Scala), one data programming language (Python, R), and scripting languages (Unix shell) as well as solid experience with git.\\nExpertise with relational databases and experience with schema design and dimensional data modeling.\\nMastery of SQL (writing complex, high performance queries in Oracle or MSSQL); experience with distributed querying (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).\\nWorking experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)\\nWorking experience with AWS cloud ecosystem.\\nExcellent communication skills in written and verbal forms, and an ability to communicate complex issues to a range of audience (management, peers, clients).\\nStrong attention to detail while excellent time management and prioritization in multitasking.\\nHighly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Become the subject matter expert on our data and its capabilities. Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.\\nDesign and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.\\nAutomate data processing using workflows tools to schedule and manage dependency of various data pipelines.\\nWork directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.\\nRecommend ways to improve data reliability, efficiency, and quality.\\nAssist eHealth’s data architect with logical and physical data model designs and documentation.\\nWork with data infrastructure team to triage issues and support issue resolution.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Get your career started at eHealth\\neHealthInsurance has many exciting career opportunities in a number of locations, across various functions. Come join us today!\\n\\nAt eHealth, we are passionate about solving our nation's toughest problems to bring more suitable, accessible, and affordable health insurance to Americans. We are seeking a talented data engineer to join our growing data team, which is already making a valuable impact on the entire company. This person will help us develop cutting-edge data tools and pipelines to drive better and faster decision making within our company and to better serve our customers. This is a fast-paced, collaborative, and iterative environment requiring quick learning, agility, and flexibility.\\n\\n\\nResponsibilities:\\nBecome the subject matter expert on our data and its capabilities. Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.\\nDesign and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.\\nAutomate data processing using workflows tools to schedule and manage dependency of various data pipelines.\\nWork directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.\\nRecommend ways to improve data reliability, efficiency, and quality.\\nAssist eHealth’s data architect with logical and physical data model designs and documentation.\\nWork with data infrastructure team to triage issues and support issue resolution.\\nMinimum Qualifications:\\nBachelors or Masters in Computer Science, Engineering, or a related quantitative field.\\n3+ years of experience with building scalable and reliable date pipelines using technologies like Spark, AWS EMR, Kafka, etc.\\n3+ years of production coding experience with at least one general software development programming language (Java, Scala), one data programming language (Python, R), and scripting languages (Unix shell) as well as solid experience with git.\\nExpertise with relational databases and experience with schema design and dimensional data modeling.\\nMastery of SQL (writing complex, high performance queries in Oracle or MSSQL); experience with distributed querying (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).\\nWorking experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)\\nWorking experience with AWS cloud ecosystem.\\nExcellent communication skills in written and verbal forms, and an ability to communicate complex issues to a range of audience (management, peers, clients).\\nStrong attention to detail while excellent time management and prioritization in multitasking.\\nHighly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.\\nNice to Have:\\nWorking experience with implementing scalable models using various statistics and machine learning toolkits (Pandas, SciPy, Scikit-learn, MLlib, Spark ML, Tensorflow, Keras, etc.).\\nStrong experience in designing and implementing data APIs.\\nProduct familiarity with Adobe Analytics, Cisco systems, Snowflake, or Informatica.\\nFamiliarity with workflow management tools (Airflow).\\nWorking experience with data warehousing.\\nAbility to create beautiful data visualizations using D3, Tableau, or similar tools.\\nWorking experience with large healthcare related datasets, including EHRs, medical claims data, and health population surveys. Experience in building healthcare data pipelines would be a big plus.\\nKnowledge of healthcare insurance industry, products, systems, business strategies, and products.\\nExperience working with call center operations.\\neHealth is an Equal Employment Opportunity employer. It is our policy to provide equal opportunity to all employees and applicants and to prohibit any discrimination because of race, color, religion, sex, national origin, age, marital status, sexual orientation, genetic information, disability, protected veteran status, or any other consideration made unlawful by applicable federal, state or local laws. The foundation of these policies is our commitment to treat everyone fairly and equally and to have a bias-free work environment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manager</td>\n",
       "      <td>Fremont, CA</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Role : Tableau + Sales Analytics\\nJD : 65533 Dashboards: Build new dashboards and make adjustments to existing dashboards, based on requirements received from stakeholders and Sales Ops Analysts on the team.\\n\\n65533 Data Models: Deliver requirements to Business Technology team to incorporate new fields, objects, and business views into the enterprise data warehouse. Document logic used to build all business views for future reference. Analyze any upcoming systems changes, determine impact on the data warehouse and dashboards, and manage that transition seamlessly.\\n65533 Data Analysis: Perform data analysis and/or gap analysis to ensure it can support Dashboard solutions\\n65533 Data Inputs: Own and maintain nonsystems data inputs, such as Excel files and Google Sheets. Optimize and automate updates where possible.\\n65533 Troubleshoot: Be the first stop for troubleshooting and resolving access or data quality issues reported by users. Help Sales Ops Analysts troubleshoot issues in dashboards they are building.\\n65533 Optimization: Optimize performance (load time) of dashboards using best practices.\\n\\nQualifications:\\n65533 Bachelor65533s degree in Information Systems, Computer Science, or related field of study.\\n65533 5+ years professional experience as a Business Intelligence Analyst, Business Intelligence Engineer, Data Engineer, or related roles.\\n65533 Extensive experience working in Tableau Desktop and publishing to Tableau Server required.\\n65533 Strong experience in SQL, data modeling, and data preparation required. Experience with Alteryx preferred.\\n65533 Experience with Sales Operations analytics preferred. Experience with Salesforce strongly preferred.\\n65533 Excellent problem solving, analytical, and project management skills.\\n65533 Strong interpersonal skills.\\n65533 Takes initiative and tackles challenges with enthusiasm.\\n65533 Ability to multitask and adapt quickly in a changing environment.\\n\\n\\nPrimary Location: US-CA-Fremont\\nSchedule: Full Time\\nJob Type: Experienced\\nTravel: No\\nJob Posting: 04/10/2019, 1:14:06 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Newark, CA 94560</td>\n",
       "      <td>Newark</td>\n",
       "      <td>CA</td>\n",
       "      <td>94560</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor or Masters in Software Engineering and Computer Science\\n8+ years of experience in design and development of large scale data platforms\\nExpert in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nProficient in Spark development with PySpark or Scala\\nExpert in Data streaming platforms such as Apache Kafka\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python, and/or C++</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Staff Data Engineer, Big Data who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a young and very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.\\nThe Role\\nLead Data Engineer and architect to design, implement a highly scalable system to ingest and process Petabytes of data per day.\\nHands-on design and develop applications for data pipeline and data management.\\nSet processes and policies for data governance and data pipeline\\nArchitect and implement best practices of big data tools such as Spark, Airflow, Kafka, Presto and Cassandra\\nLead and mentor junior Data and BI engineers.\\nSet and define the standards and best practices in data team\\nBe the point of reference for solving challenging technical problem.\\nArchitect and implement Machine Learning Pipelines for Data Science team\\n\\nQualifications\\nBachelor or Masters in Software Engineering and Computer Science\\n8+ years of experience in design and development of large scale data platforms\\nExpert in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nProficient in Spark development with PySpark or Scala\\nExpert in Data streaming platforms such as Apache Kafka\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python, and/or C++\\nExperience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architecture\\nExperienced in security and access management\\nExperience with managing distributed databases like Elasticsearch, Cassandra\\nExperience with Columnar database such as Redshift, Vertica\\nGreat verbal and written communication skills.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Our Mission\\nAt Palo Alto Networks® everything starts and ends with our mission: protecting our way of life in the digital age by preventing successful cyberattacks. It’s not a small goal. It isn’t simple either, but we aren’t in this for the easy answer. As a company with a foundation in challenging the way things are done, we’re looking for innovators with a dedication to best. In return, your career will have a tangible impact – one that's working toward technology that affects every level of society.\\nOur mission doesn’t happen by treading softly – no, it happens by defining an industry. It means building products that haven't been thought of. It means selling products with a solutions mindset. It means supporting the infrastructure of a company that moves at an incredible speed – intentionally – to stay ahead of the world’s next cyberthreat.\\n\\nYour Career\\nOur daily fight with cyber bad guys requires us to collect and analyze a lot of data…. A LOT of data. And, as our customer base continues its rapid growth, we need to look at faster and more robust tools to help us and our customers make the best decisions possible.\\nWith your knowledge of Hadoop and Big Data technologies, you will add your tools-building superpowers to a small team tasked with building out a DevOps automation environment, one that will step up our Business Analytics game and help us protect our customers from cyber intruders.\\nWe offer the chance to be part of an important mission: ending breaches and protecting our way of digital life. If you are a motivated, intelligent, creative, and hardworking individual, then this job is for you!\\n\\nOverview\\nYou will be responsible for leading technical projects to build custom applications and enhance business systems. We are looking for someone with solid project management and organization skills complemented by a strong technical background. You will work with a team of senior level managers and highly technical engineers to lead complex IT projects. The role will collaborate with multiple engineering and business organizations, understand and align with their needs, and take independent end-to-end responsibility for release of new business capabilities to production.\\nYour Impact\\nAs a Big Data Engineer, you will be an integral member of our Big Data and Analytics team responsible for design and development\\nPartner with data analyst, product owners and data scientists, to better understand requirements, finding bottlenecks, resolutions, etc.\\nDesign and develop Big Data solutions both in Cloud &amp; OnPrem\\nDesign and develop different architectural models for our scalable data processing as well as scalable data storage\\nBuild data pipelines and ETL using heterogeneous sources using Dataflow or DataProc\\nBuild data ingestion from various source systems to Hadoop or GCP using Kafka, Flume, Sqoop, Spark Streaming etc.\\nTransform data, using data mapping and data processing in Apache Beam or Spark\\nResponsible to ensure that the platform goes through Continuous Integration (CI) and Continuous Deployment (CD) with DevOps automation\\nExpands and grows data platform capabilities to tackle new data problems and challenges\\nSupports Big Data and batch/real time analytical solutions using groundbreaking technologies like Apache Beam\\nHave the ability to research and assess open source technologies and components to recommend and integrate into the design and implementation\\nWork with development and QA teams to design Ingestion Pipelines, Integration APIs, and provide Hadoop ecosystem services\\nYour Experience\\nDegree in Bachelor of Science in Computer Science or equivalent\\n5+ years of experience with the Hadoop ecosystem and Big Data technologies\\n2+ year of experience in Cloud computing\\nCompetent in writing Scala, Python or Java code.\\nDevelopment experience in Dataflow or DataProc is a Plus\\nAbility to dynamically adapt to conventional big data frameworks and tools with the use-cases required by the project\\nExperience with building stream-processing systems using solutions such as spark-streaming, Storm or Flink etc.\\nExperience in other open-sources like Druid, Elastic Search, Logstash etc. is a plus\\nKnowledge of design strategies for developing scalable, resilient, always-on data lake\\nSome knowledge of agile(scrum) development methodology is a plus\\nStrong development/automation skills\\nExcellent inter-personal and teamwork skills\\nCan-do attitude on problem solving, quality and ability to execute\\nThe Team\\nWorking at a high-tech cybersecurity company within Information Technology is a once in a lifetime opportunity. You’ll be joined with the brightest minds in technology, creating, building, and supporting tools and that enable our global teams on the front line of defense against cyberattacks. We’re joined by one mission – but driven by the impact of that mission and what it means to protect our way of life in the digital age. Join a dynamic and fast-paced team that feels excitement at the prospect of a challenge and feels a thrill at resolving technical gaps that inhibit productivity.\\nOur Commitment\\nWe’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together. To learn more about our dedication to inclusion and innovation, visit our Life at Palo Alto Networks page and our diversity website.\\nPalo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.\\nAdditionally, we are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or an accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.\\n#LI-AH1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Engineer - Data Analytics</td>\n",
       "      <td>Fremont, CA</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Data Engineer to be part of our Applications Engineering team. This person will design, develop, maintain and support our Enterprise Data Warehouse &amp; BI platform within Tesla using various data &amp; BI tools, this position offers unique opportunity to make significant impact to the entire organization in developing data tools and driving data driven culture.\\n\\nResponsibilities:\\nWork in a time constrained environment to analyze, design, develop and deliver Enterprise Data Warehouse solutions for Tesla’s Sales, Delivery and Logistics Teams\\nCreate ETL pipelines using Python, Airflow\\nCreate real time data streaming and processing using Open source technologies like Kafka , Spark etc\\nWork on creating data pipelines to maintain Datalake in AWS or Azure Cloud\\nWork with systems that handle sensitive data with strict SOX controls and change management processes\\nDevelop collaborative relationships with key business sponsors and IT resources for the efficient resolution of work requests.\\nProvide timely and accurate estimates for newly proposed functionality enhancements\\ncritical situation\\nCommunicate technical and business topics, as appropriate, in a 360 degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary.\\nDevelop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs.\\nUtilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices.\\n\\nQualifications:\\nMinimum Qualifications:\\n3+ years of experience in Cloud Technologies like AWS or Azure\\n3+ years of experience in creating data pipelines using Python\\n3+ years of experience in Data Modelling\\nMust have strong experience in Data Warehouse ETL design and development, methodologies, tools, processes and best practices\\nStrong experience in stellar dashboards and reports creation for C-level executives\\n\\nPreferred Qualifications:\\n3+ years of development experience in Open Source technologies like Python, Java\\nExperience in Big Data processing using Apache Hadoop/Spark ecosystem applications like Hadoop, Hive, Spark, Kafka and HDFS preferable\\nExcellent query writing skill and communication skills\\nFamiliarity with common API’s: REST, SOAP\\nApply\\nTesla participates in the E-Verify Program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Intern - IT Big Data Engineer</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description\\nOur Mission\\nAt Palo Alto Networks® everything starts and ends with our mission:\\n\\nBeing the cybersecurity partner of choice, protecting our digital way of life.\\n\\nWe have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.\\nYour Career\\nOur AI &amp; Analytics group is responsible for working with various business owners/stakeholders from Sales, Marketing, GCS, Infosec, Operations, and Finance to solve difficult business problems which will have a direct impact on the metrics defined to showcase the progress of Palo Alto Networks.\\nWe leverage the latest technologies from Cloud &amp; Big Data ecosystem to improve business outcomes and create through prototyping, Proof-of-Concept projects, and application development. We are seeking an intern for our team who will work closely with other Principal Engineers, Product Managers, Data Engineers &amp; Data Scientists. In this role, you will work in the AI &amp; A team to design, and deliver our next generation data services.\\nOur Summer Internship Program from May-August or June-September provides you:\\n1:1 mentorship\\nFun and engaging events that inspire your intellectual curiosity\\nOpportunities to expand your knowledge and work on challenging projects\\nConnections to other interns, recent grads, and employees across the company as well as our leaders\\nYour Impact\\nBuild Internal or external customer facing applications in Cloud &amp; Hadoop environments.\\nDevelop low latency, high throughput data pipelines in Hadoop and Cloud using leading edge technologies like Spark, Kafka, Beam, and other cloud specific technologies.\\nPartner with data analysts, product owners to gather business and system requirements\\nBuild and Integrate applications with other external applications\\nDevelop rich front-end experiences using technologies such as Tableau, Angular JS, JavaScript, Node.js, React framework, etc.\\nAssist with design, development of features to perform predictive analytics on big data sets\\nYour Experience\\nCurrently working towards a B.A./B. S/B.E./MS in a technical area (Computer Science, Engineering, etc.)\\nBasic understanding of software design and development\\nBasic understanding of developing cloud-based services (GCP, AWS, etc.)\\nGood working knowledge of at least one contemporary programming language such as Python, Java, Scala, and JavaScript.\\nBasic understanding of data analysis methods and approaches\\nExcellent oral and written communication skills\\nRequirements – All applicants must be pursuing a 4-year Undergraduate Degree, a 2-year Master’s Degree or a Doctorate degree and returning to school in the fall. You must have authorization to work within the United States.\\n\\n*Please note that we will not sponsor applicants for work visas for this position*\\nThe Team\\nOur Data engineering team is responsible for deep diving into data, figuring out issues with the quality of data and coming up with advanced techniques to bring insights out of the data to solve business problems.\\nOur Commitment\\nWe’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.\\nWe are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.\\nPalo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analytics Engineer Intern</td>\n",
       "      <td>San Jose, CA 95134</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95134</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Analytics Engineer Intern\\nSummer 2020 internship applications are open September 2019 through January 2020. Samsung Semiconductor summer internships start in May/June 2020.\\nJoin us for a unique 12-14 week paid internship that offers personal and professional development. You’ll work with the teams that create new computing system architectures needed to support emerging machine learning applications, internet of things (IoT) and edge computing that benefit millions of users. This program will give you the opportunity work on complex solutions to that address some of the world’s most complex technological challenges.\\nSamsung Semiconductor, Inc. is a world leader in Memory and Storage technologies. We are currently looking for Data Analytics Engineer (Intern) to join our team in San Jose, CA. The Data Engineer Intern will contribute to memory and storage system research in the Memory Solutions Lab. He or she will join a team of experts in researching and developing innovative memory and storage system solutions that utilize existing and emerging technologies to add substantial value to storage systems. The ideal candidate must have a strong understanding of storage technologies including file systems, Linux I/O Stack, Linux performance profiling and computer architecture.\\n\\nJOB RESPONSIBILITIES\\nImplement software and design machine learning or deep learning models using existing big data/AI frameworks to analyze collected server telemetry data for various predictions and insights\\nPropose possible approaches and methods to solve complex analytics problems related to storage workloads and management\\nAnalyze large sets of collected server telemetry data, create machine learning models and generate verifiable results\\nWork with team members to contribute towards prototyping efforts.\\nCreate new and useful IP, publish at conferences, and generate whitepapers.\\nREQUIRED SKILLS\\nPursuing an MS or PhD in Computer Science, Computer Engineering or related field, with focus on data modeling and data science.\\nGood knowledge of popular Big Data frameworks\\nResearch and development experience with Spark, Presto, Hive, Hadoop, and NoSQL Databases\\nExperience with deploying and debugging applications across server clusters\\nPrior experience with storage performance analysis and optimization will be a big plus.\\nTrack record of innovation and creativity in problem solving\\nMust be highly motivated with excellent verbal and written communication skills.\\nStrong background in C/C++/Python/Java/Scala.\\nComfortable working in a multinational environment and understands how to leverage cultural diversity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Do you want to be part of a growing data team with a huge impact on products used by millions of people around the world? Niantic, the developer of Ingress, Pokemon Go, and Harry Potter: Wizards Unite, is searching for a Senior Data Engineer. You'll lead efforts to scale and organize the data infrastructure, working closely with Machine Learning scientists and engineers to craft a highly scalable data pipeline capable of data-enabling Niantic’s next generation of augmented reality games. You'll form the cornerstone of a flexible, driven team to build game experiences which enrich millions of lives.\\nResponsibilities\\nYou'll design and implement elegant data models (tables, partitioning, dependencies ...) for use with Niantic's ML pipelines and analytics.\\nYou'll architect the technical stack for storing and processing large volumes of Niantic’s data.\\nYou'll craft ETL pipelines to ingest and structure data from diverse sources.\\nYou'll mentor and provide technical guidance to junior data engineers, with many opportunities to demonstrate leadership.\\nYou'll work hand-in-hand with the ML Science and ML Engineering teams to provide datasets which can be used in production ML systems.\\nQualifications\\nYou have a BS, MS, or PhD in Computer Science, or a related technical field.\\nYou have 4+ years of experience developing and deploying data pipelines.\\nYou've deployed large-scale data extraction pipelines which feed into ML models.\\nYou have extensive knowledge of large-scale data processing concepts and technologies.\\nYou have experience with cloud deployment of pipelines and orchestration tools (Airflow, Composer).\\nYou possess a deep knowledge of data storage and analysis technologies such as Hive, Presto, or Spark, and are comfortable with their trade-offs and optimizations.\\nYou know your way around Java or Scala, Python and SQL.\\nPlus If...\\nYou have knowledge of the Google data stack (Dataflow, Dataproc, BigTable, BigQuery, etc.).\\nYou have experience with design of data models which serve multiple applications underlying the same model (common schemas across multiple games).\\nYou have knowledge of ML models for classification, regression, and clustering.\\nYou have experience with deploying ETL pipelines on large user bases (10s of millions of users).\\nJoin the Niantic team!\\nNiantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.\\nOriginally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, record-breaking AR game Pokémon GO, and recently released third title, Harry Potter: Wizards Unite. .\\nNiantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.\\nWe're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, London, Tokyo, Hamburg, and Zurich.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data engineer</td>\n",
       "      <td>Fremont, CA</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Qualification:\\nMaster's Degree in Statistical Analytics, Data Science, or Bachelor's Degree in computer science engineering will be considered with at least three - five years of applicable work experience\\nPreferred Proficiency:\\n\\nWeb development experience (AngularJS, D3).\\nExperience in a statistical programming language like R or Python; applied machine learning techniques including dimensionality reduction strategies, supervised/unsupervised classification and natural language processing frameworks.\\nExperience in at least one data visualization tools (e.g. Tableau, QlikView) and data warehousing tools (e.g. Informatica) is preferred\\nHuge Advantage:\\nBuilding and scaling Machine Learning frameworks\\nHadoop (Hive, Spark, UDF's)\\nDefinite Plus:\\n\\nWeb development experience (AngularJS, D3).\\nExperience:\\n5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.\\n5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.\\n2+ years of experience in scripting languages like Python etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Senior Data Engineer, Apple Media Products</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Nov 1, 2018\\nRole Number: 200007033\\nApple is seeking a highly skilled data engineer to join the Data Engineering team within Apple Media Products. AMP (home to Apple Music, App Store, iTunes and more) has some of the most compelling data in the world. We are looking for a talented engineer who is motivated by challenging problems and well versed with big data technologies. This is a unique opportunity to join a focused team and work collaboratively with other groups to make a significant impact.\\nKey Qualifications\\nExperience in high level programming languages such as Java, Scala, or Python.\\nProficiency with databases and SQL is required.\\nProficiency in data processing using technologies like Spark Streaming, Spark SQL, or Map/Reduce.\\nExpertise in Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig.\\nExpertise in developing big data pipelines using technologies like Kafka, Flume, or Storm.\\nExperience with large scale data warehousing, mining or analytic systems.\\nAbility to work with analysts to gather requirements and translate them into data engineering tasks\\nAptitude to independently learn new technologies.\\nDescription\\nAs a member of the Data Engineering team, you will have significant responsibility and influence in shaping its future direction. This role is inherently cross-functional and the ideal candidate will work across disciplines. We are looking for someone with a love for data and ability to iterate quickly on all stages of data pipeline. This position involves working on a small team to develop large scale data pipelines and analytical solutions using BigData technologies. Successful candidates will have strong engineering skills and communication, as well as, a belief that data driven processes lead to great products. You will need to have a passion for quality and an ability to understand complex systems.\\nEducation &amp; Experience\\nBachelor's degree or equivalent work experience in Engineering, Computer Science, Business Information Systems.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Santa Clara, CA 95050</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>95050</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDesign, develop, and maintain a Big Data framework and ETL (Extract, Transform, and Load) processes for the collection, organization, analysis, and visualization of homogenous and non-homogenous smart grid data from sensors, meters, relays, PMUs, SCADA, and other head-end automation and enterprise systems\\nIntegrate the Big Data platform with the broader architectures and enterprise software ecosystem across the company and industry\\nSupport domain data scientists in writing complex queries and developing machine learning models, AI applications, and visual story telling\\nOwn and manage the end-to-end data management pipeline and address performance enhancement issues\\nAssist the engineering team with validation and verification of end-to-end grid analytics system performance.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree with 3+ years of relevant experience\\nMaster’s degree with 2+ years of relevant experience\\n</td>\n",
       "      <td>Description:\\nSentient Energy is looking for a Big Data Domain Engineer/Scientist to fulfill a full-time position in the Data Science and Analytics team. This team is responsible for characterizing, classifying, and predicting the evolving behavior and performance of smart electric utility systems through synergistic utilization of smart grid data coupled with state-of-the-art Analytics and AI technologies. With the projected increase in utility IoT deployments, the amount of operational and non-operational data is exploding. Deriving business value from these investments requires both small data and big data applications to effectively ingest, store, and analyze the data for both edge and cloud deployment. Real-time and batch analytics need to be developed to realize the full value of IoT investments. Our efforts help electric utilities streamline operations, enhance system reliability, integrate more renewables, and increase system safety and resiliency.\\n\\nThis position offers a unique hands-on opportunity to apply the latest database and big data technologies in the rapidly developing and highly visible electric utility space within the Green Energy sector. You will be handling and dealing with real utility data and help solve some of the long-standing industry challenges as we face an exponential growth in digitalization and grid modernization. The innovation and creativity with which we develop and implement utility solutions are among the distinctions that make Sentient Energy the leader in Distribution Network sensing and analytics.\\n\\nResponsibilities include:\\n\\nDesign, develop, and maintain a Big Data framework and ETL (Extract, Transform, and Load) processes for the collection, organization, analysis, and visualization of homogenous and non-homogenous smart grid data from sensors, meters, relays, PMUs, SCADA, and other head-end automation and enterprise systems\\nIntegrate the Big Data platform with the broader architectures and enterprise software ecosystem across the company and industry\\nSupport domain data scientists in writing complex queries and developing machine learning models, AI applications, and visual story telling\\nOwn and manage the end-to-end data management pipeline and address performance enhancement issues\\nAssist the engineering team with validation and verification of end-to-end grid analytics system performance.\\n\\nRequirements:\\nEducation and Experience\\nAdvanced degree in computer science, electrical engineering, and/or a closely related analytical field\\n\\nBachelor’s degree with 3+ years of relevant experience\\nMaster’s degree with 2+ years of relevant experience\\n\\nRequirements\\n\\nDeep understanding and practice of distributed computing and storage principles\\nHands-on proficiency with distributed storage and processing solutions and technologies for batch and streaming analytics e.g. Hadoop, MapReduce, HDFS, Spark etc...\\nComfortable with conventional open source and commercial RDBMS (e.g. MySQL, MS SQL Server)\\nExperience with data processing and visualization in Python, R, Matlab, etc.\\nExperience building stream-processing systems\\nHands-on experience integrating data from multiple heterogenous data sources\\nGood knowledge of Big Data querying tools, such as Pig, Hive, and Impala\\nIntegration and management of distributed clusters including all services\\nKnowledge of various ETL techniques and frameworks\\nGeneral familiarity with IoT and data science tools, processes, and maturity models\\nAgility to learn new technologies and adapt workflows accordingly\\nAbility to drive fast-paced projects forward independently and in a team environment\\nGood communications skills in English\\nPersistent, self-motivated, flexible\\nStrong sense of ownership and accountability\\nExcellent organizational and time-management skills\\nDemonstrated self-starter with an ability to work effectively in a matrix organization\\nCompliance to Company Policies and Procedures, and its Core Values\\n\\nNice to Have\\n\\nGeneral understanding of trends in smart grids, IoT applications, and utility analytics\\nExperience with Python libraries for database management and advanced visualization\\nStatistics and machine learning background\\n\\nCompensation\\nCompetitive salary and incentive bonuses, generous options grants, paid time off (vacation, floating holidays), paid holidays, medical, dental, vision and life insurance, 401(k) Plan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Hiretual is an AI-powered sourcing platform, and has been recognized as one of the best recruiting tools on the market (\\nhttps://goo.gl/ERiQgY [https://goo.gl/ERiQgY]). During 2018, Hiretual achieves 500% growth with minimal focus on sales to date (\\nhttps://goo.gl/ZTA1D9 [https://goo.gl/ZTA1D9]).\\n\\nAs a data engineer engineer, you will join the core engineering team to build scalable and robust data engine towards an AI and data-driven recruiting SaaS. You will be working with top AI researchers and infrastructure gurus to explore unlimited career space.\\n\\nThe core technical skills you should have:\\n\\nStrong computer science fundamentals: algorithms, data structures, and object-oriented programming\\nStrong coding capability with Python\\nmust have 2+ years of experience in data processing pipeline, including data crawling, cleaning, processing, ETL\\nProficient in working variant databases: MySQL, Redis, Cassandra, Elasticsearch, graph databases.\\nProficient with big data processing frameworks: Spark, Hadoop, Hive, Kafka, EMR\\nWriting scalable REST APIs for web services\\n\\nBenefits\\n\\nUnlimited growth/promotion space\\nCompetitive salary and options\\n401k matching program\\nFree and nice food\\nComprehensive medical, dental, and life insurance\\nPTO policy\\nCommuter benefits\\nFun, collaborative, and energetic team environment with nice office environment\\nFun events for family!\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Engineer - Growth &amp; SEO</td>\n",
       "      <td>Palo Alto, CA 94301</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>94301</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Houzz\\nHouzz is the leading home renovation and design platform in the world. We have a highly engaged community of more than 40 million unique monthly users who leverage our technologies to find inspiration for their next home project, connect with over 2.3 million home design and remodeling professionals and discover products in the Houzz Shop.\\n\\nAbout the Role\\nHouzz is looking for a world-class Data Engineer to join analytics on the Consumer Growth, Experience and SEO team. Our mission is to assist Houzz product growth and engagement initiatives across our desktop site and mobile experience with data driven insights. Our team is working on discovering information hidden in vast amounts of data to make intelligent business decisions and to deliver better products. Critical projects will focus on data-driven insights to guide product development and business direction.\\n\\nWhat You'll Do\\nCollaborate with analytics, data infrastructure, engineering and product teams to deliver reliable data used to make data-driven decisions\\nArchitect and build core datasets\\nImplement efficient ETL processes\\nDesign and create dashboards\\nPropose changes to the logging structures or supporting new kinds of data analysis\\nTranslate business requirements into projects and prioritize based on impact\\nAt a Minimum, We'd Like You to Have\\nA strong understanding of algorithms, web services and data management\\nAdvanced SQL skills with the ability to write complex and performant queries and data pipelines\\nProficiency in data architecture or data warehouse design patterns\\nProficiency in efficient batch and streaming ETL design\\nProficiency in Python or Java with the willingness to learn the other\\nExperience with Big Data technologies such as Hadoop, Spark, Hive and Presto or Impala\\nExperience with workflow management framework, such as Airflow or Luigi\\nExcellent communication, interpersonal and cross-functional skills\\nA passion for and track record of leveraging data in user product experience, preferably also in a mobile device environment\\nA B.S. or M.S. in Computer Science or a related field\\n2+ years of relevant work experience\\nIdeally, You'll Also Have\\nExperience working with session, clickstream and AB testing data\\nExperience supporting analytics teams studying user behavior and product metrics\\nExperience building analytics frameworks and tools with broad adoption\\nExperience with visualization tools such as Tableau or Superset\\n__________________\\n\\nBe Who You Are and Do What You Love at Houzz\\n\\nWe’re a Family\\nAt Houzz, we strive to create and foster a strong family environment in our workplace. We collaborate to accomplish our goals, always working as a team. We aim to build a culture of inclusion — celebrating and leveraging our differences for the betterment of one another, our products and our community.\\nHouzz team members come from many backgrounds and bring diverse experiences to the company. We take pride in making each person feel at home.\\n\\nWe Build the Future\\nJoin Houzz in revolutionizing the home remodeling and design industry and have an impact on the more than 40 million homeowners who use our platform every month and the 2.3 million-plus home professionals around the world who are active on the site. Houzz has been named one of the most innovative companies in the world by CNBC and others, and is backed by top venture capitalists. At Houzz, you can help drive the future of an industry worth $1.2 trillion in the U.S. and Europe alone.\\n\\nWe Make Things Happen\\nOur team members play a key role in guiding the direction of our company and are able to work across multiple groups to implement fresh ideas that allow Houzz to be the industry leader. If you are interested in applying your passion to create products that will transform the lives of millions of people who are designing, remodeling and decorating their homes, welcome to Houzz.\\n\\nBenefits and PerksCompetitive salaryFlexible paid time offCommuter benefitsMedical, dental, vision and pet insuranceEmployee assistance program401k retirement savings planMaternity/paternity leave programFlexible spending accountsHealthy at Houzz programCatered meals, fully stocked kitchens and much more!\\n\\nHouzz is an Equal Opportunity Employer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Integrated Marketing Analytics Manager</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Department: Marketing &amp; Customer Insights (MCI)\\nTitle: Integrated Marketing Analytics Manager\\nPosition Role/Description:\\nThe Marketing &amp; Customer Insights (MCI) organization has a dual mandate of providing objective customer research, analysis, and marketing effectiveness measurement while advancing the use of Adobe Marketing Cloud technologies to enable and track customer experiences across surfaces.\\nWithin MCI, the Advanced Analytics team was established to focus on developing deep customer insights to support integrated marketing planning across channels. The group closely partners with Global and Field Marketing, IT, business unit leaders and other corporate functions to enhance understanding of our customers and their digital journey.\\nThe team maintains a highly visible and strategically important role in delivering and facilitating understanding of insights to inform business strategies, and to track the performance of marketing specific activities against expectations. The work the team delivers is informed by the business needs for strategic customer understanding, and may range from media investment performance analysis, deep customer journey investigations, to customer segmentation or targeting overlays.\\nThe talent mix of the team is part analyst and part data engineer. This role will focus on the data side. We are looking for a self-starter with marketing science experience and a wide range of technical skills to advance our analytics and execution capabilities. This includes advancing our marketing measurement strategy across all of our online and offline marketing channels. You will also develop our next generation of data and analytics stacks to guide our customer insights work.\\nA balance of technical and analytics skills as well as a working knowledge of marketing processes is key. A team-player, collaborative, mindset is essential.\\nResponsibilities:\\n· Ability to structure problems into data and analytics plan and execute.\\n· Hands-on creation of time-series models\\n· Ability to partner with a cross-functional team of technical and analytical partners\\n· Possible engagement with Adobe Marketing Cloud teams to evolve capabilities within the products.\\n· An understanding of effective visualization techniques for complex concepts or a background in visualizing “big data” use cases.\\n· Partner with media teams, campaign strategy teams, marketing and web analysts to gather inputs to inform business problems and analytical approaches.\\n· Provide ad hoc analysis and communicate results and insights with team and executives\\nWhat you need to succeed:\\n· Intellectual curiosity, flexibility, and high attention to detail\\n· Undergraduate degree in a quantitative field preferred\\n· Experience with SQL, R, Python\\n· Project/program management experience\\n· Good intuition with information and data; experience synthesizing large data sets to generate insights\\n· Outstanding communication skills to provide actionable reporting via written and in-person presentations. Must be able to clearly communicate results and actionable recommendations to constituencies at various levels of management\\n· Strong interpersonal skills and the ability to work as part of a diverse team and build cooperative, productive relationships with colleagues and stakeholders around the world\\nDesirable Skills:\\n· Understanding in one or more marketing channels (e.g. web, mobile, display, search)\\n· Experience in machine learning tools and techniques\\n· Experience developing and working on larger scale analytics / big data implementations\\n· Track record of consistent delivery and execution\\nAt Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.\\nIf you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.\\nAdobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(Sr.) Manager, Data Management</td>\n",
       "      <td>San Carlos, CA</td>\n",
       "      <td>San Carlos</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s or Master’s degree in computer science/data processing or equivalent\\n5+ years of experience with Java programming and developing frameworks\\n2+ years’ experience with Hadoop and Spark\\n2+ years’ experience with Amazon EMR/EC2 (or equivalent)\\n2+ years’ experience with Python\\nExperience with Bitbucket and a solid understanding of core concepts with Git\\nFamiliarity with Linux\\nFamiliarity with Jenkins and CI/CD\\nA solid understanding of basic core computer science concepts\\nExperience with AWS technologies such as Aurora, Athena, EMR, Redshift, S3\\nExperience with Postgres and MySql\\nExcellent Organizational and Project Management skills\\nOutstanding communication skills\\nSpark - Machine Learning library experience a plus\\nExperience with Scala, is a plus\\nExperience with Talend Data Integration (Big Data) platform a plus</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nIn data management, data access (Big Data, traditional Data Marts, ...).\\nIn Advanced programming (python, Shell scripting, and Java)\\nWith interactive and batch processing using Spark SQL and spark scripting.\\nIn applied data technologies:\\nHadoop\\nSpark\\nKafka, Spark Streaming\\nPig\\nHive\\nMongoDB\\nOozie\\nEMR\\nLambda\\nSQL\\nCurrent data warehousing concepts (using technologies like Redshift, Spark, Hadoop, web services, etc to support business-driven decisioning)\\nIn data architecture and data assembly\\nIn Data Governance and Data Security</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\nABOUT OPORTUN\\n\\nOportun is a mission-driven, technology-powered provider of inclusive, affordable financial services and a certified Community Development Financial Institution (CDFI).\\nWe seek to serve the 100 million people in the US who are shut out of the financial mainstream because they are credit invisible or are mis-scored because they have limited credit history. By lending money to hardworking, low-to-moderate income individuals, we help them move forward in their lives, demonstrate their creditworthiness, and establish the credit history they need to access new opportunities.\\nSince 2006, we have lent over $6.8 billion through over 3.1 million affordable small dollar loans and have helped over 730,000 people start establishing credit. In recognition of inventive approach, we were recognized by Time Magazine as one of 50 Genius Companies inventing the future.\\n\\nThe Bay Area News Group recognized Oportun as a Top Workplace in 2019. Come and be a part of our community of employees, partners, and customers who are devoted to expanding financial opportunity for millions. When we work together, we can make life better.\\n\\nSUMMARY\\nDo you want to be part of a BIG data transformation journey? Do you love exploring new avenues and pioneer things in the technology space? Do you love designing and implementing business critical data management &amp; engineering solutions using emerging technologies? Do you enjoy solving complex business problems in a fast-paced, collaborative, and iterative delivery environment? If this excites you, then keep reading!\\n\\nWe're seeking a hands-on Data Engineer that can design, code and provide architecture solutions for the team. The right candidate for this role is passionate about technology, can interact with product owners and technical stakeholders, thrives under pressure, and is hyper-focused on delivering exceptional results with good teamwork skills. The candidate will have the opportunity to influence and interact with fellow technologists beyond his team and influence technology partners across the enterprise.\\n\\nEssential Functions:\\nDesign and Develop scalable Big Data solutions across the entire data supply chain.\\nCreate or implement solutions for metadata management.\\nCreate and review technical and user-focused documentation for data solutions (data models, data dictionaries, business glossaries, process and data flows, architecture diagrams, etc.).\\nExtend and enhance the business Data Warehouse and Data Lake\\nSolve for complex data integrations across multiple systems.\\nDesign and execute strategies for real-time data analysis and decisioning.\\nCollaborate with management, business partners, analysts, developers, architects, and engineers to support data quality efforts.\\nWork closely with the Data Science team to improve actionable data\\nBe open and willing to learn new skills!\\nResponsibilities\\nExpertise (can teach/instruct others):\\nIn data management, data access (Big Data, traditional Data Marts, ...).\\nIn Advanced programming (python, Shell scripting, and Java)\\nWith interactive and batch processing using Spark SQL and spark scripting.\\nIn applied data technologies:\\nHadoop\\nSpark\\nKafka, Spark Streaming\\nPig\\nHive\\nMongoDB\\nOozie\\nEMR\\nLambda\\nSQL\\nCurrent data warehousing concepts (using technologies like Redshift, Spark, Hadoop, web services, etc to support business-driven decisioning)\\nIn data architecture and data assembly\\nIn Data Governance and Data Security\\nExperience (requires little direction):\\nwith functional requirements, detailed technical specifications, and test cases for new or modified projects\\nwith and understanding of data sources (e.g., 3rd party RDBMS, MS access, SQL server, Oracle, and MySQL)\\nwith data integration tools (e.g., Talend, SIS, Cascading)\\nwith data manipulation scripting languages\\nwith Business Intelligence, MDM, XML, SOA/WebServices\\nwith executing deliverables using Agile\\nwith Data Science toolsets and technology\\nQualifications\\nBachelor’s or Master’s degree in computer science/data processing or equivalent\\n5+ years of experience with Java programming and developing frameworks\\n2+ years’ experience with Hadoop and Spark\\n2+ years’ experience with Amazon EMR/EC2 (or equivalent)\\n2+ years’ experience with Python\\nExperience with Bitbucket and a solid understanding of core concepts with Git\\nFamiliarity with Linux\\nFamiliarity with Jenkins and CI/CD\\nA solid understanding of basic core computer science concepts\\nExperience with AWS technologies such as Aurora, Athena, EMR, Redshift, S3\\nExperience with Postgres and MySql\\nExcellent Organizational and Project Management skills\\nOutstanding communication skills\\nSpark - Machine Learning library experience a plus\\nExperience with Scala, is a plus\\nExperience with Talend Data Integration (Big Data) platform a plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Software Engineer, Data Infrastructure (SF and Mountain View)</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As one of the first dedicated engineers to data infrastructure, you will build and integrate scalable backend systems, data pipelines, platforms and tools that power our data warehouse. You will also help build the data infrastructure needed for internal and customer-facing analytics needs and our machine learning models, as well as build and maintain systems for managing data clusters, handling security, disaster recovery and replication. In doing so, you will play a meaningful role in scaling our data infrastructure as we grow rapidly.\\n\\nAs Moveworks’ data multiplies during a critical time of hypergrowth, your goal is to provide best-in-class data infrastructure, analytics tools, and data components to scale efficiently. You will design and integrate systems that power processing of unstructured datasets to ultimately finetune our product and achieve fully autonomous resolution of enterprise IT.\\n\\nWhat will you do?\\nYou will design, develop and deploy our data infrastructure in the cloud\\nPromote efficient use of data and analytics within the organization\\nBuild the data infrastructure needed for internal and customer-facing analytics needs\\nYou will build data pipelines for machine learning efforts\\nYou will build and maintain systems for managing data clusters, handling security, disaster recovery and replication\\nBuild analytics tools utilizing the data pipeline to provide actionable insights for our product and engineering team\\nYou will be a data expert and champion data quality efforts across the board\\nYou will ensure that all data components are designed and implemented in compliance with our information security requirements\\nWhat do you bring to the table?\\nYou have 4+ years of experience as a data engineer, ideally with a cloud-based SaaS company\\nYou have hands-on experience building, scaling, and supporting large-scale ETL data infrastructure systems in production\\nYou have hands on experience working with different teams for their data requirements\\nYou have strong familiarity with ETL systems like Kafka, Spark, Storm, Fluentd, Hadoop, Presto, Hive\\nYou have experience with SQL and NoSQL databases like PostgreSQL, MySQL, Cassandra, DynamoDB or HBase, and cloud management systems like AWS, GCE or AzureBS or higher in Computer Science or a related field\\nNice-to-haves:\\nExperience with large-scale machine learning pipelines is a plus\\nWho we are:\\nMoveworks is an AI first company with a singular focus: fully autonomous resolution of all enterprise IT. We are building a state of the art platform that combines natural language understanding, conversational interface, and automation to enable hundreds of millions of knowledge workers get work done faster. Our engineers built foundational systems are companies like Google, Amazon, LinkedIn, and Facebook, and we are now applying our expertise to build a first of its kind enterprise machine learning platform.\\n\\nWhile we currently resolve 15-35% of all IT related requests autonomously at companies like Broadcom and Autodesk, we are just getting started. Our vision is to build a single AI platform where employees can come for help whenever they need anything from their respective companies - ranging from HR related help, facilities, legal, and more. This is all delivered through a conversational interface like Slack, Teams, or Google Chat for a fast, delightful, and frictionless user experience.\\n\\nAs we scale globally, there’s plenty of space for you to grow with us. You’ll be part of a team that thinks in terms of we, not I. Together, we're focused on simplifying life for millions of people around the world in their most pressing hours of the day by giving them an effortless, magical way to resolve issues - all powered by a strong form of AI.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SW Big Data Systems Engineer</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Aug 28, 2019\\nWeekly Hours: 40\\nRole Number: 200038890\\nThe SWE Data Analytics team at Apple collects, processes, and analyzes diagnostics and usage data from Apple devices across the world. Our data is used to generate insights that informs and drives product strategies across all of software and hardware development. We develop batch and streaming analytics solutions using Kafka, Flume, Hadoop, Spark, Jenkins, and other state of the art technologies in a large scale infrastructure.\\nWe are looking for a passionate and results-oriented senior big data engineer to join our team and work on some of the highly visible data projects in software engineering organization. You will be collaborating with data analysts, device engineers and engineering teams. You will drive the development of data pipelines and services with high degree of ownership.\\nKey Qualifications\\nExperience developing large scale distributed computing systems\\nIn-depth knowledge and experience in one or more of the following technologies: Hadoop ecosystem, Kafka, Samza, Flume, HBase, Cassandra, Redshift, Vertica, Spark.\\nDeep understanding of key algorithms and tools for developing high efficiency data processing systems\\nValidated software engineering experience and discipline in design, test, source code management and CI/CD practices\\nExperience in data modeling and developing SQL database solutions\\nProficient in working with Linux or other Posix operating systems, shell scripting, and networking technologies\\nStrong software development, problem-solving and debugging skills with experience in one or more of the following languages: Java, Python, Scala, or Ruby\\nAmbitious, passionate about software development, especially in data technologies, you love working in a fast-paced and dynamic environment\\nYou are deeply organized, detail oriented, and thorough in every undertaking. You are able to multi-task and change focus quickly\\nExcellent interpersonal skills.\\nDescription\\nAs part of a small team of highly skilled data engineers, you will own significant responsibility in crafting, developing and maintaining our large-scale ETL pipelines, storage, and processing services.\\nYou will build self-service analytics tools to help engineering teams derive actionable metrics out of large volumes of raw data.\\nYou will partner with data science and engineering teams and develop algorithms to answer complex questions on usage of our products\\nYou will work closely with the DevOps team and develop monitoring and alerting scripts on various data pipelines and jobs\\nYou will have the opportunity to learn and work on the latest Big Data technologies, lead POCs to demonstrate new ideas and influence the future direction of our technology stack\\nEducation &amp; Experience\\nBachelors in Computer Science or equivalent experience.\\nAdditional Requirements\\nExperience using data storage technologies such as Apache Parquet or Avro\\nExperience in machine learning algorithms is a plus\\nTesting tools and methodologies to test large scale distributed computing systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sr. Data Engineer (DataOps)</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor degree in Computer Science/Engineering or related field, or equivalent professional experience\\n5+ years of software engineering/DevOps/DataOps experience.\\nExperience with combination of AWS Managed Services (such as Data Pipeline, Glue, Athena, Aurora, DynamoDB) is highly preferred.\\nCommand line expertise in AWS\\nExperience with Data Orchestration (Airflow is a big plus)\\nStrong team player - ability to work directly with other developers exchanging ideas, knowledge, experience and thoughts in order to promote Operational Rigor is required.\\nUnderstanding of capacity planning is a plus\\nAbility to drive major implementations are highly preferred\\nExperience with CI/CD tools, such as Jenkins, Chef is a definite plus\\nDeep knowledge and use of command line on Linux/Mac/AWS environments\\nSoftware development experience (in any language, particularly Python and Java) is required\\nExperience as a DevOps Engineer team lead is a plus\\nKnowledge of performance and security test tools is a definite plus</td>\n",
       "      <td>\\nWork collaboratively in an Agile/Scrum team guiding the testing process for both development and other Data Engineering (DataOps) team members.\\nExplore, drive, design and implement solutions for deployment/monitoring automation.\\nWork to solve technical problems and use appropriate technology to drive solutions\\nBuild orchestration ETL and other data solutions (Apps).\\nAutomate deployment of ETL and other data solutions.\\nAutomate monitoring of Data Solutions/ETLS\\nWork closely with Cloud Services, Global DevOps and IT organizations to resolve any Data Platform PaaS issues.\\nHelp optimize costs associated with cloud resources.\\nWork with RTB Operations team to resolve production issues.\\nTake a more proactive approach to preventing defects in a sprint and the entire SDLC\\nProvide technical and leadership discipline to other more junior team members\\nBe an advocate and an example for best practices across teams\\n</td>\n",
       "      <td>\\nBachelor degree in Computer Science/Engineering or related field, or equivalent professional experience\\n5+ years of software engineering/DevOps/DataOps experience.\\nExperience with combination of AWS Managed Services (such as Data Pipeline, Glue, Athena, Aurora, DynamoDB) is highly preferred.\\nCommand line expertise in AWS\\nExperience with Data Orchestration (Airflow is a big plus)\\nStrong team player - ability to work directly with other developers exchanging ideas, knowledge, experience and thoughts in order to promote Operational Rigor is required.\\nUnderstanding of capacity planning is a plus\\nAbility to drive major implementations are highly preferred\\nExperience with CI/CD tools, such as Jenkins, Chef is a definite plus\\nDeep knowledge and use of command line on Linux/Mac/AWS environments\\nSoftware development experience (in any language, particularly Python and Java) is required\\nExperience as a DevOps Engineer team lead is a plus\\nKnowledge of performance and security test tools is a definite plus</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At realtor.com our goal is to use data to make the home buying experience a breeze for our consumers. We empower them with the most up to date information on properties, and help find their dream homes in the least amount of time and also match them with the most suitable realtor equipped to meet their individual needs. The Data Engineering team is not afraid to break and rebuild code to make it better... stronger... faster, to overcome Big Data challenges of today and tomorrow.\\nFrom a work environment perspective, realtor.com has a supportive, collaborative and open environment that fosters innovation and continuous improvement. We have a history of many industry-first innovations in our field. Scalability, Stability and reliability are the essence of any Data Platform and ours is not an exception. We need strong data engineers to ensure operational rigor, solid foundation of data orchestration, automated monitoring, scale out with minimal operational overhead.\\nThis impactful role will promote data-driven decisions and data products for realtor.com, its customers, partners, and consumers. Additionally, this role will be part of the core team contributing to design and development of realtor.com's data and analytics platform.\\n\\nDuties And Responsibilities\\nWork collaboratively in an Agile/Scrum team guiding the testing process for both development and other Data Engineering (DataOps) team members.\\nExplore, drive, design and implement solutions for deployment/monitoring automation.\\nWork to solve technical problems and use appropriate technology to drive solutions\\nBuild orchestration ETL and other data solutions (Apps).\\nAutomate deployment of ETL and other data solutions.\\nAutomate monitoring of Data Solutions/ETLS\\nWork closely with Cloud Services, Global DevOps and IT organizations to resolve any Data Platform PaaS issues.\\nHelp optimize costs associated with cloud resources.\\nWork with RTB Operations team to resolve production issues.\\nTake a more proactive approach to preventing defects in a sprint and the entire SDLC\\nProvide technical and leadership discipline to other more junior team members\\nBe an advocate and an example for best practices across teams\\n\\nEducation, Skills And Experience\\nBachelor degree in Computer Science/Engineering or related field, or equivalent professional experience\\n5+ years of software engineering/DevOps/DataOps experience.\\nExperience with combination of AWS Managed Services (such as Data Pipeline, Glue, Athena, Aurora, DynamoDB) is highly preferred.\\nCommand line expertise in AWS\\nExperience with Data Orchestration (Airflow is a big plus)\\nStrong team player - ability to work directly with other developers exchanging ideas, knowledge, experience and thoughts in order to promote Operational Rigor is required.\\nUnderstanding of capacity planning is a plus\\nAbility to drive major implementations are highly preferred\\nExperience with CI/CD tools, such as Jenkins, Chef is a definite plus\\nDeep knowledge and use of command line on Linux/Mac/AWS environments\\nSoftware development experience (in any language, particularly Python and Java) is required\\nExperience as a DevOps Engineer team lead is a plus\\nKnowledge of performance and security test tools is a definite plus\\nIdeal Candidate Profile\\nYou are passionate about data quality and have a mind which can break software apart to expose potential risks\\nYou have a thirst for learning, continuous improvement, sharing and working in a team environment\\nYou are able to work as part of a self-directed and self-managed team\\nYou hold a point of view and aren‚ Not afraid of challenging assumptions, but are humble enough to recognize and adopt the views of others\\nYou have excellent troubleshooting, problem-solving, time management, and organizational skills\\n\\n\\n#LI-KK1\\nDiversity is important to us, therefore, realtor.com is an Equal Opportunity Employer regardless of age, color, national origin, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, marital status, status as a disabled veteran and/or veteran of the Vietnam Era or any other characteristic protected by federal, state or local law. In addition, realtor.com will provide reasonable accommodations for otherwise qualified disabled individuals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>New Graduate - IT Data Analytics - Data Engineer</td>\n",
       "      <td>Palo Alto, CA 94304</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>94304</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCurrently pursuing a Bachelors or Masters degree in Computer Science, Computer Engineering, Data Analytics or related field\\nExcellent knowledge of Data Architecture and BI concepts\\nExpertise in SQL and Python\\nWorking knowledge of Hadoop, SAP HANA\\nWorking knowledge of Tableau and SAP BOBJ is a plus\\nFamiliarity with Machine Learning models.</td>\n",
       "      <td>\\nCreate data pipelines using SQL, Python, Informatica, SDI\\nWork with structured and unstructured data and loading into Hadoop and SAP HANA\\nCreate the BI &amp; Analytical reports using SAP BOBJ &amp; Tableau based on business needs\\nCreate Machine learning models for predictive analytics.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>98% of Fortune 500 Companies use VMware Technology!\\nThe most advanced companies in the world turn to VMware to manage, grow and transform their business. When you work here, you’re connected to a global community of innovative, empowered employees working together to solve the most critical technology challenges.\\nWe believe that creativity sparks innovation and inspires our employees to think differently and challenge the status quo. Whether it’s the kind of products we develop, our approach to sustainability, or how we give back to our communities, VMware finds unique ways to bring people together to fuel creative thinking. Want to know more, check out our website https://careers.vmware.com/\\nThe IT Data &amp; Analytics team is responsible for building the Data pipelines using different corporate data assets as source, working with both structured and unstructured data and transforming the data based on the business needs and maintaining the data in the Enterprise Datawarehouse with the architecture &amp; Infrastructure that supports Near Real time reporting, Big Data and Machine Learning. The team supports Data, reporting and analytical needs of all the business groups across the company and is spread across Palo Alto, Costa Rica &amp; India.\\nAs a New Graduate, you will have the opportunity to work with Structured &amp; unstructured data, building data pipelines, working with SQL, Python and creating the Machine learning modes and building the BI reports.\\nResponsibilities:\\nAs a New Graduate at VMware, you’ll create innovative solutions and solve complex problems. You’ll take ownership of meaningful, big-picture work and springboard an impactful career. Become immersed in all aspects of our innovative and collaborative culture, and ensure you get the full VMware experience. You’ll interact with industry thought leaders at one of our world class campuses and enjoy networking, community service, and career development events. Some of your responsibilities will include:\\nCreate data pipelines using SQL, Python, Informatica, SDI\\nWork with structured and unstructured data and loading into Hadoop and SAP HANA\\nCreate the BI &amp; Analytical reports using SAP BOBJ &amp; Tableau based on business needs\\nCreate Machine learning models for predictive analytics.\\nRequired Skills:\\nCurrently pursuing a Bachelors or Masters degree in Computer Science, Computer Engineering, Data Analytics or related field\\nExcellent knowledge of Data Architecture and BI concepts\\nExpertise in SQL and Python\\nWorking knowledge of Hadoop, SAP HANA\\nWorking knowledge of Tableau and SAP BOBJ is a plus\\nFamiliarity with Machine Learning models.\\nPreferred Skills:\\nWorking knowledge of SQL and Python\\nWorking knowledge of Big Data Infrastructure\\nFamiliarity with Machine learning models\\nGood Analytical and trouble shooting skills\\nThis job opportunity is not eligible for employment-based immigration sponsorship by VMware.\\n\\nVMware Company Overview: VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com.\\n\\nEqual Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Engineer – Logs</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDegree in Computer Science, Engineering, or related fields\\nStrong in data structures, algorithms, and systems\\nDeep understanding of systems behavior and affinity for logs\\nExperience with building infrastructure to collect, prepare and analyze logs, telemetry, and monitoring data\\nExperience with building and using tools like Dtrace, BTrace, etc.\\nExperience with log analytics tools like Splunk, ELK, etc.\\nKnowledge of time series models and the use of systems like OpenTSDB, Graphite, etc.\\nExperience with handling complex production and/or support escalation issues\\nHeuristic problem solving with incomplete information\\nDeep domain knowledge and hands-on experience across a very broad spectrum of backend, frontend, cloud, AI and data infrastructure platforms,\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDegree in Computer Science, Engineering, or related fields\\nStrong in data structures, algorithms, and systems\\nDeep understanding of systems behavior and affinity for logs\\nExperience with building infrastructure to collect, prepare and analyze logs, telemetry, and monitoring data\\nExperience with building and using tools like Dtrace, BTrace, etc.\\nExperience with log analytics tools like Splunk, ELK, etc.\\nKnowledge of time series models and the use of systems like OpenTSDB, Graphite, etc.\\nExperience with handling complex production and/or support escalation issues\\nHeuristic problem solving with incomplete information\\nDeep domain knowledge and hands-on experience across a very broad spectrum of backend, frontend, cloud, AI and data infrastructure platforms,\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Peritus\\nPeritus enables self-healing autonomous datacenters with automated, cognitive support for infrastructure software and hardware. It is a funded startup co-created at The Hive in Palo Alto, CA that delivers artificial intelligence-based virtual support expert systems for data center service fulfillment and incident resolution.\\n\\nAs datacenter vendors move from on-premise to the cloud their existing support system lacks the agility and cost-effectiveness for the cloud. Peritus significantly enhances operational efficiencies of existing support services and enables managed service providers &amp; system vendors to offer new business continuity entitlements. Peritus assists &amp; automates a wide spectrum of decisions in system support including incident classification, routing, contract coverage, incident resolution recipes and orchestration of incident management between subject matter experts (SMEs).\\n\\nPeritus’ unique vectorization of system log data drives predictive modeling with highly granular feature extraction for early detection of system events. The platform’s advanced natural language processing (NLP) capabilities drive Peritus’ incident modeling and predictive capabilities. The core service fulfillment engine uses a combination of supervised and unsupervised methods to predict incident features from system log data. Peritus delivers automated orchestration of incident resolution through its close integration with existing incident management platforms.\\n\\nJob Description\\nWe are building a product that helps customers fulfill service requests as well as troubleshoot and diagnose infrastructure issues that cut across domains. The product needs to collect, collate and analyze logs, telemetry and monitoring data emanating from multi-vendor and multi-datacenter/cloud systems. The analysis should help with timestamp-based correlation of events across systems, extraction of inferences derived from performance counters from multiple systems and trace the sequence of events across layers of the data center stack.\\n\\nResponsibilities\\nWe are looking to hire an engineering technical leader with deep systems knowledge. The role entails a deeper understanding of how data center systems operate requiring familiarity with computing, storage, networking and virtualization products. Think DTrace but with a scope that spans across systems and not just within a system. The logs infrastructure needs to scale horizontally and enable machine learning models to learn systems behavior. Over time, the learning translates to automatic application of resolutions and thereby reducing human involvement.\\n\\nCan you help fill in the blanks for users handling complex systems issues that affect performance and/or bring systems down?\\n\\nQualifications &amp; Expertise\\nThe successful engineer would have a proven track record of building complex log analysis platforms:\\n\\nDegree in Computer Science, Engineering, or related fields\\nStrong in data structures, algorithms, and systems\\nDeep understanding of systems behavior and affinity for logs\\nExperience with building infrastructure to collect, prepare and analyze logs, telemetry, and monitoring data\\nExperience with building and using tools like Dtrace, BTrace, etc.\\nExperience with log analytics tools like Splunk, ELK, etc.\\nKnowledge of time series models and the use of systems like OpenTSDB, Graphite, etc.\\nExperience with handling complex production and/or support escalation issues\\nHeuristic problem solving with incomplete information\\nDeep domain knowledge and hands-on experience across a very broad spectrum of backend, frontend, cloud, AI and data infrastructure platforms,\\nPlease send your resumes to jobs@peritus.ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Senior Data Engineer (Outward, Inc.)</td>\n",
       "      <td>San Jose, CA 95112</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95112</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor's degree or equivalent work experience\\n6+ years of experience with software engineering, data engineering, data visualization, and data-mining\\nStrong Python and/or Scala development experience\\n4+ years of SQL (Hive, Oracle, MySQL, PostgreSQL) experience\\nProfessional experience using XML\\nExperience with visualization frameworks\\nExperience analyzing data to identify deliverables, gaps and inconsistencies\\nExperience initiating and driving projects to completion\\nExperience communicating the results of analyses to stakeholders\\nExperience with AWS services like lambda, Cloud Formation, RDS, EC2, IAM\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nWork with Engineers, Product Owners, and Designers to understand their data needs\\nAutomate frequently requested analyses using Python\\nEvaluate and define critical business metrics and identify new levers to help move these metrics\\nDesign and evaluate A/B experiments\\nMonitor key product metrics and identify root causes behind anomalies\\nBuild and analyze dashboards and reports\\nInfluence product teams through a presentation of data-based recommendations\\nCommunicate state of business and experiment results to product teams</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Location: San Jose, CA\\n\\nAbout Outward, Inc.\\n\\nOutward, Inc. is based in San Jose, CA and is a wholly owned subsidiary of Williams Sonoma, Inc. ( www.outwardinc.com )\\n\\nAt Outward Inc. our vision is to 'lower the friction' with regards to all aspects of the customer journey for our parent company and our retail customers. We do this by offering new technology solutions that enable new experiences and top-notch visualizations of their products. We are continuously pushing the boundaries of how 3D and AR/ VR technologies will drive the next generation shopping experience.\\n\\nThrough our portfolio of premium lifestyle brands - our mission is to deepen consumer connections with the products that matter and deliver an innovative experience.\\n\\nWe are positioned as a technology leader in the visual merchandising space for retail, with a focus on improving customer experiences with next-generation product visualizations.\\n\\nCome and join a growing team of engineers as we solve technological riddles and push the envelope of what can be done on the web!\\n\\n\\nResponsibilities:\\n\\nWork with Engineers, Product Owners, and Designers to understand their data needs\\nAutomate frequently requested analyses using Python\\nEvaluate and define critical business metrics and identify new levers to help move these metrics\\nDesign and evaluate A/B experiments\\nMonitor key product metrics and identify root causes behind anomalies\\nBuild and analyze dashboards and reports\\nInfluence product teams through a presentation of data-based recommendations\\nCommunicate state of business and experiment results to product teams\\nQualifications:\\n\\nBachelor's degree or equivalent work experience\\n6+ years of experience with software engineering, data engineering, data visualization, and data-mining\\nStrong Python and/or Scala development experience\\n4+ years of SQL (Hive, Oracle, MySQL, PostgreSQL) experience\\nProfessional experience using XML\\nExperience with visualization frameworks\\nExperience analyzing data to identify deliverables, gaps and inconsistencies\\nExperience initiating and driving projects to completion\\nExperience communicating the results of analyses to stakeholders\\nExperience with AWS services like lambda, Cloud Formation, RDS, EC2, IAM\\n\\n\\nOutward's Benefits &amp; Perks\\n\\nMedical, Dental, Vision, &amp; 401K\\nFloating holidays, PTO, and game nights\\nCompany-Sponsored Team Events &amp; Staff Parties\\nRecently Relocated Headquarters, Great for Commuters!\\nTwice Weekly Catered Lunch\\nOnsite gym &amp; showers (San Jose office)\\nFree healthy snacks\\nDog-friendly, bring your furry pal with you to work\\nTax-free commuter benefits\\nA wellness program that supports your physical, financial and emotional health\\nA smart casual work environment\\nTime off to volunteer\\nAdditional discounts on nearby gyms and other local businesses\\n\\n\\nThis position will not offer relocation assistance or remote work.\\n\\nOutward, Inc. is an Equal Opportunity Employer.\\n\\nOutward, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the California Fair Employment Act (AB 1008), or other applicable state or local laws and ordinances.\\n\\n#LI-JQ1\\n\\n-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sr. Big Data Engineer</td>\n",
       "      <td>Mountain View, CA 94043</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>94043</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>7+ years of Python or Java/J2EE development experience\\n3+ years of demonstrated technical proficiency with Hadoop and big data projects\\n5-8 years of demonstrated experience and success in data modeling\\nFluent in writing shell scripts [bash, korn]\\nWriting high-performance, reliable and maintainablecode.\\nAbility to write MapReduce jobs\\nAbility to setup, maintain, and implement Kafka topics and processes\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Résumé du poste\\nBig Data Engineers serve as the backbone of the Strategic Analytics organization, ensuring both the reliability and applicability of the team’s data products to the entire Samsung organization. They have extensive experience with ETL design, coding, and testing patterns as well as engineering software platforms and large-scale data infrastructures. Big Data Engineers have the capability to architect highly scalable end-to-end pipeline using different open source tools, including building and operationalizing high-performance algorithms.\\n\\n\\nBig Data Engineers understand how to apply technologies to solve big data problems with expert knowledge in programming languages like Java, Python, Linux, PHP, Hive, Impala, and Spark. Extensive experience working with both 1) big data platforms and 2) real-time / streaming deliver of data is essential.\\n\\n\\nBig data engineers implement complex big data projects with a focus on collecting, parsing, managing, analyzing, and visualizing large sets of data to turn information into actionable deliverables across customer-facing platforms. They have a strong aptitude to decide on the needed hardware and software design and can guide the development of such designs through both proof of concepts and complete implementations.\\nRôle et Responsabilités\\nResponsibilities include:\\nTranslate complex functional and technical requirements into detailed design.\\nDesign for now and future success\\nHadoop technical development and implementation.\\nLoading from disparate data sets. by leveraging various big data technology e.g. Kafka\\nPre-processing using Hive, Impala, Spark, and Pig\\nDesign and implement data modeling\\nMaintain security and data privacy in an environment secured using Kerberos and LDAP\\nHigh-speed querying using in-memory technologies such as Spark.\\nFollowing and contributing best engineering practice for source control, release management, deployment etc\\nProduction support, job scheduling/monitoring, ETL data quality, data freshness reporting\\nSkills Required:\\n7+ years of Python or Java/J2EE development experience\\n3+ years of demonstrated technical proficiency with Hadoop and big data projects\\n5-8 years of demonstrated experience and success in data modeling\\nFluent in writing shell scripts [bash, korn]\\nWriting high-performance, reliable and maintainablecode.\\nAbility to write MapReduce jobs\\nAbility to setup, maintain, and implement Kafka topics and processes\\nCompétences et Qualifications\\nSamsung Electronics America, Inc. is committed to employing a diverse workforce, and provides Equal Employment Opportunity for all individuals regardless of race, color, religion, gender, age, national origin, marital status, sexual orientation, gender identity, status as a protected veteran, genetic information, status as a qualified individual with a disability, or any other characteristic protected by law.\\n* Please visit Samsung membership to see Privacy Policy, which defaults according to your location. You can change Country/Language at the bottom of the page. If you are European Economic Resident, please click here .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sr. Systems Analyst</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Sr. Systems Analyst - (SO00050471-1-1-1)\\n\\n\\nUST Global is a private high growth organization headquartered in Orange County California and is a leading provider of Advanced Computing and Digital Services for Global 1000 companies worldwide. Our next-generation digital consultancy is the ideal place for you to grow your career. Are you up for the challenge? Read on!\\n\\nDescription\\n\\nSr.Big Data Developer\\nJob Description\\nCustomer s Platform Engineering team is looking for motivated and experienced big data engineers, to join a highly skilled team of experienced professionals, which believe in best of breed software craftsmanship, clean and elegant coding, using the right tool for the job, and always exploring and learning new technologies and approaches. This Big Data Engineer will work on a high volume data pipeline, data integration, data transformation with Transaction State Management. This involves working with multiple BUs to ingest data into an Operational Data Store for downstream analytics by multiple stakeholders. There will be following activities in this initiative.\\nA Real time streaming and Batch enterprise data platformHigh volume data processingNear Real Time Operational Data Storeperformance tuning of big data applications and analytical queriesData Governance and Data tiering\\n\\nResponsibilities Requirements\\nAs an engineer, you will be responsible for working on applications and services that handle all types of transactions. You will work in a fast paced environment where continuous innovation and experimentation are a given. You will master both established and cutting edge technologies like Spark, Presto, Hadoop, Hive, Oracle, Casandra, Kafka, Druid among others.\\nDesign, development, and testing of features functions delivered via applications and servicesCollaborating with peers and seniors both within their team and across the organizationWorking with operations teams to ensure your applications and services are highly available and reliableSupporting your applications and or services as and when required on a 24x7 basisDesign, develop, test, and debug large scale complex applications using big data technologies.Develop tools and automation for the effective management and operation of Big Data platform.Collaborate with architects, engineers, and business on product design and feature.Apache Hadoop and data ETL extract, transform, load , ingestion, and processing with Hadoop toolsBe proactive and anticipate handle most issues before they blowupExhibit a strong backbone and challenge the status quo when neededShow pride of ownership and strive for excellence in everything they do\\n\\nJob Requirements\\n8 years software development experienceBS in Computer Science or related degree required. MS preferredExpert in performance tuning of Spark batch and streaming applicationsExpert on Big Data Technologies such as Kafka, Apache SparkExperience in database storage technologies like Oracle, Cassandra, CouchBase etc.Experience in building real time ETL pipelines using Spark or Apache FlinkExperience in validating the data in streaming and batch processesExpert in SQL, Hive, Spark, PrestoCompetent in design implementation for reliability, availability, scalability and performanceCompetent in software engineering tools and best practices\\n\\nQualifications\\n\\nBig Data\\n\\nPrimary Location: US-CA-San Jose\\nEmployee Type: Regular Employee\\nJob Type: Full-time\\nJob Posting: Aug 6, 2019, 10:07:44 AM\\n\\n\\n UST-Global is an Equal Opportunity Employer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Senior Hadoop Engineer</td>\n",
       "      <td>San Mateo, CA 94404</td>\n",
       "      <td>San Mateo</td>\n",
       "      <td>CA</td>\n",
       "      <td>94404</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5+ years of experience as technical administrator for a Hadoop/Big Data environment\\nExpertise with YARN, Spark, Kafka, Storm, Zookeeper, Hive, Hbase operations (not dev)\\nExpertise with one or more of the following languages: Python, Scala, Java, Perl, Ruby, or Bash. Shell scripting is a must have.\\nExperience with multiple open source tool sets in the Big Data space</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExpertise at maintaining hadoop/hbase cluster built using open source software (no HDP/CDH/MapR)\\nProvide strategies for future technologies that might better solve work flows\\nTune performance and operational efficiency\\nFormulate methods to enable consistent Data loading and optimize Data operationsMeet performance and SLA requirements of the Hadoop clustersPrior experience with remote monitoring and event handling using NagiosDesign metrics and monitor real-time performance of clustersExperience in scaling and architecture design of Hadoop and Hbase clustersAbility to take ownership of multiple bigdata projects</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for an experienced and enthusiastic Senior Big Data Engineer to join our team. You will work with your teammates to plan, design, test, and implement Infrastructure in our 1000+ node cluster. This is all in support of 100's of billions of transaction a day. In addition, you will get to work with a team of extremely talented engineers who develop cutting edge AI solutions for one of the largest companies in the Ad Tech industry.\\nThe ideal candidate is expected to meet tight project deadlines, excel under pressure, work well with others, be self-motivated, and be able to manage short and long term projects. You will also be expected to help architect, design and fine tune the environment and systems for best \"uptime\" and performance.\\n\\nResponsibilities:\\nExpertise at maintaining hadoop/hbase cluster built using open source software (no HDP/CDH/MapR)\\nProvide strategies for future technologies that might better solve work flows\\nTune performance and operational efficiency\\nFormulate methods to enable consistent Data loading and optimize Data operationsMeet performance and SLA requirements of the Hadoop clustersPrior experience with remote monitoring and event handling using NagiosDesign metrics and monitor real-time performance of clustersExperience in scaling and architecture design of Hadoop and Hbase clustersAbility to take ownership of multiple bigdata projects\\nQualifications:\\n5+ years of experience as technical administrator for a Hadoop/Big Data environment\\nExpertise with YARN, Spark, Kafka, Storm, Zookeeper, Hive, Hbase operations (not dev)\\nExpertise with one or more of the following languages: Python, Scala, Java, Perl, Ruby, or Bash. Shell scripting is a must have.\\nExperience with multiple open source tool sets in the Big Data space\\nExperience in tool Integration, automation, configuration management in GIT (Puppet)\\nExcellent verbal and written communication and presentation skills, analytical and problem solving skills\\nExcellent interpersonal and communication skills\\nBS or higher in Computer Science or related fields.\\n\\n#LI-MC1\\nCompany Summary\\nZeta is a data-driven marketing technology innovator whose SaaS-based marketing cloud helps 500+ Fortune 1000 and Middle Market brands acquire, retain and grow customer relationships through actionable data, advanced analytics and machine learning.\\nFounded by David A. Steinberg and John Sculley (former CEO of Apple and Pepsi-Cola) in 2007, the company's highly-rated ZetaHub technology platform has been recognized in Gartner's Magic Quadrant for Digital Marketing Hubs (February 2017) and in its Magic Quadrant for Multichannel Campaign Management (April 2017), competing with offerings from Oracle, IBM, Salesforce and Adobe.\\nOperating on four continents with 1,300+ employees, the company is headquartered in New York City, with Centers of Excellence in Silicon Valley, Boston, London, and Hyderabad, India.\\n\\nZeta Global is an Equal Opportunity/Affirmative Action employer and does not discriminate on the basis of race, gender, ancestry, color, religion, sex, age, marital status, sexual orientation, gender identity, national origin, medical condition, disability, veteran’s status, or any other basis protected by law.\\n\\nRecent News\\nZeta Global Recognized by The Relevancy Group as Industry Leader for the 4th Consecutive Year\\nZeta Global is Recognized as a Visionary by Gartner for the First Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cloud Data Engineer</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Our Mission\\n\\n\\nAt Palo Alto Networks® everything starts and ends with our mission:\\nBeing the cybersecurity partner of choice, protecting our digital way of life.\\nWe have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.\\nYour Career\\nWe are the fastest growing company in one of the fastest moving industries, it’s absolutely critical that all of our businesses and product units have all the information needed to make the fastest and smartest decisions. Our Big Data Analytics team is constantly growing and stretching as they rise to the challenges created by our meteoric growth and adding new technologies that make sense is a daily occurrence. We have recently introduced GCP into our Big Data Analytics environment and we are looking for a ‘groundbreaking’ senior level Big Data Engineer to join our team and help drive these efforts.\\n\\nYour Impact\\nIntegral team member of our AI and Analytics team responsible for design and development of Big data solutions\\nPartner with domain experts, product managers, analyst, and data scientists to develop Big Data pipelines in Hadoop or Google Cloud Platform\\nResponsible for delivering data as a service framework from Google Cloud Platform\\nResponsible for moving all legacy workloads to cloud platform\\nWork with data scientist to build ML pipelines using heterogeneous sources and provide engineering services for data science applications\\nEnsure automation through CI/CD across platforms both in cloud and on-premises\\nAbility to research and assess open source technologies and components to recommend and integrate into the design and implementation\\nBe the technical expert and mentor other team members on Big Data and Cloud Tech stacks\\nYour Experience\\n5+ years of experience with Hadoop or Cloud Technologies\\nExpert level building pipelines using Apache Beam or Spark\\nFamiliarity with core provider services from AWS, Azure or GCP, preferably having supported deployments on one or more of these platforms\\nExperience with all aspects of DevOps (source control, continuous integration, deployments, etc.)\\nYou have experience with containerization and related technologies (e.g. Docker, Kubernetes)\\nExperience in other open-sources like Druid, Elastic Search, Logstash etc is a plus\\nAdvanced knowledge of the Hadoop ecosystem and Big Data technologies\\nHands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Impala, Spark, Kafka, Kudu, Solr)\\nKnowledge of agile(scrum) development methodology is a plus\\nStrong development/automation skills\\nCompetent reading and writing Scala, Python or Java code.\\nSystem level understanding - Data structures, algorithms, distributed storage &amp; compute\\nCan-do attitude on solving complex business problems, good interpersonal and teamwork skills\\nDegree in Bachelor of Science in Computer Science or equivalent\\nThe Team\\n\\nWorking at a high-tech cybersecurity company within Information Technology is a once in a lifetime opportunity. You’ll be joined with the brightest minds in technology, creating, building, and supporting tools and that enable our global teams on the front line of defense against cyberattacks. We’re joined by one mission – but driven by the impact of that mission and what it means to protect our way of life in the digital age. Join a dynamic and fast-paced team that feels excitement at the prospect of a challenge and feels a thrill at resolving technical gaps that inhibit productivity.\\n\\nOur Commitment\\nWe’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together. To learn more about our dedication to inclusion and innovation, visit our Life at Palo Alto Networks page and our diversity website.\\nPalo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.\\nAdditionally, we are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or an accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.\\n\\n #LI-AH1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Senior Data Engineer, Human Loop - AI Infrastructure</td>\n",
       "      <td>Santa Clara, CA 95050</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>95050</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>NVIDIA is searching for a Senior UI/UX Application Engineer to work on our AI Infra team in Santa Clara, Boulder or New York. Our team is enabling NVIDIA and our customers to more easily scale up machine learning workflows - machine learning at scale requires a new vocabulary for organizing and managing data, jobs and users. We are building and optimizing human in the loop pipelines which enable massive state of the art systems in Artificial Intelligence / Machine Learning at Nvidia and for our customers in many application spaces including medical imagery and autonomous driving.\\nWhat you will be doing:\\nCreate web applications and views to enable human annotation and oversight of sensor data from NVIDIAs self-driving car platform.\\nCollaborate with a diverse team of front and back-end engineers as well as product managers and designers and interacting with machine learning, deep learning, experts.\\nCreate human in the loop and management interfaces at the frontier of what is possible in machine learning today and getting front seat view of the action in this very hot space from a team and a company driving the progress at the cutting edge.\\nWhat we need to see:\\nBS or MS in computer science, human-computer interaction, ECE, EE or a related field.\\n4+ years of experience in web application development. 7+ years of software development experience\\nSolid experience in JavaScript frameworks for complex browser applications\\nExtensive knowledge of UI/UX and interacting with gRPC/RESTful APIs\\nJavascript core language and some frameworks (we use Angular)\\nComfortable with a customer focused, high paced environment.\\nWell versed in agile methodology.\\nExperience in software shipping cycles (dev, deploy, release, CI) and open-source software dev.\\nYou are highly motivated, passionate and curious about new technologies. You take pride in your work and strive to achieve incredible results and possess excellent communication and planning skills.\\nAbility to work successfully with multi-functional teams, principals and architects.\\nCoordinates effectively across organizational boundaries and geographies.\\nNVIDIA is widely considered to be one of the technology world’s most desirable employers. In this role you will have the opportunity to work with some of the most brilliant and talented people in the world working for us and due to unprecedented growth, our best-in-class software engineering teams are rapidly growing fast. If you're creative, passionate about what you do, autonomous and love having fun, then what are you waiting for, apply today!\\nFor two decades, we have pioneered visual computing, the art and science of computer graphics. With our invention of the GPU - the engine of modern visual computing - the field has expanded to encompass video games, movie production, product design, medical diagnosis and scientific research.\\nToday, we stand at the beginning of the next era, the AI computing era, ignited by a new computing model, GPU deep learning. This new model - where deep neural networks are trained to recognize patterns from massive amounts of data - has shown to be deeply effective at solving some of the most complex problems in everyday life.\\nNVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression , sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.\\n#deeplearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>DESCRIPTION\\nDesign and develop framework to automate ingestion of structured data\\nBuild distributed data infrastructure using open source tools.\\nDesign and develop data pipeline components\\nScale pipelines to meet performance requirements in a microservice architecture\\nProvide leadership to other Data Engineers\\nResponsible for technical direction of the team\\nCollaborate with colleagues and sharing responsibility throughout the product life-cycle\\nREQUIREMENTS\\n5+ years of software development experience\\n3+ years of experience with building scalable and reliable data pipelines\\n2+ years of experience with scalable data integration technologies\\nDemonstrated experience in deploying and managing SQL, NoSQL and Time Series databases\\nHave Knowledge on big data platform infrastructure like Kafka, Flume, Spark, Hadoop\\nExtensive experience building Reactive applications with Scala, Play, Lagom, Akka\\nProficient in Scala, and Python\\nExperience with cloud technologies such as VMware, AWS, Azure, or Google Cloud\\nDemonstrated ability to communicate and collaborate with peers\\nDemonstrated skills in result-driven problem solving\\nExperience participating in talent screening and interviewing\\nBENEFITS\\nMedical, Dental &amp; Vision Insurance\\nLife &amp; Disability Coverage\\nRetirement/401K Plan\\nFlexible work and vacation schedules\\nFully-stocked kitchen with healthy snacks and drinks\\nFree catered Wednesday lunches\\nOpen and spacious office that is close to public transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Training Solutions Advisor, Google Cloud</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nUnderstand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.\\nConnect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.\\nPartner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.\\nSupport escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.\\nPartner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Note: By applying to this position your application is automatically submitted to the following locations: Sunnyvale, CA, USA; Austin, TX, USA; New York, NY, USA; Reston, VA, USA\\nMinimum qualifications:\\n\\nBachelor's degree in Computer Science or a related technical field, or equivalent practical experience.\\n5 years of experience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.\\nExperience working in a customer facing environment in a technology company and helping customers identify solutions that best fit their unique needs.\\nAbility to travel to support Customer Engagements up to 30% of the time.\\n\\nPreferred qualifications:\\n\\nGoogle Cloud Certified e.g. Cloud Architect, Data Engineer or Associate Cloud Engineer or other comparable Cloud Certification.\\nExperience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.\\nAbility to take customers’ technical requirements and architect a proposal that maps to technical skills required.\\nAbility to quickly learn and understand new training offerings.\\nAbility to work well cross functionally and understand when to pull in specialist knowledge into Customer conversations.\\nExcellent communication and presentation skills.\\nAbout the job\\nThe Google Cloud team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you'll help shape the future of businesses of all sizes and enable them to better use technology to drive innovation.This role will enable you to make a huge impact across Google Cloud’s most strategic accounts and ensure they have Learning Plans that effectively help them develop the knowledge and skills they need to adopt Google Cloud. The role is also an exciting mix of elements as you will be working in Technical Customer Facing activities (usually in partnership with the CE or PSO/TAM Account owner), working in partnership with Cloud Learning GTM leads on Learning Plan development, and working with the Curriculum Tech leads to provide curriculum feedback and validate proposals as required.\\n\\nGoogle Cloud helps millions of employees and organizations empower their employees, serve their customers, and build what’s next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. And our teams are dedicated to helping our customers and developers see the benefits of our technology come to life.\\nResponsibilities\\nUnderstand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.\\nConnect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.\\nPartner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.\\nSupport escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.\\nPartner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.\\nAt Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Mountain View, CA 94041</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>94041</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nB.S. in Computer Science or a related discipline, or related practical experience\\nMinimum 4 years of Experience with Data Engineering &amp; Data Architecture\\nExperience using advanced SQL and databases in a business environment with large-scale datasets (Hadoop, Hive, Presto)\\nExperience with statistical modeling and analyzing large data sets\\nExperience with product analytics tools and APIs (Mixpanel, Google Analytics)\\nAWS expertise (S3, EC2, Lambda, Redshift, Athena) is a plus\\nExperience developing scalable microservices also a plus\\nFamiliarity with Kimball's data warehouse lifecycle</td>\n",
       "      <td>\\nB.S. in Computer Science or a related discipline, or related practical experience\\nMinimum 4 years of Experience with Data Engineering &amp; Data Architecture\\nExperience using advanced SQL and databases in a business environment with large-scale datasets (Hadoop, Hive, Presto)\\nExperience with statistical modeling and analyzing large data sets\\nExperience with product analytics tools and APIs (Mixpanel, Google Analytics)\\nAWS expertise (S3, EC2, Lambda, Redshift, Athena) is a plus\\nExperience developing scalable microservices also a plus\\nFamiliarity with Kimball's data warehouse lifecycle</td>\n",
       "      <td>\\nWork with structured and unstructured real-world medical data\\nDesign, build and launch efficient and reliable data pipelines to move complex data\\nWrite high-quality, efficient, testable code in Python, C++, or Go.\\nBuild data expertise and own data quality\\nCollaborate with software and AI engineers to design and implement data architecture\\nIntegrate with 3rd party analytics tools and APIs (Mixpanel, Google Analytics)\\nBuild horizontally scalable infrastructure to support ML training and data mining research</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer\\nAliveCor produces and delivers rich, informative, clinical-grade, personal heart data that can be easily understood by patients - anytime, anywhere.\\nThe Opportunity:\\nAs a Data Engineer, you will architect and improve our data infrastructure, develop new products, and support medical science while protecting user privacy.\\nThe ideal candidate has a strong background in engineering, as well as experience in statistical analysis, data analysis tools, SQL and Python. You will work closely with our product management, engineering, and AI teams to architect and build fast and efficient databases, pipelines, and services.\\nResponsibilities:\\nWork with structured and unstructured real-world medical data\\nDesign, build and launch efficient and reliable data pipelines to move complex data\\nWrite high-quality, efficient, testable code in Python, C++, or Go.\\nBuild data expertise and own data quality\\nCollaborate with software and AI engineers to design and implement data architecture\\nIntegrate with 3rd party analytics tools and APIs (Mixpanel, Google Analytics)\\nBuild horizontally scalable infrastructure to support ML training and data mining research\\nQualifications and Skills:\\nB.S. in Computer Science or a related discipline, or related practical experience\\nMinimum 4 years of Experience with Data Engineering &amp; Data Architecture\\nExperience using advanced SQL and databases in a business environment with large-scale datasets (Hadoop, Hive, Presto)\\nExperience with statistical modeling and analyzing large data sets\\nExperience with product analytics tools and APIs (Mixpanel, Google Analytics)\\nAWS expertise (S3, EC2, Lambda, Redshift, Athena) is a plus\\nExperience developing scalable microservices also a plus\\nFamiliarity with Kimball's data warehouse lifecycle\\nThe Perks:\\nAliveCor is located in downtown Mountain View, CA. We are close to Caltrain and supply car and bike parking for employees. We strive to make your life outside work as smooth as possible while you're at work, and we offer a long list of benefits to make that happen.\\nCompetitive salary and competitive stock options\\nFlexible and generous vacation policy\\nMaternity / Paternity Leave\\n401(k)\\nTriNet medical coverage, employee 100% covered for medical, dental and vision\\nEquipment allowance to build out your ideal workspace\\nMobile reimbursement\\nStocked kitchen + weekly catered lunches\\nA supportive, collaborative group of people who understand that success depends on the team\\nAliveCor is an equal opportunity employer and will not discriminate against any employee or applicant on the basis of age, color, disability, gender, national origin, race, religion, sexual orientation, veteran status, or any other classification protected by federal, state, or local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Senior Data Engineer Sandipan</td>\n",
       "      <td>San Mateo, CA</td>\n",
       "      <td>San Mateo</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nProven expertise in production software development\\n7+ years of experience programming in Java, Python, SQL, or C/C++\\nProficient in SQL, NoSQL, relational database design and methods for efficiently storing &amp; retrieving data\\nStrong analytical and science skills\\nCreative problem solver\\nExcellent verbal and written communications skills\\nStrong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..\\nExperience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies &amp; frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)\\n</td>\n",
       "      <td>\\nPrior Data Platform Engineering experience\\nExperience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.\\nDynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)\\nExperience designing and tuning high performance systems\\nPrior experience with data warehousing and business intelligence systems\\nExperience with Elasticsearch, SolrWeb, and Lucene\\nExperience with Star Schema, fact vs dimensions, updates/restatements and views\\nProfessional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform\\nLinux expertise\\nPrior work and/or research experience with unstructured data and data modeling\\nFamiliarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)\\nDemonstrate understanding of \"var\" vs. \"val\", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem\\nFirm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants\\nUnderstanding of various analytic and visualization utilities available in R\\nConfigure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build\\nAbility to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access\\nUnderstanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.\\nAble to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output\\nAble to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar\\nAble to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue\\nImplementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data\\nImplement a graph (line or pie etc.) backed by a live (changing) data set, something like \"requests per minute\" or similar\\nUnderstand basic modeling techniques, tools sets. Implement simple Python or R analytic routines\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Data Engineer, you will provide technical leadership to the team that designs and develops path-breaking large scale cluster data processing systems.\\n\\nDesign and develop code, scripts and data pipelines that leverage structured and unstructured data integrated from multiple sources. Software installation and configuration. Participate in and help lead requirements and design workshops with our clients. Develop project deliverable documentation. Mentor junior members of the team in software development best practices. Other duties as assigned.\\n\\nAdditionally, as a senior member of our development &amp; Ops team, you will help establish thought leadership in the big data space by contributing best practices, white papers, technical commentary and representing our section as one Leads.\\n\\nJob Qualifications:\\n\\nProven expertise in production software development\\n7+ years of experience programming in Java, Python, SQL, or C/C++\\nProficient in SQL, NoSQL, relational database design and methods for efficiently storing &amp; retrieving data\\nStrong analytical and science skills\\nCreative problem solver\\nExcellent verbal and written communications skills\\nStrong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..\\nExperience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies &amp; frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)\\n\\nPreferred Knowledge, Skills and Abilities:\\n\\nPrior Data Platform Engineering experience\\nExperience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.\\nDynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)\\nExperience designing and tuning high performance systems\\nPrior experience with data warehousing and business intelligence systems\\nExperience with Elasticsearch, SolrWeb, and Lucene\\nExperience with Star Schema, fact vs dimensions, updates/restatements and views\\nProfessional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform\\nLinux expertise\\nPrior work and/or research experience with unstructured data and data modeling\\nFamiliarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)\\nDemonstrate understanding of \"var\" vs. \"val\", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem\\nFirm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants\\nUnderstanding of various analytic and visualization utilities available in R\\nConfigure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build\\nAbility to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access\\nUnderstanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.\\nAble to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output\\nAble to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar\\nAble to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue\\nImplementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data\\nImplement a graph (line or pie etc.) backed by a live (changing) data set, something like \"requests per minute\" or similar\\nUnderstand basic modeling techniques, tools sets. Implement simple Python or R analytic routines\\n\\nJob Abilities:\\nMust be able to sit for long periods of time working on computers. Must be able to interact and communicate with the senior management in meetings. Must be able to write programming code in applicable languages and write project documentation in English.\\n\\nEducation:\\nBachelor's Degree or foreign equivalent in Computer Science or related technical field followed by six (6-8) years of progressively responsible professional experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).\\n\\nOR\\n\\nMaster's Degree or foreign equivalent in Computer Science or related technical field. Four (4-5) years of experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).\\n\\nEmployer will accept any suitable combination of education, training, or experience.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Data Software Engineer</td>\n",
       "      <td>Pleasanton, CA</td>\n",
       "      <td>Pleasanton</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nOwn product data set features from the development phase through to production deployment.\\nSupport building distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time.\\nDeploy and maintain Hadoop/Big Data/Spark and database storage Infrastructures in AWS cloud.\\nSupport managing data environments and/or data sets to serve a wide range of data users, including but not limited to Data Scientists, Data Analysts, Business Analysts etc.\\nPerform offline analysis of large data sets using components of a big data software ecosystem.\\nEvaluate big data technologies and prototype solutions to improve data processing architecture.\\nMonitor installation of HDFS/Hadoop/Spark and related software releases, third-party utilities with emphasis on overall system performance.\\nSupport the activity of troubleshoot and determine root cause of complex data provenance, metadata issues and engineering questions that may involve interfacing with various technical staff in multiple organizations and with differing levels of expertise.\\nDevelop tools and procedures to monitor and automate system tasks on servers and clusters\\nAuthor extract, transform and load (ETL) scripts for moving and curating data into data sets for storage and use by a datalake, data warehouse and datamart.</td>\n",
       "      <td>Bachelor’s degree in computer science, computer engineering, or a related field, or the equivalent combination of education and related experience.\\n5 years of professional experience as a data software engineer.\\n1 years of experience with AWS/Cloud Big Data computing design, provisioning, and tuning.\\nPrevious experience as a Data Engineer / Database Administrator and/or Business Intelligence Analyst.</td>\n",
       "      <td>Knowledge of database concepts, object and data modeling techniques and design principles.\\nDetailed knowledge of database architectures, software, and facilities.\\nSuccessful history of manipulating, processing, and extracting value from large disconnected data sets.\\nExperience with programming languages – Python (required), Scala, Ruby, R.\\nDatabase technologies - SQL, performance tuning concepts, AWS RDS, RedShift, MySQL.\\nExperience with big data batch processing tools: Hadoop MapReduce, ElasticSearch, PIG, Hive, Cascading/Scalding, Apache Spark, AWS EMR.\\nExperience with stream-processing systems: Kinesis, Kafika, MQTT.\\nExperience with relational NoSQL databases including DyanamoDB.\\nAbility to write JSON, XML, YAML and other data definition schemas.\\nGood verbal and written communication skills necessary to effectively collaborate in a team environment and present and explain technical information and provide advice to management.\\nA seasoned, experienced professional with a full understanding of area of specialization; resolves a wide range of issues in creative ways.\\nWork is independent and collaborative in nature.\\nContributes to moderately complex aspects of a project. Provides regular updates to manager on project status.</td>\n",
       "      <td>Job Summary:\\n\\nResponsible for all aspects of data acquisition, data transformation, analytics scheduling and operationalization to drive high-visibility, cross-division outcomes. Investigate, evaluate, test and recommend technical solutions for future systems. Support software developers, database architects, data scientists on data initiatives and will ensure optimal data delivery architecture.\\n\\nMajor Responsibilities:\\nData Design Management\\nOwn product data set features from the development phase through to production deployment.\\nSupport building distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time.\\nDeploy and maintain Hadoop/Big Data/Spark and database storage Infrastructures in AWS cloud.\\nSupport managing data environments and/or data sets to serve a wide range of data users, including but not limited to Data Scientists, Data Analysts, Business Analysts etc.\\nPerform offline analysis of large data sets using components of a big data software ecosystem.\\nEvaluate big data technologies and prototype solutions to improve data processing architecture.\\nMonitor installation of HDFS/Hadoop/Spark and related software releases, third-party utilities with emphasis on overall system performance.\\nSupport the activity of troubleshoot and determine root cause of complex data provenance, metadata issues and engineering questions that may involve interfacing with various technical staff in multiple organizations and with differing levels of expertise.\\nDevelop tools and procedures to monitor and automate system tasks on servers and clusters\\nAuthor extract, transform and load (ETL) scripts for moving and curating data into data sets for storage and use by a datalake, data warehouse and datamart.\\nTechnical Advisor\\nCollaborate with other teams to deploy data tools and data sets that support both operations and product use cases.\\nEvaluate and advise on technical aspects of open work requests in the product backlog for other projects as assigned.\\nKnowledge/Skill Requirements:\\n\\nKnowledge of database concepts, object and data modeling techniques and design principles.\\nDetailed knowledge of database architectures, software, and facilities.\\nSuccessful history of manipulating, processing, and extracting value from large disconnected data sets.\\nExperience with programming languages – Python (required), Scala, Ruby, R.\\nDatabase technologies - SQL, performance tuning concepts, AWS RDS, RedShift, MySQL.\\nExperience with big data batch processing tools: Hadoop MapReduce, ElasticSearch, PIG, Hive, Cascading/Scalding, Apache Spark, AWS EMR.\\nExperience with stream-processing systems: Kinesis, Kafika, MQTT.\\nExperience with relational NoSQL databases including DyanamoDB.\\nAbility to write JSON, XML, YAML and other data definition schemas.\\nGood verbal and written communication skills necessary to effectively collaborate in a team environment and present and explain technical information and provide advice to management.\\nA seasoned, experienced professional with a full understanding of area of specialization; resolves a wide range of issues in creative ways.\\nWork is independent and collaborative in nature.\\nContributes to moderately complex aspects of a project. Provides regular updates to manager on project status.\\nEducation/Experience Requirements:\\nBachelor’s degree in computer science, computer engineering, or a related field, or the equivalent combination of education and related experience.\\n5 years of professional experience as a data software engineer.\\n1 years of experience with AWS/Cloud Big Data computing design, provisioning, and tuning.\\nPrevious experience as a Data Engineer / Database Administrator and/or Business Intelligence Analyst.\\nPanasonic is proud to be an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability status, protected veteran status, and any other characteristic protected by law or company policy. All qualified individuals are required to perform the essential functions of the job with or without reasonable accommodation. Pre-employment drug testing is required for safety sensitive positions or as may otherwise be required by contract or law. Due to the high volume of responses, we will only be able to respond to candidates of interest. All candidates must have valid authorization to work in the U.S. Thank you for your interest in Panasonic Corporation of North America.\\n#LI-SR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Apple Media Products - Commerce Data Engineer</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Mar 18, 2019\\nWeekly Hours: 40\\nRole Number: 200040350\\nThe Apple Media Products Commerce Engineering team has a proud tradition of delivering state of the art products in a competitive marketplace. We seek to maintain a meaningful and rewarding environment where the best engineers and scientists can collaborate and produce real-world improvements in customers' online experience. You will solve problems unrivaled in scale and concept in the pursuit of new and creative features. We seeking a talented, experienced Applied Researcher/Data Scientist to work on high visibility projects that affect millions of customers globally. At Apple, great ideas have a way of becoming great products, services, and customer experiences very quickly. If you are a self-motivated, high-energy individual who is not afraid of challenges, we're looking for you.\\nKey Qualifications\\nStrong knowledge of coding practices and experience.\\nDeep understanding of the full software development lifecycle.\\nExperience of object oriented programming languages like Scala/Java.\\nExperience of processing large-scale production-level data sets.\\nExperience working in a large code base.\\nExperience of building and running large-scale data pipelines.\\nExperience with large scale data processing frameworks like Spark/Hadoop.\\nExperience with low-latency big data stores, e.g. Cassandra, Voldemort or HBase.\\nHighly self-motivated, results driven and data driven.\\nAbility to stay focused and prioritize a heavy workload while achieving exceptional quality.\\nAbility to work in a fast-paced dynamic environment.\\nAttention to detail, data accuracy and quality of output.\\nFamiliarity with scalability and performance issues.\\nExcellent judgment and integrity with the ability to make timely and sound decisions.\\nExperience in payment science is a plus.\\nDescription\\nThe Apple Media Products Commerce Engineering team is looking for someone with a love for data. This position involves working on very large scale data mining, cleaning, analysis, deep level processing, machine learning or statistic modeling, metrics tracking and evaluation. The goal is to improve Apps Store/iTunes Store payment experience and engagement. You should have a passion for quality and an ability to understand complex systems while being able to iterate quickly on all stages of data understanding and modeling to solve the real-world problems.\\nEducation &amp; Experience\\nMS in Computer Science or related field. PhD preferred.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Sr. Big Data Developer</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What You'll Do\\nAs a Senior Data Engineer of the Portals and Data Analytics Engineering team, you will have a unique opportunity to use your creativity in cloud infrastructure to develop innovative big data analytics platform. We are seeking a talented, self-motivated, and highly creative senior software engineer to join our team to develop new big data platform project.\\nLeads team to develop big data platform, propose solutions and technical evolution, establish Cisco credibility platform for a Cisco Collaboration solution.\\nWorks closely with global team on engagements by estimating the complexity and duration of technical tasks; and providing business value justification and risk evaluations.\\nAdopt new advanced data analytic technical skills and knowledge in the cloud architecture, particularly for collaboration based big data service\\nWho You'll Work With\\nThe Cloud Collaboration Technology Group (CCTG) is a $1B business unit that develops primarily cloud based collaboration software solutions, and is an integral part of the $4.4B Collaboration portfolio at Cisco, which is investing heavily to transform the future of collaboration experiences for our end users. CCTG's vision is to be the recognized industry leader in the Cloud Collaboration market as well as to be the industry beacon that attracts top talent. Our strategy is to sustain innovation in our current $1B product line, and to develop innovative new products for the future that expand and positively disrupt our market. We will do all of this, and build a next generation cloud platform with APIs and an ecosystem that deliver tangible business outcomes enabling our customers to get more jobs done!\\nWho You Are\\nBring a strong perspective that drives change and motivates engineers to develop simple solutions to complex problems. Have deep understanding of data analytics best practices, including skills such as data modeling, data cleaning, data mining, machine learning and data virtualization. Be an expert in below two or more areas\\nA BS/CS/EE Bachelors Degree in addition to a minimum of 8-10 years of distributed systems development experience with strong focus on the design and development of cloud and data analytic applications * Previous experience as hands-on technical lead * Data storage - experienced in implementing Big Data technologies successfully in an enterprise;\\nKnowledge discovery - advanced knowledge in entity and relationship extraction from unstructured data;\\nData governance - experienced in developing and integrating software allowing for flexible and scalable data transformation with data quality controls.\\nData analytics - strong backgrounds in statistics, machine learning and similar technologies.\\nData visualization - knowledge of tools that are cost-effective and make it easy for end users to better understand and produce reports and graphs.\\nHadoop Administration - Contribute to the evolving architecture to meet growth requirements for scaling, reliability, performance and security. * 4-8 years hands on experience with big data tools such as Hadoop, Cassandra, Kafka, Storm, Spark etc * Experienced Java/Scala engineer.\\nFlexible, self-motivated, problem solver who can work both individually as well as an effective team player with excellent attention to detail and great interpersonal skills (both verbal and written)\\n* Excellent communications skills * Be willing/able to work crossing boundary to reach best fit solution in real world cloud service\\nCisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\\nCisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.\\nWe Are Cisco\\n#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.\\nWe embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!\\nBut “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)\\nDay to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.\\nSo, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Big Data Engineer - Masters (Co-Op) – United States</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What You’ll Do\\nDesign and deliver automated transformation of large data sets leveraging MapReduce, streaming, and other emerging technologiesLeverage HBase, Elasticsearch, etc. to ingest transformed data at scaleCollaborate with security experts to deliver high-impact web-based APIs\\n Implement high-volume data integration solutions Analyze, monitor, and optimize for performance\\nProduce and maintain high-quality technical documentation\\nWho You'll Work With\\nJoin us as we transform the world of tomorrow. Develop creative ideas on how to work better and smarter. Influence and participate in top-priority projects that have a real impact.\\nWho You Are\\nCurrently enrolled in an accredited university co-op program pursuing a Master’s degree in Computer Science, Computer Engineering, Electrical Engineering, or a related major such as Math, PhysicsMinimum of a 3.0 GPA or equivalentTrack record of developing technology to enable large scale data transformationStrong Java experience and hands-on Hadoop ecosystem experience – HBase, Hive, Spark, etc.Possess knowledge of software engineering best practicesPassion for solving hard problems and exploring new technologiesExcellent communication and technical documentation skills\\nWhy Cisco\\n#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.\\nWe embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!\\nBut “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)\\nDay to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.\\nSo, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!\\n\\nThis position is available to Master’s level Students. Positions are located East Coast, West Coast and Central US. Not all positions offer sponsorship or are available at all locations. Relocation is available for some locations and or positions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Big Data Engineer, Apple Media Products Analytics</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Nov 1, 2018\\nRole Number: 200007010\\nThe iTunes Store is looking for a top-notch Big Data engineer to develop an analytics infrastructure that will generate insights into customer experiences on products such as the iTunes Store, App Store, and iBookstore. Our products reach hundreds of millions of customers around the world, and have revolutionized how people interact with their music, movies, TV shows, apps, books, and podcasts.\\nKey Qualifications\\nLanguage: Java or Scala\\nWorking knowledge on the following distributed data processing platforms: Spark, Hadoop\\nGreat if you also know: HBase, Kafka, Java Map Reduce\\nAlgorithms: You will be working on developing new algorithms to process large scale data efficiently.\\nWe expect you to know: basic computer science algorithms, data structures and distributed algorithms to process and mine data, e.g. Map Reduce Algorithm\\nGreat, but not required, if you also know about how to develop: graph, data classification and clustering algorithms in distributed environment\\nGood debugging, critical thinking, and communication skills\\nKnowledge in engineering machine learning, feature engineering systems is a plus.\\nAble to gather cross-functional requirements and translate them into practical engineering tasks\\n5+ years of programming experience\\nDescription\\nThe iTunes Store Analytics team is responsible for collecting, analyzing, and reporting on customer experience data. From this data we generate insights into how customers interact with our products, and use these insights to drive improvements to user-facing features.\\nYou will be working on a small team and will be responsible for processing large amounts of data and developing platforms to process, analyze and mine that data to extract intelligence. Prepare data for visualization, ad-hoc exploration, reporting, and further analysis. We are looking for a well-rounded data engineer who has good design sense.\\nThe ideal candidate pays close attention to details - caring about the quality of the input data as well as how the processed data is ultimately interpreted and used. You are also a team player - ready to contribute during design sessions, and able to give and receive constructive code reviews. Your curiosity drives you to explore new technologies and apply creative solutions to problems.\\nEducation &amp; Experience\\nBS degree in Computer Science or a related field\\nAdditional RequirementsBuild large scale data processing, mining and analysis projects and features, ensuring robust &amp; maintainable solutions are implemented with special attention to data quality, performance and usability details.Effectively demonstrate feature prototypes to executivesDevelop, advocate for, and build consensus on, coding best practices.Ability to effectively work with cross functional teams to understand requirements and identify design and engineering impactsExperience with architecting big data and analytical applications that scale to petabytes highly preferred.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Senior Application Support Analyst (Temp. 6 months)</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Named as one of Fortunes’ 100 Fastest Growing Companies for 2019, EPAM is committed to providing our global team of 30,100+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential.\\n\\nDescription\\n\\nYou are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Application Support Analyst. Scroll down to learn more about the position’s responsibilities and requirements.\\n\\nEPAM Systems is seeking a Cloud Platform/Big Data Support Specialist to provide enterprise-level support to customers of a major cloud service provider. As a cloud support specialist, you will work in a team of experienced support engineers to resolve customer's concerns and issues for using cloud platform and big data products.\\n\\nYou would use your technical expertise and communication skills to understand customer's problem, provide technical assistance, then guide them to resolution. You would also participate in discussion with product engineers to share your insights on customer needs and issues to help product improvements. You will be fully exposed to the cutting edge technologies of a prominent cloud service provider, and play a key role in the growth of their cloud computing products.\\n\\n#LI-DNI\\nWhat You’ll Do\\nProvide technical assistance and support over e-mail, chat and phone as part of a global 24x7-support organization\\nProvide initial response to customer's inquiry, troubleshoot, provide updates, identify root case, and resolve the issue to the satisfaction of customer\\nHandle escalation from customer and lead to satisfactory resolution\\nCo-work with engineers across technical and product domains to resolve complex cross-domain issues\\nConsult with senior engineers and subject matter experts (SME) to accelerate problem resolution\\nHand-off or take-over cases to/from other geographical region to provide around-the-clock issue resolution for premium customers\\nFollow communication guidelines and security policies when communicating with customer\\nCategorize support requests for support and service analytics\\nProduce support documents, perform knowledge sharing and training\\nKeep technical skills up to date with latest cloud technologies\\nWhat You Have\\nA degree in an associated field and/or other advanced certification along with significant experience\\nStrong analytical / troubleshooting / problem solving skills\\nStrong verbal and written communication skills\\nExcellent customer service skills\\nAbility to perform job functions under stress and pressure\\nCommitment to continuous self-learning\\nRegular, reliable attendance\\n3+ years of experience as developer or a combination developer + big data engineer\\nProficient in at least one of the following development languages: Java, Python, .NET, Ruby, PHP, Go or Javascript (NodeJS)\\nHands on experience with RESTful APIs\\nExperience with relational databases (e.g. MySQL, PostgreSQL, etc.)\\nExperience with Big Data architectures and technologies and BI solutions\\nExperience in CI/CD, DevOps and related automation tools (e.g. Jenkins, Chef, Puppet, etc.)\\nAbility to read and understand code and able to write code samples to reproduce customer issues\\nAbility to read and understand logs and stack traces to troubleshoot issues\\nGood oral and written business communication skills in English (CEF Level C1 or above)\\nMust be able to work on the following shifts:\\nEarly week shift from 7:00 AM to 6:00 PM, Sunday to Wednesday\\nLate week shift from 7:00 AM to 6:00 PM, Wednesday to Saturday\\nYes, you will work 4 days and take 3 days off\\nMay need to work on public holidays. If worked on a public holiday, you will be provided with a day-off in lieu\\nNice to have\\nBA/BS degree preferred\\n2+ years of customer support experience preferably in Enterprise software support\\nExperience with PaaS and IaaS technologies\\nExperience with distributed computing frameworks (e.g. Hadoop, Spark, Flink, Storm, Samza, Beam, Airflow, Google Big Query, etc.)\\nExperience with distributed data stores (HBase, Cassandra, Riak, Google Bigtable, Amazon Dynamo DB, etc.) and/or distributed message brokers (Kafka, RabbitMQ, ActiveMQ, Google Pub/Sub, Amazon Kinesis, etc.)\\nExperience with ETL processes and tools (e.g. AWS Glue, Google Dataprep and/or Datafusion, MS SSIS, ODI, IPC, etc.)\\nExperience with any ML library (scikit-learn, XGBoost, pytorch, tensorflow, Spark mllib) or basic understanding of ML concepts\\nWhat We Offer\\nMedical, Dental and Vision Insurance (Subsidized)\\nHealth Savings Account\\nFlexible Spending Accounts (Healthcare, Dependent Care, Commuter)\\nShort-Term and Long-Term Disability (Company Provided)\\nLife and AD&amp;D Insurance (Company Provided)\\nEmployee Assistance Program\\nUnlimited access to LinkedIn learning solutions\\nMatched 401(k) Retirement Savings Plan\\nPaid Time Off\\nLegal Plan and Identity Theft Protection\\nAccident Insurance\\nEmployee Discounts\\nPet Insurance\\nEPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Santa Clara, CA 95054</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>95054</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nJob Title: Data Engineer\\nLocation: San Francisco, CA, Austin, TX, San Jose, CA\\nTerms: Full-time\\nAbout Trianz\\nTrianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.\\nWhat We Stand For\\nOur clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.\\nAs a result, Trianz is focusing on three important themes in our engagement model with clients.\\nCrystallize business impact from a top management point of view\\nHelp Clients achieve results from strategy-by making execution predictable through innovative execution techniques\\nCreate a positive, enriching partnership experience in everything we do\\nIndustries, Clients &amp; Practices\\nTrianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:\\nCloud\\nAnalytics\\nDigitization\\nInfrastructure\\nSecurity\\nJob Description\\nOverview\\nData is the way our clients make decisions. It is the core to their business, helping create an experience for customers and providing insights into the effectiveness of our product launch &amp; features.\\n\\nAs a Data Engineer , you will be a part of an early stage team that builds the data pipelines, collection, and storage, and exposes services that make data a first-class citizen. We are looking for a Data Engineer to build a scalable data platform. You'll have ownership of core data pipelines that powers top line metrics; You will also use data expertise to help evolve data models in several components of the data stack; You will help architect, building, and launching scalable data pipelines to support growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, using huge amounts of data to fuel several teams such as Analytics, Data Science, Marketplace and many others.\\n\\nResponsibilities\\n\\nOwner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth\\nEvolve data model and data schema based on business and engineering needs\\nImplement systems tracking data quality and consistency\\nDevelop tools supporting self-service data pipeline management (ETL)\\nSQL and MapReduce job tuning to improve data processing performance\\n\\nExperience\\n\\n3+ years of relevant professional experience\\nExperience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)\\nProficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)\\nGood understanding of SQL Engine and able to conduct advanced performance tuning\\nStrong skills in scripting language (Python, Ruby, Bash)\\n1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)\\nComfortable working directly with data analytics to bridge Lyft's business goals with data engineering\\n\\nWe are Growing Rapidly: 2019 Highlights\\nTrianz is growing above the average of the professional services industry. Here are some highlights.\\nVoted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.\\nWon the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.\\nWon UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.\\nFeatured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.\\nAchieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.\\nTalk to us, Join us &amp; Develop into Leaders\\nCome join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.\\nWe are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!\\nEqual Opportunity Employer\\nTrianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Data Engineer, Hardware Consumer Care</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBuild, own and maintain end to end consumer hardware support operations backend analytics infrastructure.\\nProvide thought leadership around designing the backend data architecture that scales well with the growing needs of the business.\\nWork closely with Product Management, Software Engineering and other cross-functional teams to further improve existing customer relationship management tools for all key channels of customer support.\\nBuild new reporting tools and technologies to help measure quality and efficiency of customer care operations.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum qualifications:\\n\\nBachelor's degree in Data Science, CS, Engineering, Mathematics/Statistics, Operations Research, or equivalent practical experience.\\n9 years work experience. Experience designing and developing data models and data warehouses.\\nExperience in one or more programming languages (e.g., Python, Java, etc.).\\nExperience managing projects, troubleshooting technical issues and working with Engineering/Sales/Services teams and customers.\\n\\nPreferred qualifications:\\n\\nMaster's degree or PhD in a technical or scientific field of study.\\nExperience with Unix or GNU/Linux systems.\\nExperience developing reporting portals for users with various security access levels.\\nExperience in operational analytics and new evolving machine learning space.\\nExpertise in data management and writing/maintaining ETL for structured and unstructured data sources.\\nAbility to communicate complex findings in a structured and clear manner to a non-technical audience.\\nAbout the job\\nWe are a part of Hardware Customer Care team that engages with dedicated customers across the globe through our online resources and communities, social outreach and 1:1 care. We represent the voice of the consumer and work closely with cross-functional partners across Google to make our products and policies better.\\nAs a data analytics and insights group within the Customer Support organization, we are responsible for providing strategic insights, analysis and reporting to various stakeholders in support organization. As a Data Engineer, you’ll be responsible for building and maintaining data analytics infrastructure. This role requires deep technical expertise, shaping discussions on relevant metrics and data signals to define an excellent user support experience.\\n\\nGoogle's mission is to organize the world's information and make it universally accessible and useful. Our Devices &amp; Services team combines the best of Google AI, Software, and Hardware to create radically helpful experiences for users. We research, design, and develop new technologies and hardware to make our user's interaction with computing faster, seamless, and more powerful. Whether finding new ways to capture and sense the world around us, advancing form factors, or improving interaction methods, the Devices &amp; Services team is making people's lives better through technology.\\nResponsibilities\\nBuild, own and maintain end to end consumer hardware support operations backend analytics infrastructure.\\nProvide thought leadership around designing the backend data architecture that scales well with the growing needs of the business.\\nWork closely with Product Management, Software Engineering and other cross-functional teams to further improve existing customer relationship management tools for all key channels of customer support.\\nBuild new reporting tools and technologies to help measure quality and efficiency of customer care operations.\\nAt Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Data Engineer, Calibra (Blockchain)</td>\n",
       "      <td>Menlo Park, CA</td>\n",
       "      <td>Menlo Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.\\nIn June 2019 the Libra Association announced plans for Libra – a new global cryptocurrency that runs on the Libra network. Libra's mission is to create a simple global financial infrastructure that empowers billions of people around the world. It's powered by blockchain technology and the plan is to launch it in 2020.\\n\\n\\nCalibra is a newly formed Facebook subsidiary whose goal is to provide financial services that will enable people to access and participate in the Libra network. Our first product will be a digital wallet for Libra, a new global currency powered by blockchain technology, and it will be available in 2020 in Messenger, WhatsApp, and as a standalone app. Our vision is to reinvent money and transform the global economy so people everywhere can live better lives. Calibra will let you send Libra to almost anyone with a smartphone, as easily and instantly as you might send a text message or photo, and at low to no cost. And, in time, we plan to offer additional services for people and businesses, such as paying bills with the push of a button, buying a cup of coffee with the scan of a code, or riding your local public transit without needing to carry cash or a metro pass.\\n\\n\\nWe are seeking an experienced Data Engineer to join the Calibra Data Engineering team. Do you like working with data? Do you want to use data to influence product decisions for products leveraging fascinating blockchain technology? Our data engineering team works very closely with Product Managers, Data Scientists, Software Engineers, Economic Researchers, Compliance, and Risk Management to build intuitive, secure products to solve some of the most challenging problems, at a scale that few companies can match. This is technically challenging, will have massive global impact, and you will get to work closely with smart folks on an intellectually challenging initiative.\\n\\n\\nThis position is located in our Menlo Park office.\\nRESPONSIBILITIES\\nBuild cross-functional relationships with engineers, product managers, data scientists and other cross- functional partners to understand data needs and deliver on those needs\\nIdentify all possible use cases and business outcomes for allocated areas of ownership\\nPrepare logging specifications and collaborate with engineering teams to instrument logging\\nEnvision and build a self-serve data platform which teams can use to answer their own questions with data\\nIdentify, collect and transform real-world user interaction, API's and server events data into scalable/extensible schema models\\nCreate foundational data capabilities by leveraging your existing data leadership in addition to constantly learning about new technologies\\nDesign and implement scalable data repositories to integrate qualitative and quantitative research data\\nManage the delivery of high impact dashboards and data visualizations\\nInform, influence, support, and execute our product decisions and product launches\\nAlong with Product and Engineering teams, use data to solve problems and identify trends, growth levers, and opportunities\\nDesign, build and launch new data extraction, transformation and loading processes in production\\nWork with data infrastructure to triage infrastructure issues and drive to resolution\\nMINIMUM QUALIFICATIONS\\nBS/BA in Technical Field, Computer Science, Mathematics, or equivalent experience.\\n4+ years experience in the data warehouse space.\\n4+ years experience in custom ETL design, implementation and maintenance.\\n4+ years experience working with either a MapReduce or a MPP system.\\n4+ years experience with schema design and dimensional data modeling.\\n4+ years experience in writing SQL statements.\\nExperience in analyzing data to identify deliverables, gaps, and inconsistencies.\\nExperience in identifying and communicating data-driven insights.\\nExperience in managing and communicating data warehouse plans to internal clients.\\nFacebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Data Engineer - Business Growth</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer - Business Growth: JD.com\\n\\nJD.com is China's largest online retailer and its biggest overall retailer, as well as the country's biggest Internet company by revenue. JD.com sets the standard for online shopping through its commitment to quality, authenticity, and its vast product offering covering everything from fresh food and apparel to electronics and cosmetics. Its unrivaled nationwide fulfillment network provides standard same- and next-day delivery covering a population of more than 1 billion - a level of service and speed that is unmatched globally. In 2017, JD had 292.5M annual active consumers and net revenue of US$55.7B. JD.com is currently on pace to become China's largest B2C e-commerce platform by 2021. JD.com is listed on the NASDAQ under the ticker \"JD\". In 2014, the year of its listing, this was the largest IPO on the NASDAQ. JD.com was the first Chinese Internet company to be included on the Fortune Global 500 List.\\n\\nJD plans to strengthen its leadership position as a technology-driven company over the next 10+ years, with a focus on big data, AI technology and smart logistics, and to become the dominant e-commerce platform in China. To achieve these goals, we aim to more deeply understand consumer behavior in China, better understand what drives purchases, how to forecast demand, and how to target promotions, advertising and other efforts across the whole range of technologies available to sophisticated Chinese consumers today. As global brands enter China and as China in turn becomes more globalized, we will provide innovative ways for these brands to deliver new products, help drive new monetization strategies and provide solutions for brand safety and product authenticity. In addition, we are constantly evolving our already state-of-the-art ad-serving platform to serve digital ads across all of China. For this platform, we are developing sophisticated analytics around attribution, return and exposure. To drive this exciting agenda forward, we are looking for people with strong social science, quantitative, statistical and machine learning skills who can partner with our team in Mountain View and in Beijing to leverage the vast data and computational assets of JD (which now involves collaboration and data sharing with Baidu, NetEase, Quihoo, Sogou, Toutiao and WeChat. This powerful consortium combines the largest search, gaming, mobile-security, news-aggregation and social network companies in China).\\n\\nThe ideal candidate for this position will be quantitatively trained (advanced master's or PhD) with expertise in data science. Individuals who are interested in using computing and data to more deeply understanding consumer behavior, marketplace behavior, competition and who have a passion for solving complex business problems through the combined use of data, technology and strategy will thrive in our environment. They will also be comfortable working cross-functionally and thrive in a fast-paced organization. Interest in e-commerce and in economics/quantitative marketing/business-analytics is a plus.\\n\\nResponsibilities\\n\\n\\nDesign, build and launch new data extraction, transformation and loading pipelines to develop data-based tools for analyzing consumer behaviors.\\nProduce high quality code with reliability and scalability; efficiently collaborate across different teams to ensure smooth transition of POC stage code stacks to productization.\\nAnalyze large-scale structured and unstructured data; develop new machine learning, statistical models and visualization tools to help answer JD advertising and marketing related business questions.\\n\\nMinimum Qualifications\\n\\n\\nMaster's or PhD degree in Computer Science, Statistics, Mathematics or related fields.\\nProficiency in SQL and a Unix/Linux environment for automating processes with shell scripting.\\nDeep understanding of big data technologies (some subset of MapReduce, Hadoop, Pig, Spark, Hive, Kafka, etc.).\\nProficiency in at least one of the following programming languages: Java, C++, Scala, Python, R.\\nSolid foundation in data structures and algorithms, with excellent debugging and troubleshooting skills.\\nExcellent communication skills and a strong team player.\\nInterest in consumer behavior, e-commerce and related business questions.\\nTraining and experience in ML and building statistical models a plus.\\n\\nAbout JD-Business Growth\\n\\nThe Business Growth Business Unit manages JD's ad-business. JD's ad-tech comprises a set of large scale publishing, programmatic ad-exchange, ad-network and data management platforms for serving digital ads on JD-owned and affiliated properties across China. JD Business Group also works on JD's marketing technology platform efforts aimed at helping brands leverage JD and its partners' large-scale data and computing assets to improve their marketing in China.\\n\\nJD Business Growth research group in Silicon Valley is focused on using science to suggest, support and shape new and existing data-driven advertising, e-commerce, and marketing products. We combine large-scale experimentation, statistical-econometric, machine learning and artificial intelligence tools, with social-science, economics and computer science to find innovative ways to drive growth and monetization. Some of the problems we work on include advertising attribution, recommendation systems, advertising targeting, using applied economics, causal inference, deep learning, and reinforcement learning methods.\\n\\nJD.com is an Equal Opportunity Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.\\n\\n------\\n\\n------</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Hiretual is an AI-powered sourcing platform, and has been recognized as one of the best recruiting tools on the market. During 2018, Hiretual achieves 500% growth with minimal focus on sales to date.\\n\\nAs a data engineer engineer, you will join the core engineering team to build scalable and robust data engine towards an AI and data-driven recruiting SaaS. You will be working with top AI researchers and infrastructure gurus to explore unlimited career space.\\n\\nThe core technical skills you should have:\\nStrong computer science fundamentals: algorithms, data structures, and object-oriented programmingStrong coding capability with Pythonmust have 2+ years of experience in data processing pipeline, including data crawling, cleaning, processing, ETLProficient in working variant databases: MySQL, Redis, Cassandra, Elasticsearch, graph databases.Proficient with big data processing frameworks: Spark, Hadoop, Hive, Kafka, EMRWriting scalable REST APIs for web services\\nBenefits\\nUnlimited growth/promotion spaceCompetitive salary and options401k matching programFree meals, snacks, and drinksComprehensive medical, dental, and life insurancePTO policyCommuter benefitsFun, collaborative, and energetic team environment with nice office environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Mateo, CA</td>\n",
       "      <td>San Mateo</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are changing the way healthcare uses data. Currently, about 80% of healthcare data is underused because it is too messy and unstructured for humans to efficiently analyze. The healthcare industry needed an intelligent technology that could extract insights quickly and accurately. And that is where we came in. Apixio augments the ability to read, decipher, and understand patient information. This ultimately translates into better care delivery, lower costs, and streamlined processes.\\n\\nThe opportunity at Apixio:\\nApixio is a mission-driven data science company passionate to make the practice of medicine more science than art. We are looking for a high-level Data Engineer, who shares our vision, to join our winning team and help us make a difference. This role will be filled by someone who can gain a clear understanding of the current infrastructure and be able to understand how to align with the needs and objectives of the science team. Abilities for training and testing will come into play with their motivation to make constant improvements.\\n\\nWe will be asking you to:\\n\\nBuild and maintain Scala spark / Hadoop pipelines at Apixio that are involved in our training / testing infrastructure for machine learning models\\nBuild and maintain Spark pipelines for ETL of data that write to our data warehouse and maintain the Apixio data warehouse\\nContribute to design and implementation of software pipelines that enable Apixio to update its internal proprietary algorithms in a scalable and reliable manner\\nContribute to our core code base that performs machine learning in a production setting\\n\\nTo be effective at this role you will need to be:\\n\\nCurious, willing to learn new things and try new things\\nData Driven\\nCommunicative - (willing to stand by decisions / ideas that they have)\\n\\nWe are assuming that you have:\\nA degree in computer science\\n\\n\\nExperience working with Scala / Java (min 2 years) in a professional setting\\nExperience working with Big Data Technologies (Hadoop, spark, etc… )\\n\\nAbout Apixio:\\nWe are a mission-driven company developing insights from data for a healthier world. We are helping people make better decisions using artificial intelligence to deliver quality healthcare. We are the technology leader in our space and growing at a 60% year over year rate and show no signs of slowing down. Our success is driven by a team of experienced, passionate and fun engineers, data scientists and business professionals that are working to solve some of the most complex problems, with some of the most innovative technologies and techniques in AI and machine learning. We are well funded by leading organizations such as Bain Capital and SSM, and serve more than 35 national and regional health plan and provider clients, and growing.\\n\\nWhat Apixio can offer you:\\n\\nMeaningful work to improve the healthcare industry\\nCompetitive compensation, including pre-IPO equity\\nExceptional benefits, including medical, dental and vision, FSA\\n401k\\nCatered, free lunches\\nParties, picnics and Friday Happy Hour\\nGenerous Vacation Policy\\nFree Parking\\nSubsidized Gym membership/Wellness Program\\nModern open office in beautiful San Mateo, CA\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Senior Cloud Data Engineer</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDesign and develop framework to automate data ingestion and integration of structured data from a wide variety of enterprise data sources, at scale.\\nDesign and develop data pipeline components and integrate them with the Splunk and other ETL Platforms.\\nDesign data quality monitoring and automated data cleaning.\\nAssist the business liaison and ETL function with data related issues such as assessing data quality, data consolidation, evaluating existing data sources, etc.\\nExperience with handling large data infrastructure platform and driving stability through automated monitoring, alerting, and actions.\\nExperience developing for, configuring, and supporting Cloud computing solutions</td>\n",
       "      <td>\\nBachelor degree in Computer Science or related field</td>\n",
       "      <td>\\nExperience with building scalable and reliable data pipelines using Data engine technologies like APIs, AWS Redshift, Snowflake, Talend.\\n8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.\\n4+ years experience designing and developing complex ETL/ELT programs with Python, Visual ETL Tools etc\\n3+ years experience developing complex SQL\\nExperience using Cloud Storage and computing technologies such as RedShift, Snowflake\\n3+ years experience programming in Python\\n2+ years experience with Bitbucket\\n2+ years experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues\\n2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)\\nExperience with API based integration from multiple SaaS data sources\\nExperience developing and implementing streaming data ingestion solutions\\nExperience in Agile methodology (2+ years)</td>\n",
       "      <td>Job Description: That’s a cool job - I want it!\\nReady to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year.\\nAs a Cloud Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data lake solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions.\\nResponsibilities: I want to and can do that!\\nDesign and develop framework to automate data ingestion and integration of structured data from a wide variety of enterprise data sources, at scale.\\nDesign and develop data pipeline components and integrate them with the Splunk and other ETL Platforms.\\nDesign data quality monitoring and automated data cleaning.\\nAssist the business liaison and ETL function with data related issues such as assessing data quality, data consolidation, evaluating existing data sources, etc.\\nExperience with handling large data infrastructure platform and driving stability through automated monitoring, alerting, and actions.\\nExperience developing for, configuring, and supporting Cloud computing solutions\\nRequirements: I’ve already done that or have that!\\nExperience with building scalable and reliable data pipelines using Data engine technologies like APIs, AWS Redshift, Snowflake, Talend.\\n8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.\\n4+ years experience designing and developing complex ETL/ELT programs with Python, Visual ETL Tools etc\\n3+ years experience developing complex SQL\\nExperience using Cloud Storage and computing technologies such as RedShift, Snowflake\\n3+ years experience programming in Python\\n2+ years experience with Bitbucket\\n2+ years experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues\\n2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)\\nExperience with API based integration from multiple SaaS data sources\\nExperience developing and implementing streaming data ingestion solutions\\nExperience in Agile methodology (2+ years)\\nEducation: Got it!\\nBachelor degree in Computer Science or related field\\nWe value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which you are applying.\\nFor job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Azure Data Engineer</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of consulting or client service delivery experience on Azure\\n</td>\n",
       "      <td>DevOps on an Azure platform</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\n</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n People in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications\\n\\n Role &amp; Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts\\n\\n (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions\\nExtensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.\\nMinimum of 5 years of RDBMS experience\\nExperience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training &amp; Certification\\nMCSE Cloud Platform &amp; Infratsructiure Training &amp; Certification\\nMCSD Azure Solutions Architect Training &amp; Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nIoT, event-driven, microservices, containers/Kubernetes in the cloud\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Los Altos, CA 94022</td>\n",
       "      <td>Los Altos</td>\n",
       "      <td>CA</td>\n",
       "      <td>94022</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proficient using Python / SQL or equivalent technologies for complex data manipulations and transformations\\nComfortable with shell scripting, preferably Linux\\nFamiliarity with stream processing (Kafka) is desirable\\nAbility to drive and implement ideas in a fast-paced environment.\\nEffective project management, interpersonal and organizational skills.\\nDeveloped communication, problem-solving and analytical skills.</td>\n",
       "      <td>Build, maintain and enhance a data pipeline and analytics warehouse used to serve reporting and insights.\\nImplement batch and streaming ETL processes for ingesting data from heterogeneous sources\\nDeploy capabilities to allow internal and external customers to access the data for insights to guide customer success initiatives, product development, and corporate marketing efforts.\\nBuild capabilities to enable the organization to develop and deploy machine learning applications at scale.\\nInvolvement in all phases of software development from review of functional specification through assisting with test plans and final QA cycle. Actively participates in monitoring and troubleshooting of production platform related issues\\nPerform day-to-day tasks that ensure technology platform remains stable and available to users. Closely work with cross functional team to enhance systems, trouble-shoot data issues, etc.</td>\n",
       "      <td>\\nBachelor's degree in Computer Engineering or equivalent is desired</td>\n",
       "      <td>None Found</td>\n",
       "      <td>FinTech is a fast-paced, rapidly growing, and engaging space. Jemstep is a market-leading FinTech provider of digital advice solutions to investment advisory firms including banks, broker dealers, and independent advisors. Our Advisor Pro solution enables firms and their advisors to connect with investors digitally, delivering investment advice to help them achieve their financial goals. We are helping banks and financial advisors transform their business, making it more efficient for firms to serve their clients and onboard new ones. While most 'robo' advice solutions in market have been designed to replace advisors, our solution is designed to help advisors by extending their reach and enriching their clients' experience. We firmly stand behind our platform's flexibility, ease of integration and adoption, and value in delivering measurable results with a full suite of technology solutions.\\n\\nIn this position, you'll be joining a high-growth company brimming with intelligent people, optimism, collaboration, and passion for progress, with ties to Invesco, a globally recognized and established asset management firm with a long history of success. Invesco has presence in over 25 countries and Jemstep has offices throughout the United States, South Africa, and India. As we further our mission and expand our presence and position, we're seeking individuals of demonstrable success, with energetic, motivating personalities to join our team.\\n\\nJob Purpose (Job Summary):\\n\\nAs a Data Engineer you will be responsible for building, expanding, and optimizing Jemstep's data and data pipeline architecture. You will contribute to and support Software Development, Product Management, and Customer Success initiatives that enable the company to make data-based decisions. You will need strong strategic, collaboration and communication skills, as well as an entrepreneurial mindset.\\n\\nKey Responsibilities / Duties:\\nBuild, maintain and enhance a data pipeline and analytics warehouse used to serve reporting and insights.\\nImplement batch and streaming ETL processes for ingesting data from heterogeneous sources\\nDeploy capabilities to allow internal and external customers to access the data for insights to guide customer success initiatives, product development, and corporate marketing efforts.\\nBuild capabilities to enable the organization to develop and deploy machine learning applications at scale.\\nInvolvement in all phases of software development from review of functional specification through assisting with test plans and final QA cycle. Actively participates in monitoring and troubleshooting of production platform related issues\\nPerform day-to-day tasks that ensure technology platform remains stable and available to users. Closely work with cross functional team to enhance systems, trouble-shoot data issues, etc.\\n\\nWork Experience / Knowledge:\\nAt least 5 years of hands on experience in a data engineering capacity\\nExperience with digital marketing technologies (e.g. web analytics, email marketing platforms)\\nExperience implementing BI solutions (e.g. Tableau, Qlik, Looker) and scaling to large user audiences\\n\\nSkills / Other Personal Attributes Required:\\nProficient using Python / SQL or equivalent technologies for complex data manipulations and transformations\\nComfortable with shell scripting, preferably Linux\\nFamiliarity with stream processing (Kafka) is desirable\\nAbility to drive and implement ideas in a fast-paced environment.\\nEffective project management, interpersonal and organizational skills.\\nDeveloped communication, problem-solving and analytical skills.\\n\\nFormal Education: (minimum requirement to perform job duties)\\nBachelor's degree in Computer Engineering or equivalent is desired\\n\\nFLSA (US Only): Exempt\\n\\nThe above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.\\n\\nInvesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Palo Alto, CA 94303</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>94303</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Bill.com\\n\\nBill.com is a leader in financial process automation for small businesses and mid-size companies. Making it simple to connect and do business, the Bill.com Back Office Cloud digitizes, automates and simplifies legacy payment and financial processes. With an integrated, end-to-end platform, Bill.com leverages artificial intelligence to reduce manual work, and provides a cloud workspace to help run your business anytime, anywhere. The company partners with four of the largest U.S. financial institutions, more than 70 of the top 100 U.S. accounting firms, and major accounting software providers. Bill.com manages more than $70B in annual payment volume across ACH, virtual cards, checks, and international payments. The company has offices in Palo Alto, California and Houston, Texas. For more information, visitwww.bill.com or follow @billcom.\\n\\nMission: Bill.com moves over $60B per year and we have 10 years worth of customer data. We are leveraging this data to make data driven decisions, and apply data science and machine learning to solve a variety of tough problems. We are in the middle of a large-scale transformation to the public cloud and are developing data pipelines, data warehouse, and machine learning infrastructure in AWS.\\n\\nData engineers at Bill.com will be responsible for building data pipelines and the infrastructure to enable data science, data analytics, and machine learning at scale in AWS. Some of the problems we are currently working on include: detecting payment fraud, extracting semantic data from customer documents, and increasing customer acquisition through advanced analytics. Data engineers will own and build the data platform that makes all of this possible. We have multiple positions available at different levels of seniority.\\nProfessional Experience/Background to be successful in this role:\\n5+ years of experience owning and building data pipelines.\\nExtensive knowledge of data engineering tools, technologies and approaches\\nAbility to absorb business problems and understand how to service required data needs\\nDesign and operation of robust distributed systems\\nProven experience building data platforms from scratch for data consumption across a wide variety of use cases (e.g data science, ML, scalability etc)\\nDemonstrated ability to build complex, scalable systems with high quality\\nExperience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases.\\nExperience with specific AWS technologies (such as S3, Redshift, EMR, and Kinesis) a plus\\nExperience in SQL and one or more of Python, Java and Scala\\nExpected Outcomes:\\nDesign and implement data infrastructure and processing workflows required to support data science, machine learning, BI and reporting in AWS\\nBuild robust, efficient and reliable data pipelines consisting of diverse data sources\\nDesign and develop real time streaming and batch processing pipeline solutions\\nOwn the data expertise and data quality for the pipelines\\nDrive the collection of new data and refinement of existing data sources\\nIdentify shared data needs across Bill.com, understand their specific requirements, and build efficient and scalable pipelines to meet various needs\\nBuild data stores for feature variables required for machine learning\\nBill.com Culture:Humble – No egoFun – Celebrate the momentsAuthentic – We are who we arePassionate – Love what you do\\nDedicated – To each other and the customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Data Engineer / Scientist (Spark + AI 2019)</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Apr 23, 2019\\nRole Number: 200044728\\nWe are looking for excellent software engineers with experience in big data engineering. You will have the opportunity to engage with exciting new-product teams around Apple, and use your data science, systems, machine learning and artificial intelligence skills to tackle challenging technical problems in our next generation products that will delight millions of people.\\nWe are hiring in Cupertino, Seattle, and Pittsburgh.\\nKey Qualifications\\nStrong programming skills in C++, Java, Scala, or Python\\nDeep understanding of basic data structures and algorithms\\nExperience with scaling data platforms to hundreds of terabytes or petabytes using Spark or Hadoop\\nFamiliarity with modern machine learning techniques\\nCreative, collaborative, &amp; product focused\\nCurious about new technologies and passionate about exploring new use cases\\nDescription\\nAt Apple, you will design, develop and deploy large scale services and platforms. You will also collaborate with teams across Apple, who are building the newest, most compelling intelligent applications in the world. You will have strong engineering and communication skills, as well as a belief that data driven processes lead to great products.\\nEducation &amp; Experience\\nB.S., M.S., or PhD in Computer Science, Computer Engineering, Statistics, Bioinformatics, Applied Mathematics, or equivalent practical experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>San Jose, CA 95110</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95110</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMaster's Degree in Computer Science or related field\\nSoftware Engineering, Object-Oriented Design\\nStrong coding skills\\nJava, Python, RESTful API, SQL\\nLinux Shell, GIT\\nExcellent problem-solving skills and analysis\\nTechnical Documentation\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDemonstrates up-to-date expertise in Software Engineering and applies this to the development, execution, and improvement of action plans.\\nProvides and supports the implementation of business solutions.\\nProvides support to the business for new and existing systems.\\nTroubleshoot system production issues.\\nWork with Data Engineer to automate solutions where appropriate\\nWork on automation of data pipelines and ETLs\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We’re Eargo and we’re on a mission to disrupt and transform the hearing health industry! We’re looking for passionate individuals who jump out of bed in the morning dreaming of new ways to improve the lives of our customers.\\n\\nWe’re tech geeks and dreamers. We believe that every voice is unique and every idea is worth talking about. Moreover, we believe in passion and having fun. Because it takes passion to change the world and if you have passion in what you’re doing, you naturally have fun doing it. But, we do have a serious side. Over 48 million Americans currently experience hearing loss and 80% of them do nothing about it. More often than not it’s related to social stigma. This is why we exist…to give people their hearing without the social stigma so that they can go on being the cool person they are. That’s why we get out of bed in the morning and we take that seriously.\\n\\nDoes this sound like a place you want to work? Well, you’re in luck! We’re looking for some great minds to join us on our journey.\\n\\nWe are seeking self-driven and self-starter Software Engineer to join the Business Systems and Data Science Team. The Software Engineer will be responsible for implementing new business solutions on custom Integration Platform. The ideal candidate will have a background in Computer science or related field with experience in Java, Python, and RESTful APIs.\\n\\nKey Responsibilities:\\n\\nDemonstrates up-to-date expertise in Software Engineering and applies this to the development, execution, and improvement of action plans.\\nProvides and supports the implementation of business solutions.\\nProvides support to the business for new and existing systems.\\nTroubleshoot system production issues.\\nWork with Data Engineer to automate solutions where appropriate\\nWork on automation of data pipelines and ETLs\\n\\nMinimum Qualifications:\\n\\nMaster's Degree in Computer Science or related field\\nSoftware Engineering, Object-Oriented Design\\nStrong coding skills\\nJava, Python, RESTful API, SQL\\nLinux Shell, GIT\\nExcellent problem-solving skills and analysis\\nTechnical Documentation\\n\\nPreferred Qualifications:\\n\\nKnowledge of database technologies like MySQL, DynamoDB\\nKnowledge of AWS\\nWorking with IDE's such as Eclipse, IntelliJ\\nSelf-starter who is capable of taking innovative initiatives\\nExperience with startup companies\\nExperience with ETL tools like Talend, Pentaho, SSIS etc.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Data Strategy Specialist - Business &amp; Data Analysis, Cloud, AWS, Azure, Big Data</td>\n",
       "      <td>San Jose, CA 95113</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95113</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe North America Data Strategy &amp; Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.\\n\\n Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model\\n\\nRequired Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:\\no Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling\\no Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.\\no Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel\\n Able to travel up to 100% (Mon-Thu)\\n\\nOptional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model\\n Experience in compiling business cases and roadmaps for data, analytics and technology investments\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Staff Cybersecurity Data Engineer</td>\n",
       "      <td>Fremont, CA</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Staff Cybersecurity Data Engineer\\nEnphase Energy is looking for an experienced Staff Cybersecurity Data Engineer. This position is a collaborative role between our CTO and Software Engineering teams focusing on the security and resiliency of Enphase’s energy management systems. The ideal candidate is an experienced, senior cybersecurity data engineer who can help acquire new security infrastructure and application data, based on multiple open source technologies and proprietary components, to promote early detection of cybersecurity/Advanced Persistent Threats (ATPs) to our globally distributed fleet of IoT customer assets. This role will require you to research and provide designs and prototype implementations for packaging, forwarding, ingesting, indexing, querying and defending massive data flows from globally distributed sources. Enphase Energy is committed to the highest standards of privacy and cyber-security, and you possess the experience and the personal drive to build compliant systems that wrangle data safely. Your work products will produce an operationally efficient data analysis system which enjoys the benefits of your strong skills in machine learning, multi sensor fusion, and the design of game-changing visualizations.\\nResponsibilities\\nArchitecting and implementing an advanced cybersecurity data pipeline for SecOps infrastructure to ingest, index, package, forward, query and defend massive data flows from globally distributed sources enabling the detection of intrusions and driving remediation responses\\nCollaborate with embedded software engineering to design flexible data capture sources with multiple collection methodologies for an international IoT fleet\\nIdentify technologies that enable rapid and continuously evolving suite of analyses on our data repository, from home-grown machine learning to third party integrations\\nEngage monitoring personnel in design and deployment of a 24/7 visualization system that enables a tight analyst-to-designer feedback loop\\nPrototype implementations for data collection frameworks in the cloud\\nDefine principles of fault tolerance to create a system that side-steps outages\\nProvide cloud and embedded system architects with design constraints based on data handling and privacy compliance best practices\\nParticipate in security certification, generating technical documentation, presenting to internal and external customers\\nRequirements\\nMust have a BS in Computer Security, Computer Science, Software Engineering or other related fields with a minimum of 8 years industry experience in software and big data engineering role\\nStrong experience with host-based and network telemetry used by security analytics platforms to detect cybersecurity/APT threats\\nExperience with object-oriented and scripting languages such as Python and JavaPrior experience with data management at scale in at least one public cloud vendor\\nGood understanding of in cloud technologies, such as ELK, Apache Kafka, Hadoop, MapReduce, HIVE, PIG, Apache Spark, AWS Kinesis, Glue, S3, Athena, Redshift\\nWork experience with machine learning applied to product workflows\\nKnowledge of NoSQL Database systems like MongoDB or CouchDB, including Graph Databases\\nProject experience working with the nuts and bolts of big data management including clusters, sharding, and planning\\nAdvantage but not required\\nAdvantageous to have experience working in or performing supporting engineering for a modern Security Operations Center, especially work in using data for threat mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Earnin:\\nEvery year, while Americans wait for their paychecks, more than $1 trillion of their hard-earned money is held up in the pay cycle. As a result, we accumulate over $50 billion in late and overdraft fees and turn to high-interest loans. Overdraft charges and bank fees often trap people in a cycle of debt that can lead to unhealthy decisions and falling victim to predatory businesses disguised as helpful services. We don't accept that.\\n\\nEarnin is an app that creates products that help people gain control of their finances. Cash Out lets people get paid as soon as they leave work, with no fees, interest, or hidden costs. With Health Aid, Earnin negotiates on behalf of community members to lower their total unpaid medical bill and work out a budget-friendly payment plan. Cash Back Rewards is a way for members to earn up to 10% cash back on purchases from over a thousand local and national businesses without needing a credit card or having to reach spend thresholds to earn cash rewards — and they can withdraw the money at any time. We also offer free tools to help avoid overdrafts, to remind people when recurring bills are due, and we're working on more! There is never any required cost to use any of these products or services, users can choose to tip what they think is fair to support the service and pay it forward to keep the movement going.\\n\\nEarnin is supported by funding partners including Andreessen Horowitz, Matrix Partners, Ribbit Capital, Felicis Venture, Thrive Capital, and others. Join us and help build a new financial system focused on fairness and people's needs.\\n\\nYou can help make a difference.\\n\\nAbout the Team:\\nWe are a data driven mobile financial tech company and we're looking for a Data Engineer to join us and help us build out our data infrastructure to aid in our mission of enabling people to gain access to their paycheck on demand.\\n\\nData engineers are an important function to interact with every team within Earnin and you will be interfacing heavily with our analytics, engineering, and data science teams to help them advance our product utilizing machine learning intelligence.\\n\\nAs a Data Engineer you will:\\n\\nFocus on designing, building, and launching efficient and reliable data infrastructure to scale and compute for our business\\nHelp us build a world class data lake/data warehouse, by building data pipelines\\nDesign and develop new systems and tools to enable folks to consume and understand data faster\\nUse your expert coding skills across a number of languages from Python, Java, C++, Go etc.\\nWork across multiple teams in high visibility roles and own the solution end-to-end\\nDesign, build and launch new data extraction, transformation and loading processes in production\\nWork with data infrastructure to triage infra issues and drive to resolution.\\n\\nSome skills we consider critical to being a Data Engineer:\\n\\nBS or MS degree in Computer Science or a related technical field\\nFamiliarity with Python\\nFamiliarity with Hadoop stack, Spark, AWS Glue, AWS Athena etc\\nDiverse data storage technologies (RDBMS, Sql Server, Mysql, ElasticSearch, dynamodb, s3 etc.)\\nDeep familiarity with schemas, metadata catalogs etc.\\nAbility to manage and communicate data warehouse plans to internal clients\\nStrong communication skills, including the ability to identify and communicate data driven insight\\n\\nEarnin does not unlawfully discriminate on the basis of race, color, religion, sex (including pregnancy, childbirth, breastfeeding or related medical conditions), gender identity, gender expression, national origin, ancestry, citizenship, age, physical or mental disability, legally protected medical condition, family care status, military or veteran status, marital status, registered domestic partner status, sexual orientation, genetic information, or any other basis protected by local, state, or federal laws. Earnin is an E-Verify participant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Fremont, CA</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer\\n6 Months+\\nSanta Clara CA\\n\\n5+ years of experience with Hadoop or Cloud TechnologiesExpert level building pipelines using Apache Beam or Spark using Dataflow or DataProcExperience in building ETL using data from Big Query. Should have prior projects experience on GCP projects.Expert in reading and writing Scala, Python or Java code.Familiarity with GCP services, preferably having supported deployments on one or more of GCP servicesExperience in Managing code in GithubExperience with all aspects of DevOps (source control, continuous integration, deployments, etc.)Advanced knowledge of the Hadoop ecosystem and Big Data technologiesHands-on experience with the Hadoop ecosystem (HDFS, Hive, Impala, Spark, Kafka, Kudu, Solr)Knowledge of agile(scrum) development methodologyStrong development/automation skillsSystem level understanding - Data structures, algorithms, distributed storage &amp; computeCan-do attitude on solving complex business problems, good interpersonal and teamwork skills\\n\\n\\nPrimary Location: US-CA-Fremont\\nSchedule: Full Time\\nJob Type: Experienced\\nTravel: No\\nJob Posting: 05/06/2019, 3:54:04 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Senior Scala Data Engineer</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Scala Data Engineer – HPE InfoSight\\n\\nHPE is seeking an outstanding software engineer to play a key role in helping the HPE InfoSight team build AI for the datacenter. HPE InfoSight allow HPE partners and customers to optimize, manage, and protect their datacenter infrastructure, while helping customer support, engineering, and sales to deliver more value to our customers.\\n\\nYou will be joining a small start up within HPE, agile, empowered team, focused on analyzing call-home data sent from HPE storage and enterprise products to provide business value through analytics. The team leverages a modern big-data and microservice-based technology stack for our end-to-end data processing, analysis, API, and web application – to provide our users with the insights they need to be successful.\\n\\nResponsibilities\\n\\nTechnical contributor as a full-stack developer in a small, cross-functional development team, focused on providing data analytics as a service to internal and external HP customers.Contribute to the continuous improvement of our IoT analytics platform, powered by Scala, Spark, Mesos, Akka, Cassandra, Kafka, Elasticsearch, and Vertica.Develop unit, integration, system or any tests that are needed to help the team deliver value quickly, with high quality, to our customers.Leverage big-data technologies for data analytics, including Hadoop/Spark, Vertica (SQL), and Elasticsearch.Develop automation for continuous delivery, testing, and monitoring of our application and infrastructure, using Mesosphere DCOS, Jenkins, Ansible, Kibana, and others.\\nEducation and Experience\\n\\nBachelor/Master's in Computer Science/Engineering, or equivalent, and a minimum of 5-7 years’ experience.\\n\\nKnowledge and Skills\\nTeam player with a passion for learning, programming, automation, and data analytics.Excellent programming skills, with experience or an interest in learning functional programming.Excellent analytical and problem solving skills.Excellent communications skills.\\nWe are looking for a candidate with some or all of the following:\\n\\nExperience building a data pipeline using Scala, Java, or Python, preferably with Spark and KafkaData analytics experience with SQL, NoSQL, Hadoop, or ideally Spark.Machine learning experienceLinux development or system administration experience, including Python or BASH scripting.Automation experience with Ansible, Chef, Puppet, or other.\\n1038195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Senior Data Engineer, Customer Experience</td>\n",
       "      <td>Los Gatos, CA 95032</td>\n",
       "      <td>Los Gatos</td>\n",
       "      <td>CA</td>\n",
       "      <td>95032</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Los Gatos, California\\nData Science and Engineering\\nNetflix is revolutionizing entertainment and shaping the evolution of storytelling around the world. With over 150 million members in 190 countries, we are focused on delivering an incredible customer experience.\\n\\nWhen someone encounters a problem watching their favorite Netflix show, the data products we build and teams we support get members back to streaming as soon as possible. We collect millions of data points via phone, chat, and social media to assess customer feedback in 25 different languages. This allows us to measure the effectiveness of different customer service strategies, detect issues to proactively assist members with personalized support, and test different paths for members to easily discover useful resources.\\n\\nIn this role, you will collaborate with a team of data engineers to build reliable, scalable data pipelines using Apache Spark/Flink. These pipelines will power analytic dashboards, custom viz applications with different storage engines (Snowflake, Druid, Elasticsearch), A/B experiments, feature generation for training production ML models, and NLP driven insights to better understand customer issues.\\nWho you are:\\nYou have a strong background in distributed data processing (Batch or Streaming).\\nYou have extensive data modeling skills. You design structures that are adaptable to changes in the source data or business processes.\\nYou are a technical thought leader with a perspective on how to build great data products. You can adopt and help evolve our engineering best practices.\\nYou are proficient in at least one major programming language and are passionate about writing clean, supportable code.\\nYou are an advocate for data quality. You have a strong opinion on when data audits, unit tests, and documentation can be most effective.\\nYou have strong SQL skills.\\nYou have strong communication skills to effectively partner with data scientists and engineering stakeholders.\\nYou are curious about the rapidly evolving technologies in this domain. You are eager to learn and master new tech when it can have a big impact on our team.\\nYou can relate to many of the aspects of the Netflix culture and love to operate independently while collaborating and giving/receiving strong, candid feedback to your team members.\\nAPPLY NOW\\nShare this listing:\\nLINK COPIED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nJoin an agile SaaS team to design, develop and maintain features and iteratively deploy services using Infoblox’s cloud-based architecture\\nDesign and implement components of our Next Generation Platform\\nRecommend ways to improve data reliability, efficiency and quality\\nExpand and grow data platform capabilities to solve new data problems and challenges\\nBuild large-scale data processing systems using cloud computing technologies\\nBuild high-performance algorithms, prototypes, and proof of concepts\\nApply complex big data concepts with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights\\nWork closely with various cross functional product teams\\nStay current on key trends especially in the area of technologies and frameworks like: Mesos/Marathon, Kubernetes, Docker etc.</td>\n",
       "      <td>\\nBachelor’s degree in CS, CE or EE is required\\nMasters in CS, CE, or EE is preferred</td>\n",
       "      <td>\\n8+ years experience, 2+ in Big Data Engineering\\nProficient in Java, Scala, Golang, or Python\\nGood understanding of Microservices architecture\\nExpertise in BigData - MapReduce, HIVE, HBase, Spark streaming, Apache Flink, Storm, Kafka, In memory Database, JMS\\nExperience with NoSQL databases such as Cassandra/DynamoDB\\nGood exposure in application performance tuning, memory management, scalability\\nAbility to design highly scalable distributed systems, using different open source technologies\\nExperience building high-performance algorithms</td>\n",
       "      <td>Infoblox is the global leader in providing actionable network intelligence through network services, security and threat intelligence. We give companies total control and visibility of their network, allowing them to operate more efficiently and intelligently.\\nWe are looking for a Staff Software Engineer to join our SaaS Next Generation Platform Team in Santa Clara, CA. In this role, you will be responsible for developing, maintaining, evaluating and testing big data technologies. Our organization is extremely data driven where technical innovations happen and you will have an opportunity to use cutting edge technology across all stages of development lifecycle and be part of our exciting and innovative initiatives.\\nResponsibilities:\\n\\nJoin an agile SaaS team to design, develop and maintain features and iteratively deploy services using Infoblox’s cloud-based architecture\\nDesign and implement components of our Next Generation Platform\\nRecommend ways to improve data reliability, efficiency and quality\\nExpand and grow data platform capabilities to solve new data problems and challenges\\nBuild large-scale data processing systems using cloud computing technologies\\nBuild high-performance algorithms, prototypes, and proof of concepts\\nApply complex big data concepts with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights\\nWork closely with various cross functional product teams\\nStay current on key trends especially in the area of technologies and frameworks like: Mesos/Marathon, Kubernetes, Docker etc.\\nRequirements:\\n8+ years experience, 2+ in Big Data Engineering\\nProficient in Java, Scala, Golang, or Python\\nGood understanding of Microservices architecture\\nExpertise in BigData - MapReduce, HIVE, HBase, Spark streaming, Apache Flink, Storm, Kafka, In memory Database, JMS\\nExperience with NoSQL databases such as Cassandra/DynamoDB\\nGood exposure in application performance tuning, memory management, scalability\\nAbility to design highly scalable distributed systems, using different open source technologies\\nExperience building high-performance algorithms\\nEducation\\nBachelor’s degree in CS, CE or EE is required\\nMasters in CS, CE, or EE is preferred\\nIt’s an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologies—and having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox team—our future looks bright, and so will yours. To check out what it’s like to be a Bloxer click here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Data Engineer who is excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data infrastructure across our products and internal tools. At Applied, we encourage all engineers to take ownership over technical and product decisions, closely interact with users to collect feedback, and contribute to a thoughtful, dynamic team culture.\\n\\nAt Applied, you will\\nDesign powerful data pipelines that process fast sensor streams, leverage appropriate data stores, and offer easy-to-use APIs\\nDevelop and deploy high-quality software using modern tooling and frameworks\\nWork with products and teams across Applied Intuition\\nWork with customers across the AV ecosystem to understand their needs and the innards of their data systems\\n\\nWe’re looking for someone who\\nHas 1.5+ years experience building scalable big data pipelines\\nHas experience with open source data processing frameworks (Spark, Kafka, etc.)\\nHas experience with different data storages (e.g., relational and NoSQL)\\nHas experience with containerization and other modern software development workflows\\nTakes initiative and ownership in a fast-paced environment\\n\\nNice to have\\nExpertise with multiple modern programming languages (Python, C++, Go, etc.)\\nPrior work in enterprise software, including on-prem and/or cloud deployments\\nPrior work in either autonomy or simulation products\\n\\nAutonomy is one of the leading technological advances of this century that will come to impact our lives. The work you’ll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting edge technology while working with major players across the industry and the globe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Newark, CA</td>\n",
       "      <td>Newark</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor or Masters in Software Engineering or Computer Science\\n+4 years of experience in Data Engineering and Business Intelligence.\\nProficient in IoT tools such as MQTT, Kafka, Spark\\nProficient with AWS, S3, Redshift\\nExperience with Presto and Parquet/ORC\\nProficient with Apache Spark and data frame.\\nExperienced in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python,\\nExperience with Columnar database such as Redshift, Vertica\\nGreat verbal and written communication skills.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Leading the future of luxury mobility\\n\\nLucid’s mission is to inspire the adoption of sustainable energy by creating the most captivating luxury electric vehicles, centered around the human experience. Working at Lucid Motors means having a shared vision to power the future in revolutionary ways. Be part of a once-in-a-lifetime opportunity to transform the automotive industry.\\n\\nWe are looking for a Data Engineer, Streaming who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.\\nThe Role\\nHands-on design and develop streaming and IoT data pipelines.\\nDeveloping streaming pipeline using MQTT, Kafka, Spark Structure Streaming\\nOrchestrate and monitor pipelines using Prometheus and Kubernetes\\nDeploy and maintain streaming jobs in CI/CD and relevant tools.\\nPython scripting for automation and application development\\nDesign and implement Apache Airflow and other dependency enforcement and scheduling tools.\\nHands-on data modeling and data warehousing\\nDeploy solution using AWS, S3, Redshift and Docker/Kubernetes\\nDevelop storage and retrieval system using Presto and Parquet/ORC\\nScripting with Apache Spark and data frame.\\nQualifications\\nBachelor or Masters in Software Engineering or Computer Science\\n+4 years of experience in Data Engineering and Business Intelligence.\\nProficient in IoT tools such as MQTT, Kafka, Spark\\nProficient with AWS, S3, Redshift\\nExperience with Presto and Parquet/ORC\\nProficient with Apache Spark and data frame.\\nExperienced in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python,\\nExperience with Columnar database such as Redshift, Vertica\\nGreat verbal and written communication skills.\\nBe part of something amazing\\n\\nCome work alongside some of the most accomplished minds in the industry. Beyond providing competitive salaries, we’re providing a community for innovators who want to make an immediate and significant impact. If you are driven to create a better, more sustainable future, then this is the right place for you.\\n\\nAt Lucid, we don’t just welcome diversity - we celebrate it! Lucid Motors is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, national or ethnic origin, age, religion, disability, sexual orientation, gender, gender identity and expression, marital status, and any other characteristic protected under applicable State or Federal laws and regulations.\\n\\nTo all recruitment agencies: Lucid Motors does not accept agency resumes. Please do not forward resumes to our careers alias or other Lucid Motors employees. Lucid Motors is not responsible for any fees related to unsolicited resumes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Senior Data/Server Engineer</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Niantic, the developer behind popular games like Pokemon GO and Harry Potter: Wizards Unite is searching for a Senior Data Engineer with an extensive server infrastructure background. Join a group of experienced engineers to help build and scale Niantic's core data infrastructure. This is a nimble, motivated team responsible for building trust, owning data integrity, and supporting data-driven decision-making at Niantic.\\nResponsibilities\\nArchitect the data stack responsible for storing and processing enormous volumes of analytics data.\\nDesign efficient, extensible data models for use with Niantic's data pipelines and analytics systems.\\nImprove and extend core data competency for the User Acquisition pipeline to facilitate tools and reporting in support of growing UA efforts.\\nOrganize and secure data drawn from diverse sources and build streamlined ETL pipelines to transform and validate it.\\nMentor and offer technical guidance to data engineers, data scientists, and infrastructure engineers.\\nWork with the Product Team and Management to define a shared vision, an execution strategy, and communicate timeline and trade-offs.\\nQualifications\\n4+ years of experience developing and deploying robust, large-scale data pipelines.\\nA high degree of attention to detail and clear aptitude for finding and resolving data integrity issues.\\nProven success in securing and auditing data stores and implementing legal compliance, e.g. GDPR.\\nDeep knowledge of available data storage technologies such as Hadoop, Cassandra, Druid, and their trade-offs.\\nExcel in developing general purpose solutions to difficult problems and building elegant solutions.\\nStrong communicator to both technical and non-technical people and demonstrated ability to document technical design decisions.\\nExpert in Java or Scala, Python and SQL.\\nBS, MS, or PhD in Computer Science or a related technical field.\\nPlus If...\\nFamiliarity with mobile advertising, user acquisition and associated data processing and metrics (e.g. attribution, retention, CPI, ROAS).\\nDetailed knowledge of and experience with the large advertising networks, e.g. Google, Facebook, Twitter, Apple.\\nKnowledge of the Google data stack (e.g. Dataflow, BigQuery, BigTable).\\nProficient in the use of Airflow, Composer.\\nJoin the Niantic team!\\nNiantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.\\nOriginally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, record-breaking AR game Pokémon GO, and recently released third title, Harry Potter: Wizards Unite.\\nNiantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.\\nWe're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, London, Tokyo, Hamburg, and Zurich.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Us:\\n---------\\n\\nWe are The Selling App. The fast and easy way to sell or buy almost anything, from fashion to toys, sporting goods to electronics, jewelry to shoes. Launched in 2013, Mercari quickly became the #1 shopping app in Japan. Now we're on a global mission to build a future where people everywhere feel empowered to sell the things they don't use, a future where all useful things are used. And with a fast-growing user base in the U.S. of over 45 million downloads, we are on our way to doing just that.\\n\\nThe ideal candidate is eager to take responsibilities on data products and machine learning platform spontaneously and has a passion to make our systems reliable and sustainable with the modern technologies. The candidate will take responsibility for not only data engineering, but also machine learning platform. We expect the candidate is able to design and implement data pipelines at scale and is deeply familiar with container technologies and related topics to offer intelligent system as microservices.\\n\\nWhat You'll Be Doing:\\n---------------------\\n\\n\\nDesign, build and operate ETL pipeline at scale.\\nAutomation of processes related to data products and machine learning products.\\nSupport stakeholders with designing data structure for data products.\\nDevelopment and operation of API/tools related to data products and machine learning products.\\n\\nWhat You'll Need:\\n-----------------\\n\\n\\nIndustry experience building and productionizing data pipelines.\\nSolid understanding / experience in the machine learning space.\\nIndustry experience building and productionizing system to serve machine learning.\\nAbility and desire to take full ownership of projects, driving them forward to completion.\\n\\nNice-to-haves:\\n--------------\\n\\n\\nExperience with Apache Spark, Apache Beam or related distributed processing frameworks.\\nExperience with Google Cloud Platform or related cloud services.\\nExperience with Apache Airflow or related workflow scheduling products.\\nExperience with developing microservices with docker and kubernetes to serve machine learning models.\\n\\nTechnologies We Use:\\n--------------------\\n\\n\\nETL: Apache Airflow\\nContainer: Docker/Kubernetes\\nAPI: gRPC/Tensorflow Serving/Flask(REST)\\nDatabase: Google Datastore/MySQL/Google Spanner\\nDistributed Processing: Apache Beam/Apache Spark, Hadoop\\nCloud: Google Cloud(BigQuery/ML Engine/Google Dataflow/Google Dataproc, etc.)\\n\\nWhy Mercari?\\n------------\\n\\nMercari nurtures an all for one environment where teamwork and innovative thinking is the priority.\\n\\nPerks:\\n\\nCompetitive ‌medical, dental, and vision insurance options\\n401k match\\nLife &amp; disability insurance\\nEmployee Assistant Program\\nNew parent paid leave\\nRocket Lawyer legal services\\nFond perks and rewards\\nCommuter reimbursement\\nTime when you need it - unlimited vacation days\\nCatered lunches everyday\\nTeam outings and events\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title          Location          City State  \\\n",
       "0    Software Engineer            Redwood City, CA  Redwood City  CA     \n",
       "1    Data Engineer                Santa Clara, CA   Santa Clara   CA     \n",
       "2    Manager                      Fremont, CA       Fremont       CA     \n",
       "3    Big Data Engineer            Newark, CA 94560  Newark        CA     \n",
       "4    Big Data Engineer            Santa Clara, CA   Santa Clara   CA     \n",
       "..                 ...                        ...           ...   ..     \n",
       "176  Big Data Engineer            Santa Clara, CA   Santa Clara   CA     \n",
       "177  Data Engineer                Sunnyvale, CA     Sunnyvale     CA     \n",
       "178  Senior Data Engineer         Newark, CA        Newark        CA     \n",
       "179  Senior Data/Server Engineer  Sunnyvale, CA     Sunnyvale     CA     \n",
       "180  Senior Data Engineer         Palo Alto, CA     Palo Alto     CA     \n",
       "\n",
       "            Zip     Country  \\\n",
       "0    None Found  None Found   \n",
       "1    None Found  None Found   \n",
       "2    None Found  None Found   \n",
       "3    94560       None Found   \n",
       "4    None Found  None Found   \n",
       "..          ...         ...   \n",
       "176  None Found  None Found   \n",
       "177  None Found  None Found   \n",
       "178  None Found  None Found   \n",
       "179  None Found  None Found   \n",
       "180  None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Qualifications  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "1    Bachelors or Masters in Computer Science, Engineering, or a related quantitative field.\\n3+ years of experience with building scalable and reliable date pipelines using technologies like Spark, AWS EMR, Kafka, etc.\\n3+ years of production coding experience with at least one general software development programming language (Java, Scala), one data programming language (Python, R), and scripting languages (Unix shell) as well as solid experience with git.\\nExpertise with relational databases and experience with schema design and dimensional data modeling.\\nMastery of SQL (writing complex, high performance queries in Oracle or MSSQL); experience with distributed querying (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).\\nWorking experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)\\nWorking experience with AWS cloud ecosystem.\\nExcellent communication skills in written and verbal forms, and an ability to communicate complex issues to a range of audience (management, peers, clients).\\nStrong attention to detail while excellent time management and prioritization in multitasking.\\nHighly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.\\n   \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "3    Bachelor or Masters in Software Engineering and Computer Science\\n8+ years of experience in design and development of large scale data platforms\\nExpert in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nProficient in Spark development with PySpark or Scala\\nExpert in Data streaming platforms such as Apache Kafka\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python, and/or C++                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "176  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "177  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "178  \\nBachelor or Masters in Software Engineering or Computer Science\\n+4 years of experience in Data Engineering and Business Intelligence.\\nProficient in IoT tools such as MQTT, Kafka, Spark\\nProficient with AWS, S3, Redshift\\nExperience with Presto and Parquet/ORC\\nProficient with Apache Spark and data frame.\\nExperienced in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python,\\nExperience with Columnar database such as Redshift, Vertica\\nGreat verbal and written communication skills.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "179  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "180  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "\n",
       "         Skills  \\\n",
       "0    None Found   \n",
       "1    None Found   \n",
       "2    None Found   \n",
       "3    None Found   \n",
       "4    None Found   \n",
       "..          ...   \n",
       "176  None Found   \n",
       "177  None Found   \n",
       "178  None Found   \n",
       "179  None Found   \n",
       "180  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Responsibilities  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1    Become the subject matter expert on our data and its capabilities. Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.\\nDesign and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.\\nAutomate data processing using workflows tools to schedule and manage dependency of various data pipelines.\\nWork directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.\\nRecommend ways to improve data reliability, efficiency, and quality.\\nAssist eHealth’s data architect with logical and physical data model designs and documentation.\\nWork with data infrastructure team to triage issues and support issue resolution.\\n   \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "176  \\nJoin an agile SaaS team to design, develop and maintain features and iteratively deploy services using Infoblox’s cloud-based architecture\\nDesign and implement components of our Next Generation Platform\\nRecommend ways to improve data reliability, efficiency and quality\\nExpand and grow data platform capabilities to solve new data problems and challenges\\nBuild large-scale data processing systems using cloud computing technologies\\nBuild high-performance algorithms, prototypes, and proof of concepts\\nApply complex big data concepts with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights\\nWork closely with various cross functional product teams\\nStay current on key trends especially in the area of technologies and frameworks like: Mesos/Marathon, Kubernetes, Docker etc.                                                                          \n",
       "177  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "178  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "179  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "180  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                  Education  \\\n",
       "0    None Found                                                                               \n",
       "1    None Found                                                                               \n",
       "2    None Found                                                                               \n",
       "3    None Found                                                                               \n",
       "4    None Found                                                                               \n",
       "..          ...                                                                               \n",
       "176  \\nBachelor’s degree in CS, CE or EE is required\\nMasters in CS, CE, or EE is preferred   \n",
       "177  None Found                                                                               \n",
       "178  None Found                                                                               \n",
       "179  None Found                                                                               \n",
       "180  None Found                                                                               \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Requirement  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "176  \\n8+ years experience, 2+ in Big Data Engineering\\nProficient in Java, Scala, Golang, or Python\\nGood understanding of Microservices architecture\\nExpertise in BigData - MapReduce, HIVE, HBase, Spark streaming, Apache Flink, Storm, Kafka, In memory Database, JMS\\nExperience with NoSQL databases such as Cassandra/DynamoDB\\nGood exposure in application performance tuning, memory management, scalability\\nAbility to design highly scalable distributed systems, using different open source technologies\\nExperience building high-performance algorithms   \n",
       "177  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "178  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "179  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "180  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         FullDescriptions  \n",
       "0    Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.\\n\\nSenior Data Engineer\\n\\nPosition Summary\\nAt Auction.com, we are embarking on a journey to transform the real estate market with technological innovations. A critical prerequisite for this transformation is a robust data infrastructure. As a senior data engineer, you will help us design and implement our big data environment that is real-time, stable and scalable. You will work with a talented data engineering team to improve our data processing pipeline and developing new capabilities to support mission-critical initiatives. You will mentor junior engineers and help evaluating new technology along the way. You impact will be felt across the team as well as the entire Auction.com organization.\\n\\nResponsibilities/Duties\\n\\nMake major contribution to the implementation of our real time big data initiative\\nBuild and automate productized data processing pipelines in AWS big data platform\\nHelp improve our development process and standards through mentoring and leading-by-example\\nHelp define and implement data ingestion contracts\\nCreate data environment to support our data analytics, reporting and data science teams\\n\\nKnowledge, Skills and Abilities\\n\\nIn-depth understanding of modern big data technology, including Hadoop and Spark\\nKnowledge of real time data streaming and aggregation architectural patterns and practice\\nProficient in programming languages such as Python, Scala and Java\\nFamiliarity with NoSQL as well as SQL databases\\nData modeling and machine learning skill is a plus\\n\\nEducation/Experience\\n\\nBachelor's Degree in computer science, data science or related fields\\nFamiliarity with agile developmental process\\nPrevious experience developing data product required\\nHands-on experience with real time data streaming, aggregation and presentation strongly preferred\\nPrevious experience with production ETL pipeline development required\\nAt least a years’ experience with AWS cloud or another cloud platform\\n\\nTo all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1    Get your career started at eHealth\\neHealthInsurance has many exciting career opportunities in a number of locations, across various functions. Come join us today!\\n\\nAt eHealth, we are passionate about solving our nation's toughest problems to bring more suitable, accessible, and affordable health insurance to Americans. We are seeking a talented data engineer to join our growing data team, which is already making a valuable impact on the entire company. This person will help us develop cutting-edge data tools and pipelines to drive better and faster decision making within our company and to better serve our customers. This is a fast-paced, collaborative, and iterative environment requiring quick learning, agility, and flexibility.\\n\\n\\nResponsibilities:\\nBecome the subject matter expert on our data and its capabilities. Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.\\nDesign and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.\\nAutomate data processing using workflows tools to schedule and manage dependency of various data pipelines.\\nWork directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.\\nRecommend ways to improve data reliability, efficiency, and quality.\\nAssist eHealth’s data architect with logical and physical data model designs and documentation.\\nWork with data infrastructure team to triage issues and support issue resolution.\\nMinimum Qualifications:\\nBachelors or Masters in Computer Science, Engineering, or a related quantitative field.\\n3+ years of experience with building scalable and reliable date pipelines using technologies like Spark, AWS EMR, Kafka, etc.\\n3+ years of production coding experience with at least one general software development programming language (Java, Scala), one data programming language (Python, R), and scripting languages (Unix shell) as well as solid experience with git.\\nExpertise with relational databases and experience with schema design and dimensional data modeling.\\nMastery of SQL (writing complex, high performance queries in Oracle or MSSQL); experience with distributed querying (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).\\nWorking experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)\\nWorking experience with AWS cloud ecosystem.\\nExcellent communication skills in written and verbal forms, and an ability to communicate complex issues to a range of audience (management, peers, clients).\\nStrong attention to detail while excellent time management and prioritization in multitasking.\\nHighly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.\\nNice to Have:\\nWorking experience with implementing scalable models using various statistics and machine learning toolkits (Pandas, SciPy, Scikit-learn, MLlib, Spark ML, Tensorflow, Keras, etc.).\\nStrong experience in designing and implementing data APIs.\\nProduct familiarity with Adobe Analytics, Cisco systems, Snowflake, or Informatica.\\nFamiliarity with workflow management tools (Airflow).\\nWorking experience with data warehousing.\\nAbility to create beautiful data visualizations using D3, Tableau, or similar tools.\\nWorking experience with large healthcare related datasets, including EHRs, medical claims data, and health population surveys. Experience in building healthcare data pipelines would be a big plus.\\nKnowledge of healthcare insurance industry, products, systems, business strategies, and products.\\nExperience working with call center operations.\\neHealth is an Equal Employment Opportunity employer. It is our policy to provide equal opportunity to all employees and applicants and to prohibit any discrimination because of race, color, religion, sex, national origin, age, marital status, sexual orientation, genetic information, disability, protected veteran status, or any other consideration made unlawful by applicable federal, state or local laws. The foundation of these policies is our commitment to treat everyone fairly and equally and to have a bias-free work environment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2    Role : Tableau + Sales Analytics\\nJD : 65533 Dashboards: Build new dashboards and make adjustments to existing dashboards, based on requirements received from stakeholders and Sales Ops Analysts on the team.\\n\\n65533 Data Models: Deliver requirements to Business Technology team to incorporate new fields, objects, and business views into the enterprise data warehouse. Document logic used to build all business views for future reference. Analyze any upcoming systems changes, determine impact on the data warehouse and dashboards, and manage that transition seamlessly.\\n65533 Data Analysis: Perform data analysis and/or gap analysis to ensure it can support Dashboard solutions\\n65533 Data Inputs: Own and maintain nonsystems data inputs, such as Excel files and Google Sheets. Optimize and automate updates where possible.\\n65533 Troubleshoot: Be the first stop for troubleshooting and resolving access or data quality issues reported by users. Help Sales Ops Analysts troubleshoot issues in dashboards they are building.\\n65533 Optimization: Optimize performance (load time) of dashboards using best practices.\\n\\nQualifications:\\n65533 Bachelor65533s degree in Information Systems, Computer Science, or related field of study.\\n65533 5+ years professional experience as a Business Intelligence Analyst, Business Intelligence Engineer, Data Engineer, or related roles.\\n65533 Extensive experience working in Tableau Desktop and publishing to Tableau Server required.\\n65533 Strong experience in SQL, data modeling, and data preparation required. Experience with Alteryx preferred.\\n65533 Experience with Sales Operations analytics preferred. Experience with Salesforce strongly preferred.\\n65533 Excellent problem solving, analytical, and project management skills.\\n65533 Strong interpersonal skills.\\n65533 Takes initiative and tackles challenges with enthusiasm.\\n65533 Ability to multitask and adapt quickly in a changing environment.\\n\\n\\nPrimary Location: US-CA-Fremont\\nSchedule: Full Time\\nJob Type: Experienced\\nTravel: No\\nJob Posting: 04/10/2019, 1:14:06 PM                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "3    We are looking for a Staff Data Engineer, Big Data who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a young and very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.\\nThe Role\\nLead Data Engineer and architect to design, implement a highly scalable system to ingest and process Petabytes of data per day.\\nHands-on design and develop applications for data pipeline and data management.\\nSet processes and policies for data governance and data pipeline\\nArchitect and implement best practices of big data tools such as Spark, Airflow, Kafka, Presto and Cassandra\\nLead and mentor junior Data and BI engineers.\\nSet and define the standards and best practices in data team\\nBe the point of reference for solving challenging technical problem.\\nArchitect and implement Machine Learning Pipelines for Data Science team\\n\\nQualifications\\nBachelor or Masters in Software Engineering and Computer Science\\n8+ years of experience in design and development of large scale data platforms\\nExpert in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nProficient in Spark development with PySpark or Scala\\nExpert in Data streaming platforms such as Apache Kafka\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python, and/or C++\\nExperience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architecture\\nExperienced in security and access management\\nExperience with managing distributed databases like Elasticsearch, Cassandra\\nExperience with Columnar database such as Redshift, Vertica\\nGreat verbal and written communication skills.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4    Our Mission\\nAt Palo Alto Networks® everything starts and ends with our mission: protecting our way of life in the digital age by preventing successful cyberattacks. It’s not a small goal. It isn’t simple either, but we aren’t in this for the easy answer. As a company with a foundation in challenging the way things are done, we’re looking for innovators with a dedication to best. In return, your career will have a tangible impact – one that's working toward technology that affects every level of society.\\nOur mission doesn’t happen by treading softly – no, it happens by defining an industry. It means building products that haven't been thought of. It means selling products with a solutions mindset. It means supporting the infrastructure of a company that moves at an incredible speed – intentionally – to stay ahead of the world’s next cyberthreat.\\n\\nYour Career\\nOur daily fight with cyber bad guys requires us to collect and analyze a lot of data…. A LOT of data. And, as our customer base continues its rapid growth, we need to look at faster and more robust tools to help us and our customers make the best decisions possible.\\nWith your knowledge of Hadoop and Big Data technologies, you will add your tools-building superpowers to a small team tasked with building out a DevOps automation environment, one that will step up our Business Analytics game and help us protect our customers from cyber intruders.\\nWe offer the chance to be part of an important mission: ending breaches and protecting our way of digital life. If you are a motivated, intelligent, creative, and hardworking individual, then this job is for you!\\n\\nOverview\\nYou will be responsible for leading technical projects to build custom applications and enhance business systems. We are looking for someone with solid project management and organization skills complemented by a strong technical background. You will work with a team of senior level managers and highly technical engineers to lead complex IT projects. The role will collaborate with multiple engineering and business organizations, understand and align with their needs, and take independent end-to-end responsibility for release of new business capabilities to production.\\nYour Impact\\nAs a Big Data Engineer, you will be an integral member of our Big Data and Analytics team responsible for design and development\\nPartner with data analyst, product owners and data scientists, to better understand requirements, finding bottlenecks, resolutions, etc.\\nDesign and develop Big Data solutions both in Cloud & OnPrem\\nDesign and develop different architectural models for our scalable data processing as well as scalable data storage\\nBuild data pipelines and ETL using heterogeneous sources using Dataflow or DataProc\\nBuild data ingestion from various source systems to Hadoop or GCP using Kafka, Flume, Sqoop, Spark Streaming etc.\\nTransform data, using data mapping and data processing in Apache Beam or Spark\\nResponsible to ensure that the platform goes through Continuous Integration (CI) and Continuous Deployment (CD) with DevOps automation\\nExpands and grows data platform capabilities to tackle new data problems and challenges\\nSupports Big Data and batch/real time analytical solutions using groundbreaking technologies like Apache Beam\\nHave the ability to research and assess open source technologies and components to recommend and integrate into the design and implementation\\nWork with development and QA teams to design Ingestion Pipelines, Integration APIs, and provide Hadoop ecosystem services\\nYour Experience\\nDegree in Bachelor of Science in Computer Science or equivalent\\n5+ years of experience with the Hadoop ecosystem and Big Data technologies\\n2+ year of experience in Cloud computing\\nCompetent in writing Scala, Python or Java code.\\nDevelopment experience in Dataflow or DataProc is a Plus\\nAbility to dynamically adapt to conventional big data frameworks and tools with the use-cases required by the project\\nExperience with building stream-processing systems using solutions such as spark-streaming, Storm or Flink etc.\\nExperience in other open-sources like Druid, Elastic Search, Logstash etc. is a plus\\nKnowledge of design strategies for developing scalable, resilient, always-on data lake\\nSome knowledge of agile(scrum) development methodology is a plus\\nStrong development/automation skills\\nExcellent inter-personal and teamwork skills\\nCan-do attitude on problem solving, quality and ability to execute\\nThe Team\\nWorking at a high-tech cybersecurity company within Information Technology is a once in a lifetime opportunity. You’ll be joined with the brightest minds in technology, creating, building, and supporting tools and that enable our global teams on the front line of defense against cyberattacks. We’re joined by one mission – but driven by the impact of that mission and what it means to protect our way of life in the digital age. Join a dynamic and fast-paced team that feels excitement at the prospect of a challenge and feels a thrill at resolving technical gaps that inhibit productivity.\\nOur Commitment\\nWe’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together. To learn more about our dedication to inclusion and innovation, visit our Life at Palo Alto Networks page and our diversity website.\\nPalo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.\\nAdditionally, we are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or an accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.\\n#LI-AH1  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...  \n",
       "176  Infoblox is the global leader in providing actionable network intelligence through network services, security and threat intelligence. We give companies total control and visibility of their network, allowing them to operate more efficiently and intelligently.\\nWe are looking for a Staff Software Engineer to join our SaaS Next Generation Platform Team in Santa Clara, CA. In this role, you will be responsible for developing, maintaining, evaluating and testing big data technologies. Our organization is extremely data driven where technical innovations happen and you will have an opportunity to use cutting edge technology across all stages of development lifecycle and be part of our exciting and innovative initiatives.\\nResponsibilities:\\n\\nJoin an agile SaaS team to design, develop and maintain features and iteratively deploy services using Infoblox’s cloud-based architecture\\nDesign and implement components of our Next Generation Platform\\nRecommend ways to improve data reliability, efficiency and quality\\nExpand and grow data platform capabilities to solve new data problems and challenges\\nBuild large-scale data processing systems using cloud computing technologies\\nBuild high-performance algorithms, prototypes, and proof of concepts\\nApply complex big data concepts with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights\\nWork closely with various cross functional product teams\\nStay current on key trends especially in the area of technologies and frameworks like: Mesos/Marathon, Kubernetes, Docker etc.\\nRequirements:\\n8+ years experience, 2+ in Big Data Engineering\\nProficient in Java, Scala, Golang, or Python\\nGood understanding of Microservices architecture\\nExpertise in BigData - MapReduce, HIVE, HBase, Spark streaming, Apache Flink, Storm, Kafka, In memory Database, JMS\\nExperience with NoSQL databases such as Cassandra/DynamoDB\\nGood exposure in application performance tuning, memory management, scalability\\nAbility to design highly scalable distributed systems, using different open source technologies\\nExperience building high-performance algorithms\\nEducation\\nBachelor’s degree in CS, CE or EE is required\\nMasters in CS, CE, or EE is preferred\\nIt’s an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologies—and having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox team—our future looks bright, and so will yours. To check out what it’s like to be a Bloxer click here.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "177  We are looking for a Data Engineer who is excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data infrastructure across our products and internal tools. At Applied, we encourage all engineers to take ownership over technical and product decisions, closely interact with users to collect feedback, and contribute to a thoughtful, dynamic team culture.\\n\\nAt Applied, you will\\nDesign powerful data pipelines that process fast sensor streams, leverage appropriate data stores, and offer easy-to-use APIs\\nDevelop and deploy high-quality software using modern tooling and frameworks\\nWork with products and teams across Applied Intuition\\nWork with customers across the AV ecosystem to understand their needs and the innards of their data systems\\n\\nWe’re looking for someone who\\nHas 1.5+ years experience building scalable big data pipelines\\nHas experience with open source data processing frameworks (Spark, Kafka, etc.)\\nHas experience with different data storages (e.g., relational and NoSQL)\\nHas experience with containerization and other modern software development workflows\\nTakes initiative and ownership in a fast-paced environment\\n\\nNice to have\\nExpertise with multiple modern programming languages (Python, C++, Go, etc.)\\nPrior work in enterprise software, including on-prem and/or cloud deployments\\nPrior work in either autonomy or simulation products\\n\\nAutonomy is one of the leading technological advances of this century that will come to impact our lives. The work you’ll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting edge technology while working with major players across the industry and the globe.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "178  Leading the future of luxury mobility\\n\\nLucid’s mission is to inspire the adoption of sustainable energy by creating the most captivating luxury electric vehicles, centered around the human experience. Working at Lucid Motors means having a shared vision to power the future in revolutionary ways. Be part of a once-in-a-lifetime opportunity to transform the automotive industry.\\n\\nWe are looking for a Data Engineer, Streaming who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.\\nThe Role\\nHands-on design and develop streaming and IoT data pipelines.\\nDeveloping streaming pipeline using MQTT, Kafka, Spark Structure Streaming\\nOrchestrate and monitor pipelines using Prometheus and Kubernetes\\nDeploy and maintain streaming jobs in CI/CD and relevant tools.\\nPython scripting for automation and application development\\nDesign and implement Apache Airflow and other dependency enforcement and scheduling tools.\\nHands-on data modeling and data warehousing\\nDeploy solution using AWS, S3, Redshift and Docker/Kubernetes\\nDevelop storage and retrieval system using Presto and Parquet/ORC\\nScripting with Apache Spark and data frame.\\nQualifications\\nBachelor or Masters in Software Engineering or Computer Science\\n+4 years of experience in Data Engineering and Business Intelligence.\\nProficient in IoT tools such as MQTT, Kafka, Spark\\nProficient with AWS, S3, Redshift\\nExperience with Presto and Parquet/ORC\\nProficient with Apache Spark and data frame.\\nExperienced in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python,\\nExperience with Columnar database such as Redshift, Vertica\\nGreat verbal and written communication skills.\\nBe part of something amazing\\n\\nCome work alongside some of the most accomplished minds in the industry. Beyond providing competitive salaries, we’re providing a community for innovators who want to make an immediate and significant impact. If you are driven to create a better, more sustainable future, then this is the right place for you.\\n\\nAt Lucid, we don’t just welcome diversity - we celebrate it! Lucid Motors is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, national or ethnic origin, age, religion, disability, sexual orientation, gender, gender identity and expression, marital status, and any other characteristic protected under applicable State or Federal laws and regulations.\\n\\nTo all recruitment agencies: Lucid Motors does not accept agency resumes. Please do not forward resumes to our careers alias or other Lucid Motors employees. Lucid Motors is not responsible for any fees related to unsolicited resumes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "179  Niantic, the developer behind popular games like Pokemon GO and Harry Potter: Wizards Unite is searching for a Senior Data Engineer with an extensive server infrastructure background. Join a group of experienced engineers to help build and scale Niantic's core data infrastructure. This is a nimble, motivated team responsible for building trust, owning data integrity, and supporting data-driven decision-making at Niantic.\\nResponsibilities\\nArchitect the data stack responsible for storing and processing enormous volumes of analytics data.\\nDesign efficient, extensible data models for use with Niantic's data pipelines and analytics systems.\\nImprove and extend core data competency for the User Acquisition pipeline to facilitate tools and reporting in support of growing UA efforts.\\nOrganize and secure data drawn from diverse sources and build streamlined ETL pipelines to transform and validate it.\\nMentor and offer technical guidance to data engineers, data scientists, and infrastructure engineers.\\nWork with the Product Team and Management to define a shared vision, an execution strategy, and communicate timeline and trade-offs.\\nQualifications\\n4+ years of experience developing and deploying robust, large-scale data pipelines.\\nA high degree of attention to detail and clear aptitude for finding and resolving data integrity issues.\\nProven success in securing and auditing data stores and implementing legal compliance, e.g. GDPR.\\nDeep knowledge of available data storage technologies such as Hadoop, Cassandra, Druid, and their trade-offs.\\nExcel in developing general purpose solutions to difficult problems and building elegant solutions.\\nStrong communicator to both technical and non-technical people and demonstrated ability to document technical design decisions.\\nExpert in Java or Scala, Python and SQL.\\nBS, MS, or PhD in Computer Science or a related technical field.\\nPlus If...\\nFamiliarity with mobile advertising, user acquisition and associated data processing and metrics (e.g. attribution, retention, CPI, ROAS).\\nDetailed knowledge of and experience with the large advertising networks, e.g. Google, Facebook, Twitter, Apple.\\nKnowledge of the Google data stack (e.g. Dataflow, BigQuery, BigTable).\\nProficient in the use of Airflow, Composer.\\nJoin the Niantic team!\\nNiantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.\\nOriginally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, record-breaking AR game Pokémon GO, and recently released third title, Harry Potter: Wizards Unite.\\nNiantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.\\nWe're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, London, Tokyo, Hamburg, and Zurich.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "180  About Us:\\n---------\\n\\nWe are The Selling App. The fast and easy way to sell or buy almost anything, from fashion to toys, sporting goods to electronics, jewelry to shoes. Launched in 2013, Mercari quickly became the #1 shopping app in Japan. Now we're on a global mission to build a future where people everywhere feel empowered to sell the things they don't use, a future where all useful things are used. And with a fast-growing user base in the U.S. of over 45 million downloads, we are on our way to doing just that.\\n\\nThe ideal candidate is eager to take responsibilities on data products and machine learning platform spontaneously and has a passion to make our systems reliable and sustainable with the modern technologies. The candidate will take responsibility for not only data engineering, but also machine learning platform. We expect the candidate is able to design and implement data pipelines at scale and is deeply familiar with container technologies and related topics to offer intelligent system as microservices.\\n\\nWhat You'll Be Doing:\\n---------------------\\n\\n\\nDesign, build and operate ETL pipeline at scale.\\nAutomation of processes related to data products and machine learning products.\\nSupport stakeholders with designing data structure for data products.\\nDevelopment and operation of API/tools related to data products and machine learning products.\\n\\nWhat You'll Need:\\n-----------------\\n\\n\\nIndustry experience building and productionizing data pipelines.\\nSolid understanding / experience in the machine learning space.\\nIndustry experience building and productionizing system to serve machine learning.\\nAbility and desire to take full ownership of projects, driving them forward to completion.\\n\\nNice-to-haves:\\n--------------\\n\\n\\nExperience with Apache Spark, Apache Beam or related distributed processing frameworks.\\nExperience with Google Cloud Platform or related cloud services.\\nExperience with Apache Airflow or related workflow scheduling products.\\nExperience with developing microservices with docker and kubernetes to serve machine learning models.\\n\\nTechnologies We Use:\\n--------------------\\n\\n\\nETL: Apache Airflow\\nContainer: Docker/Kubernetes\\nAPI: gRPC/Tensorflow Serving/Flask(REST)\\nDatabase: Google Datastore/MySQL/Google Spanner\\nDistributed Processing: Apache Beam/Apache Spark, Hadoop\\nCloud: Google Cloud(BigQuery/ML Engine/Google Dataflow/Google Dataproc, etc.)\\n\\nWhy Mercari?\\n------------\\n\\nMercari nurtures an all for one environment where teamwork and innovative thinking is the priority.\\n\\nPerks:\\n\\nCompetitive ‌medical, dental, and vision insurance options\\n401k match\\nLife & disability insurance\\nEmployee Assistant Program\\nNew parent paid leave\\nRocket Lawyer legal services\\nFond perks and rewards\\nCommuter reimbursement\\nTime when you need it - unlimited vacation days\\nCatered lunches everyday\\nTeam outings and events\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "\n",
       "[181 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_Cupertino.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
