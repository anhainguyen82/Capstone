{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Bellevue%2C+WA&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL\\nStrong Python coding skills\\nExperience developing and implementing ETL architectures with large, complex data sets\\nUnderstanding of database architecture and data lakes\\nDistributing computing (parallel processing, multi-threading) – Hadoop, MapReduce, Spark\\nHands-on experience with web crawling/web scraping required (6+ months)\\nExperience developing APIs\\nExperience with Node.js and familiarity with Machine Learning are pluses\\nStrong quantitative data analysis skills\\nBeyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech\\nCuriosity about anomalies in the data and the ability to identify the business opportunities they represent.\\nStrong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels\\nAzure experience is a plus\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL\\nStrong Python coding skills\\nExperience developing and implementing ETL architectures with large, complex data sets\\nUnderstanding of database architecture and data lakes\\nDistributing computing (parallel processing, multi-threading) – Hadoop, MapReduce, Spark\\nHands-on experience with web crawling/web scraping required (6+ months)\\nExperience developing APIs\\nExperience with Node.js and familiarity with Machine Learning are pluses\\nStrong quantitative data analysis skills\\nBeyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech\\nCuriosity about anomalies in the data and the ability to identify the business opportunities they represent.\\nStrong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels\\nAzure experience is a plus\\n</td>\n",
       "      <td>About us\\n\\nLaunched in October 2018, the Likewise app is the fun, social and incredibly useful way for people to discover, curate, and share recommendations on TV shows, movies, books, podcasts, restaurants, travel and more. Best of all, Likewise helps people quickly find recommendations from their friends, family, and other trusted sources.\\nImagined and backed by Bill Gates’ private office, Likewise is a rare early-stage startup that is thinking big, playing to win, and investing to continue its rapid growth trajectory. If you are passionate about what you do, and want to be a core part of creating a household consumer name, then come talk to us about getting in on the fun!\\nHere's a link to a Geekwire article about us: https://bit.ly/2RuxBlx. And Built In Seattle named us one of Seattle’s 50 Startups to Watch in 2019! https://bit.ly/2VXup3o\\nRole\\nWith the Likewise app launched, we have a lot of fun and creative work ahead of us in making Likewise’s AI into the end-all-be-all for determining the best recommendations to consumers across any category – movies, podcasts, books, restaurants, travel, and more! The work you will be doing is the foundation to making Likewise AI real, and it won’t happen without you. You’ll redefine how AI makes recommendations, and in doing so, change people’s lives for the better. We expect to grow the team as the company grows, and the right candidate will have the potential to lead that growth.\\nObjectives\\nCreate a process that handles our disparate internal and external data sources and automatically converts that unstructured data into structured data to be consumed by machine learning and our product, marketing, and executive teams\\nBuild data process pipelines for new and existing data sources\\nGlean insights and business opportunities from the data, and champion ideas for improvement based on those insights to the product team\\nLead the external data sources collection effort, and creatively identify new, relevant data sources that will positively impact our products and users\\nWork closely with the Data Science team to complete all data needs\\nFind the handful of outliers in massive data sets and define processes to handle them\\nRequirements\\nQualifications\\n4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL\\nStrong Python coding skills\\nExperience developing and implementing ETL architectures with large, complex data sets\\nUnderstanding of database architecture and data lakes\\nDistributing computing (parallel processing, multi-threading) – Hadoop, MapReduce, Spark\\nHands-on experience with web crawling/web scraping required (6+ months)\\nExperience developing APIs\\nExperience with Node.js and familiarity with Machine Learning are pluses\\nStrong quantitative data analysis skills\\nBeyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech\\nCuriosity about anomalies in the data and the ability to identify the business opportunities they represent.\\nStrong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels\\nAzure experience is a plus\\nBenefits\\nWorking here\\nLocated in downtown Bellevue, close to restaurants, shopping, parks and transit, we are proud to offer a competitive benefits package including stock options, health care where we pay 100% of employee premiums, 401(k) plan, commuter benefits, flexible paid time off, and a culture that’s team-based, open, casual and fun. If you’re looking for a rare opportunity to be a part of an innovative, exciting company and become a key member on our team, join us!\\nWe support workplace diversity and do not discriminate on the basis of race, color, religion, gender identity or expression, national origin, age, military service eligibility, veteran status, sexual orientation, marital status, physical or mental disability, or any other protected class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer I, Amazon Payment Products</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for Data Engineer who has a passion for their customers and a passion for working with data. You like working with your customers, understanding their challenges, and partnering with them to invent great solutions. You like working with large data sets, and bringing data together from multiple systems to answer critical business questions and drive change. You are analytical and creative. You should also have the following skills or experiences:Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance or related technical field.\\n3+ years developing end-to-end Business Intelligence solutions: data modeling, ETL and reporting3+ years in relational database concepts with a solid knowledge of star schema, Oracle, SQL, PL/SQL, SQL Tuning, OLAP, Big Data technologiesExperience with coding languages like Python/Java/ScalaExperience in working with business customers to drive requirements analysisHave analytical skills and be creativeExperience with Big Data solutions: Hadoop, Hive or other frameworksExposure to large databases, BI applications, data quality and performance tuningExcellent written and spoken communication skill\\n\\nHundreds of millions of customers, billions of transactions, petabytes of data… How to use the world’s richest collection of e-commerce data to provide superior value and better paying experience to customers ? The Amazon Payments Team manages all Amazon branded payment offerings globally. These offerings are growing rapidly and we are continuously adding new market-leading features and launching new products. Amazon.com has a culture of data-driven decision-making and demands business intelligence that is timely, accurate, and actionable. This team provides a fast-paced environment where every day brings new challenges and new opportunities.\\n\\nOur team of high caliber software developers, data scientists, statisticians and product managers use rigorous quantitative approaches to ensure that we target the right product to the right customer at the right moment, managing tradeoffs between click through rate, approval rates and lifetime value. In order to accomplish this we leverage the wealth of Amazon’s information to build a wide range of probabilistic models, set up experiments that ensure that we are thriving to reach global optimums and leverage Amazon’s technological infrastructure to display the right offerings in real time.\\n\\nAs a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. You should be passionate about working with huge data sets and be someone who loves to bring datasets together to answer business questions. You should have deep expertise in creation and management of datasets. You will build data analytical solutions that will address increasingly complex business questions.\\n\\nYou should be expert at implementing and operating stable, scalable data flow solutions from production systems into end-user facing applications/reports. These solutions will be fault tolerant, self-healing and adaptive. You will be working on developing solutions that provide some of the unique challenges of space, size and speed. You will implement data analytics using cutting edge analytics patterns and technologies that are inclusive of but not limited to various AWS Offerings -EMR, Lambda, Kinesis, and Spectrum. You will extract huge volumes of structured and unstructured data from various sources (Relational /Non-relational/No-SQL database) and message streams and construct complex analyses. You will write scalable code and tune performance running over billion of rows of data. You will implement data flow solutions that process data on Spark ,Redshift and store in Redshift ,Filebased system (S3) for reporting and adhoc analysis.\\n\\nYou should be detail-oriented and must have an aptitude for solving unstructured problems. You should work in a self-directed environment, own tasks and drive them to completion.\\n\\nYou should have excellent business and communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions. You own customer relationship about data and execute tasks that are manifestations of such ownership, like ensuring high data availability, low latency, documenting data details and transformations and handling user notifications and training.\\n\\nExperience partnering with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies.Experience with web technology to develop dashboards.Practical Knowledge of Linux or Unix shell scriptingExperience in processing large volume of data.Strong troubleshooting and problem solving skillsDemonstrated experience in dealing with Senior Management on addressing their reporting and metrics requirements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HR Finance Data Engineer III</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.5+ years of experience in scripting languages like Python, Scala, etc.Demonstrated strength in data modeling, ETL development, and Data warehousing. Data WarehousingExperience with Redshift, Oracle, NoSQL etc.Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS.Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Experience in working and delivering end-to-end projects independently.Knowledge of distributed systems as it pertains to data storage and computing.\\n\\nThe Human Resources Finance Technology team within Amazon's Financial Planning &amp; Analysis organization is seeking a highly skilled and motivated Data Engineer to join our team in Seattle. You will be building world class big data applications to support Amazon Human Resources. If you enjoy innovating, thinking big and want to contribute directly to the success of a growing team, you may be a prime candidate for this position.\\n\\nAs a Sr. Data Engineer on the HR Finance Tech team, you will\\nBuild robust and scalable data integration (ETL) pipelines using SQL, EMR, Python, Spark, Redshift, Lambda, and MatillionBuild and deliver high quality data architecture to support business intelligence engineer and program managers’ reporting needs.Interface with other technology teams to extract, transform, and load data from a wide variety of data sources.Drive both business and technology solutions to improve visibility into key Amazon HR metrics.Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.Translate data into actionable insights for the stakeholders.\\n\\nProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategyExperience providing technical leadership and mentoring other engineers for best practices on data engineeringKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsMasters in computer science, mathematics, statistics, economics, or other quantitative field\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation/Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Intelligence Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelors in engineering, science, math, statistics or computer science3+ years of work experience as a business intelligence engineer, data engineer or data scientist role3+ years of experience in SQL programming3+ years of experience in building data warehouses and dimensional modeling3+ years of experience with business intelligence and data visualization tools (e.g. Tableau)3+ years of experience with a modern programming language (e.g. Python, R, Scala etc.)Experience with AWS Suite\\n\\nAmazon Lab126 is an inventive research and development company that designs and engineers high-profile consumer electronics. Lab126 began in 2004 as a subsidiary of Amazon.com, Inc., originally creating the best-selling Kindle family of products. Since then, we have produced groundbreaking devices like Fire tablets, Fire TV, Amazon Echo and Amazon Show. The Amazon Devices group delivers delightfully unique Amazon experiences, giving customers instant access to everything, digital or physical.\\n\\nAre you interested in a fast-paced, high-growth environment with the opportunity to work on business-critical decisions? Amazon Device Accessories is looking for an outstanding Business Intelligence Engineer to join our Operational Excellence Team. We’re looking for someone who can provide insight on KPI’s, understand inferential statistics and advise business teams on how to optimize for profit.\\n\\nAs an engineer on the team, you'll leverage tools and services including Amazon Redshift, Tableau, AWS Glue, AWS Athena, Spark, EMR, Machine Learning and Time Series models to build solutions that deliver data-driven reports, dashboards, and recommendations to high level leadership.\\n\\nYou'll work directly with business leaders and stakeholders to understand different business problems and use cases. You'll work with Finance, Tech and Business teams to identify and consume data sources, transform the data, and build the reports and visualizations needed to meet the requirements. You’ll have the opportunity to get hands on experience with Machine Learning, Time Series Modelling and high impact business analysis.\\n\\nDeveloping this capability will provide insights that are used to lead decision making around product allocation, product effectiveness, productivity analysis and business impact. Consumers of these insights will include Directors, VP’s and SVP’s.\\n\\nOur tenets for analytics team members are as follows:\\nUtilizing the Scientific Method to make tangible business impactMetrics before Messes\\no Ensuring we’re measuring the right business metrics to guide the business\\nForecast or be Last\\no Developing state of the art predictive models for ensuring we’re moving in the right direction\\n\\nRoles and Responsibilities:\\nBuild data solutions using AWS services that deliver data-driven reports, dashboards, and tools.Develop and implement Time Series and Machine Learning Forecast ModelsManage marketing and sales data for the organizationManage ETL pipelines using AWS EMR and SparkDistill problem definitions, models, and constraints from informal business requirements.Provide innovative self-service tools to our customers to self-serve and scale dataFollow established engineering best practices and define new best practices where required.Identify critical metrics/reports that measure product performance, efficiency/effectiveness and create client facing dashboards to facilitate decision making.Collaborate on the design, development, maintenance, and delivery/presentation of forecasting models, metrics, reports, analyses, tools, and dashboardsPerform proactive diagnostic analysis on the various product measures and surface meaningful insights to the leadership team.Collaborate with Data Scientists, Data Engineers and Economists to develop Product Insights on Marketing and Sales data.\\n\\nMasters in engineering, science, math, statistics or computer scienceExperience using AWS services for data analytics (i.e., Athena, Glue, Redshift, EMR, etc.)Experience developing custom ETL solutions using Python and SQLExperience with Tableau Desktop and Tableau ServerStrong written and verbal communications skills. Having the ability to translate scientific findings into business recommendations and outputs.The ability to influence stakeholders through delivering results and earning trustBasic statistical tests (but not limited to) t-tests, chi-square and regressionExpert SQLProficiency in PythonExperience delivering the best Products to customers\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer, Global Specialty Fulfillment</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BS in Computer Science, Math, Physics, or Engineering6+ years relevant work experience in software development or related data-driven fieldKnowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computingDemonstrated experience in relational database concepts with an expert knowledge of SQLDemonstrated ability in data modeling, ETL development, and Data warehousing\\n\\nLove food? We do! The AmazonFresh and Prime Now operations finance team is seeking an experienced and innovative Data Engineer to build tools that support Operations teams in AmazonFresh and Prime Now. We are an analytics team responsible for building tools, analysis, and reporting to support internal leaders within fulfillment, last mile, and supply chain operations. This is a unique opportunity for someone interested in Amazon’s start-up consumables-focused environment. AmazonFresh and Prime Now experiment, fail fast, learn, and scale rapidly.\\n\\nUltra-fast delivery delights Amazon customers by delivering what they want quickly: medication for a sick kid, lunch at work when you forgot, food and drinks for a party, last minute gifts, dinner from a local restaurant, and so many more uses.\\n\\nThe business model of ultra-fast delivery is attractive, and offers our Engineering team the opportunity to work on any number of complex technical problems. Our team designs, builds and owns our end-to-end services from the ground up and works on large scale back-end systems to support the entirety of our order and inventory pipelines.\\n\\nWe are seeking Data Engineer. In this role you will:\\n\\nYou help build the infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, data storage principles, and recent advances in distributed systems\\nYou manage AWS resources.\\nYou collaborate with Business Intelligence Engineers (BIEs) to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation\\nYou help drive the architecture and technology choices that enable a world-class user experience\\nYou develop expertise in a broad range of Amazon’s data resources and know when, how, and which to use and which not to use\\nYou encourage the organization to adopt next-generation data architecture strategies, proposing both data flows and storage solutions\\nYou are comfortable with a degree of ambiguity and willing to develop quick proof of concepts, iterate and improve\\nYou create extensible designs and easy to maintain solutions with the long term vision in mind\\nYou have an understanding and empathy for business objectives, and continually align your work with those objectives and seek to deliver business value. You listen effectively.\\nYou are comfortable presenting your findings to large groups\\n\\nWe have a very flat team structure, and offer a unique opportunity for technical leaders who want to work closely with the business in defining, designing, building and operating products that are in the early stage of fast expansion.\\n\\nExperience working with AWS Big Data TechnologiesExperience working with Open Source Big Data toolsProven track record of delivering a big data solutionExperience developing tools for data engineers and machine learningExperience working with both Batch and Real Time data processing systems\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Vet / Gender Identity / Sexual Orientation / Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Business Intel Engineer III</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BA/BS in Computer Science, Engineering, Statistics, Mathematics, Finance or related field.5+ years’ experience as a BIE, data scientist, data engineer or similar job function with a technology company.Demonstrated strength in SQL, data modeling, ETL development, and data warehousing.Advanced skills in Excel as well as any data visualization tools like Quicksight, Tableau or similar BI tools.Experience of working in very large data warehouse environment and multi data sources.Familiarity with AWS solutions such as Redshift, S3.Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management.Have ability to influence and drive program from end to end.\\n\\nAmazon seeks an experienced Business Intelligence Engineer (BIE) to join Enterprise Risk Management and Compliance (ERMC) team. Our mission is to prevent denied parties from transacting with Amazon businesses, including AWS, customers, vendors, sellers, subsidiaries etc. We screen events in billion every day.\\n\\nYou efficiently and routinely deliver the right things. You are seasoned BI. You have broad knowledge on Amazon businesses, data, systems and tools. You have a department-wide view of the analytical solutions that you build, and you consistently think in terms of automating or expanding the results beyond our business. You are a key influencer in your team’s strategy and contribute significantly to team projects. You will drive reporting/analytics projects from end-to-end. You supervise the creation and implementation of BI projects, provide mentor and guidance to team members, help them to achieve their goals.\\n\\nThis role has opportunity to grow to a BI manager in the future.\\n\\nDuties &amp; responsibilities for this role will include:\\n\\nInterface with business customers to gather requirements, drive reporting or analytic projects to help solve complex challenges.\\nDesign, implement, and support datasets that provide structured data for reporting and analysis.\\nDevelop Tableau/QuickSight dashboards across various business teams to drive adoption and increase visibility into key measures of business performance.\\nAnalyze billions of rows of data to find the root causes behind variances and drive business insights.\\nInvestigate and implement new big data technologies to provide automatic resolutions to address business needs.\\nMentor junior team members and help them grow.\\nThe successful candidate will demonstrate good business acumen, experience in developing reports and conducting analysis, strong communication skills, an ability to work effectively with cross functional teams, and an ability to work in an ever-changing environment.\\n\\nMBA or Master’s in Computer Science, Engineering, Statistics, Mathematics, Finance or related field.Experience in projects involving complex data sets and high variability.Working knowledge of statistical methodologies.Experience to conduct complex data analysis and use ML models.A history of teamwork and willingness to roll up one’s sleeves to get the job done.Excellent communication (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams.Experience to handle confidential and sensitive data.Design and develop data infrastructure to support business growth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Manager III, Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experience mentoring and managing other Data Engineers or Software Development Engineers, ensuring data engineering best practices are being followedA desire to work in a collaborative, intellectually curious environment.Bachelors degree in Computer Science, Mathematics, Physical Sciences or a related STEM field5+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc.Experience in maintaining data warehouse systems and working on large scale data transformation using EMR, Hadoop, Hive, or other Big Data technologiesDemonstrated strength in SQL, data modeling, ETL development, and data warehousingExperience with administering and supporting multiple relational and non relational DBs managing data at peta byte scaleExperience with hardware provisioning, forecasting hardware usage, and managing to a budget\\n\\nThe S3C Compliance team owns the end-to-end compliance experience for over two million active Selling Partners. We own innovation in food and product safety, compliance for global trade, and accuracy for dangerous goods classifications. We support worldwide product, program, data science, and analytics teams. We provide scalable technology to improve safety and compliance in throughout the supply chain.\\n\\nWe are looking for an experienced Data Engineering Manager to lead the Risk Data Technologies team, manage existing data resources, implement new technologies and tooling to further enable science and analytics, as well as help drive scalable data sharing practices. In this role you will split your time between hands on development and managerial activities. You will own data environments, integrate with new technologies, and oversee the development of new processes that support teams across the global compliance organization. You will gather requirements through direct interaction with business, science, as well as software development teams. You will track the performance of our resources and related capabilities, constantly evolving our offering in order to scale our capability set with the growth of the business and needs of our customers.\\n\\nThe ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.\\n\\nExtensive experience working with AWS with a strong understanding of Redshift, EMR, Athena, Aurora, DynamoDB, Kinesis, Lambda, S3, EC2, etc.Extensive Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Strong interpersonal skills and the ability to communicate complex technology solutions to senior leadership, gain alignment, and drive progress\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer, Talent Assessment</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Degree in Computer Science, Engineering, Mathematics, or a related field, or substantial industry experience5+ years of experience data warehousingExpertise in Redshift, Netezza, Teradata or other MPP databasesExperience with data modeling and ETL (load/transform) developmentExceptional command of SQLHands on experience with a business intelligence tool such as Tableau, OBIEE, Cognos, SSRS/SSAS, or QuickSightKnowledge of a programming or scripting language (Python, Perl, or Javascript)\\n\\nAmazon’s Talent Assessment team designs and implements groundbreaking hiring solutions for one of the world’s fastest growing companies. We work in a fast-paced, global environment where we must solve complex problems (ranging from research-based to technical) and deliver solutions that have impact.\\n\\nWe are seeking a Data Engineer who can lead the development and support of the analytic technologies as we use data-based solutions to design and experiment with new hiring solutions that predict success for highly complex roles (technical and non-technical). Be part of a global team that helps select for roles having great impact on Amazon globally (for example, AWS, Kindle, Retail, and Business &amp; Development). You will enable flexible and structured access to our global candidate data to support our strategies. This includes implementation of a BI platform, promotion of scalability through automation and reporting tools, and adherence to the highest data quality and governance standards. The candidate will also drive the design and implementation of world class big data infrastructure to support machine learning and econometric analysis. This role is based in Seattle, WA.\\n\\nMaster’s degree in Information Systems or a related fieldExperience with Java and Map Reduce frameworks such as Hive/Hadoop.Familiarity with EMR, NoSql, Dynamo DBStrong organizational and multitasking skills with ability to balance competing priorities.Exceptional written communication skills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>A desire to work in a collaborative, intellectually curious environment.Degree in Computer Science, Engineering, Mathematics, Physics, or a related field and at least 4 years work experienceIndustry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.Demonstrated ability in data modeling, ETL development, and data warehousing.Experience with Oracle, Redshift, Teradata, etc.Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)\\n\\nMachine Learning is changing the way new products, programs and services are created and intelligently marketed to customers to increase customer engagement. The Consumer Analytics team is uniquely organized with SDEs, DEs, BIEs and Research Scientists to solve challenging predictive analytics problems at Amazon scale. Are you passionate about Big Data (Amazon scale), Machine Learning and Predictive Analytics software?\\n\\nBig Data Processing\\nWe are responsible for the production, processing, and analysis of several TB’s of customer grain data on a daily basis. We analyze data coming from various traffic channels such as onsite, free search, paid search, social, paid social email, associates etc. We heavily use AWS services such as AWS Flow, S3, EC2, EMR (Hadoop/Spark), Kinesis, DynamoDb to manage our data workflows.\\n\\nMachine Learning\\nWe build various Machine Learning solutions that learn and become better with time by the addition of new data and validation methodologies. We work with both supervised and unsupervised machine learning approaches not limited to regression, classification, clustering etc.\\n\\nOur products have become Amazon wide programs and are accelerating in adoption across businesses. We have services that provide predictions from our models to influence Amazon customer experience. However, as we look forward our rate of innovation is dependent on the quality and breadth of data we input to these models. As Amazon is pivoting towards worldwide simultaneous launches of programs like Prime and Amazon Video our fundamental data models need to be re-engineered. We also need serious engineering behind the data pipelines to establish and support stronger SLAs as businesses take dependency on our outputs.\\n\\nWe are looking for an outstanding individual who combines superb technical, communication, and analytical capabilities with a demonstrated ability to get the right things done quickly and effectively. This person must be comfortable working with a team of software development engineers to raise the bar of the data pipelines we build and maintain. Given the cross Amazon nature of our products, the individual should be highly self-directed having good cross-team collaboration skills.\\n\\nThe ideal candidate for our team is a thinker and a doer: someone who loves sophisticated algorithms and mathematical precision, but at the same time enjoys implementing real systems, and is motivated by the prospect of spectacular business returns.\\n\\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience building data products incrementally and integrating and managing datasets from multiple sourcesExperience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologiesExperience providing technical leadership and mentor other engineers for the best practices on the data engineering spaceExperience using machine learning and statistical tools such as Python/Pandas, R etc .Linux/UNIX including to process large data sets.\\nAmazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Engineer I</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>A desire to work in a collaborative, intellectually curious environment.Degree in Computer Science, Engineering, Mathematics, or a related field and 4-5+ years industry experienceOne year of experience in the following skill(s):Developing and operating large-scale data structures for business intelligence analytics using: ETL/ELT processes; OLAP technologies; data modeling; SQL;Experience with at least one relational database technology such as Redshift, Oracle, MySQL or MS SQLExperience with at least one massively parallel processing data technology such as Redshift, Teradata, Netezza, Spark, or Hadoop based big data solution\\n\\nHealth, Safety, Security, and Compliance (HS3C) is responsible for keeping our Customers and partners safe, and ensuring we maintain WW compliance. We build scalable solutions that grow with the Amazon business. HS3C-Compliance team collects petabytes of data from thousands of data sources inside and outside Amazon including the Amazon catalog system, inventory system, customer order system, and page views on the website. We provide interfaces for our internal customers to access and query the data hundreds of thousands of times per day, using Amazon Web Service’s (AWS) Redshift, Hive, and Spark.\\n\\nHS3C-Compliance is growing, and the data processing landscape is shifting. Our data is consumed by teams across HS3C including Research Scientists, Machine Learning Specialists, Business Analysts, and Data Engineers. We are seeking an outstanding Data Engineer to join the HS3C-Compliance data technologies team. The HS3C-Compliance data technologies team manages the core HS3C business data from hundreds of source systems. Amazon has culture of data-driven decision-making, and demands business intelligence that is timely, accurate, and actionable. If you join the HS3C-Compliance data technologies team, your work will have an immediate influence on day-to-day decision making at Amazon.\\n\\nAs an Amazon Data Engineer II, you will be working in one of the world's largest cloud-based data lakes. You should be skilled in the architecture of data warehouse solutions for the Enterprise using multiple platforms (EMR, RDBMS, Columnar, Cloud). You should have extensive experience in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.\\nAs a Data Engineer II on the HS3C-Compliance data technologies team, you will design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for analytics and deep learning. Implement data ingestion routines both real time and batch using best practices in data modeling, ETL/ELT processes leveraging AWS technologies and Big data tools. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers. Evaluate and make decisions around the use of new or existing software products and tools. Mentor junior data engineers.\\n\\nIndustry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience building data products incrementally and integrating and managing datasets from multiple sourcesQuery performance tuning skills using Unix profiling tools and SQLExperience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologiesExperience providing technical leadership and mentor other engineers for the best practices on the data engineering spaceLinux/UNIX including to process large data sets.Experience with AWS\\nOpportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AWS Data Engineer</td>\n",
       "      <td>Seattle, WA 98104</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98104</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.</td>\n",
       "      <td>DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:\\nProvide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.\\n- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)\\n\\nBasic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\n§ Certified AWS Developer - Associate\\n§ Certified AWS DevOps – Professional (Nice to have)\\n§ Certified AWS Big Data Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud\\nExperience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus\\n\\nProfessional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Bellevue, WA 98004</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>98004</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree required\\n3-5 years of experience in database technologies\\nMinimum 3 years of experience in Data Warehousing\\nExperience with AWS tools (S3/EC2/Athena/Redshift Spectrum/IAM)\\nExperience with Python\\nFamiliarity with Tableau or other data visualization tools.\\nExperienced with Linux administration and scripting\\nDeep experience with data modeling, data pipelines, SQL, AWS, and distributed compute platforms\\nExperience building effective relationships with a broad range of partners\\nAbility to communicate well with partners, both technical and non-technical</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCollect, transform, analyze, and refine operational and customer data.\\nBuild-out data structures designed to efficiently answer business questions.\\nAssist in evolving data structures from a MSSQL Server footprint into a data lake environment.\\nDevelop, implement and tune current ETL processes.\\nAssist with ad hoc report generation and analysis for merchandise and customers.\\nCreate pipelines from internal and external data sources to AWS using custom python scripts.\\nAssist in managing our Tableau ecosystem and provide technical support.</td>\n",
       "      <td>\\nBachelor’s degree required\\n3-5 years of experience in database technologies\\nMinimum 3 years of experience in Data Warehousing\\nExperience with AWS tools (S3/EC2/Athena/Redshift Spectrum/IAM)\\nExperience with Python\\nFamiliarity with Tableau or other data visualization tools.\\nExperienced with Linux administration and scripting\\nDeep experience with data modeling, data pipelines, SQL, AWS, and distributed compute platforms\\nExperience building effective relationships with a broad range of partners\\nAbility to communicate well with partners, both technical and non-technical</td>\n",
       "      <td>\\nWhile performing the duties of this job, the associate is regularly required to talk or hear. The associate is frequently required to sit; stand; walk; use hands to finger, handle or feel; as well as reach with hands and arms.\\nThe associate must frequently lift and/or move up to 15 pounds and occasionally lift and/or move up to 35 pounds. Specific vision abilities required by this job include close vision, distance vision, depth perception and ability to adjust focus.\\nAbility to work in open environment with fluctuating temperatures and standard lighting\\nAbility to work on computer and mobile phone for multiple hours; with frequent interruptions\\nRequired to travel in elevator or stairwells to attend meetings and engage with associates on multiple floors throughout building\\nHotel, Airplane, and Car Travel Required</td>\n",
       "      <td>Overview\\nResponsible for unlocking the value in our ever growing data by creating new and improved techniques and solutions for data collection, management, usage and reporting.\\nResponsibilities\\nCore Accountabilities:\\nCollect, transform, analyze, and refine operational and customer data.\\nBuild-out data structures designed to efficiently answer business questions.\\nAssist in evolving data structures from a MSSQL Server footprint into a data lake environment.\\nDevelop, implement and tune current ETL processes.\\nAssist with ad hoc report generation and analysis for merchandise and customers.\\nCreate pipelines from internal and external data sources to AWS using custom python scripts.\\nAssist in managing our Tableau ecosystem and provide technical support.\\nQualifications\\nEducation/Experience Required:\\nBachelor’s degree required\\n3-5 years of experience in database technologies\\nMinimum 3 years of experience in Data Warehousing\\nExperience with AWS tools (S3/EC2/Athena/Redshift Spectrum/IAM)\\nExperience with Python\\nFamiliarity with Tableau or other data visualization tools.\\nExperienced with Linux administration and scripting\\nDeep experience with data modeling, data pipelines, SQL, AWS, and distributed compute platforms\\nExperience building effective relationships with a broad range of partners\\nAbility to communicate well with partners, both technical and non-technical\\n\\nPhysical Requirements:\\nThe physical demands described here are representative of those that are required by an associate to successfully perform the essential functions of this job.\\nWhile performing the duties of this job, the associate is regularly required to talk or hear. The associate is frequently required to sit; stand; walk; use hands to finger, handle or feel; as well as reach with hands and arms.\\nThe associate must frequently lift and/or move up to 15 pounds and occasionally lift and/or move up to 35 pounds. Specific vision abilities required by this job include close vision, distance vision, depth perception and ability to adjust focus.\\nAbility to work in open environment with fluctuating temperatures and standard lighting\\nAbility to work on computer and mobile phone for multiple hours; with frequent interruptions\\nRequired to travel in elevator or stairwells to attend meetings and engage with associates on multiple floors throughout building\\nHotel, Airplane, and Car Travel Required\\n\\nPosition Type/Expected Hours of Work:\\nThis is a full-time position. As an International Retailer, occasional evening and/or weekend work may be required during periods of high volume. This role operates in a professional office environment and routinely uses standard office equipment.\\n\\nOther Considerations:\\nPlease note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the associate for this job. Duties, responsibilities and activities may change at any time with or without notice. Reasonable accommodations may be made to qualified individuals with disabilities to enable them to perform the essential functions of the role.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Senior Data Engineer - Turn 10 Studios</td>\n",
       "      <td>Redmond, WA</td>\n",
       "      <td>Redmond</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nEnthusiasm for cars and/or gaming - Our priority is making amazing racing games. The ideal candidate must have enthusiasm for our products and empathy for our customers.\\nEnthusaism for cloud data technology - Our pipelines are fully supported by Azure leveraging things like Data Explorer (Kusto), Data Warehouse, Data Factory, Data Lake, SQL and Power BI. The ideal candidate has a passion for cloud technology and a minimum of 5 years' experience.\\nA drive to develop data insights - Collecting data is the easy part. Helping business leaders and game designers ask the right questions and answering these questions with a relentless attention to details (accuracy) is where the fun begins. The ideal candidate is a meticulous gatekeeper for data and code quality, passionate about generating insights from data, and a strong communicator and collaborator.\\nEnthusiasm for AI/ML or an interest to learn - We don't do science projects, but we have an aspiration to build AI/ML capabilities that generate customer value.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nEvolving our big data pipelines to streamline data collection (measure things) and democratize the consumption of data (generate information).\\nWorking with business leaders and game designers to answer the key questions that enable the team to drive franchise growth and create experiences that thrill customers.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Games. Xbox. Big data. AI/ML. Turn 10 Studios, makers of the award winning, billion-dollar Forza franchise, is searching for a senior engineer to help build our next generation data pipelines. You would be joining a small team of awesome people that move fast, innovate daily, and have fun (we make games!).\\n\\nAnalytics is a crucial part of the business at Turn 10. Understanding user motivations and behaviors enables the team to build fun, engaging racing experiences on PC, Xbox and mobile. The gaming industry is evolving to a GaaS model (Games as a Service) where the most successful studios will learn continuously from data and respond rapidly to customer's needs. This is our challenge. As a leader on the data engineering team, you will be front and center building the platform that will enable the entire studio to quickly learn, adapt and transform our games.\\nResponsibilities\\nYou'll focus on:\\n\\nEvolving our big data pipelines to streamline data collection (measure things) and democratize the consumption of data (generate information).\\nWorking with business leaders and game designers to answer the key questions that enable the team to drive franchise growth and create experiences that thrill customers.\\n\\nExpanding the studios capabilities in AI/ML, building an intelligent cloud for Forza.\\nQualifications\\nWe only have a few requirements:\\n\\nEnthusiasm for cars and/or gaming - Our priority is making amazing racing games. The ideal candidate must have enthusiasm for our products and empathy for our customers.\\nEnthusaism for cloud data technology - Our pipelines are fully supported by Azure leveraging things like Data Explorer (Kusto), Data Warehouse, Data Factory, Data Lake, SQL and Power BI. The ideal candidate has a passion for cloud technology and a minimum of 5 years' experience.\\nA drive to develop data insights - Collecting data is the easy part. Helping business leaders and game designers ask the right questions and answering these questions with a relentless attention to details (accuracy) is where the fun begins. The ideal candidate is a meticulous gatekeeper for data and code quality, passionate about generating insights from data, and a strong communicator and collaborator.\\nEnthusiasm for AI/ML or an interest to learn - We don't do science projects, but we have an aspiration to build AI/ML capabilities that generate customer value.\\n\\nMicrosoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.\\n\\nBenefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Engineer- Python</td>\n",
       "      <td>Seattle, WA 98104</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98104</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of experience in core JAVA and SQL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Senior Consultant, you will focus on managing the information supply chain from acquisition to ingestion, storage and the provisioning of data to points of impact by modernizing and enabling new capabilities. Information value is enhanced through enterprise-scale applications that enable visualization, consumption and monetization of both structured and unstructured data. Big data is becoming one of the most important technology trends that has the potential for dramatically changing the way organizations use information to enhance the customer experience and transform their business models.\\nWork you'll do\\n\\nSenior Consultants work within an engagement team. Key responsibilities will include:\\n Function as integrators between business needs and technology solutions, helping to create technology solutions to meet clients’ business needs.\\n Identifying business requirements, requirements management, functional design, prototyping, process design (including scenario design, flow mapping), testing, training, defining support procedures and supporting implementations.\\n\\nThe Team\\n\\nAnalytics &amp; Cognitive\\n\\nIn this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.\\n\\n\\nThe Analytics &amp; Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy &amp; Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.\\n\\n\\nAnalytics &amp; Cognitive will work with our clients to:\\n Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms\\n Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions\\n Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements\\n\\n\\nQualifications\\n\\nRequired:\\n\\n 5+ years of experience in core JAVA and SQL\\n 3+ years of experience in Python&amp; Unix Shell Scripting\\n 3+ years of experience in building scalable and high performance data pipelines using Apache Hadoop, Map Reduce, Pig &amp; Hive\\n Experience with bigdata cross platform compatible file formats like Apache Avro &amp; Apache Parquet\\n Experience in Apache Spark is a plus\\n 1+ years of experience with data lake implementations, core modernizations and data ingestion\\n\\n 1 or more years of hands on experience designing and implementing data ingestion techniques for real time and batch processes for video, voice, weblog, sensor, machine and social media data into Hadoop ecosystems and HDFS clusters.\\n 2+ years of experience leading workstreams or small teams\\n Willingness for weekly client-based travel, up to 80-100% (Monday — Thursday/Friday)\\n Bachelor’s Degree or equivalent professional experience\\n\\n Preferred:\\n\\nAWS Certification, Hadoop Certification or Spark Certification\\nExperience with Cloud using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)\\nExperience with data integration products like Informatica Power Center Big Data Edition (BDE), IBM BigInsights, Talend etc.\\nExperience designing and implementing reporting and visualization for unstructured and structured data sets\\nExperience in designing and implementing scalable, distributed systems leveraging cloud computing technologies like AWS EC2, AWS Elastic Map Reduce and Microsoft Azure\\nExperience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching\\nKnowledge of data, master data and metadata related standards, processes and technology\\nExperience working with multi-Terabyte data sets\\nExperience with Data Integration on traditional and Hadoop environments\\nAbility to work independently, manage small engagements or parts of large engagements.\\nStrong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint).\\nStrong problem solving and troubleshooting skills with the ability to exercise mature judgment.\\nEagerness to mentor junior staff.\\nAn advanced degree in the area of specialization is preferred.\\n\\nHow you’ll grow\\n\\nAt Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.\\n\\n\\nBenefits\\n\\nAt Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.\\nDeloitte’s culture\\n\\nOur positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.\\n\\n\\nCorporate citizenship\\n\\nDeloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.\\n\\n\\nRecruiter tips\\n\\nWe want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.\\n\\n#LI:PTY\\n#IND:PTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Senior Data Engineer- Enterprise Data Platform</td>\n",
       "      <td>Seattle, WA 98101</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDesign, implement, and support a platform providing secured access to large datasets.\\nCollaborate with customers from Finance, HR and other shared service functions understanding their requirements and delivering data solutions they need.\\nUnderstand the data sources, develop an ETL strategy with these sources, perform data modeling to meet customers’ data needs.\\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.\\nRecognize and adopt best practices in reporting and analysis: data integrity, data security, analysis, validation, and documentation.\\nMonitor, manage and administer the Enterprise Applications Data Warehouse and ensure optimal performance at scale.\\nDesign, implement and manage the access to the datasets based on the Zillow’s data access policies.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About the team\\nAre you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to create the next-generation tools for intuitive data access?\\n\\nZillow Group's Enterprise Applications organization is seeking a savvy Data Engineer to join our team and shape the future of our Enterprise data platform. This data platform serves Analytics and reporting needs of Zillow's Finance, HR and other organizational functions. You will be responsible for building data collection, transformation and processing pipeline automation for Zillow’s Enterprise function data to support Zillow’s rapidly growing and dynamic businesses, and use it to deliver the BI and Insights critical for success of these functions.\\n\\nOur team enjoys a good challenge and we celebrate our successes together. If you enjoy working in a supportive environment that encourages creativity and promotes ownership and career growth, come and join us.\\nAbout the role\\nThe right candidate has experience in building and maintaining data warehouses and BI systems for Enterprise functions and shared services, and is excited at the opportunity to build and optimize our data systems from the ground up.\\nKey Responsibilities:\\nDesign, implement, and support a platform providing secured access to large datasets.\\nCollaborate with customers from Finance, HR and other shared service functions understanding their requirements and delivering data solutions they need.\\nUnderstand the data sources, develop an ETL strategy with these sources, perform data modeling to meet customers’ data needs.\\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.\\nRecognize and adopt best practices in reporting and analysis: data integrity, data security, analysis, validation, and documentation.\\nMonitor, manage and administer the Enterprise Applications Data Warehouse and ensure optimal performance at scale.\\nDesign, implement and manage the access to the datasets based on the Zillow’s data access policies.\\nWho you are\\nAs a Data Engineer, you should be an expert with data warehousing components (e.g. Data Modeling, ETL and Reporting) and integrations. You should have a deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management of large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. You should have strong analytical Skills.\\nQualifications:\\nBS in CS or similar and 8+ years professional experience, or MS with 5+ years of Professional experience.\\nSolid Experience in dimensional data modeling, ETL development, and Data Warehousing.\\nHands on experience building and supporting Enterprise level Data warehouses by sourcing data from SaaS applications like Workday, Anaplan, Zuora or other ERPs.\\nSolid experience with data modelling, SQL, writing, debugging and performance tuning.\\nExperience with any BI reporting tools like Tableau, Domo.\\nBasic database administration.\\nSystem monitoring and alerting, dashboarding experience.\\nExperience with Snowflake is a plus.\\nExperience with middleware tools such as Boomi, Workato or Mulesoft.\\nYou know how to work with high volume of rapidly changing data.\\nGet to know us\\nZillow Group houses the largest portfolio of real estate brands on mobile and the web. We are on a mission to rewire the real estate transaction and are building transformational tools and services that make it easier for everyone to find and get into a home they love. We are working to create an on-demand real estate transaction experience for every stage of the home lifecycle - for buyers, sellers, renters and borrowers - and we're well on our way. No matter what job you're in, you will play a critical role in making this vision a reality for millions of people.\\nAt Zillow Group, we're powered by our inclusive work culture, where everyone has the support and resources to do the best work of their careers. Our efforts to streamline the real estate transaction is supported by our passion to empower people and enrich lives around everything home, a deep-rooted culture of innovation, a fundamental commitment to Equity and Belonging, and world-class benefits. But, don't just take our word for it. Read our reviews on Glassdoor and recent recognition from multiple organizations, including: Fortune 100 Best Companies to Work For (#69), Fortune Best Workplaces for Diversity (#38), Fortune Best Workplaces for Parents (#31), Fortune Best Workplaces for Women (#20), Fatherly's Best Workplaces for New Dads (#37), JUST Capital 100 Company (#69), Bloomberg Gender Equality Index constituent.\\nZillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sr. Data Engineer - Worldwide Operations Talent Management</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree in a quantitative/technical discipline such as Computer Science, Engineering, Statistics5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analyticsDemonstrable skills and experience writing complex, highly-optimized SQL queries across large data setsExperience in scripting languages such as PythonExperience with AWS technologies such as Redshift, EMR, S3Experience in building reporting solutions using BI tools\\n\\nAt WW Operations (WWOps) Talent Management, we use data to drive all our decision making. The Talent Analytics team acts as an internal provider of analytical solutions, using quantitative approaches to uncover insight to drive changes and inform decisions. As the Data Engineer, you will work on a team transforming massive, complex data into quantifiable relationships, trends and actionable insights. We are looking for a strong problem-solver, who is skilled using data and technologies to implement solutions.\\n\\nIn this role, you will design, implement and manage an analytical data infrastructure. You will collaborate closely with other engineers and scientists across the organization to create robust and scalable solutions to flow data from various source systems into the data warehouse and into end-user facing applications.\\n\\nThis role also requires a significant understanding of data mining and analytical techniques. You will be expected to deliver at a high level against ambiguous problems with minimal technical supervision. The successful candidate will have strong technical and analytical capabilities, business acumen, effective communication skills, and the ability to work effectively with cross-functional teams in a fast paced environment.\\n\\nExperience working with Open Source Big Data tools (Parquet, Spark, Hadoop)Advanced knowledge in AWS technologiesFamiliarity with statistical modeling and machine learning techniquesStrong verbal and written communication skills and ability to build rapport with cross-functional partners\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Business Intelligence Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Own the design, development, and maintenance of scalable solutions for ongoing metrics, reports, analyses, dashboards, etc. to support analytical and business needs.Translate basic business problem statements into analysis requirements. Work with internal customers to define best output based on expressed stakeholder needs.Review and audit existing ETL jobs and SQL queries.Experience using Python, Ruby, or other scripting languages to automate data retrieval, manipulation, and analysis.Experience using R, SAS, or other statistical packages.\\n\\nAs we strive to be Earth's most customer-centric company, Amazon has reinvented how hundreds of millions of people shop online – providing customers with the opportunity to find and discover virtually anything they want to buy and providing millions of sellers with a platform for growing successful businesses. We are looking for an exceptional business analyst to help us develop new ways to build trust and loyalty with sellers, a crucial component of our flywheel\\n\\nSellers’ trust in Amazon is our top priority and in this role, you will be tasked with building that trust over time by diving deep into how we measure our progress and helping to identify and prioritize key areas of focus. Amazon’s growth requires leaders who move fast, have an entrepreneurial spirit to create new solutions, have an unrelenting tenacity to get things done, and are capable of breaking down and solving complex problems.\\n\\nThe successful candidate will be a self-starter, comfortable with ambiguity and be able to create and maintain efficient &amp; automated processes. They will set the vision, strategy, and roadmap, and work alongside with stakeholders in the organization to make it happen. They know and love working with business intelligence tools, can model multidimensional datasets, and can partner effectively with business leaders to answer key business questions. They are analytical and creative, and don’t quit. This is a role with high visibility to senior leadership and with high opportunity for impact for those willing to roll up their sleeves and dive deep to achieve results.\\n\\nAdvanced degree in data science, statistics, or a related field.5+ years in the analysis space as a Business Intelligence Engineer, Business Analyst, Data Engineer, or similar roles.\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Engineer I</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>A desire to work in a collaborative, intellectually curious environment.Degree in Computer Science, Engineering, Mathematics, or a related field and 4-5+ years industry experienceOne year of experience in the following skill(s):Developing and operating large-scale data structures for business intelligence analytics using: ETL/ELT processes; OLAP technologies; data modeling; SQL;Experience with at least one relational database technology such as Redshift, Oracle, MySQL or MS SQLExperience with at least one massively parallel processing data technology such as Redshift, Teradata, Netezza, Spark, or Hadoop based big data solution\\n\\nHealth, Safety, Sustainability, Security, and Compliance (HS3C) is responsible for keeping our Customers and partners safe, and ensuring we maintain WW compliance. We build scalable solutions that grow with the Amazon business. HS3C-Compliance team collects petabytes of data from thousands of data sources inside and outside Amazon including the Amazon catalog system, inventory system, customer order system, and page views on the website. We provide interfaces for our internal customers to access and query the data hundreds of thousands of times per day, using Amazon Web Service’s (AWS) Redshift, Hive, and Spark.\\nHS3C-Compliance is growing, and the data processing landscape is shifting. Our data is consumed by teams across HS3C including Research Scientists, Machine Learning Specialists, Business Analysts, and Data Engineers. We are seeking an outstanding Data Engineer to join the HS3C-Compliance data technologies team. The HS3C-Compliance data technologies team manages the core HS3C business data from hundreds of source systems. Amazon has culture of data-driven decision-making, and demands business intelligence that is timely, accurate, and actionable. If you join the HS3C-Compliance data technologies team, your work will have an immediate influence on day-to-day decision making at Amazon.\\n\\nAs an Amazon Data Engineer II, you will be working in one of the world's largest cloud-based data lakes. You should be skilled in the architecture of data warehouse solutions for the Enterprise using multiple platforms (EMR, RDBMS, Columnar, Cloud). You should have extensive experience in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.\\n\\nAs a Data Engineer II on the HS3C-Compliance data technologies team, you will design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for analytics and deep learning. Implement data ingestion routines both real time and batch using best practices in data modeling, ETL/ELT processes leveraging AWS technologies and Big data tools. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers. Evaluate and make decisions around the use of new or existing software products and tools. Mentor junior data engineers.\\n\\nIndustry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience building data products incrementally and integrating and managing datasets from multiple sourcesQuery performance tuning skills using Unix profiling tools and SQLExperience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologiesExperience providing technical leadership and mentor other engineers for the best practices on the data engineering spaceLinux/UNIX including to process large data sets.Experience with AWS\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Staff Software Engineer, Data Engineering</td>\n",
       "      <td>Seattle, WA 98107</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98107</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What you’ll be doing…\\nAre you a senior Data Engineer with experience in cloud technologies and a passion for data? As a Staff Software Engineer in the Office of the CTO Data Science &amp; Engineering team, you will be drive solutions to wide-ranging data engineering and infrastructure challenges for product and internal operations. You will partner with world-class developers, engineers, architects, and data scientists to drive thinking, provide technical leadership, and collaborate in defining best practices around data engineering. You will also work alongside local product management, engineering, and research teams to develop innovative solutions that will influence our product line.\\n\\nSome of your responsibilities include…\\nProvide technical leadership to efforts around tooling and infrastructure that enable teams to efficiently complete and maintain data science projects\\nPartner with teams on modeling and analysis problems – from transforming problem statements into analysis problems, to working through data modeling and engineering, to analysis and communication of results\\nLead code reviews, design, and best practices\\nCoach and mentor senior engineers\\nParticipate in the evolution of overall approach to Data Engineering at Tableau\\nBuild up and share best practices for the development and deployment of ML based solutions\\nUse experience gained in the above and expertise in this space to influence our product roadmap, potentially working with prototype engineering team to add additional capabilities to our products to solve more of these problems\\n\\nWho you are...\\nExperienced . 10+ years of experience in a Data Engineer role with a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\\nTechnically Savvy. A background in some specific technologies is preferred: AWS (EC2, EMR, RDS, Redshift), Python (including DS packages), SQLServer, Cassandra, Snowflake, and Data pipeline/streaming tools (Airflow, NiFi, Kafka)\\nData Rockstar. Experience building and optimizing data pipelines, architectures and data sets. A successful history of manipulating, processing and extracting value from large disconnected datasets. Deep knowledge of stream processing, and highly scalable ‘big data’ data stores. Demonstrated experience in designing or implementing enterprise-wide data strategy.\\nTeam Player. Experience supporting and working with cross-functional teams in a dynamic environment. Strong oral and written communication skills.\\nYou are a Recruiter! Tableau hires company builders and, in this role, you will be asked to be on the constant lookout for the best talent to bring onboard to help us continue to build one of the best companies in the world!\\n#LI-EF1\\nTableau Software is an Equal Opportunity Employer.\\nTableau Software is a company on a mission. We help people see and understand their data. After a highly successful IPO in 2013, Tableau has become a market-defining company in the business intelligence industry. Our culture is casual and high-energy. We are passionate about our product and our mission and we are loyal to each other and our company. We value work/life balance, efficiency, simplicity, freakishly friendly customer service, and making a difference in the world!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Business Analyst Manager</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's Degree, preferably in an analytical field - e.g. Economics, Operations, Business, Mathematics, Statistics, Finance or an in applicable social sciences field such as Criminal Justice or Law.At least four years of experience working in Analytics/Business Intelligence environmentExperience in working with databases and relational data set solutions in a business or government environmentDemonstrated use of commercial and proprietary analytical software solutions as well as query languages such as SAS, SPSS and SQL.Prior experience in design and execution of analytical projectsWorked extensively in large scale data bases and data warehousesExcellent oral/written communication and presentation skills, including an ability to effectively communicate with both internal and external stakeholders.\\n\\nEach day, hundreds of thousands of developers make billions of transactions worldwide on our cloud. They are harnessing the power of Amazon Web Services (AWS) to enable innovative applications, websites, and businesses. However, there are always a few people that try to take unfair advantage of a good thing...\\n\\nAre you interested in taking your skills and career to the next level, while having fun and fighting fraud in the cloud? How would you like to be the driving force for developing the data and insights strategy for our global AWS Investigations Fraud Team? You will be leading our Analytics Team that is central in shaping the definition and execution of the long-term data and insights strategy for AWS Investigations.\\n\\nAs the BA Manager, you will oversee a highly skilled team (Data Engineer; Business Intelligence Engineer; Business Analysts) who will design, develop, and evaluate highly innovative business intelligence tools. Your team will partner with business stakeholders to develop automated reports for AWS Investigations. Your team will constantly be seeking out new data that is useful in improving AWS Investigations, while maintaining data integrity of existing data. Your team will have the opportunity to Think Big on ways to evolve measuring, reporting, evaluating the AWS Investigator group.\\n\\nResponsibilities Include:\\nInterfacing with business customers, gathering requirements and developing new datasets in data warehouseOptimizing the performance of business-critical queries and dealing with ETL job related issuesIdentifying the data integrity issues to address them immediately to provide great user experienceExtracting and combining data from various heterogeneous data sourcesCreate, update, maintain, data visualization dashboards used by internal stakeholdersModelling data and metadata to support ad-hoc and pre-built reportingWorking with customers to fulfill their data requirement using DW tables &amp; maintain metadata for all DW TablesLead the effort to shape the definition and execution of the long-term data and insights strategy for AWS Investigations\\n\\nExperience in e-commerce/on-line companies in fraud / risk control functionsExperience in leading strategic and analytical projects related to; Fraud or Cyber Crime or Financial Crimes or White Collar Crimes or Financial Risk ManagementExposure to software development life cycle (SDLC) processHistory of constructing operational dashboards utilized by global teamsExperience working with teams across regions (U.S.; EMEA; APAC)Someone who is keen to leverage their existing skills while trying new approaches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SQL Developer</td>\n",
       "      <td>Renton, WA 98057</td>\n",
       "      <td>Renton</td>\n",
       "      <td>WA</td>\n",
       "      <td>98057</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Company Information\\nPACCAR is a Fortune 500 company established in 1905. PACCAR Inc is recognized as a global leader in the commercial vehicle, financial, and customer service fields with internationally recognized brands such as Kenworth, Peterbilt, and DAF trucks. PACCAR is a global technology leader in the design, manufacture and customer support of premium light-, medium- and heavy-duty trucks under the Kenworth, Peterbilt and DAF nameplates and also provides customized financial services, information technology and truck parts related to its principal business.\\nWhether you want to design the transportation technology of tomorrow, support the staff functions of a dynamic, international leader, or build our excellent products and services, you can develop the career you desire with PACCAR. Get started!\\nDivision Information\\nPACCAR's Information Division (ITD), located in Renton, WA utilizes cutting-edge technology to provide systems development, consulting, voice and data communications services to the entire Corporation, which has high visibility in the technology sector.\\nRequisition Summary\\nDoes empowering teams to make data driven decisions excite you? Do you wake up in the morning wondering what possibilities could be unlocked with more data? PACCAR is looking for a seasoned data engineer with AWS experience to join the team. Data Engineering focuses on making possible fast, accurate, and reliable access to data. We build data pipelines, manage a data warehouse, and support the production use of our data. We advocate for good data practices and make sure that our business users are able to make good data driven decisions.\\nJob Functions / Responsibilities\\nDoes empowering teams to make data driven decisions excite you? Do you wake up in the morning wondering what possibilities could be unlocked with more data? PACCAR is looking for a seasoned data engineer to join the team. Data Engineering focuses on understanding and modeling business and application data requirements and designing data structures and pipelines that ensure fast, accurate, and reliable access to high-quality data. We advocate for sound data practices and make sure that our business users are able to make good data driven decisions.\\nWork with business users and application architects to understand data requirements, definitions and business rules\\nCreate conceptual and logical data models that accurately reflect these requirements in a way easily understood by business users and development teams\\nWork with dev teams to create sound physical data designs that reflect the project architecture and choice of data/database technology\\nImplement data structures on a variety of database platforms, including SQL Server, Oracle, Teradata and Snowflake\\nWork with dev teams to create database objects (views, functions, stored procedures) that improve application performance, functionality and scalability\\nBuild data pipelines (including data migration from legacy data sources, cleansing and transformation), data validation frameworks, job schedules with emphasis on automation and scale\\nContribute to overall architecture, framework, and design patterns to store and process high data volumes\\nEnsure product and technical features are delivered to spec and on-time\\nDesign and implement features in collaboration with product owners, reporting analysts / data analysts, and business partners within an Agile / Scrum methodology\\nProactively support product health by building solutions that are automated, scalable, and sustainable – be relentlessly focused on minimizing defects and technical debt\\nQualifications\\nMasters’ or Bachelors' degree in Software Engineering or a related field\\n5+ years of experience in large-scale software development (preferably Agile) with emphasis on data modeling and database development\\n5+ years of experience with data modeling tools (Erwin, ER/Studio, PowerDesigner)\\n5+ years of experience with relational DBMSs and SQL coding (SQL Server, Oracle, Teradata, Snowflake)\\nAbility to communicate effectively (both orally and in writing) with business users, project team leaders and application developers\\nProficiency with ETL tools and techniques (SSIS, Attunity, Informatica)\\n2+ years experience with AWS and related services (EC2, S3, DynamoDB, ElasticSearch, SQS, SNS, Lambda, Airflow, Snowflake, etc.)\\nEducation\\n5+ years exerience or Bachelors' degree in Computer Science\\nAdditional Job Board Information\\nPACCAR is an Equal Opportunity Employer/Protected Veteran/Disability.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Rational Consulting is growing, and we need amazing talent to join our team. If you have an entrepreneurial attitude with a deep spirit of service and killer subject expertise, we want to talk to you. We are always looking for the next great consultant to raise the bar and push Rational Consulting to be better than we were yesterday. Staying hungry, curious, and looking forward to what's next is part of our DNA.\\n\\nWe are currently seeking a Data Engineer to join our tech client in Bellevue, WA. Our client drives a very progressive approach to retail technology sales. A world where passionate innovators come to collaborate and empower. This team is reinventing the way we all work, play, learn and do business. Bring your vision to their mission!\\n\\nWhat You'll Do:\\n\\nCreate and edit reports and dashboards in Power BI\\nSupport the client in identifying how to get access to required data to drive desired insights\\nHelp the client and other Rational team members identify new insights that might be available from available data\\nUphold data integrity standards\\nSupport making sure data is secured and shared with only the appropriate audiences\\nAsses new data sources and how they can enhance and augment current reporting\\n\\nWhat You'll Bring:\\n\\n2-3 years of enterprise level experience working with Power BI\\nExperience writing queries in DAX, SQL, M, and Power Query\\nKnowledge Azure SQL DB, SSIS, and Logic Apps/Flow\\nAn understanding of:\\nData modeling\\nData cleansing\\nData visualization\\nWorking knowledge of the Smartsheets platform\\n\\nWho You Are:\\n\\nWell-rounded Professional. You have amazing communication and organizational skills along with high EQ.\\nSelf-sufficient. You get stuff done, you are able to work with little direct supervision but know when to ask for help.\\nCollaborative to the Core. Demonstrated ability to work in a team environment, as a leader and member.\\nData Lover. You know your numbers and value the metrics behind the consumer story.\\n\\nRational is a results-oriented CX Solutions firm designing premium customer experiences. Each of our business practices are deeply rooted in delivering client success. We see ourselves as partners to our clients, and we invest in each of their business goals, ensuring that our work helps deliver on these goals. Client success is our ultimate metric, and what drives our mindset, skillset, and company culture.\\n\\nRational wants to make sure that all candidates have an equal opportunity to be considered for employment. Please let us know if you need any reasonable accommodation to participate in the job application or interview process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Renton, WA</td>\n",
       "      <td>Renton</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>COMPANY OVERVIEW\\nFounded in 2009, Cyborg Mobile is a human-centered consultancy providing Technology and Management Consulting Services. Cyborg Mobile provides solutions in Experience Design, Program Leadership, Organizational Change, Product Innovation, Digital Strategy, and Consumer Mobile Technology.\\nAt Cyborg Mobile, we use our combined expertise and empathetic approach to build products that delight customers. Our team members are passionate about growth, innovation and collaboration. We enjoy learning for fun and staying curious. If you have growth and ownership mindset and can work cohesively with a team, you are probably a great fit for our team!\\nTHE POSITION\\nCyborg Mobile seeks a Data Engineer to support its growing consultancy firm. This is a contract position for a public sector client of ours, with opportunity to extend into a full-time role. The key project is a data modernization effort that will help the client collect, manage, process and extract insights from data to better serve internal stakeholders and the public. This role will be part of the Data Services IT team and work closely with business analysts and SMEs.\\nPOSITION RESPONSIBILITIES\\nAs a Data Engineer, you will be responsible for all aspects of the software development lifecycle, including design, coding, integration testing, deployment and documentation. You will work in an Agile team setting to create and maintain new data applications relying heavily on experience and judgment to plan and accomplish goals.\\nSpecific breakdown of responsibilities:\\nAnalyze and resolve complex challenges around data and tools. Optimize analytical workflows by identifying opportunities and automating them\\nCollaborate with members of your team (eg, business analysts, data architect, subject matter experts) on the project's goals to understand and document business requirements\\nTranslate customer requirements into unambiguous, scalable, robust and flexible technical solutions for implementation\\nCreate and maintain architecture diagrams, data models, mapping documents, business rules, data flow diagrams and other design related artifacts\\nAssist the data warehouse team in designing efficient processes to load and manage data, including assessment of data quality in the source systems and implement appropriate business rules, data mappings, and transformation rules\\nActively participate in code reviews, unit testing, system integration testing and remedy solution defects\\nAnalyze and troubleshoot production issues quickly to ensure system uptime meets service level agreements\\nQUALIFICATIONS/REQUIREMENTS\\nWorking knowledge in data migration/integration with off-premise/cloud services such as Azure and/or AWS\\nProven handling of cost-conscious, enterprise-grade data movements and current on cloud/hybrid analytical technologies (big data, lakes, stream analytics, NoSQL, DWH, ETL/ELT)\\n5+ years of experience building, administering and managing scalable analytical platforms containing both structured and unstructured data\\nExposure to full-stack engineering (application, data, infrastructure and platform administration) and technology trends / best practices\\nStrong coder in any object-oriented language: Java, C#, Python, Javascript, etc.\\nExperience building infrastructure required for optimal ETL/ELT process for large data sets in a variety of structured and unstructured formats\\nKnowledge of big data ecosystem using tools like Hadoop, MapReduce, HBase, oozie, Flume, MongoDB, Cassandra and Pig\\nExperience working with NoSQL databases and DevOps tools: ADO, Git, Jenkins, Docker, etc.\\nKnowledge of machine learning, including pattern recognition clustering, text mining, etc.\\nAbility to work in version control and change/release management processes, alongside experience with source control mediums such as Team Foundation Server (TFS), Visual Studio Team Services (VSTS) or Git (preferred)\\nSolid understanding of data warehouse principles and multi-dimensional data modeling concepts, source to target mapping and data integration architecture. Foundational knowledge of traditional end-end ETL/OLAP solutions, preferably but not required, on the Microsoft SSIS/SSAS stack\\nExcellent written and verbal communication skills with the ability to communicate to non-technical audiences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Seattle, WA 98103</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98103</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor CS degree and 7+ years of experience in software engineering.\\nTrack record of developing and maintaining reliable, highly available, secure, high throughput web-scale data systems (e.g. Social, AdTech, MarTech, Heathcare, FinTech, etc.).\\nExperience with big data pipelines and processing (e.g. MapReduce, Hadoop, Big Query, Hive, Tez, Spark, etc.).\\nExperience with realtime streaming event logs (e.g. Kafka, GCP Cloud Pub/Sub, SNS/SQS).\\nYou influence your peers, advise senior leaders, coach and mentor junior team members.\\nYou facilitate cross-team collaboration among engineers and contribute to the broader community of senior engineers.\\nMust pass a Criminal Justice Information Services (CJIS) background check and maintain confidential and highly sensitive information.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Our mission is to protect life.\\nWe’re out to make the world a safer place by solving big problems and taking on the public safety challenges of our time. From our company's inception building the TASER device to a full suite of hardware and software solutions, we are focused on providing police agencies with the state-of-the-art devices and services they need to successfully serve and protect us. In the next few years, we're going to eliminate the burden of paperwork in policing, so officers can increase the time they spend building relationships and serving in their communities. We’ll put video at the heart of the police record so our justice system can get to the truth faster. And we won't stop innovating until the bullet is rendered obsolete.\\n\\nIt’s a big mission, but it’s one we’ll pursue relentlessly every single day.\\n\\nYour Impact\\n\\nYou are a Senior Full Stack Software Engineer with experience building large-scale software applications. At Axon, you’ll create and maintain a data architecture that is the connective tissue between Axon products and public safety systems. Successfully completing this work means that you are:\\nProtecting life by surfacing key information to ensure Officer and Jail SafetyEnabling Crime Analysts to unlock that missing piece of data to solve crimeAllowing justice to be swift and accurate by giving Prosecutors and Courts accurate informationAllowing Investigators and Forensic Technicians easy access to crime scene evidence\\n\\nWhen you are successful, you will equip public safety professionals at all levels with the information they need to protect life and truth.\\n\\nYour Day to Day\\nHelp build one of the largest cloud solutions on the planet. What you build will accelerate product innovation and help scale our platform to meet the ever-expanding needs of our growing customer base.\\nPartner with internal teams and agencies to make law enforcement data highly accessible and actionable.\\nDevelop the core platform capabilities that support Axon's product development and evolution - at scale.\\nSolve challenging problems of scale, latency, reliability, and availability. Our team will draw from your experience in these areas and support your continued growth.\\nWrite performant, maintainable code that is easy to read and well-documented.\\nBasic Qualifications\\nBachelor CS degree and 7+ years of experience in software engineering.\\nTrack record of developing and maintaining reliable, highly available, secure, high throughput web-scale data systems (e.g. Social, AdTech, MarTech, Heathcare, FinTech, etc.).\\nExperience with big data pipelines and processing (e.g. MapReduce, Hadoop, Big Query, Hive, Tez, Spark, etc.).\\nExperience with realtime streaming event logs (e.g. Kafka, GCP Cloud Pub/Sub, SNS/SQS).\\nYou influence your peers, advise senior leaders, coach and mentor junior team members.\\nYou facilitate cross-team collaboration among engineers and contribute to the broader community of senior engineers.\\nMust pass a Criminal Justice Information Services (CJIS) background check and maintain confidential and highly sensitive information.\\nPreferred Qualifications\\nExperience with Azure cloud components and .NET.\\nExperience creating and maintaining containerized web applications and serverless components.\\nFamiliarity with build and CI tools/processes like Kubernetes, TeamCity, Azure DevOps, etc.\\nETL Development for Business Intelligence.\\nExperience using statistical programming languages (e.g. Python, R) and to deliver results for real-world problems.\\nCompensation and Benefits\\nCompetitive salary and 401K with employer match\\nDiscretionary paid time off\\nRobust parental leave policy\\nAn award-winning office/working environment\\nRide along with real police officers in real life situations, see them use technology, get inspired\\nAnd more...\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Software Development Engineer, Big Data</td>\n",
       "      <td>Seattle, WA 98101</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About the team\\nJob Description Summary\\nWe build the pipelines and processes responsible for daily ingestion of terabytes of data. We productionalize intelligent, data-driven systems to help Zillow capture strategic opportunities in the market. Our work enriches Zillow's unparalleled living database of all homes and hundreds of millions of customers and empowers teams downstream to build analytics tools and products to delight our users.\\n\\nSmall team = big impact. Engineering teams are highly decentralized in order to create the small team speed and autonomy of a start-up environment but backed by big company resources.Fast-moving, developer driven organization full of brilliant and ambitious people.Learn more about what we are doing at www.zillow.com/engineering and www.zillow.com/data-science\\nAbout the role\\nWe are looking for a strong technical contributor with a background in software development to create intelligent data driven systems and pipelines. As a Data Engineer, you will be responsible for all phases of the development cycle: design, implementation, testing, and release. You will leverage your deep knowledge and experience to provide technical leadership for the team, take ideas from zero to completion, and provide the bridge between raw data and actionable business insights. You will:\\nBuild and maintain highly-scalable ETL pipelines and data-driven systems\\nWork closely with business stake holders, data analysts and machine learning engineers to productionalize analytic solutions\\nDesign and implement new data pipelines to support business analysts and data scientists\\nDesign and build infrastructure to support our petabyte scale data lake\\nWho you are\\nData engineer with experience with building and shipping highly scalable distributed systems on cloud platforms (AWS/Azure/GCP) and database technologies (SQL/NoSQL/column-orienteddatastores/distributed databases)\\nExperience with the Big Data ecosystem (Hadoop/Hive/Spark/Presto/Airflow)\\nProven track record of leading and delivering large projects independently\\nProven ability to learn new technologies quickly\\nA degree (BS/MS+) in Computer Science or a related technical discipline\\nExperience with Hive, Spark, Presto, Airflow and or Python a plus\\nGet to know us\\nZillow Group houses the largest portfolio of real estate brands on mobile and the web. We are on a mission to rewire the real estate transaction and are building transformational tools and services that make it easier for everyone to find and get into a home they love. We are working to create an on-demand real estate transaction experience for every stage of the home lifecycle - for buyers, sellers, renters and borrowers - and we're well on our way. No matter what job you're in, you will play a critical role in making this vision a reality for millions of people.\\nAt Zillow Group, we're powered by our inclusive work culture, where everyone has the support and resources to do the best work of their careers. Our efforts to streamline the real estate transaction is supported by our passion to empower people and enrich lives around everything home, a deep-rooted culture of innovation, a fundamental commitment to Equity and Belonging, and world-class benefits. But, don't just take our word for it. Read our reviews on Glassdoor and recent recognition from multiple organizations, including: Fortune 100 Best Companies to Work For (#69), Fortune Best Workplaces for Diversity (#38), Fortune Best Workplaces for Parents (#31), Fortune Best Workplaces for Women (#20), Fatherly's Best Workplaces for New Dads (#37), JUST Capital 100 Company (#69), Bloomberg Gender Equality Index constituent.\\nZillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Principal Engineer, Software</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>One or more of the following: SQL, Hive, Pig, R, Matlab, SAS, Python, Java, Ruby, C++, Perl, MDX, DAX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Technical System Expertise: Understands system protocols, how systems operate, and data flows. Aware of current technology benefits and trends. Understand the building blocks, interactions, dependencies and tools required to complete software and automation work. In-depth understanding of enterprise data warehouse patterns and technologies. Have experience with \"Big Data\" model paradigms such as MapReduce in order to build a scale out data processing solutions.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>This role is tasked with developing, constructing, testing and maintaining architectures and software in support of data transfer between systems. This is done by aligning architectural capabilities with requirements from the business to engineer data pipelines from data sourcing, ingestion, enrichment, and modeling. The Principal Data Engineer also optimizes existing ETL/ELT processes, builds APIs for data access, and analyzes source systems for optimization. They do this by partnering with Technical Product Managers, Data Scientists, Data Architects, and Business Functional Analysts, to build and deliver analytics solutions with a product mindset. Their experience and knowledge allows them to apply DevOps principals to data engineering processes to ensure reliable, accurate, and complete data pipelines.\\n\\nResponsibilities\\nTechnical System Expertise: Understands system protocols, how systems operate, and data flows. Aware of current technology benefits and trends. Understand the building blocks, interactions, dependencies and tools required to complete software and automation work. In-depth understanding of enterprise data warehouse patterns and technologies. Have experience with \"Big Data\" model paradigms such as MapReduce in order to build a scale out data processing solutions.\\nTechnical Engineering Services: Supports engineering projects by developing software solutions for data and analytics; conducting tests and inspections; preparing reports and calculations. Expected to supervise engineering teams on occasion.\\nInnovation: Presents new ideas which improve an existing system/process/service. Presents new ideas which utilize new frameworks to improve an existing system/process/services. Express new perspectives based on independent study of the industry. Review current company processes to highlight questions that may drive process refinement.\\nTechnical Writing: Maintains knowledge of existing technology documents. Writes basic documentation on how technology works. Contributes clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption at the VP level.\\nTechnical Leadership: Collaborates with technical teams and utilizes system experience to deliver technical solutions. Continuously learns new technologies.\\nTechnology Strategy: Understand current technology that supports business goals. Understand technology trends and how technical investments may be affected by changes within those trends. Identifies risks and mitigation strategies from a technical perspective as it relates to this field.\\nAlso responsible for other Duties/Projects as assigned by business management as needed.\\n\\nQualifications\\nOne or more of the following: SQL, Hive, Pig, R, Matlab, SAS, Python, Java, Ruby, C++, Perl, MDX, DAX\\nSQL and SSAS Expert\\nProficient in BI/Analytics and Data Prep tools such as Tableau, PowerBI, CLIQ, Alteryx, SAP Data Hub Modeler, etc.\\nExpert in ETL/ELT Deployment Environments such as on-prem (SSIS, Nifi, etc), cloud-based (Azure Data Factory, AWS Glue, etc) &amp; containerized (Kubernetes)\\nWorking knowledge of TCP/IP, Firewalls, SSH, SSL, SFTP, port forwarding, NTFS Security, ACLs, Least Privilege, Active Directory, LDAP\\nData Warehouse Virtualization Platforms (eg: Denodo)\\n\\nMinimum Qualifications\\nBachelors Degree in Computer Science, Engineering or similar.\\n5-8 years of Technical Experience Required.\\nMaster of Science, MBA or Engineering Preferred\\n\\n\\n\\n\\nCompany Profile\\nAs America's Un-carrier, T-Mobile USA, Inc. (NASDAQ: \"TMUS\") is redefining the way consumers and businesses buy wireless services through leading product and service innovation. The company's advanced nationwide 4G and 4G LTE network delivers outstanding wireless experiences for customers who are unwilling to compromise on quality and value. Based in Bellevue, Washington, T-Mobile USA. Inc. provides services through its subsidiaries and operates its flagship brands, T-Mobile and Metro by T-Mobile. For more information, please visit http://www.t-mobile.com\\n\\nEOE Statement\\nWe Take Equal Opportunity Seriously - By Choice. T-Mobile USA, Inc. is an Equal Opportunity Employer. All decisions concerning the employment relationship will be made without regard to age, race, ethnicity, color, religion, creed, sex, sexual orientation, gender identity or expression, national origin, marital status, citizenship status, veteran status, the presence of any physical or mental disability, or any other status or characteristic protected by federal, state, or local law. Discrimination or harassment based upon any of these factors is wholly inconsistent with our Company values and will not be tolerated. Furthermore, such discrimination or harassment may violate federal, state, or local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Data Engineer - Internal Audit</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline.Proficiency in SQL &amp; Python.Experience working with AWS big data technologies (Redshift, S3, DynamoDB) and moving data between different accounts &amp; technology.2+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets.Demonstrated strength in data modeling and ETL development for optimal performance.\\n\\nInternal Audit’s mission is to help our businesses improve controllership, operational efficiency, and customer experience.\\n\\nAmazon is seeking a Data Engineer for our Internal Audit Data Analytics team that provides data analytics services and solutions to enable our audit programs to scale with Amazon’s growth and complexity. This is a highly visible opportunity to provide custom-build solutions and services that will increase the productivity of Amazon’s audit function using data.\\n\\nIn this role, you will be a technical expert with massive impact. You work tightly with the Business Intelligence Engineers (BIEs) on our team to onboard data and construct data models that will be designed for optimal analysis. You will be an expert in collecting data from different types of sources and normalizing them into a consumable data models that are accessible to less technical users. Our team will maintain an ever changing infrastructure which will require data engineering problem solving, building unique high quality reliable, accurate, consistent, and architecturally sound data sets that are aligned with our business needs.\\n\\nTasks include, but are not limited to:\\nManaging AWS resources including S3, Glue, Redshift, Lambda etc.Design, implement and support an analytical data infrastructure and recommending complimentary technology additions.Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using internally built and AWS technologies.Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency.Collaborate with Data Scientists and Business Intelligence Engineers (BIEs) to recognize and help adopt best practices in analysis: data integrity, test design, analysis, validation, and documentation.Lend domain knowledge of datasets and practices to BIE’s.Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.\\n\\nMasters in computer science, mathematics, statistics, economics, or other quantitative fields.Experience working with bigger AWS big data technologies (EMR, Neptune, Athena).Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.Experience providing technical leadership and educating other engineers for best practices on data engineering.Familiarity with statistical models and data mining algorithms.\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Principal Data Engineer</td>\n",
       "      <td>Renton, WA 98057</td>\n",
       "      <td>Renton</td>\n",
       "      <td>WA</td>\n",
       "      <td>98057</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s Degree in computer science, engineering, mathematics, MIS or similar field.\\n\\n10 years in technology roles.\\n\\nMust have experience with the following technologies:\\n\\nC#\\n\\nASP.net\\n\\nT-SQL\\n\\nHTML/CSS\\n\\nJavaScript\\n\\nNodejs\\n\\nDemonstrated analytical skills\\n\\nDemonstrated problem solving skills\\n\\nPromotes information sharing\\n\\nAbility to work within tight timeframes and meet strict deadlines.\\n\\nPossesses strong technical Aptitude.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Providence St. Joseph Health is calling a Principal Data Engineer to one of our following locations: Renton, WA (preferred), Spokane, WA, Richland, WA, Everett, WA, Olympia, WA, Anchorage, AK, Missoula, MT, Portland, OR, Beaverton, OR, Anaheim, CA, Burbank, CA or Lubbock, TX. This position is 1 year-long in duration with full medical, dental, vision and vacation benefits. We are open to the possibility of working remotely within our 7 state region of operation: AK, WA, OR, MT, CA, NM and TX.\\nWe are seeking a Principal Data Engineer to design and build modern data-centric software applications to support clinical and operational processes across all parts of the health system. These applications leverage cloud computing, big data, mobile, data science, and modern software development methodologies and frameworks. Data Engineers build data pipelines, enrichment processes, provisioning layers, APIs and user interfaces to meet the requirements of key initiatives. The Principal Data Engineer will take point on development of best practices and standards across the engineering team and participate in research and development of new technologies. A Principal Data Engineer should be able to and emphasize mentoring less experienced Data Engineers and training the team, as needed, to develop a robust skillset among the entire team. Strongly encourages and places a priority on collaboration with meticulous source control and documentation. An emphasis on simple solutions to complex problems through the use of modern and emerging methods and tools is critical. This position will works closely with the Product, Platform, and Architecture teams to deliver on joint efforts.\\nIn this position you will have the following responsibilities:\\nDesign, build and deliver quantitative applications that improve operations and generate value\\n\\nParticipate in DevOps, Agile, and continuous integration frameworks\\n\\nStay abreast of emerging technologies, open source projects, and best practices in the field\\n\\nData warehousing, big data, enterprise search, business intelligence, analytics, modern and mobile applications\\n\\nBuild processes that are fault-tolerant, self-healing, reliable, resilient and secure\\n\\nWork effectively and in real-time with other developers, product managers, and customers to deliver on collective goals\\n\\nActively participate in code reviews, support the overall code base, and support the establishment of standard processes and frameworks. Take a lead role in the development of standard practices and enforce following standard processes.\\n\\nQualifications:\\nRequired qualifications for this position include:\\nBachelor’s Degree in computer science, engineering, mathematics, MIS or similar field.\\n\\n10 years in technology roles.\\n\\nMust have experience with the following technologies:\\n\\nC#\\n\\nASP.net\\n\\nT-SQL\\n\\nHTML/CSS\\n\\nJavaScript\\n\\nNodejs\\n\\nDemonstrated analytical skills\\n\\nDemonstrated problem solving skills\\n\\nPromotes information sharing\\n\\nAbility to work within tight timeframes and meet strict deadlines.\\n\\nPossesses strong technical Aptitude.\\n\\nPreferred qualifications for this position include:\\nMaster’s Degree.\\n\\nCloud computing, Linux, Hadoop, MapReduce, Spark, Hbase, Kudu and NoSQL platforms in general; Apache Solr and Lucene\\n\\nJava, Scala, C#, Python, shell scripting and/or similar languages\\n\\nRelational database platforms, database design, and SQL\\n\\nAPIs, JSON, REST and other relevant W3C open standards\\n\\nModern application development frameworks\\n\\nFamiliarity with commercial or open source ETL tools\\n\\nAbout the department you will serve.\\nProvidence Strategic and Management Services provides a variety of functional and system support services for all eight regions of Providence Health &amp; Services from Alaska to California. We are focused on supporting our Mission by delivering a robust foundation of services and sharing of specialized expertise.\\nWe offer a full comprehensive range of benefits - see our website for details\\nhttp://www.providenceiscalling.jobs/rewards-benefits/\\nOur Mission\\nAs expressions of God’s healing love, witnessed through the ministry of Jesus, we are steadfast in serving all, especially those who are poor and vulnerable.\\nAbout Us\\nProvidence Health &amp; Services is a not-for-profit Catholic network of hospitals, care centers, health plans, physicians, clinics, home health care and services guided by a Mission of caring the Sisters of Providence began over 160 years ago. Providence is proud to be an Equal Opportunity Employer. Providence does not discriminate on the basis of race, color, gender, disability, veteran, military status, religion, age, creed, national origin, sexual identity or expression, sexual orientation, marital status, genetic information, or any other basis prohibited by local, state, or federal law.\\nSchedule: Full-time\\nShift: Day\\nJob Category: Information Technology\\nLocation: Alaska-Anchorage\\nOther Location(s): Oregon-Portland, Oregon-Beaverton, Montana-Missoula, Washington-Everett, Washington-Renton, Washington-Richland, Washington-Spokane, California-Anaheim\\nReq ID: 234029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>BI Engineer - Alexa Engagement</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in math, statistics, computer science, or finance or equivalent experience.5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems AnalystSQL writing experience and experience with ETLRedshift experience\\n\\nCome help us shape the future of the best voice controlled computer in the cloud. Alexa and Echo are shaping the future of voice recognition and cloud-based content/services. Alexa is the name of the Amazon cloud-based voice service and the brain that powers Echo, the award-winning and groundbreaking Amazon device designed around your voice. Echo connects to Alexa, to provide information, answer questions, play music, read the news, check sports scores or the weather, and more—instantly. It's hands-free, and always ready. All you have to do is ask.\\n\\nTo achieve this, we blend of a variety of disciplines (such as NLP, data mining, machine learning, big data, semantic web, graph stores, cloud computing) in an effort to understand our customers and the things they're excited about. To complement our complex algorithms and extensive data analyses, we create elevated and inspirational mobile and web features across the entire communication experience. We use artificial intelligence, data mining and usability studies to develop new features, and we test them through hundreds of R &amp; D experiments a year. We are also incredibly intent on solving some of the most complex computing problems to be found in industry and academia, and we get to test our solutions in the real world every day. And most importantly, we relentlessly ask: \"What haven't we thought of yet?\"\\n\\nThe business intelligence engineer will work closely with data scientists, software engineers, and product managers to build out reporting to inform key stakeholders and decision-makers. In this role, you’ll design, execute and iterate on high visibility business intelligence reporting for the teams of people that are actively building out Alexa's capabilities. If you love working with huge data sets and delivering the insights you discover through business intelligence reporting and automated systems, then this is the job for you.\\n\\nKey Responsibilities\\nCollect, analyze and share data to help product teams drive improvement in key business metrics and customer experiencePropose and prioritize changes to reporting and create additional metrics and processes based on program changes and customer requirementsWork closely with Alexa program teams to create ad-hoc reports to support timely business decisions and project workIdentify and implement new capabilities and best practices to develop and improve automated data analysis processesLearn and understand a growing range of Amazon data resources and discover how, and when to use which datasets\\n\\nExpert understanding of best practices to handle extremely large volume of dataAbility to create extensible and scalable data schema that lay the foundation for downstream analysisA clear passion for learning new BI skills and techniques independently and continuouslyAbility to prioritize multiple concurrent projects while still delivering timely and accurate resultsExperience working in a lean, successful start-up or on a new product team where continuous innovation is desired and ambiguity is the normExperience mentoring others in SQL, modeling, forecasting and the use of large datasetsProficiency with scripting languages and Unix systems (Python, perl, bash, etc.)Experience with the following is a plus: Looker, Tableau, MicrostrategyExperience in an internet-based company with large, complex data sources.\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Business Intelligence Engineer II</td>\n",
       "      <td>Bellevue, WA 98004</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>98004</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExposure to Web Analytics such as standard Clickstream analysis tools (e.g. Omniture) and Multivariate test software a plus\\nExperience with Tableau, SAS, SPSS or similar a plus\\nExperience with Hadoop development a plus\\nTravel industry and/or e-Commerce experience a plus</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Expedia\\nBusiness Intelligence Engineer/ Data Engineer (Partner Marketing)\\nAt Expedia Group, the data landscape is defined by complexity. As a lodging marketplace, we connect customers with our millions of hotel partners; displaying a multitude of rates, promotions and content to facilitate the best possible purchase decisions. All of this results in an abundance of data ready to be transformed and interpreted into something meaningful. At Expedia Group we have a large Analytics function focused on delivering these insights, within which the Business Intelligence team has a critical role to play.\\nThe Business Intelligence team is focused on building reliable data structures, analytical tools, and process automation applications to support our Partner Marketing group as they become more scientific and efficient in engagement with our hotel suppliers. As engineers are directly integrated with the business analyst and data scientist communities across the organization, the team shares in the ownership of key projects and deliverables. Working with platform teams, we utilize advanced technology to optimize a complex Big Data ecosystem and enable better accessibility to data. As a Business Intelligence Engineer, you will serve as an expert in our systems and processes and be positioned to provide guidance to various teams to ensure that deliverables exceed expectations.\\nWhat You’ll Do:\\nServe as an expert on Expedia’s datamarts/warehouses – with a deep understanding of the architecture and how to map and consolidate information across various sources\\nBe creative by developing automated solutions and improving existing features for business process efficiency\\nBe ready and willing to go extra miles for business needs.\\nRun ad-hoc reports for financial modeling, business intelligence, and project level data analysis\\nBuild and maintain business intelligence reports and dashboards\\nAdministrate reporting databases and delivery systems to support the ongoing reporting operations\\nCreate monitoring tools to track the quality and performance of solutions\\nWork with global teams, including database developers, product managers and business partners to construct appropriate BI framework for projects/programs\\nWork with other developers to maintain and enhance our custom-built applications which will encompass both database and user interface modules\\nWho You Are:\\nDo you have good experience working with AWS cloud-based services: S3, EC2, RDS?\\nHave you been working with Hadoop, Hive, Presto, Spark, Qubole, Snowflake?\\nDo you love working with Marketing data, Google Analytics, Facebook APIs etc…?\\nAre you familiar with building APIs and authentications services?\\nYou have background in Dotnet development (MVC, ASP.Net; C#, etc.) and web technologies (Javascript; HTML; CSS, etc.)\\nMinimum of 3 years’ experience as a developer in a Business Intelligence/data mining/report writing role. Solid foundation in SQL, particularly Business Intelligence and Analytics solutions\\nPossess strong scripting skills to perform data/file manipulation\\nSolid background in Data Warehousing and Business Analytics environment\\nYou have ability to read and interpret data schemas as well as develop reporting and visualizations\\nYou have excellent problem solving skills with experience identifying and solving data quality issues\\nYou are flexible and motivated working in a dynamic environment\\nHave excellent verbal and written communication skills (English)\\nExperience with data mining methodologies and statistical methods\\nAbility to logically translate and disseminate analytical insights to non-analytical business partners\\nYou have strong emphasis on testing, validating and overall data quality\\nA MS/BS degree in Computer Science or related technical field preferred. Additional education in business or statistics a plus\\nOther Qualifications:\\nExposure to Web Analytics such as standard Clickstream analysis tools (e.g. Omniture) and Multivariate test software a plus\\nExperience with Tableau, SAS, SPSS or similar a plus\\nExperience with Hadoop development a plus\\nTravel industry and/or e-Commerce experience a plus\\nWhy Join Us:\\nExpedia Group recognizes our success is dependent on the success of our people. We are the world's travel platform, made up of the most knowledgeable, passionate, and creative people in our business. Our brands recognize the power of travel to break down barriers and make people's lives better – that responsibility inspires us to be the place where exceptional people want to do their best work, and to provide them to tools to do so.\\nWhether you're applying to work in engineering or customer support, marketing or lodging supply, at Expedia Group we act as one team, working towards a common goal; to bring the world within reach. We relentlessly strive for better, but not at the cost of the customer. We act with humility and optimism, respecting ideas big and small. We value diversity and voices of all volumes. We are a global organization but keep our feet on the ground so we can act fast and stay simple. Our teams also have the chance to give back on a local level and make a difference through our corporate social responsibility program, Expedia Cares.\\nOur family of travel brands includes: Brand Expedia®, Hotels.com®, Expedia® Partner Solutions, Egencia®, trivago®, HomeAway®, Orbitz®, Travelocity®, Wotif®, lastminute.com.au®, ebookers®, CheapTickets®, Hotwire®, Classic Vacations®, Expedia® Media Solutions, CarRentals.com™, Expedia Local Expert®, Expedia® CruiseShipCenters®, SilverRail Technologies, Inc., ALICE and Traveldoo®.LI-BSTEWARD\\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Data Engineer Manager</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)3+ years of hands-on experience hiring and managing teams and5+ years as a hands-on Data Engineer or developer7+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics4+ years experience working with Open Source Big Data tools (Parquet, Spark, Hadoop, Presto)\\n\\nAre you inspired by innovation? Do you like to build products that are global? Do you like extending machine learning and NLP in the areas of data engineering and analytics? Answer yes to any of these and you’ll fit right in here at AWS-BTS Engineering. We are a team of doers who work passionately to apply cutting edge advances in technology and software to solve challenges that are unique to AWS and transform our customers’ experiences.\\n\\nAs an Amazon Data Engineering and Analytics leader, you will be working on and building large scale data environments that power our next generation products. We are passionate about building highly scalable real time data engines and seek a leader who can drive the vision, lead data engineers, BI engineers, ML and data scientists to deliver the solutions.\\n\\nThe Data Engineering &amp; Analytics team will build next generation data stores and real time insights by working with multiple data sources, warehouses and platforms. These solutions will power real time web applications and recommendation platforms. We collaborate with product managers, business stakeholders and data platform teams to make feature trade-offs, design, and power new applications. The solutions will include statistical modeling, machine learning, predictive analytics, and data visualization.\\n\\nA successful candidate should have a background in business analytics, data science, data visualization, and data engineering.\\n\\nMaster's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)Experience working with AWS Big Data Technologies (EMR, Redshift, S3)Proven track record of delivering a big data solution with ML and predictive use cases.Experience working with both Batch and Real Time data processing systemsProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategyExperience providing technical leadership and mentoring other engineers for best practices on data engineeringKnowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsAbility to plan roadmap, prioritize tasks, manage dependencies and deliver on time.Excellent understanding of software development life cycle and/or agile development environment with emphasis on BI practices.Ability to create collaborative relationships with partners, stakeholders and customers while managing expectations, managing concerns and risks, and communicating progress.Proficiency in at least one modern programming language such as Java, Scala, or PythonKnowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and cloud computingStrong problem-solving skills and ability to prioritize conflicting requirements.Excellent written and verbal communication skills and ability to succinctly summarize key findings.Meets/exceeds Amazon’s leadership principles requirements for this roleMeets/exceeds Amazon’s functional/technical depth and complexity for this role\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Age/ Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Business Intelligence Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BA/BS in Computer Science, Engineering, Information Systems Finance or related field3+ years of demonstrated experience in data modeling, ETL, data warehousing, and transformation of large scale data sources using SQL, Redshift, Oracle, or other Big Data technologiesStrong analytical skills with SQL/R, including the ability to pull data and use data to identify and solve ambiguous problem.Advanced skills in Excel as well as any data visualization tools like Tableau or similar BI toolsExperience of complex operational, process, and performance improvement projectsBe self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty dataAbility and interest in working in a fast-paced and rapidly-changing environment\\n\\nAmazon is seeking an exceptional Business Intelligence Engineer (BIE) to join the Amazon Fashion (Softlines) Marketing team. This is a unique, high visibility opportunity for someone with a passion to dive deep into large-scale data sets, surface data that provide unique insights to leaders of the Softlines organization and ultimately have an impact on the direction of our business.\\n\\nThis position requires excellent statistical and analytical abilities, deep knowledge of business intelligence solutions and data engineering practices, and the ability to collaborate with various teams across Amazon. The successful candidate will be a self-starter comfortable with ambiguity, capable of working in a fast-paced environment, possess a strong attention to detail, and able to collaborate with customers to understand and transform business problems into requirements and deliverables.\\n\\nResponsibilities:\\n\\nDesign, implement, and support a platform providing ad hoc access to large datasets.\\nInterface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL.Apply mathematical modelling to data and metadata for ad hoc and pre-built reporting.Interface with business customers to gather requirements and deliver complete reporting solutions.Design, develop, and maintain ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.Recognize and adopt best practices in reporting and analysis, including data integrity, test design, analysis, validation, and documentation.Improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.Participate in strategic and tactical planning discussions, including annual budget processes.\\n\\nMaster’s degree in Computer Science, Engineering, Information Systems, Statistics, Finance or related fieldAdvanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management as required.Expert in writing and tuning SQL scriptsExperience working in very large data warehouse environmentsExperience in a data engineer or BIE role with a technology companyExperience conducting large scale data analysis to support business decision makingStrong verbal/written communication and data presentation skills, including an ability to effectively communicate with both business and technical teams\\nAmazon.com is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Data Engineer - Alexa</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience.Relevant experience in analytics, data engineering, business intelligence, market research or related fieldExperience gathering business requirements, using industry standard business intelligence tool(s) to extract data, formulate metrics and build reportsExperience using SQL, ETL and databases with large-scale, complex datasets\\n\\nInterested in Amazon Alexa? Come work on it. We’re building the speech and language solutions behind Amazon Echo and other Amazon products and services. We’re working hard, having fun, and making history!\\n\\nWe are looking for candidates who want to help shape the future of human-computer interactions. Specifically, we are looking for an outstanding Data Engineer who is looking to work in a new space to help define how we use data to understand customer behavior and satisfaction. In this role, you will develop and support the analytic technologies that give our teams flexible and structured access to their data, including implementation of a BI platform, defining metrics and KPIs, and automating reporting and data visualization.\\n\\nThe successful candidate will be an expert with SQL, ETL (and general data wrangling) and have exemplary communication skills. The candidate will need to be a self-starter, comfortable with ambiguity in a fast-paced and ever-changing environment, and able to think big while paying careful attention to detail.\\n\\nResponsibilities\\n\\nYou know and love working with business intelligence tools, can model multidimensional datasets, and can partner with customers to answer key business questions. You will also have the opportunity to display your skills in the following areas:\\n\\n· · Design, implement, and support a platform providing ad hoc access to large datasets\\n· · Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL\\n· · Manage AWS Resources\\n· · Model data and metadata for ad hoc and pre-built reporting\\n· · Interface with business customers, gathering requirements and delivering complete reporting solutions\\n· · Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions\\n· · Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation\\n· · Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\\n\\nGraduate degree in computer science, business, mathematics, statistics, economics, or other quantitative fieldBoth technically deep and business savvy enough to interface with all levels and disciplines within the organizationDemonstrated ability to coordinate projects across functional teams, including engineering, IT, product management, marketing, finance, and operationsKnowledge of Advanced SQL and a programming languageExperience with data visualization using Tableau or similar toolsExperience with large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologiesProven track record of successful communication of analytical outcomes through written communication, including an ability to effectively communicate with both business and technical teams\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Data Engineer, Internal Benchmarking</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience3+ years of relevant work experience in software development, analytics, data engineering, business intelligence or related IT fields, with 1+ years in data engineeringExperience in writing and optimizing SQL queries in a business environment with large-scale, complex datasetsDetailed knowledge of data warehouse technical architecture, infrastructure components, ETL and reporting/analytic tools and environmentsExperience in data visualization software (Tableau/Quicksight) or open-source projects\\n\\nWould you like to be a Data Engineering and Analytics SME who constantly learns new domains, tools and data sources to identify best practices from across Amazon and propagate them globally?\\n\\nAmazon’s Internal Benchmarking team is looking for a Data Engineer (DE) to join our Seattle-based team to design, manage, and continuously enhance our BEnchmarking Analytics Foundation (BEAF). BEAF is a shared foundation upon which multiple Benchmarking teams develop and deliver a wide variety of analytics applications including metrics generation, metrics correlation, econometric models, knowledge modeling, and many other use cases to help improve process effectiveness, customer experience, supplier experience, and employee and candidate experience. As the member of the BEAF team, you will manage the Redshift/Spectrum/EMR infrastructure and analytics tools, as well as build data pipelines, tools, and reports that enable product managers, analysts, BIEs, solution architects, and executives to design, deliver, and consume benchmarking services. You will work on three fronts: (1) collaborate with the 15+ specialized Benchmarking teams (4-5 of them are most active) to gather requirements for data collection, logging, storage, transforming, analysis, and reporting; (2) when appropriate, work directly with a benchmarking service team to design and develop data pipelines to process structured and unstructured data to support ML and other analytics applications; and (3) design and execute experiments to test new tools and new benchmarking ideas.\\n\\nKey Responsibilities:\\nManage Redshift/Spectrum/EMR infrastructure, and drive architectural plans and implementation for future data storage, reporting, and analytic solutionsDesign data schema and operate internal data warehouses and SQL/NoSQL database systemsDesign, implement, automate, and monitor data pipelinesDevelop Extract-Transform-Load (ETL) jobs and Redshift/Spectrum/EMR jobs to design and generate business metricsOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions\\n\\nGraduate degree in Computer Science, Mathematics, Statistics, Finance, or related technical field1+ years of experience in implementing big data processing technology (Hadoop, etc.)Strong ability to effectively communicate with both business and technical teamsDemonstrated experience delivering actionable insights for a consumer businessCoding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)Experience with AWS technologies including Redshift, RDS, S3, EMR (or equivalent with other cloud-based technologies)Experience and interest in statistical analysis would be highly useful\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Data Streaming Platform Engineer</td>\n",
       "      <td>Seattle, WA 98101</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About the team\\nWe build the real time streaming platform responsible for ingesting of billions of real-time events and terabytes of data. We aim to provide teams across all of Zillow Group a robust, scalable and lightning fast way of passing their data to diverse set of use cases which enrich Zillow’s unparalleled living database of all homes and hundreds of millions of customers and empowers teams downstream to build analytics tools and products to delight our users.\\n\\nSmall team = huge impact. Engineering teams are highly decentralized in order to create the small team speed and autonomy of a start-up environment but backed by big company resources.Fast-moving, developer driven organization full of forward-thinking and ambitious people.Learn more about what we are doing at https://www.zillow.com/engineering and https://www.zillow.com/data-science\\nAbout the role\\nWe are looking for a strong software engineer with a background in real time streaming to create an end to end streaming platform. As a Data Engineer, you will be responsible for all phases of the development cycle: design, implementation, release and operations. You will leverage your knowledge and experience to provide technical leadership for the team, take ideas from zero to completion, and get down to the details for building a system from scratch. You will:\\nBuild and maintain highly scalable, low-latency, fault-tolerant streaming data platform that empowers Data Scientists, Engineers to build real time data applications.\\nWork closely with business and technology stakeholders to build the next generation Distributed Streaming Data Platform using Apache Kafka\\nWork closely with the real time data processing platform team to build shared infrastructure abstractions and self service tooling\\nWho you are\\nData Engineer with experience using streaming technologies (Kafka/Kinesis/EventHubs)\\nExperience running applications on top of cloud platforms (AWS/Azure/GCP)\\nExperience with Java\\nExcellent communication skills and demonstrative empathy\\nUnderstanding of distributed systems and concepts\\nProven track record of leading and delivering large projects independently\\nProven ability to learn new technologies quickly\\nKnowledge of Computer Science fundamentals (CS Degree or related)\\nExperience running Apache Kafka cluster is a plus\\nGet to know us\\nZillow Group houses the largest portfolio of real estate brands on mobile and the web. We are on a mission to rewire the real estate transaction and are building transformational tools and services that make it easier for everyone to find and get into a home they love. We are working to create an on-demand real estate transaction experience for every stage of the home lifecycle - for buyers, sellers, renters and borrowers - and we're well on our way. No matter what job you're in, you will play a critical role in making this vision a reality for millions of people.\\nAt Zillow Group, we're powered by our inclusive work culture, where everyone has the support and resources to do the best work of their careers. Our efforts to streamline the real estate transaction is supported by our passion to empower people and enrich lives around everything home, a deep-rooted culture of innovation, a fundamental commitment to Equity and Belonging, and world-class benefits. But, don't just take our word for it. Read our reviews on Glassdoor and recent recognition from multiple organizations, including: Fortune 100 Best Companies to Work For (#69), Fortune Best Workplaces for Diversity (#38), Fortune Best Workplaces for Parents (#31), Fortune Best Workplaces for Women (#20), Fatherly's Best Workplaces for New Dads (#37), JUST Capital 100 Company (#69), Bloomberg Gender Equality Index constituent.\\nZillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>----------------\\nRole Description\\n----------------\\n\\nThe Data Engineer is responsible for designing and developing robust, scalable solutions for collecting, analyzing large data sets, creating and maintaining data pipelines, data structures and reports to be used by the revenue organization at Dropbox.\\n\\n----------------\\nResponsibilities\\n----------------\\n\\n\\nUnderstand business processes, applications and how data is gathered; and tie application telemetry to transactional data model.\\nDesign, build and manage data marts to satisfy our growing data needs.\\nDevelop and manage data pipelines at enterprise scale\\nBuild data expertise and own data quality for various data flows\\nLaunch and support new data models that provide intuitive analytics to internal customers\\nDesign and develop new framework and automation tools to enable teams to consume and understand data faster\\nUse your expert coding skills across a number of languages like SQL, Python and Java to support analysts and data scientists\\nInterface with internal data consumers to understand data needs\\nCollaborate with multiple teams in high visibility roles and own the solution end-to-end\\n\\n------------\\nRequirements\\n------------\\n\\n\\n5+ years of SQL (Oracle, AWS Redshift, Hive, etc) experience required, No-SQL experience is a plus.\\n5+ years of Python or Java development experience.\\n5+ years of experience with schema design and dimensional data modeling.\\nHands-on experience working in data warehousing, data architecture and/or data engineering environments at enterprise scale.\\nAbility to analyze data to identify deliverables, gaps and inconsistencies.\\nExperience designing, building and maintaining data processing systems\\nExperience working with visualization tools like Tableau or MicroStrategy\\nCommunication skills including the ability to identify and communicate data driven insights\\nBS or MS degree in Computer Science or a related technical field\\n\\n------------------\\nBenefits and Perks\\n------------------\\n\\n\\n100% company paid individual medical, dental, &amp; vision insurance coverage\\n401k + company match\\nMarket competitive total compensation package\\nFree Dropbox space for your friends and family\\nWellness Reimbursement\\nGenerous vacation policy\\n10 company paid holidays\\nVolunteer time off\\nCompany sponsored tech talks (technology and other relevant professional topics)\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Sr. Machine Learning Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Data Science, Applied Science, Computer Science, Computer Engineering or related technical disciplin5+ years of experience as a data/software developer/scientist or related technical job5+ years of experience with SQLExperience managing platforms or infrastructure software systemsExperience building/managing big data or traditional DW platform componentsExperience as tech lead for data/software engineering/science team\\n\\nPassionate about books and data science? Kindle/Books Demand Science and Analytics team is seeking an experienced Sr. Data Engineer to work with ML Scientists, Economists, SDEs and BA/IEs on the next generation of science and ML-based products that will help customers discover and buy more of their favorite books on Amazon (in Kindle, Print, or Audio format).\\nThe Kindle/Books Demand Science and Analytics team owns the development of science-based/ML applications aimed at growing customer engagement through\\nshopping and discovery CX (pricing, deals, personalized rewards, and ranking of recommendations and relevant content across multiple surfaces)targeted marketing (email, push, on-site, paid media)subscription products (including selection and marketing of books for programs like Kindle Unlimited and Prime Reading)\\nAs a Sr. Data Engineer, you will transform billions of daily customer interactions (e.g. browsing, buying, and reading/listening behavior) into highly reliable, quality, and low-latency data structures for analytics, data science, and ML use cases. Ultimately, your work will deepen and accelerate the extent to which we can understand and delight book customers. You will work with a large variety of data sources and will experiment and apply the latest set of big data technologies (ETL and Data Lakes, Redshift, S3, EMR, EC2) to transform data into a better CX with books.\\n\\nCore Responsibilities\\nUse SparkSQL, Redshift, EDX and other big data technologies to build and maintain data infrastructure using software engineering best practicesManage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda etcBuild and deliver high-quality data architecture and automated pipelines to support data science and ML use casesInterface with other technology teams to extract, transform, and load data from a wide variety of data sourcesContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersDevelop understanding and documentation of data sources and appropriate logic for consumption from various data providers (e.g. search, clickstream, reading, marketing, transactions)Enable science and analytics teams to discover and access new data sources\\nCore Leadership capabilities\\nOwnership (think long term, don’t sacrifice long-term value for short-term results)Invent and simplify (look for new ideas from everywhere, find ways to simplify to implement innovation)Insist on high standards (continuously raise the bar on data quality, access, speed, and documentation)Deliver results (focus on the key inputs to deliver the high quality-solutions in a timely fashion)\\n\\nMasters in computer science, mathematics, statistics, economics, engineering or other quantitative field5+ years of experience as a data/software developer/scientist or related technical portfolio\\n· 2+ years of experience with object-oriented languages such as Python and Scala · Experience with Machine Learning applications (feature generation/selection, supervised/unsupervised learning)\\nExcellent verbal and written communication skills and technical writing skillsExperience providing technical leadership and mentoring other engineers for best practices on data engineering.Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsExperience developing cloud software services and an understanding of design for scalability, performance, privacy, security and reliability\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Senior Business Intelligence Engineer, AWS Training &amp; Certification</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Business, Technology, or a similar discipline7+ years of relevant work experience as a business intelligence engineer or data engineer role5+ years of experience in SQL programming4+ years of experience in building data warehouses and dimensional modeling4+ years of experience with business intelligence and data visualization tools (e.g. Tableau)Experience with ETL tools and processesExperience with on boarding and developing data structures\\n\\nThe AWS Training and Certification organization educates customers, partners, and AWS Employees globally on AWS products, solutions, and best practices. We are seeking a talented and experienced Business Intelligence Engineer to help us develop reporting solutions and processes to support the analysis and reporting of metrics for the AWS Training and Certification organization.\\n\\nAs part of the AWS Training and Certification (T&amp;C) Business Intelligence and Data Analytics team you will develop data and reporting solutions to provide insights into the KPIs and metrics that drive the organization. We are looking for someone who will help create automated reporting processes, develop data structures for reports, build insightful dashboards, and deliver data insights to our teams across the globe. The right candidate will be passionate about working with large datasets and should be someone who loves to bring data together to answer complex business questions that deepen our understanding of our business drivers. As a Business Intelligence Engineer within AWS, you will have the exciting opportunity to help shape and deliver our strategy to build a scalable reporting infrastructure that will broaden AWS’s penetration in the cloud computing market.\\n\\nThe ideal candidate will have a background in data warehousing (SQL, ETL techniques, data modeling, Redshift, etc.) and business intelligence tools (e.g. Tableau). A successful candidate should have the technical proficiency to query data in a Redshift data warehouse environment, structure data to be leveraged within business intelligence tools, and be able to develop insightful reports that are delivered to business stakeholders.\\n\\nResponsibilities:\\nServe as a key member of the AWS Training &amp; Certification Business Intelligence and Data Analytics team by supporting all reporting requests from global operations, helping procure new data streams (from both internal and external sources), and ultimately turn data into actionable insights by building meaningful reports.Conduct analysis, perform analysis, and prototype new metrics that will be shared with the Operations Team and executive stakeholdersLeverage internal tools including Tableau and AWS Redshift to deliver data-driven recommendations, tools, dashboards, reports, and findings to internal stakeholders.Manage numerous requests concurrently and strategically, prioritizing when necessary.Support cross-functional teams on the day-to-day execution of projects and initiatives.Drive small to medium projects that help build AWS into the most customer-centric platform.\\n\\nMaster’s degree in Business, Technology, or a similar disciplineAbility to distill problem definitions, models, and constraints from informal business requirementExperience with supporting enterprise business analyticsExperience with Tableau Desktop and Tableau ServerExperience administrating Tableau ServerExperience with RedshiftStrong verbal, written, and presentation skills\\nAmazon.com is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Data Engineer with testing</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Details\\nJob Code\\nJPSC-7555\\nPosted Date\\n06/13/18\\nExperience\\n7 Years\\nPrimary Skills\\n• 3 + years’ experience with 1 or more Databases like Oracle,SQL Server ( basic SQL Concepts,writing simple to medium PL/SQL queries a must) • 2 + yrs Experience with Java server-side web application technology Spring,Hibernate and/or SpringBoot • 2+ yrs Experience with web services and REST architecture (using or building XML/JSON web/serverside APIs) • 1 + yrs of hands-on coding experience in an Agile based multi-tier application framework and environment.\\nRequired Documents\\nResume\\nOverview\\nRole: Data Engineer with testing\\nLocation: Seattle, WA\\nDuration: 6+ Months\\n\\nJob Description:\\n3 + years’ experience with 1 or more Databases like Oracle, SQL Server ( basic SQL Concepts , writing simple to medium PL/SQL queries a must)\\n2 + yrs Experience with Java server-side web application technology Spring, Hibernate and/or SpringBoot\\n2+ yrs Experience with web services and REST architecture (using or building XML/JSON web/serverside APIs)\\n1 + yrs of hands-on coding experience in an Agile based multi-tier application framework and environment.\\nGood understanding of software development frameworks, terminology\\nCompetent using version control systems such as GIT, SVN, VSS\\nKnowledge of continuous integration and development (CI/CD) methodologies\\nKnowledge/Understanding of RESTful APIs\\nKnowledge/Understanding of Cloud native Platforms like PCF (Pivotal Cloud Foundry)\\nKnowledge/Understanding of Cloud Datastores – In-Memory/Persistent, NOSQL / Relational\\nKnowledge/Understanding of Logging - using tools like SPLUNK.\\n\\nIn the COMMENTS include:\\nLegal name:\\nPhone #:\\nEmail address:\\nDaily Rate:\\nDOB (Date and Month) :\\nSkype ID :\\nLocation (City and State):\\nRelocate:\\nAvailability to start:\\nVisa type and expiration:\\nHiring Status: C2C/W2/1099\\nOpen for CTH:\\nTimeslots for phone conversation:\\nTimeslots for WebEx/video interview :\\n\\nSummary:\\n\\nThanks Regards,\\nSyed Raza\\nDesk Number: 585 - 532 - 7200 Extension 9002\\n687 Lee Road, Suite 250, Rochester, NY 14606\\nEmail : Syed.j@avanitechsolutions.com\\nEmail: syed_j@iic.com\\nGmail hangouts: syedraza199025@gmail.com\\nwww.iic.com\\nwww.avanitechsolutions.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Assurance\\nAt Assurance we are disrupting the antiquated and inefficient world of insurance and financial services. Our team of world class software engineers, data scientists, and business professionals are modernizing how people obtain and manage their financial life all through our powerful platform ecosystem. We are rapidly growing as we expand our product offerings and global footprint, and this growth continues to present new and exciting challenges as we push our industry into its future. We eliminate waste throughout the industry and calculate the complex into simple, valuable solutions to improve people's lives. We are humble, driven, and committed to improving the lives of millions.\\n\\nAbout the Position\\nAs we build the future of consumer insurance in a modern age, data is at the core of everything that we do. The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business. Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations. Our Data Engineers design and build the backbone that makes this development possible with no support from engineering (we own our stack end to end). At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise.\\nTo be successful in this role, you must possess the following:\\nExpertise in modeling data\\nExperience with Spark, Hadoop/EMR, SQL\\nAbility to optimize data access for speed/reliability/velocity as needed by the business\\nComfort with QA’ing your own data, to include ‘menial tasks’ like listening to calls or scrubbing excel files to ensure everything is correct\\nComfort with learning new technologies to help the team explore new solutions to existing problems\\nA drive to move fast and deliver business value\\nExcellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked.\\nBusiness Acumen – you are always eager to understand how the business works, and more specifically, how your work impacts the business.\\nEnthusiastic yet humble – you are excited about the work you do, but you are also humble enough to embrace feedback – you don’t need to be the smartest person in the room.\\nThe following additional experience is desired:\\nCapable of modifying an existing job to add a new field and get it into production within a day.\\nCapable of creating a new data pipeline/job within 2-3 days.\\nYou have a proven ability to drive business results by building the right infrastructure that enables data-based insights. You are comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes. We’re growing at a rapid pace, so it’s important that you embrace the opportunity to blaze your own trail. You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity. You can work independently, with little oversight or guidance.\\n\\nAt Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise. If this sounds like a good fit for you, give us a shout, we’d love to chat!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Azure Data Architect</td>\n",
       "      <td>Seattle, WA 98104</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98104</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of consulting or client service delivery experience on Azure\\n</td>\n",
       "      <td>DevOps on an Azure platform</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\n</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS &amp; NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nQualifications\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 10 years of experience in big data, database and data warehouse architecture and delivery\\nMinimum of 5 years of professional experience in 2 of the following areas:\\n§ Solution/technical architecture in the cloud\\n§ Big Data/analytics/information analysis/database management in the cloud\\n§ IoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training &amp; Certification\\nMCSE Cloud Platform &amp; Infratsructiure Training &amp; Certification\\nMCSD Azure Solutions Architect Training &amp; Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nStrong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Software Solutions Architect Seattle, WA</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nSplunk Certified Consultant (SCC-2, SCC-II, Core Certification)\\nExperience designing and implementing distributed Splunk installations including all Splunk server roles (Search Head, Indexers, Heavy Forwarders, and Universal Forwarders, etc.)\\nExperience with advanced configuration of Splunk including Indexer Clustering and Search Head Clustering\\nExperience maintaining and administering enterprise-scale implementations\\nExperience developing custom Splunk content including scheduled searches, reports, dashboards and alerts\\nProficient at data on-boarding activities including custom parsing rules, custom TAs, props, transforms, and adhering to the Common Information Model (CIM)\\nExperience configuring indexes, index routing, retention policies, etc.\\nExcellent written and oral skills, ability to work closely with multiple customers, manage expectations, and track engagement scope\\nMinimum Education: Bachelors Degree</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Arcus Data is seeking experienced Splunk consultants to join their team. Arcus Data is a high growth solutions provider that celebrates and rewards innovation. Join a world-class team of extremely technical engineers that work together in a collaborative culture.\\nRequired Skills\\nSplunk Certified Consultant (SCC-2, SCC-II, Core Certification)\\nExperience designing and implementing distributed Splunk installations including all Splunk server roles (Search Head, Indexers, Heavy Forwarders, and Universal Forwarders, etc.)\\nExperience with advanced configuration of Splunk including Indexer Clustering and Search Head Clustering\\nExperience maintaining and administering enterprise-scale implementations\\nExperience developing custom Splunk content including scheduled searches, reports, dashboards and alerts\\nProficient at data on-boarding activities including custom parsing rules, custom TAs, props, transforms, and adhering to the Common Information Model (CIM)\\nExperience configuring indexes, index routing, retention policies, etc.\\nExcellent written and oral skills, ability to work closely with multiple customers, manage expectations, and track engagement scope\\nMinimum Education: Bachelors Degree\\nRequired Experience\\nSplunk engineer with experience managing and configuring Splunk environments, performing data on-boarding, developing custom content on Splunk platform, troubleshooting methodologies, and ability to walk customers through requirements gathering phase and develop appropriate system designs.\\nRecommended Skills\\nSplunk premium apps (ITSI, Exchange, Enterprise Security, VMWare, Business Flow)\\nUnderstanding of Syslog daemon configuration principles, ideally Syslog-NG\\nUnderstanding of modern data pipelines (Kafka, Cribl, Kinesis, Firehose)\\nCloud experience (AWS, Azure, GCP)\\nDevelopment and API experience (Python, REST, XML)\\nAutomation Experience (Ansible, TerraForm, Puppet, Chef)\\nHigh-level Job Description\\nWe’re looking for candidates who have a strong technical background and ability to think creatively. You’ll be responsible for building, deploying, and enhancing Splunk for our diverse customer base. This involves working with the customers directly, understanding their business needs, and building innovative solutions to solve interesting &amp; challenging problems.\\nDetailed Job Description\\nOur customer deployments range from on-prem, to hybrid, to 100% cloud. You will be supporting a wide variety of clients — some who are just getting started on their Splunk journey — all the way up to advanced 20+ TB customers with thousands of daily active users. This will include the installation and configuration of Splunk Enterprise according to Splunk best practices. Our customers need assistance developing solutions to support their use cases, which requires data onboarding, dashboard development, custom alerting, and third-party tool integrations.\\nYou will also be involved in planning and requirements gathering discussions with the client and will need to possess excellent written and verbal communication skills. The candidate will be required to document system design, capacity planning guides, status reports, and standard operating procedures.\\nAs an Arcus Data engineer you’ll have full access to our internal knowledge base and use case libraries, which is driven by our internal development teams. You’ll also have the technical backing of our entire engineering team to encourage collaboration and growth through information sharing and knowledge workshops. We celebrate and reward innovative thinking!\\nAbout Arcus Data\\nArcus Data is a proven leader in business analytics, cloud adoption, and automation. We empower organizations to solve some of their most challenging business needs through innovative solutions and advanced use case development. We deliver solutions across all industries including Technology, Healthcare, Energy &amp; Utilities, Retail, and Financial Services.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Redmond, WA</td>\n",
       "      <td>Redmond</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of relevant experience regarding designing and implementing software systemsSoftware engineering skill in one or more high level languages (C#, C++, Java, Python)</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of relevant experience regarding designing and implementing software systemsSoftware engineering skill in one or more high level languages (C#, C++, Java, Python)</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We live in the Age of Data. And we LOVE Data! Data and insights from data power an increasing range of applications, transforming not just the technology industry but society at large. Individuals and businesses need a powerful, elastic, and highly available Data Platform that can help them derive insights from large volumes of data. The Database Systems organization is delivering such a platform for a range of Relational Database workloads, from online transaction processing workloads, to data warehousing solutions, from on premise enterprise systems, to on-demand cloud services. Uniquely in the industry, we deal with the full breadth of environments, from server-based to cloud-based systems, delivering features that work across these environments, providing differentiated value to our customers.\\nWe are a full stack team, tackling the breadth of technology from distributed systems, availability, scalability, security, query processing, storage, operating systems, networking, management tools, web development, and most other fields in Computer Science. As an organization, we are also proud of our world class team, our deep investments in growing and retaining talent, and the diversity of skills, experiences, interests and personalities that makes our team strong. Along with the positive impact to society from our technology, our team also prides itself on direct involvement in various social causes, particularly related to broadening access to technology to all sections of society. To help our business to succeed in its ambitions, the Database Systems team is building up Business Analytics capabilities that allow us to track and improve customer experience, provide metrics around growth of existing and new offerings we are constantly developing, and want to apply Machine Learning techniques to answer various business questions. We are creating and maintaining data pipelines and Data Warehouses that allow us to analyze very large data sets that are emitted from our cluster telemetry, and relevant data from across the company. We won’t work in isolation but will leverage analytical work that has been done across the company.\\nResponsibilities\\nWe are looking for engineers who are passionate about data and want to work in a multi-disciplinary team of Software Engineers, Data Engineers and Data Scientists to solve real-world business problems. You will work on either data from diverse structured and unstructured data sources, and various formats including tabular, text and time series or can contribute to our core data pipeline. In addition, we have deep investments in on-prem telemetry that we need to update/modernize. This business-critical telemetry stack enables business insights for our existing and new features on Windows and Linux. We expect all team members to contribute broadly, with an agile and growth mindset, so your ability to step into multiple areas will be a key attribute that we are looking for.\\nQualifications\\nBasic Qualifications:\\n5+ years of relevant experience regarding designing and implementing software systemsSoftware engineering skill in one or more high level languages (C#, C++, Java, Python)BS, MS, or PhD in computer science (or equivalent)\\n\\nPreferred, but not required:\\nCommon ML and analysis tools (R, SAS, SPSS, MatLab)Experience with large scale applied machine learning techniques is a plus but not requiredExperience with SQL or equivalent query language.\\n\\nAZDAT #ENGGJOBS\\n#AZDATABASE\\n\\nAbility to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.\\nMicrosoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.\\n\\nBenefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBuild large-scale batch and real-time data pipelines with data processing frameworks by leveraging Google Cloud Platform, AWS, and Python data science tools.\\nHelp drive optimization, testing and tooling to improve data quality.\\nCollaborate with other software engineers, ML experts and business stakeholders. And the opportunity to learn and lead every single day.\\nWork in multi-functional and agile teams to continuously experiment, iterate and deliver on new product objectives to meet customers needs.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>This is an onsite position, i.e. not working remotely. The office is located in downtown Seattle.\\nAre you energized by the idea of disrupting an established industry with cutting-edge technologies? Are you motivated by the opportunity to shape the next milestone of a growing company?\\nAI and Machine learning (ML) is ripe for broad market adoption. At Kavout, we are the pioneer in AI for investing. As a FinTech company, Kavout’s mission is to empower institutions and investors with augmented intelligence to generate alpha, manage wealth and do more with less.\\nThe company was founded in 2016 and is headquartered in Seattle.\\nWe are looking for a Data Engineer with a solid background in Python, Google Cloud Platform, AWS. In this role you will be exposed to different challenging tasks from data engineering to analytics, and be part of a team to build the next generation investing platform for capital markets with AI and machine learning.\\nResponsibilities\\nBuild large-scale batch and real-time data pipelines with data processing frameworks by leveraging Google Cloud Platform, AWS, and Python data science tools.\\nHelp drive optimization, testing and tooling to improve data quality.\\nCollaborate with other software engineers, ML experts and business stakeholders. And the opportunity to learn and lead every single day.\\nWork in multi-functional and agile teams to continuously experiment, iterate and deliver on new product objectives to meet customers needs.\\nWho you are\\nYou know how to work with high volume heterogeneous data, preferably with distributed systems such as GCP, AWS, Hadoop, BigTable, Redis, MongoDB etc.\\nYou are knowledgeable about data modeling, data access, data analysis, and data storage techniques.\\nYou appreciate agile software processes, data-driven development, reliability, and responsible experimentation.\\nThe ability to rapidly learn and understand complex data systems.\\nComfortable dealing with ambiguity and working independently.\\nYou understand the value of partnership within teams.\\nThis is an onsite role, i.e. not a remote position. The office is at downtown Seattle.\\nBenefits\\nWe offer a collaborative working environment. And we encourage accountability and integrity.\\nCompetitive salary\\nHealth insurance coverage\\nPaid time-off\\nHoliday pay\\nPlease submit your resume to contact@kavout.co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Data Engineer, Amazon Air</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor degree in Engineering, Computer Science, or Statistics; or Masters in Math/Statistics/Finance or related disciplineMinimum three (3) years of experience data modeling, ETL, data warehousing, and transformation of large scale data sources using SQL, Redshift, Oracle, or other Big Data technologiesAbility to source and combine disparate data sets to answer business questionsMinimum of (2) years Advanced SQL with Oracle, SQL or MySQL, and Columnar Databases\\n\\nAmazon seeks a passionate, results-oriented, Data Engineer to identify strategic initiatives that will form the next generation of air delivery to delight Amazon customers. As an Amazon.com Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. This individual should have deep expertise in the design, creation, management, and business use of extremely large datasets. This high impact role will have an opportunity to help design and build our data infrastructure from the ground up, work with emerging technologies such as Redshift, while driving business intelligence solutions end-to-end: business requirements, workflow instrumentation, data modeling, ETL, metadata, reporting, and dashboard development. He/she should be an expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications. This individual should also be able to work with business customers in understanding the business requirements and implementing reporting solutions. The role requires someone who loves data, understands enterprise information systems, has a strong business sense, and is an excellent communicator.\\n\\nResponsibilities include:\\nConduct deep-dive investigations into business problems and identify potential opportunitiesIdentify and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentationContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersWork with in-house data scientists, global supply chain, transportation and logistics teams, and software teams to identify new features and projectsIdentify ways to automate analysis through smarter software systemsTrack realized savings and impacts, and communicate results with senior leadersThis is an individual contributor role that will partner with internal stakeholders across multiple teams, gathering requirements and deliver complete solutions\\n\\nMaster degree in Engineering or Math/Statistics/Finance or related disciplineAbility to work in a deadline-driven work environment; ability to re-prioritize on a regular basis in order to remain current with business needsExperience guiding and mentoring other data engineers and influencing large scale projects or organizationsQuantitative and qualitative data science experience with impact to a business, a track record of problem solving using software systems, and the desire to create and maintain data warehouse systems\\nAmazon.com is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.\\n\\n#AmazonAir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Hadoop/MapR Certification</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>B.S. in Computer Science (or similar)</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Splunk Data Engineer is responsible for supporting the efforts of the data science team and it’s overall mission of providing a platform for the ingestion, management, storage, and analysis of unstructured and semi-structured machine generated data. This includes supporting the deployment and management of the underlying big data infrastructure including Splunk, HUNK, Hadoop/MapR.\\n\\nPrimary Duties and Responsibilities:Assist in the deployment and management of the Splunk, HUNK, Hadoop/MapR infrastructureDeploy various configurations in the lab for testing and evaluation by the data science teamDeploy and Manage the staging and production Splunk and MapR environmentsWork with the operations team as needed for support and maintenance issues related to Splunk and MapRProvide level 2 on call support as needed for Splunk, HUNK and MapR environmentsTroubleshoot production issues related to data ingestion and other big data related issues as needed\\nQualifications\\n 2+ years experience with data engineering concepts\\n 2+ years experience deploying and maintaining Splunk environments\\n Working knowledge of event logging and key performance indicators\\n Analytical and problem solving skills\\n Very strong troubleshooting skills\\n Solid written and verbal communication skills\\n Ability to work directly with customers and vendors in a congenial manner\\n Willingness to be a team player\\n Self-motivated with the ability to work independently with minimal supervision\\n Proficient with common business software (Microsoft Office, Adobe Acrobat, etc.)\\n Command line proficiency in server management with a wide variety of UNIX environments\\nOptional Desired Qualifications:\\n Hadoop/MapR Certification\\n Hadoop/MapR ecosystem experience\\n Working knowledge of telephony and related concepts\\n Splunk administration certification or training\\n Experience with Splunk ITSI\\nEducation and Certifications:\\n B.S. in Computer Science (or similar)\\nWe have been delivering industry-leading solutions for the payments, financial and telecommunications industries since 1990. We are the preferred supplier of networking, integrated data and voice services to many leading organizations in the global payments and financial communities, as well as a provider of extensive telecommunications network solutions to service providers.\\nWe are a privately held company with a healthy balance sheet, secure assets and a loyal customer base that includes some of the largest global blue-chip companies in the world. Many of the world’s leading companies continue to count on us as their primary provider of a range of networking and communication services, enabling them to expand regionally, nationally and globally. We provide services to customers in over 60 countries throughout the world.\\nWe manage some of the largest real-time community networks in the world, enabling industry participants to simply and securely interact and transact with other businesses, to access the data and applications they need, over managed and secure communications platforms. Our existing footprint supports millions of connections and provide access to critical databases. Our network securely blends private and public networking to enable customers to utilize a single connection for \"one-to-many\" and \"many-to-many\"connections over a global platform.\\nSince our launch we have helped our customers and communities of interest, requiring secure and reliable communications solutions, to evolve from legacy to leading-edge technologies. Today the company provides a full range of services from dedicated connections to managed IP network solutions, providing local support and global reach to medium and large enterprises and service providers.\\nApplication Instructions\\nApplicants are encouraged to submit an electronic resume when applying for our positions. Job postings are open until filled, unless otherwise specified.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Data Engineer- ACES</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Ø Bachelor’s degree in Computer Science (or similar), or 4+ years of software development/data engineering experience\\nØ Experience in at least one modern object-oriented programming language (Ruby, Python, Java)\\nØ Demonstrated strength in data modeling, ETL development, and Data warehousing.\\nØ Experience with Redshift, Oracle, NoSQL etc.\\nØ Experience in working and delivering end-to-end projects independently.\\nØ Knowledge of distributed systems as it pertains to data storage and computing\\n\\nBusiness/Team Introduction\\nACES (Amazon Customer Excellency System) is seeking a highly talented and motivated Data Engineer to join the Quantitative Business Intelligence Tools (QuBIT) team. In this role, you will be for responsible for building a data lake and implementing data pipelines from various Fulfillment and Transportation applications used to provide seamless data access/visibility across North American Customer Fulfillment (NACF) Network with visibility spanning from Fulfillment Center teams to corporate leadership. This NACF network includes hundreds of fulfillment centers charged with completing and shipping hundreds of millions of packages to customers each year! Every package we ship out is an opportunity for improvement, translating to better prices and service to our customers.\\n\\nData Engineer Responsibilities\\nYou should be enthusiastic about learning new technologies and be able to design and implement solutions using these technologies to empower internal customers and scale the existing platforms. This team has a broad scope including all of NACF, allowing creative freedom to explore broad business problems and areas of opportunity needing scalable and fast moving solutions. Application breadth can range from creating a new solution to an old problem, or diving into a space to create new value for our operators in Fulfillment Centers. You should also be enthusiastic about building deep domain knowledge about Amazon’s business and the fulfillment quality space. You must possess strong verbal and written communication skills, be self-driven and deliver high quality results in a fast paced environment. You need to really enjoy working closely with your peers in a group of very smart and talented engineers.\\n\\n1. Experience with Agile development methodologies\\nØ Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS.\\nØ Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)\\nØ Experience with building pipelines from application database.\\n\\nAmazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Data Engineer, Runner Performance Lab</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCreate and maintain optimal data pipeline architecture for the Run Research Lab\\n Lead and drive the re-structuring of the current data architecture, development and implementation of new data management projects &amp; capabilities, data applications and data cleansing.\\n Collaborate with appropriate data owners and key stakeholders including Research, Assessment, Run Sights and Product Creation to identify and map data from the source environment to the target data environment\\nClean, prepare and optimize data at scale for ingestion and consumption including interfaces between Brooks and third-party systems to enable real time data consumption and preparation for analysis\\n Identify data quality gaps and work with data owners to develop solutions and close gaps. Participate in on-going service delivery, including documentation and ownership of relevant change control requests (including evaluation, test, implementation, and verification).\\nWrite code or use specialized development tools to create product features, enhance and/or customize software components\\n Anticipate, identify and solve issues concerning data management in the Lab to improve data quality.\\nTroubleshoot data issues and perform root cause analysis to proactively resolve product and operational issues\\n Build continuous integration, test-driven development and production deployment frameworks. Drive collaborative reviews of design, code, test plans and data set implementation in support of maintaining data engineering standards. Test developed programs and integration of data from various sources.\\nLiaise with enterprise data teams to ensure that development adheres to organizational architecture guidelines.\\nParticipate in key architectural and technical decisions as they apply to the Run Research Lab\\nCoordinate and conduct application testing (new support packages, releases, functionality and customizing) in close cooperation with the technology team.\\nEngage system owners to filter, size and prioritize business requests and drive towards appropriate decision points.\\nEstablish consistent technical architecture &amp; contribute to development policies, standards and conventions\\nMaintain expert knowledge of development tools, technologies and related delivery methods.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree in computer science, statistics or applicable engineering fields with a focus in biomechanics and a research environment a plus.\\n3+ years’ experience with data management tools and industry standard relational database systems preferably in the lab based setting.\\n An expert in database technologies (SQL, Big Data frameworks (Hadoop, Spark), advanced data modelling, cloud platforms (AWS, Azure) as well as real-time (Kafka) and batch data integration frameworks\\nSignificant experience in writing programs to analyze biomechanical data is strongly preferred (matlab, visual 3D, Labview, ATL, Python, Jave, C/C++)\\nAdvanced knowledge and experience in use of biomechanics systems for analyzing running/walking gait (3D mocap systems (Motion Analysis, Vicon, Qualysis), Visual 3D, plantar pressure systems (Novel)\\nExperience in algorithms, especially in the field of AI and machine learning\\nExperienced in Agile/ Scrum methodologies and collaboration with cross functional teams\\nStrong project management and analytical skills\\nAbility to work cross functionally in a fast paced, dynamic environment\\nCurious and open minded; always open for a challenge, inventive, creative. Ability to challenge the status quo – always looking at improving our products and processes while also displaying a willingness to dive into the details.\\nUnwavering demonstration of Brooks’ corporate values: Serve People, Lead Thought, Compete as a Team, Have Integrity, Be Active, Have Fun!\\nA passion for the running enthusiast and active lifestyle\\nTravel 5% of the time</td>\n",
       "      <td>Who We Are:\\nBrooks is a team of passionate people united by a desire to do meaningful work, lead healthy lives and make a difference. We share a focused mission: to inspire everyone to run and be active. That’s it. No distractions—it’s all about the run. Through science, creativity, service, authenticity and connection, we obsess over delivering the best running gear on the planet. We do it our way, with our unique spirit, with a goal of being more relevant to runners than any other brand, day after day and mile after mile. We are determined to innovate, challenging ourselves to lead thought at every turn. Inside these walls and on the roads, tracks and trails, we live and breathe Run Happy, celebrating the positive impact running has on our lives and others. We inject it into all we do because it makes everything better, smarter, more fun and more memorable. Our company culture defines us, bonds us together and creates the conditions for success. It is lived daily as a behavioral expression of our collective set of brand values: Connect with People, Innovate for our Customer, Compete as a Team, Build Trust, Have Fun &amp; Bring Passion, and Be Active. If you’re on our team, it means you’re part of creating something extraordinary. You’re part of Brooks.\\n\\nWe are looking for a passionate Data Engineer on the Run Research team to help us build and create the future of the run. From optimizing performance to assessing injury risk to improving the experience on the run, you’ll help design &amp; build the components, frameworks and libraries to support and scale our analytics programs that will enable our teams to create amazing products and elevate experiences for our runners. In this role, you will work cross functionally, collaborating with the research teams, our product assessment teams and our product creation teams to develop data &amp; analytics capabilities that will allow us to leverage data to inform how we help runners achieve their path to a better self. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection. The right candidate is excited and passionate about optimizing the Run Research Lab’s data architecture to support our next generation of products and data initiatives.\\n\\nJob Responsibilities\\nCreate and maintain optimal data pipeline architecture for the Run Research Lab\\n Lead and drive the re-structuring of the current data architecture, development and implementation of new data management projects &amp; capabilities, data applications and data cleansing.\\n Collaborate with appropriate data owners and key stakeholders including Research, Assessment, Run Sights and Product Creation to identify and map data from the source environment to the target data environment\\nClean, prepare and optimize data at scale for ingestion and consumption including interfaces between Brooks and third-party systems to enable real time data consumption and preparation for analysis\\n Identify data quality gaps and work with data owners to develop solutions and close gaps. Participate in on-going service delivery, including documentation and ownership of relevant change control requests (including evaluation, test, implementation, and verification).\\nWrite code or use specialized development tools to create product features, enhance and/or customize software components\\n Anticipate, identify and solve issues concerning data management in the Lab to improve data quality.\\nTroubleshoot data issues and perform root cause analysis to proactively resolve product and operational issues\\n Build continuous integration, test-driven development and production deployment frameworks. Drive collaborative reviews of design, code, test plans and data set implementation in support of maintaining data engineering standards. Test developed programs and integration of data from various sources.\\nLiaise with enterprise data teams to ensure that development adheres to organizational architecture guidelines.\\nParticipate in key architectural and technical decisions as they apply to the Run Research Lab\\nCoordinate and conduct application testing (new support packages, releases, functionality and customizing) in close cooperation with the technology team.\\nEngage system owners to filter, size and prioritize business requests and drive towards appropriate decision points.\\nEstablish consistent technical architecture &amp; contribute to development policies, standards and conventions\\nMaintain expert knowledge of development tools, technologies and related delivery methods.\\nRequirements\\nBachelor’s degree in computer science, statistics or applicable engineering fields with a focus in biomechanics and a research environment a plus.\\n3+ years’ experience with data management tools and industry standard relational database systems preferably in the lab based setting.\\n An expert in database technologies (SQL, Big Data frameworks (Hadoop, Spark), advanced data modelling, cloud platforms (AWS, Azure) as well as real-time (Kafka) and batch data integration frameworks\\nSignificant experience in writing programs to analyze biomechanical data is strongly preferred (matlab, visual 3D, Labview, ATL, Python, Jave, C/C++)\\nAdvanced knowledge and experience in use of biomechanics systems for analyzing running/walking gait (3D mocap systems (Motion Analysis, Vicon, Qualysis), Visual 3D, plantar pressure systems (Novel)\\nExperience in algorithms, especially in the field of AI and machine learning\\nExperienced in Agile/ Scrum methodologies and collaboration with cross functional teams\\nStrong project management and analytical skills\\nAbility to work cross functionally in a fast paced, dynamic environment\\nCurious and open minded; always open for a challenge, inventive, creative. Ability to challenge the status quo – always looking at improving our products and processes while also displaying a willingness to dive into the details.\\nUnwavering demonstration of Brooks’ corporate values: Serve People, Lead Thought, Compete as a Team, Have Integrity, Be Active, Have Fun!\\nA passion for the running enthusiast and active lifestyle\\nTravel 5% of the time\\n\\n\\nAt Brooks, we celebrate diversity &amp; equity. We are committed to creating an inclusive environment, and encourage people of all backgrounds, perspectives, experiences, and skills to apply. Brooks is proud to be an equal employment opportunity employer. All employment decisions are made without regard to race, religion, color, national origin, gender, gender identity, the presence of a sensory, physical or mental disability, medical condition, military status, marital status, pregnancy or child birth, sexual orientation, age, genetic information, status as a victim of domestic violence, sexual assault or stalking, political ideology, or any other non-merit based factors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in computer science, mathematics, statistics, economics, or other quantitative field5+ years of relevant work experience in a role requiring application of data modeling and analytic skillsStrong experience with ETL development, data modeling, data warehousing, MySQL, Tableau, and databases in a business environment with large-scale, complex datasetsAdvanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management as requiredSelf-driven, with demonstrated ability to deliver on ambiguous projects as well as projects/requests where the underlying data is incompleteStrong verbal/written communication and presentation skillsExperience in gathering requirements and formulating business metrics for reportingExperienced working in a fast-paced, high-tech environment and comfortable navigating conflicting priorities and ambiguous problems\\n\\nAre you interested in defining the future operating model for Amazon’s NA Capacity Planning? Are you excited by high-visibility, strategic supply chain solutions and like to help drive Amazon's operations planning and forecasting? As a team player, you have an opportunity to work with some of the best Technical Engineers, Program Managers, 160+ FCs located across the network and Business Leaders to design the best fulfillment network on the planet. North America Supply Chain Operations team is looking for a Sr. Business Intelligence Engineer who will work on highly visible strategic projects that will influence business critical decisions.\\n\\nThe ideal candidate will have excellent statistical and analytical abilities, outstanding business acumen and judgment, intense curiosity, strong technical skills, and superior written and verbal communication skills. S/he will be a self-starter, comfortable with ambiguity, able to think big and be creative (while paying careful attention to detail), and enjoys working in a fast-paced dynamic environment. To be successful in this role, you should have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards and using visualization tools, while always applying analytical rigor to solve business problems. In addition to leading the design, development, and management of our analytical tools and reporting, we will also look to this person to provide thought leadership and business analysis support as needed. Your analytics will be used by the Capacity Planning, Capacity Execution, and Demand Planning teams. You will gain knowledge about Amazon’s operations in the capacity planning and forecasting space. You will work as a liaison with different stakeholders (Product Managers, Program Managers, Technical Engineers, Ops Engineering, Finance, and SCOT) in order to diagnose and solve complex business problems by analyzing data and providing recommendations. You will experience a wide range of problem solving situations, strategic to real-time, requiring extensive use of data collection and analysis.\\n\\nJob duties include:\\nDesign, develop and maintain scalable, automated, user-friendly systems, reports, dashboards, etc. that will support our analytical and business needsAnalyze key metrics to uncover trends and root causes of issuesSuggest and build new metrics and analysis that enable better perspective on businessCapture the right metrics to influence stakeholders and measure successDevelop domain expertise and apply to operational problems to find solutionIn-depth research of capacity-related issues, i.e., fullness, space utilization etcWork across teams with different stakeholders to prioritize and deliver data and reportingRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation\\n\\nMBA or Master’s degree in Computer Science, Engineering, Statistics, Mathematics or related fieldFamiliarity with supply chain management concepts including planning, forecasting and optimization gained through work experienceExpert in writing and tuning SQL scriptsExperience working in very large data warehouse environments3+ years of experience in a data engineer or BIE role with a technology companyAdvanced capabilities with productivity software such as Excel and Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Data Engineer - Business Intelligence</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree in Math/Statistics/Engineering or other equivalent quantitative discipline.3+ years in relevant experience as data engineer, data scientist, business intelligence engineer, or equivalent.Advanced working knowledge of large data manipulation and data mining using SQLAdvanced knowledge in developing insightful visualizations and dashboards for large user base using BI tools such as Tableau, Quicksight, OBIEE, or QlikViewAdvanced ability to draw insights from data and clearly communicate them (verbal/written) to the stakeholders and senior management as requiredExperienced in building self-service reporting solutions using business intelligence softwareKnowledge of ETL Tools and Data Warehousing.Knowledge of any programming language (Java, Python, etc) and/or scripting language (Perl, Unix Shell, etc) to process data for modelingBe self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty dataStrong as an individual contributor creating, managing standard operating procedures and internal processStrong sense of ownership, urgency, and drive\\n\\nBACKGROUND\\nEC2 Capacity Lifecycle is part of Amazon Web Services. We are responsible for writing software for perfecting capacity elasticity science, delivering the best possible instance lifecycle experience considering optimum hardware and software configurations, managing our ever-growing multi-billion-dollar in-fleet hardware for most optimum utilization, and expanding the AWS footprint in many new regions every quarter.\\nThis role is in EC2 Capacity Engagement team which is a team that is within the EC2 Capacity Lifecycle organization. EC2 Capacity Engagement team manages, escalates, and drives resolution for all active and anticipated capacity shortages that impact Amazon EC2's compute utility service. This teams also documents and reports on all of these issues, and serves as the voice that brings visibility to these events and their impact on customers. They are responsible for all the management of capacity and customers that is not yet automated (e.g., reviewing current and future pool health, driving actions across internal teams, and coordinating with customers) as well as building or evolving the UI tools that enable humans to better meet these needs and feeding back into automation roadmaps to solve recurring failure modes\\nThe EC2 Capacity Engagement team is searching for a passionate and talented Data Engineer or Business Intelligence Engineer (BIE) to help us manage our supply and demand management systems on a global scale.\\n\\nData Engineer -BIE Role Core Responsibilities within the EC2 CE team\\nCreate, maintain, and expand a central repository for high-quality cleaned and audited data from the various sourcesDesign, develop and maintain scalable, automated, user-friendly systems, reports, dashboards, scorecards, etc. that will support our analytical and business needs, as well as, and present insights to stakeholdersAnalyze key metrics to uncover trends and root causes of issuesWork closely with various stakeholders to define the information needed and how best to present itLeverage Amazon EC2 AWS capacity knowledge to explore for insights in data that prove or disprove long-held beliefs and helps to guide the ability to anticipate future capacity needs that have not yet been identified.Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation\\n\\nAdvanced knowledge of ETL Tools and Data Warehousing.Advanced understanding of AWS services (e.g. DynamoDB, S3, Redshift, and Athena), including complex database ingestion operationsFamiliarity with analytics methods. At least three (Data Prep / Wrangling, Custom Reports / Dashboards, Statistical Inference, Clustering, regression &amp; classification).Demonstrated ability to manage and prioritize workload and roadmapsPrior success working with large, complex data setsDemonstrated ability to achieve stretch goals in a highly innovative and fast-paced environment.Experience in machine learningExperience in Root Cause Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Software Engineer II</td>\n",
       "      <td>Redmond, WA</td>\n",
       "      <td>Redmond</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of professional software development experience.Experience in cloud computing and distributed systems.Customer obsession and a dedication for delivering delightful consumer experiences.Good problem solving and debugging skills associated with production cloud services.Sincere love for the product and seeks out opportunity to innovate and follow through with execution.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Contribute to and learn from a diverse community of developers at Team Xbox.You will collaborate with teams across Microsoft, building cloud services used in numerous customer experiences that reach across multiple client platforms.Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements.Work with data scientist team members that assist them in building and optimizing our product into an innovative industry leader.You are experienced in a wide array of ML techniques, with a penchant for problem solving.As a product innovator, you will ideate, scope work, and drive implementation, validation, and release of new product features.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Software &amp; Data II Engineer in Xbox Live Community\\n\\nXbox Live, the leading gamer network spanning console, PC and mobile games, is central to Microsoft’s gaming strategy. Xbox Live provides the breadth of social gaming experiences with Xbox Live clubs, friends, chat, messaging, community hubs, game hubs, game activity feed, and much more. Xbox Live has an exemplary future as it expands to serve many more scenarios where consumers, and creators inspire each other.\\n\\nWe are looking for a creative, analytical and knowledgeable software and data engineer to join our group of similarly inspired and hardworking engineers. You will help us build the next generation of experiences that will make Xbox Live the best community for everyone. You will closely collaborate with Xbox developers, designers, and data scientists, helping us deepen our understanding of gamers and their relationship with other gamers and game content, to provide more meaningful and richer experiences.\\n\\nXbox Live is an exceptional place to work – for every gamer, and every non-gamer with a real passion for deeply understanding our current and future gaming fans. It is a place where we recognize that we are better when we work together, and even better still when we lift and grow one another. We are made stronger by our array of dynamic perspectives and ideas. And we have fun, a lot of fun. You should join us.\\nResponsibilitiesContribute to and learn from a diverse community of developers at Team Xbox.You will collaborate with teams across Microsoft, building cloud services used in numerous customer experiences that reach across multiple client platforms.Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements.Work with data scientist team members that assist them in building and optimizing our product into an innovative industry leader.You are experienced in a wide array of ML techniques, with a penchant for problem solving.As a product innovator, you will ideate, scope work, and drive implementation, validation, and release of new product features.\\n\\nWe do not have a fixed list of requirements. Instead, your passion to deeply understand what will excite our existing users and motivate new users to join Xbox Live will set you apart. You will work directly with product owners, designers, client developers, deliver actionable insights, recommendations, and more.\\nQualifications5+ years of professional software development experience.Experience in cloud computing and distributed systems.Customer obsession and a dedication for delivering delightful consumer experiences.Good problem solving and debugging skills associated with production cloud services.Sincere love for the product and seeks out opportunity to innovate and follow through with execution.\\nPreferred Qualifications:\\nBA/BS or degree in computer science, statistics, math, economics, business or engineering preferred but not required.Dedication for design and enjoy crafting highly performance servicesExperience with integrating services with different client side architectures, caching, experimentation etc.Experience with an OO programming language like Java/C#/C++; Experience with at least one scripting language (Python, Perl, Ruby, Shell etc.).Experience in data transformation and data visualization.Familiar with machine learning toolkits and productionalizing machine learning models.Have experience building production data pipelines using one or more frameworks such as Spark, Flink or Hive/Hadoop.Experience with Data Warehouse design, ETL (Extraction, Transformation &amp; Load), architecting efficient software designs for DW platform.Experience with highly scalable, distributed service architectures, productionalizing ML models, high velocity streaming pipelines or high scale data pipelines, etc.Experience with technologies such as Azure Data Lake, Hadoop, Spark, Hive, Kusto, Elasticsearch, etc.Familiarity with Gaming and Social Network domains.\\n#xboxlivejobs\\n\\n#getjobs\\n#gamingjobs\\n\\nMicrosoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.\\n\\nBenefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.\\n\\nXGAXGETXBL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Data Strategy Specialist - Business &amp; Data Analysis, Cloud, AWS, Azure, Big Data</td>\n",
       "      <td>Seattle, WA 98104</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98104</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe North America Data Strategy &amp; Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.\\n\\n Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model\\n\\nRequired Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:\\no Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling\\no Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.\\no Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel\\n Able to travel up to 100% (Mon-Thu)\\n\\nOptional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model\\n Experience in compiling business cases and roadmaps for data, analytics and technology investments\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>New York Hiring Conference - Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Economics, Finance, Mathematics, Statistics, Engineering).4+ years of relevant experience in one of the following areas: data science, data engineering, business intelligence or business analytics.Strong analytical and problem-solving skills.Expertise in the design, creation and management of large datasets/data models.Expert-level proficiency in writing complex, highly-optimized SQL queries across large datasets.Ability to work with business owners to define key business requirements and convert to technical specifications.Ability to manage priorities simultaneously and drive projects to completion.\\n\\nAmazon Global Finance &amp; Finance Tech teams are coming to New York!\\nWe are hosting an exclusive hiring event for lead engineers in the data space on October 10th &amp; 11th, 2019 – if you are passionate about Big Data, BI systems, Cloud/AWS &amp; ML, and always enjoy a good challenge of highly complex technical contexts, we have the opportunity for you!\\n\\nEven the best analysts’ and scientists’ impact is dependent on having access to high quality, reliable data at scale. We are looking for top data engineers to join various teams within the Finance &amp; Finance Tech space in our Seattle HQ, and the person will be responsible of partnering with our research team to understand data needs, establish/manage a data store, work with teams across multiple functions to identify normative data sources, and build data pipelines for production level systems. As a Data Engineer, you will be owning the technical architecture of BI and Data platforms, working with very large data sets in one of the world's largest and most complex data warehouse environments, and you will work closely with the business and technical teams in analyzing many unique business problems and use creative problem-solving to deliver results. You will work in a fast paced environment with some of the brightest engineers to innovate on behalf of the customer. You should be somebody that is passionate about solving customers’ problems and gets excited about owning infrastructure services that serve critical finance systems. You will also guide the team on software development best practices and set examples by using them in the solutions you build.\\n\\nIn summary, a typical Data Engineer in Amazon works on:\\nArchitecture design and implementation of next generation BI solutions, enabling stakeholders to manage the business and make effective decisions.Designing, planning, and building for secure, available, scalable, stable, and cost-effective data solutions in the various engineering subject areas as it relates to data storage and movement solutions: data warehousing, enterprise system data architecture, data design (e.g., Logical and Physical Modeling), data persistence technologies, data processing, data management, and data analysis.\\n\\nMasters in computer science, mathematics, statistics, economics, or other quantitative field.Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.Experience providing technical leadership and mentoring other engineers for best practices on data engineering.Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.Experience with AWS services including S3, Redshift, EMR and RDS.Knowledge with statistical and/or econometric modeling.Experience in BI/DW as a change leader providing strategic research, recommendations, and implementations.\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Data Engineer, Analytics - Seattle</td>\n",
       "      <td>Seattle, WA 98101</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98101</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.\\nDo you like working with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.\\n\\n\\nThis is a full-time position based in our office in Seattle.\\nRESPONSIBILITIES\\nManage data warehouse plans for a product or a group of products.\\nInterface with engineers, product managers and product analysts to understand data needs.\\nBuild data expertise and own data quality for allocated areas of ownership.\\nDesign, build and launch new data models in production.\\nDesign, build and launch new data extraction, transformation and loading processes in production.\\nSupport existing processes running in production.\\nDefine and manage SLA for all data sets in allocated areas of ownership.\\nWork with data infrastructure to triage infra issues and drive to resolution.\\nMINIMUM QUALIFICATIONS\\n2+ years experience in the data warehouse space.\\n2+ years experience in custom ETL design, implementation and maintenance.\\n2+ years experience working with either a MapReduce or an MPP system.\\n2+ years experience with object-oriented programming languages.\\n2+ years experience with schema design and dimensional data modeling.\\n2+ years experience in writing SQL statements.\\nExperience analyzing data to identify deliverables, gaps and inconsistencies.\\nExperience managing and communicating data warehouse plans to internal clients.\\nPREFERRED QUALIFICATIONS\\nBS/BA in Technical Field, Computer Science or Mathematics.\\nKnowledge in Python or Java.\\nFacebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\n\\nOur Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.\\nA Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.\\nWhat you'll own:\\nData pipeline / ETL development:\\nBuilding and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies\\nFocus on data curation on top of datalake data to produce trusted datasets for analytics teams\\nData Curation:\\nProcessing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists\\nMigrating self-serve data pipeline to centrally managed ETL pipelines\\nAdvanced SQL development and performance tuning\\nSome exposure to Spark, Glue or other distributed processing frameworks helpful\\nWork with business data stewards &amp; analytics team to research and identify data quality issues to be resolved in the curation process\\nData Modeling:\\nDesign and build master dimensions to support analytic data requirements\\nReplacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems\\nDesign, build and support pipelines to deliver business critical datasets\\nResolve complex data design issues &amp; provide optimal solutions that meet business requirements and benefit system performance\\nQuery Engine Expertise &amp; Performance Tuning:\\nAssist Analytics teams with tuning efforts\\nCurated dataset design for performance\\nOrchestration:\\nManagement of job scheduling\\nDependency management mapping and support\\nDocumentation of issue resolution procedures\\nData Access\\nDesign and management of data access controls mapped to curated datasets\\nLeveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment\\n\\nExperience you'll need:\\nStrong experience designing and building end-to-end data pipelines\\nExtensive SQL development experience\\nKnowledge of data management fundamentals and data storage principles\\nData modeling:\\nNormalization\\nDimensional/OLAP design and data warehousing\\nMaster data management patterns\\nModeling trade-offs impacting data management &amp; processing/query performance\\nKnowledge of distributed systems as it pertains to data storage, data processing and querying\\nExtensive experience in ETL and DB performance tuning\\nHands on experience with a scripting language (Python, bash, etc.)\\nSome experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful\\n\\nFamiliarity with the technology stacks available for:\\nMetadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nData management, data processing and curation:\\nPostgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.\\nExperience in data modeling for batch processing and streaming data feeds; structured and unstructured data\\nExperience in data security / access management, data cataloging and overall data environment management\\n\\nExperience with cloud services such as AWS and APIs helpful\\nYou’d be a great fit if your current track record looks like this:\\n5+ years of progressive experience data engineering and data warehousing\\nExperience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))\\nExperience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)\\nStrong capability to manipulate and analyze complex, high-volume data from a variety of sources\\nEffective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language\\nAbility to problem solve independently and prioritize work based on the anticipated business value\\n\\nQualifications\\n\\nnull\\n\\nAdditional Information\\n\\nAll your information will be kept confidential according to EEO guidelines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Sr Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience5+ years of relevant work experience in analytics, data engineering, business intelligence, market research or related fieldExperience in data modeling, ETL development, and data warehousing, or similar skillsExperience using SQL with large data sets (e.g. Oracle, SQL Server, Redshift)Experience with AWS technologies including Redshift, RDS, S3, EMR, Kinesis\\n\\nAlexa is the cloud service that powers Amazon Echo, the groundbreaking device designed around your voice. This is an opportunity to join a growing team that is working to build an exciting new Amazon business in voice.\\n\\nWe are looking for an exceptional Data Engineer who will own building and maintaining Alexa Skill’s data model and architecture. This includes implementation of a BI platform, promotion of scalability through automation and reporting tools, and adherence to the highest data quality and governance standards. The candidate will also drive the design and implementation of world class big data infrastructure to support machine learning and econometric analysis using Skills data.\\n\\nThe successful candidate will be an expert with SQL, Python, AWS technologies and have exemplary communication skills. The candidate will need to be a self-starter, comfortable with ambiguity in a fast-paced and ever-changing environment, and able to think big while paying careful attention to detail.\\n\\nResponsibilities:\\n\\nDesign, implement, and support an analytical data infrastructure providing ad hoc access to large datasets and computing powerManaging AWS resources including EC2, RDS, Redshift, et ceteraInterface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologiesExplore and learn the latest AWS technologies to provide new capabilities and increase efficiencyCollaborate with Business Intelligence Engineers (BIEs) to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentationCollaborate with Data Scientists to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learningHelp continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\\n\\nGraduate degree in computer science, business, mathematics, statistics, economics, or other quantitative field10 or more years' of prior experience in a Data Engineer role with a technology company or financial institutionBoth technically deep and business savvy enough to interface with all levels and disciplines within the organizationKnowledge of Advanced SQL and scripting for automation (e.g. Python, Perl or Ruby)Familiarity with statistical models and data mining algorithmsExperience with Hadoop or other map/reduce \"big data\" systems and services\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Kent, WA</td>\n",
       "      <td>Kent</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of data engineering, ETL and/or data warehouse development\\nMaster’s Degree in Computer Science (or similar area of study)\\nTechnical expertise and experience both SQL and NOSQL databases\\nAdvanced understanding of a wide array of data models including relational, dimensional, document-based, object oriented, object-relational, and graphical\\nAdvanced experience in database interrogation of SQL and NOSQL databases\\nExperience implementing High Availability systems requirement\\nExperience with web based APIs (e.g. REST, SOAP)\\nExperience with AWS Stack (RDS, Kinesis, Lambda, Redshift, SQS, etc)\\nProficiency in scripting languages (e.g. Python, Bash)\\nStrong analytic skill set and a high degree of proficiency in data mining\\nExcellent written communication and presentation skills\\nMust be a U.S. citizen or national, U.S. permanent resident (current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Collaborate with departments and technical product managers to collect, transform and aggregate information that leads to business insights\\nBuild and maintain tools, data pipelines, analytics, reports to highlight technical performance metrics and other key information identified by programs and functional leadership\\nWork with application developers to collect data from custom applications\\nEstablish processes and tools for monitoring and improving performance and effectivity of new and existing data integrations and pipelines\\nPerform quality assurance and code reviews to ensure both functional and non-functional requirements are being met\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description:\\nAs part of a small, passionate and accomplished team of experts, you will work with stakeholders and technical product managers to create a world class decision support system. To successfully accomplish this task, you will design and implement data pipelines from scores of source systems, create flexible and powerful data models and pathways to allow reliable and timely information to be securely delivered downstream to systems and people. This position requires a commitment to quality and attention to detail that will directly impact the history of space exploration and will require your dedicated commitment and detailed attention towards safe and repeatable spaceflight.\\nResponsibilities:\\nCollaborate with departments and technical product managers to collect, transform and aggregate information that leads to business insights\\nBuild and maintain tools, data pipelines, analytics, reports to highlight technical performance metrics and other key information identified by programs and functional leadership\\nWork with application developers to collect data from custom applications\\nEstablish processes and tools for monitoring and improving performance and effectivity of new and existing data integrations and pipelines\\nPerform quality assurance and code reviews to ensure both functional and non-functional requirements are being met\\nQualifications:\\n5+ years of data engineering, ETL and/or data warehouse development\\nMaster’s Degree in Computer Science (or similar area of study)\\nTechnical expertise and experience both SQL and NOSQL databases\\nAdvanced understanding of a wide array of data models including relational, dimensional, document-based, object oriented, object-relational, and graphical\\nAdvanced experience in database interrogation of SQL and NOSQL databases\\nExperience implementing High Availability systems requirement\\nExperience with web based APIs (e.g. REST, SOAP)\\nExperience with AWS Stack (RDS, Kinesis, Lambda, Redshift, SQS, etc)\\nProficiency in scripting languages (e.g. Python, Bash)\\nStrong analytic skill set and a high degree of proficiency in data mining\\nExcellent written communication and presentation skills\\nMust be a U.S. citizen or national, U.S. permanent resident (current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum.\\nDesired:\\nExperience with and knowledge of project management principles and practices\\nExperience in manufacturing processes such as Integrated Supply Chain\\nExperience with OLAP Cubes or similar BI constructs\\nExperience with Kafka, Spark and other big data pipeline technologies\\nExperience with IoT / Smart Factory data collection and aggregation\\nBlue Origin offers a phenomenal work environment and awesome culture with competitive compensation, benefits, 401K, and relocation.\\n\\n\\nBlue Origin is an equal opportunity employer . In addition to EEO being the law, it is a policy that is fully consistent with Blue's principles. All qualified applicants will receive consideration for employment without regard to status as a protected veteran or a qualified individual with a disability, or other protected status such as race, religion, color, national origin, sex, sexual orientation, gender identity, genetic information, pregnancy or age. Blue Origin prohibits any form of workplace harassment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Data Engineer In Test</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's in Computer Science or related degree\\n</td>\n",
       "      <td>Bachelor's in Computer Science or related degree\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.\\nOne thing that's stayed the same since our founding: our commitment to our customers, partners and employees.\\nJoin us on our journey as we continue to grow into a powerful contender in the field of insurance.\\nThis position contributes to developing, implementing, and sustaining manual &amp; automation testing including performance testing processes, practices, and controls in support of application and system requirement throughout the software development and sustainment lifecycles. Provides direction on the development and implementation of test automation and performance testing processes, methods and tools.\\n\\nThe position requires understanding &amp; experience in AWS Data Platform\\n\\nExperience is required in creating test plans/test cases, executing tests for applications &amp; validating data using Tableau.\\nSkills Required:\\nHands-on Engineer with experience in development/testing software with big data components in AWS Cloud infrastructure\\nExperience in testing AWS data pipelines using S3, AWS GLUE, Athena, PySpark, AWS Code Pipeline, Jupyter Notebooks, XML/JSON, Redshift, Tableau, etc.\\nSkills in SQL, Python, pytest, Git, Code deployment &amp; CI/CD practices\\nExperience scripting, running ETL jobs, troubleshooting errors, analyzing data and performance testing.\\nExperience working with Agile SDLC frameworks i.e. SCRUM, Kanban, DevOps.\\nExperience developing or working with commercial or open source automation tools and frameworks\\nDemonstrate knowledge using version control and defect tracking methods, including an understanding of associated tools\\nDemonstrated collaboration working with diverse teams including project managers, business analysts, and Engineers related to quality assurance roles and responsibilities\\nQualifications:\\nBachelor's in Computer Science or related degree\\n3-5 years of experience in Data Engineering in Test\\n3+ years of experience with SQL, Python &amp; Tableau\\nUnderstanding of key QA metrics and defect management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>This position is full time and based at our Seattle, U.S. office.\\n\\n\\nWHO YOU ARE\\nThe ideal candidate has extensive experience designing and building data solutions with the open-source technology stack including: Hadoop, Spark, Hive, Airflow, or any other. Experience developing and maintaining a commercial quality B2B SaaS platform is highly preferred as well as involvement on AWS (Amazon Web Services). You must enjoy collaborating with team members and acting as scrum master to successfully deliver projects using an agile methodology. Even if you do not possess skill in these technologies and architectures, but you are a knowledgeable, experienced data management profession, please do apply!\\n\\n\\nRESPONSIBILITIES\\nDefine, develop, and maintain the TenPoint7 Cloud platform including the technology components that provide: data ingestion, data integration, data modeling, data processing and data visualization\\nProvide data related consulting to clients for the creation of custom analytics apps or the implementation of packaged analytics apps\\nGather, analyze, and document project functional requirements\\nDefine epics, user stories, tasks, and subtasks for projects\\nDesign and develop new features for the SaaS platform\\nMaintain and enhance existing features of the SaaS platform\\nCreate unit tests, perform unit testing and fix bugs\\nAssist in project management responsibilities including scope, schedule, issues and risks\\nHelp validate that solutions meet requirements and service/quality level agreements\\nCommunicate project status to team members\\nMaintain quality standards of excellence and ensure compliance with TenPoint7 delivery standards and best practices\\n\\n\\nTECHNICAL REQUIREMENTS\\nPython, Java or JavaScript, SQL\\nSpark, Spark is nice to have\\nExperience with AWS, EMR or SageMaker is preferred\\n\\n\\nPERSONAL ATTRIBUTES\\nDrive – determined to work hard and get things done\\nIntegrity – always reliable and professional for our clients and our team members\\nTeam Oriented – Collaboratively create productive, cohesive, intercontinental teams\\nInnovative – solve complex problems in new and unique ways\\nAnalytical – Understand data and all its potential\\nSelf-Reliant and self-confident\\nPersistent and fearless\\nPowerfully passionate\\n\\n\\nQUALIFICATIONS\\nBachelor of Science degree in computer science\\nExperience in back-end web application development\\nExperience in managing projects, and providing clarity and transparency on project status to all stakeholders\\nVery good verbal and written English language with strong communication skills\\n\\n\\nABOUT TENPOINT7\\nTenPoint7 is an Analytics Software-as-a-Service company based in Seattle, WA with a global development office located in Ho Chi Minh City, Vietnam. We deliver high value analytics apps hosted in the cloud that are infused with Data Science based algorithms. We are driven by these 3 simple things: Data, People &amp; Value. If you find interest, please send your resume in English to careers@tenpoint7.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Senior Cloud Solutions Architect</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMastery in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio\\nApplication Development: building custom web and mobile applications on top of the GCP stack\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExperience providing oversight and direction of cloud projects\\nExperience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states\\nExperience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP\\nExperience across multiple cloud platforms: GCP, AWS, Azure\\nExperience with container engines: Kubernetes, Docker, AWS Elastic Container Service\\nExperience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation\\nExperience working with engineering and sales teams to elicit customer requirements\\nAbility to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders\\nTime management skills with the ability to manage multiple streams and lead less experienced architects\\nExperience as a technical consultant or another customer-facing technical role\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Sr. Cloud Solutions Architect!\\n\\nYour Mission\\n\\nAs a Sr. Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.\\n\\nYou will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures that other engineers and architects demur to you for lack of expertise. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.\\n\\nPathway to Success\\n\\n#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.\\n\\nYou will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the solution architecture or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events.\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives\\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Cloud Architect Certified\\n\\n[https://cloud.google.com/certification/cloud-architect] and/or Google\\nProfessional Data Engineer Certified\\n[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.\\n\\nRequired Qualifications:\\n\\nMastery in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio\\nApplication Development: building custom web and mobile applications on top of the GCP stack\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExperience providing oversight and direction of cloud projects\\nExperience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states\\nExperience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP\\nExperience across multiple cloud platforms: GCP, AWS, Azure\\nExperience with container engines: Kubernetes, Docker, AWS Elastic Container Service\\nExperience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation\\nExperience working with engineering and sales teams to elicit customer requirements\\nAbility to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders\\nTime management skills with the ability to manage multiple streams and lead less experienced architects\\nExperience as a technical consultant or another customer-facing technical role\\n\\nUseful Qualifications:\\n\\nHands-on experience designing and recommending elegant solutions that drive business outcomes\\nExperience building, designing and migrating complex cloud architectures\\nStrong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team\\nAbility to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security\\nDeep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed\\nKnowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes\\nHighly self-motivated and able to work independently as well as in a team environment\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Title      Location      City  \\\n",
       "0    Big Data Engineer                            Bellevue, WA  Bellevue   \n",
       "1    Data Engineer I, Amazon Payment Products     Seattle, WA   Seattle    \n",
       "2    HR Finance Data Engineer III                 Seattle, WA   Seattle    \n",
       "3    Business Intelligence Engineer               Seattle, WA   Seattle    \n",
       "4    Data Engineer, Global Specialty Fulfillment  Seattle, WA   Seattle    \n",
       "..                                           ...          ...       ...    \n",
       "159  Sr Data Engineer                             Seattle, WA   Seattle    \n",
       "160  Data Engineer                                Kent, WA      Kent       \n",
       "161  Data Engineer In Test                        Seattle, WA   Seattle    \n",
       "162  Data Engineer                                Seattle, WA   Seattle    \n",
       "163  Senior Cloud Solutions Architect             Seattle, WA   Seattle    \n",
       "\n",
       "    State         Zip     Country  \\\n",
       "0    WA    None Found  None Found   \n",
       "1    WA    None Found  None Found   \n",
       "2    WA    None Found  None Found   \n",
       "3    WA    None Found  None Found   \n",
       "4    WA    None Found  None Found   \n",
       "..   ..           ...         ...   \n",
       "159  WA    None Found  None Found   \n",
       "160  WA    None Found  None Found   \n",
       "161  WA    None Found  None Found   \n",
       "162  WA    None Found  None Found   \n",
       "163  WA    None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Qualifications  \\\n",
       "0    \\n4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL\\nStrong Python coding skills\\nExperience developing and implementing ETL architectures with large, complex data sets\\nUnderstanding of database architecture and data lakes\\nDistributing computing (parallel processing, multi-threading) – Hadoop, MapReduce, Spark\\nHands-on experience with web crawling/web scraping required (6+ months)\\nExperience developing APIs\\nExperience with Node.js and familiarity with Machine Learning are pluses\\nStrong quantitative data analysis skills\\nBeyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech\\nCuriosity about anomalies in the data and the ability to identify the business opportunities they represent.\\nStrong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels\\nAzure experience is a plus\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "159  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "160  5+ years of data engineering, ETL and/or data warehouse development\\nMaster’s Degree in Computer Science (or similar area of study)\\nTechnical expertise and experience both SQL and NOSQL databases\\nAdvanced understanding of a wide array of data models including relational, dimensional, document-based, object oriented, object-relational, and graphical\\nAdvanced experience in database interrogation of SQL and NOSQL databases\\nExperience implementing High Availability systems requirement\\nExperience with web based APIs (e.g. REST, SOAP)\\nExperience with AWS Stack (RDS, Kinesis, Lambda, Redshift, SQS, etc)\\nProficiency in scripting languages (e.g. Python, Bash)\\nStrong analytic skill set and a high degree of proficiency in data mining\\nExcellent written communication and presentation skills\\nMust be a U.S. citizen or national, U.S. permanent resident (current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "161  Bachelor's in Computer Science or related degree\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "162  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "163  \\nMastery in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio\\nApplication Development: building custom web and mobile applications on top of the GCP stack\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExperience providing oversight and direction of cloud projects\\nExperience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states\\nExperience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP\\nExperience across multiple cloud platforms: GCP, AWS, Azure\\nExperience with container engines: Kubernetes, Docker, AWS Elastic Container Service\\nExperience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation\\nExperience working with engineering and sales teams to elicit customer requirements\\nAbility to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders\\nTime management skills with the ability to manage multiple streams and lead less experienced architects\\nExperience as a technical consultant or another customer-facing technical role\\n   \n",
       "\n",
       "                                                 Skills  \\\n",
       "0    None Found                                           \n",
       "1    None Found                                           \n",
       "2    None Found                                           \n",
       "3    None Found                                           \n",
       "4    None Found                                           \n",
       "..          ...                                           \n",
       "159  None Found                                           \n",
       "160  None Found                                           \n",
       "161  Bachelor's in Computer Science or related degree\\n   \n",
       "162  None Found                                           \n",
       "163  None Found                                           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Responsibilities  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "159  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "160  Collaborate with departments and technical product managers to collect, transform and aggregate information that leads to business insights\\nBuild and maintain tools, data pipelines, analytics, reports to highlight technical performance metrics and other key information identified by programs and functional leadership\\nWork with application developers to collect data from custom applications\\nEstablish processes and tools for monitoring and improving performance and effectivity of new and existing data integrations and pipelines\\nPerform quality assurance and code reviews to ensure both functional and non-functional requirements are being met\\n   \n",
       "161  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "162  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "163  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "\n",
       "      Education  \\\n",
       "0    None Found   \n",
       "1    None Found   \n",
       "2    None Found   \n",
       "3    None Found   \n",
       "4    None Found   \n",
       "..          ...   \n",
       "159  None Found   \n",
       "160  None Found   \n",
       "161  None Found   \n",
       "162  None Found   \n",
       "163  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Requirement  \\\n",
       "0    \\n4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL\\nStrong Python coding skills\\nExperience developing and implementing ETL architectures with large, complex data sets\\nUnderstanding of database architecture and data lakes\\nDistributing computing (parallel processing, multi-threading) – Hadoop, MapReduce, Spark\\nHands-on experience with web crawling/web scraping required (6+ months)\\nExperience developing APIs\\nExperience with Node.js and familiarity with Machine Learning are pluses\\nStrong quantitative data analysis skills\\nBeyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech\\nCuriosity about anomalies in the data and the ability to identify the business opportunities they represent.\\nStrong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels\\nAzure experience is a plus\\n   \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "159  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "160  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "161  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "162  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "163  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        FullDescriptions  \n",
       "0    About us\\n\\nLaunched in October 2018, the Likewise app is the fun, social and incredibly useful way for people to discover, curate, and share recommendations on TV shows, movies, books, podcasts, restaurants, travel and more. Best of all, Likewise helps people quickly find recommendations from their friends, family, and other trusted sources.\\nImagined and backed by Bill Gates’ private office, Likewise is a rare early-stage startup that is thinking big, playing to win, and investing to continue its rapid growth trajectory. If you are passionate about what you do, and want to be a core part of creating a household consumer name, then come talk to us about getting in on the fun!\\nHere's a link to a Geekwire article about us: https://bit.ly/2RuxBlx. And Built In Seattle named us one of Seattle’s 50 Startups to Watch in 2019! https://bit.ly/2VXup3o\\nRole\\nWith the Likewise app launched, we have a lot of fun and creative work ahead of us in making Likewise’s AI into the end-all-be-all for determining the best recommendations to consumers across any category – movies, podcasts, books, restaurants, travel, and more! The work you will be doing is the foundation to making Likewise AI real, and it won’t happen without you. You’ll redefine how AI makes recommendations, and in doing so, change people’s lives for the better. We expect to grow the team as the company grows, and the right candidate will have the potential to lead that growth.\\nObjectives\\nCreate a process that handles our disparate internal and external data sources and automatically converts that unstructured data into structured data to be consumed by machine learning and our product, marketing, and executive teams\\nBuild data process pipelines for new and existing data sources\\nGlean insights and business opportunities from the data, and champion ideas for improvement based on those insights to the product team\\nLead the external data sources collection effort, and creatively identify new, relevant data sources that will positively impact our products and users\\nWork closely with the Data Science team to complete all data needs\\nFind the handful of outliers in massive data sets and define processes to handle them\\nRequirements\\nQualifications\\n4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL\\nStrong Python coding skills\\nExperience developing and implementing ETL architectures with large, complex data sets\\nUnderstanding of database architecture and data lakes\\nDistributing computing (parallel processing, multi-threading) – Hadoop, MapReduce, Spark\\nHands-on experience with web crawling/web scraping required (6+ months)\\nExperience developing APIs\\nExperience with Node.js and familiarity with Machine Learning are pluses\\nStrong quantitative data analysis skills\\nBeyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech\\nCuriosity about anomalies in the data and the ability to identify the business opportunities they represent.\\nStrong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels\\nAzure experience is a plus\\nBenefits\\nWorking here\\nLocated in downtown Bellevue, close to restaurants, shopping, parks and transit, we are proud to offer a competitive benefits package including stock options, health care where we pay 100% of employee premiums, 401(k) plan, commuter benefits, flexible paid time off, and a culture that’s team-based, open, casual and fun. If you’re looking for a rare opportunity to be a part of an innovative, exciting company and become a key member on our team, join us!\\nWe support workplace diversity and do not discriminate on the basis of race, color, religion, gender identity or expression, national origin, age, military service eligibility, veteran status, sexual orientation, marital status, physical or mental disability, or any other protected class.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "1    We are looking for Data Engineer who has a passion for their customers and a passion for working with data. You like working with your customers, understanding their challenges, and partnering with them to invent great solutions. You like working with large data sets, and bringing data together from multiple systems to answer critical business questions and drive change. You are analytical and creative. You should also have the following skills or experiences:Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance or related technical field.\\n3+ years developing end-to-end Business Intelligence solutions: data modeling, ETL and reporting3+ years in relational database concepts with a solid knowledge of star schema, Oracle, SQL, PL/SQL, SQL Tuning, OLAP, Big Data technologiesExperience with coding languages like Python/Java/ScalaExperience in working with business customers to drive requirements analysisHave analytical skills and be creativeExperience with Big Data solutions: Hadoop, Hive or other frameworksExposure to large databases, BI applications, data quality and performance tuningExcellent written and spoken communication skill\\n\\nHundreds of millions of customers, billions of transactions, petabytes of data… How to use the world’s richest collection of e-commerce data to provide superior value and better paying experience to customers ? The Amazon Payments Team manages all Amazon branded payment offerings globally. These offerings are growing rapidly and we are continuously adding new market-leading features and launching new products. Amazon.com has a culture of data-driven decision-making and demands business intelligence that is timely, accurate, and actionable. This team provides a fast-paced environment where every day brings new challenges and new opportunities.\\n\\nOur team of high caliber software developers, data scientists, statisticians and product managers use rigorous quantitative approaches to ensure that we target the right product to the right customer at the right moment, managing tradeoffs between click through rate, approval rates and lifetime value. In order to accomplish this we leverage the wealth of Amazon’s information to build a wide range of probabilistic models, set up experiments that ensure that we are thriving to reach global optimums and leverage Amazon’s technological infrastructure to display the right offerings in real time.\\n\\nAs a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. You should be passionate about working with huge data sets and be someone who loves to bring datasets together to answer business questions. You should have deep expertise in creation and management of datasets. You will build data analytical solutions that will address increasingly complex business questions.\\n\\nYou should be expert at implementing and operating stable, scalable data flow solutions from production systems into end-user facing applications/reports. These solutions will be fault tolerant, self-healing and adaptive. You will be working on developing solutions that provide some of the unique challenges of space, size and speed. You will implement data analytics using cutting edge analytics patterns and technologies that are inclusive of but not limited to various AWS Offerings -EMR, Lambda, Kinesis, and Spectrum. You will extract huge volumes of structured and unstructured data from various sources (Relational /Non-relational/No-SQL database) and message streams and construct complex analyses. You will write scalable code and tune performance running over billion of rows of data. You will implement data flow solutions that process data on Spark ,Redshift and store in Redshift ,Filebased system (S3) for reporting and adhoc analysis.\\n\\nYou should be detail-oriented and must have an aptitude for solving unstructured problems. You should work in a self-directed environment, own tasks and drive them to completion.\\n\\nYou should have excellent business and communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions. You own customer relationship about data and execute tasks that are manifestations of such ownership, like ensuring high data availability, low latency, documenting data details and transformations and handling user notifications and training.\\n\\nExperience partnering with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies.Experience with web technology to develop dashboards.Practical Knowledge of Linux or Unix shell scriptingExperience in processing large volume of data.Strong troubleshooting and problem solving skillsDemonstrated experience in dealing with Senior Management on addressing their reporting and metrics requirements                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2    Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.5+ years of experience in scripting languages like Python, Scala, etc.Demonstrated strength in data modeling, ETL development, and Data warehousing. Data WarehousingExperience with Redshift, Oracle, NoSQL etc.Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS.Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Experience in working and delivering end-to-end projects independently.Knowledge of distributed systems as it pertains to data storage and computing.\\n\\nThe Human Resources Finance Technology team within Amazon's Financial Planning & Analysis organization is seeking a highly skilled and motivated Data Engineer to join our team in Seattle. You will be building world class big data applications to support Amazon Human Resources. If you enjoy innovating, thinking big and want to contribute directly to the success of a growing team, you may be a prime candidate for this position.\\n\\nAs a Sr. Data Engineer on the HR Finance Tech team, you will\\nBuild robust and scalable data integration (ETL) pipelines using SQL, EMR, Python, Spark, Redshift, Lambda, and MatillionBuild and deliver high quality data architecture to support business intelligence engineer and program managers’ reporting needs.Interface with other technology teams to extract, transform, and load data from a wide variety of data sources.Drive both business and technology solutions to improve visibility into key Amazon HR metrics.Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.Translate data into actionable insights for the stakeholders.\\n\\nProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategyExperience providing technical leadership and mentoring other engineers for best practices on data engineeringKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsMasters in computer science, mathematics, statistics, economics, or other quantitative field\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation/Age                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "3    Bachelors in engineering, science, math, statistics or computer science3+ years of work experience as a business intelligence engineer, data engineer or data scientist role3+ years of experience in SQL programming3+ years of experience in building data warehouses and dimensional modeling3+ years of experience with business intelligence and data visualization tools (e.g. Tableau)3+ years of experience with a modern programming language (e.g. Python, R, Scala etc.)Experience with AWS Suite\\n\\nAmazon Lab126 is an inventive research and development company that designs and engineers high-profile consumer electronics. Lab126 began in 2004 as a subsidiary of Amazon.com, Inc., originally creating the best-selling Kindle family of products. Since then, we have produced groundbreaking devices like Fire tablets, Fire TV, Amazon Echo and Amazon Show. The Amazon Devices group delivers delightfully unique Amazon experiences, giving customers instant access to everything, digital or physical.\\n\\nAre you interested in a fast-paced, high-growth environment with the opportunity to work on business-critical decisions? Amazon Device Accessories is looking for an outstanding Business Intelligence Engineer to join our Operational Excellence Team. We’re looking for someone who can provide insight on KPI’s, understand inferential statistics and advise business teams on how to optimize for profit.\\n\\nAs an engineer on the team, you'll leverage tools and services including Amazon Redshift, Tableau, AWS Glue, AWS Athena, Spark, EMR, Machine Learning and Time Series models to build solutions that deliver data-driven reports, dashboards, and recommendations to high level leadership.\\n\\nYou'll work directly with business leaders and stakeholders to understand different business problems and use cases. You'll work with Finance, Tech and Business teams to identify and consume data sources, transform the data, and build the reports and visualizations needed to meet the requirements. You’ll have the opportunity to get hands on experience with Machine Learning, Time Series Modelling and high impact business analysis.\\n\\nDeveloping this capability will provide insights that are used to lead decision making around product allocation, product effectiveness, productivity analysis and business impact. Consumers of these insights will include Directors, VP’s and SVP’s.\\n\\nOur tenets for analytics team members are as follows:\\nUtilizing the Scientific Method to make tangible business impactMetrics before Messes\\no Ensuring we’re measuring the right business metrics to guide the business\\nForecast or be Last\\no Developing state of the art predictive models for ensuring we’re moving in the right direction\\n\\nRoles and Responsibilities:\\nBuild data solutions using AWS services that deliver data-driven reports, dashboards, and tools.Develop and implement Time Series and Machine Learning Forecast ModelsManage marketing and sales data for the organizationManage ETL pipelines using AWS EMR and SparkDistill problem definitions, models, and constraints from informal business requirements.Provide innovative self-service tools to our customers to self-serve and scale dataFollow established engineering best practices and define new best practices where required.Identify critical metrics/reports that measure product performance, efficiency/effectiveness and create client facing dashboards to facilitate decision making.Collaborate on the design, development, maintenance, and delivery/presentation of forecasting models, metrics, reports, analyses, tools, and dashboardsPerform proactive diagnostic analysis on the various product measures and surface meaningful insights to the leadership team.Collaborate with Data Scientists, Data Engineers and Economists to develop Product Insights on Marketing and Sales data.\\n\\nMasters in engineering, science, math, statistics or computer scienceExperience using AWS services for data analytics (i.e., Athena, Glue, Redshift, EMR, etc.)Experience developing custom ETL solutions using Python and SQLExperience with Tableau Desktop and Tableau ServerStrong written and verbal communications skills. Having the ability to translate scientific findings into business recommendations and outputs.The ability to influence stakeholders through delivering results and earning trustBasic statistical tests (but not limited to) t-tests, chi-square and regressionExpert SQLProficiency in PythonExperience delivering the best Products to customers\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "4    BS in Computer Science, Math, Physics, or Engineering6+ years relevant work experience in software development or related data-driven fieldKnowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computingDemonstrated experience in relational database concepts with an expert knowledge of SQLDemonstrated ability in data modeling, ETL development, and Data warehousing\\n\\nLove food? We do! The AmazonFresh and Prime Now operations finance team is seeking an experienced and innovative Data Engineer to build tools that support Operations teams in AmazonFresh and Prime Now. We are an analytics team responsible for building tools, analysis, and reporting to support internal leaders within fulfillment, last mile, and supply chain operations. This is a unique opportunity for someone interested in Amazon’s start-up consumables-focused environment. AmazonFresh and Prime Now experiment, fail fast, learn, and scale rapidly.\\n\\nUltra-fast delivery delights Amazon customers by delivering what they want quickly: medication for a sick kid, lunch at work when you forgot, food and drinks for a party, last minute gifts, dinner from a local restaurant, and so many more uses.\\n\\nThe business model of ultra-fast delivery is attractive, and offers our Engineering team the opportunity to work on any number of complex technical problems. Our team designs, builds and owns our end-to-end services from the ground up and works on large scale back-end systems to support the entirety of our order and inventory pipelines.\\n\\nWe are seeking Data Engineer. In this role you will:\\n\\nYou help build the infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, data storage principles, and recent advances in distributed systems\\nYou manage AWS resources.\\nYou collaborate with Business Intelligence Engineers (BIEs) to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation\\nYou help drive the architecture and technology choices that enable a world-class user experience\\nYou develop expertise in a broad range of Amazon’s data resources and know when, how, and which to use and which not to use\\nYou encourage the organization to adopt next-generation data architecture strategies, proposing both data flows and storage solutions\\nYou are comfortable with a degree of ambiguity and willing to develop quick proof of concepts, iterate and improve\\nYou create extensible designs and easy to maintain solutions with the long term vision in mind\\nYou have an understanding and empathy for business objectives, and continually align your work with those objectives and seek to deliver business value. You listen effectively.\\nYou are comfortable presenting your findings to large groups\\n\\nWe have a very flat team structure, and offer a unique opportunity for technical leaders who want to work closely with the business in defining, designing, building and operating products that are in the early stage of fast expansion.\\n\\nExperience working with AWS Big Data TechnologiesExperience working with Open Source Big Data toolsProven track record of delivering a big data solutionExperience developing tools for data engineers and machine learningExperience working with both Batch and Real Time data processing systems\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Vet / Gender Identity / Sexual Orientation / Age                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "159  Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience5+ years of relevant work experience in analytics, data engineering, business intelligence, market research or related fieldExperience in data modeling, ETL development, and data warehousing, or similar skillsExperience using SQL with large data sets (e.g. Oracle, SQL Server, Redshift)Experience with AWS technologies including Redshift, RDS, S3, EMR, Kinesis\\n\\nAlexa is the cloud service that powers Amazon Echo, the groundbreaking device designed around your voice. This is an opportunity to join a growing team that is working to build an exciting new Amazon business in voice.\\n\\nWe are looking for an exceptional Data Engineer who will own building and maintaining Alexa Skill’s data model and architecture. This includes implementation of a BI platform, promotion of scalability through automation and reporting tools, and adherence to the highest data quality and governance standards. The candidate will also drive the design and implementation of world class big data infrastructure to support machine learning and econometric analysis using Skills data.\\n\\nThe successful candidate will be an expert with SQL, Python, AWS technologies and have exemplary communication skills. The candidate will need to be a self-starter, comfortable with ambiguity in a fast-paced and ever-changing environment, and able to think big while paying careful attention to detail.\\n\\nResponsibilities:\\n\\nDesign, implement, and support an analytical data infrastructure providing ad hoc access to large datasets and computing powerManaging AWS resources including EC2, RDS, Redshift, et ceteraInterface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologiesExplore and learn the latest AWS technologies to provide new capabilities and increase efficiencyCollaborate with Business Intelligence Engineers (BIEs) to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentationCollaborate with Data Scientists to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learningHelp continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\\n\\nGraduate degree in computer science, business, mathematics, statistics, economics, or other quantitative field10 or more years' of prior experience in a Data Engineer role with a technology company or financial institutionBoth technically deep and business savvy enough to interface with all levels and disciplines within the organizationKnowledge of Advanced SQL and scripting for automation (e.g. Python, Perl or Ruby)Familiarity with statistical models and data mining algorithmsExperience with Hadoop or other map/reduce \"big data\" systems and services\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "160  Description:\\nAs part of a small, passionate and accomplished team of experts, you will work with stakeholders and technical product managers to create a world class decision support system. To successfully accomplish this task, you will design and implement data pipelines from scores of source systems, create flexible and powerful data models and pathways to allow reliable and timely information to be securely delivered downstream to systems and people. This position requires a commitment to quality and attention to detail that will directly impact the history of space exploration and will require your dedicated commitment and detailed attention towards safe and repeatable spaceflight.\\nResponsibilities:\\nCollaborate with departments and technical product managers to collect, transform and aggregate information that leads to business insights\\nBuild and maintain tools, data pipelines, analytics, reports to highlight technical performance metrics and other key information identified by programs and functional leadership\\nWork with application developers to collect data from custom applications\\nEstablish processes and tools for monitoring and improving performance and effectivity of new and existing data integrations and pipelines\\nPerform quality assurance and code reviews to ensure both functional and non-functional requirements are being met\\nQualifications:\\n5+ years of data engineering, ETL and/or data warehouse development\\nMaster’s Degree in Computer Science (or similar area of study)\\nTechnical expertise and experience both SQL and NOSQL databases\\nAdvanced understanding of a wide array of data models including relational, dimensional, document-based, object oriented, object-relational, and graphical\\nAdvanced experience in database interrogation of SQL and NOSQL databases\\nExperience implementing High Availability systems requirement\\nExperience with web based APIs (e.g. REST, SOAP)\\nExperience with AWS Stack (RDS, Kinesis, Lambda, Redshift, SQS, etc)\\nProficiency in scripting languages (e.g. Python, Bash)\\nStrong analytic skill set and a high degree of proficiency in data mining\\nExcellent written communication and presentation skills\\nMust be a U.S. citizen or national, U.S. permanent resident (current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum.\\nDesired:\\nExperience with and knowledge of project management principles and practices\\nExperience in manufacturing processes such as Integrated Supply Chain\\nExperience with OLAP Cubes or similar BI constructs\\nExperience with Kafka, Spark and other big data pipeline technologies\\nExperience with IoT / Smart Factory data collection and aggregation\\nBlue Origin offers a phenomenal work environment and awesome culture with competitive compensation, benefits, 401K, and relocation.\\n\\n\\nBlue Origin is an equal opportunity employer . In addition to EEO being the law, it is a policy that is fully consistent with Blue's principles. All qualified applicants will receive consideration for employment without regard to status as a protected veteran or a qualified individual with a disability, or other protected status such as race, religion, color, national origin, sex, sexual orientation, gender identity, genetic information, pregnancy or age. Blue Origin prohibits any form of workplace harassment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "161  Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.\\nOne thing that's stayed the same since our founding: our commitment to our customers, partners and employees.\\nJoin us on our journey as we continue to grow into a powerful contender in the field of insurance.\\nThis position contributes to developing, implementing, and sustaining manual & automation testing including performance testing processes, practices, and controls in support of application and system requirement throughout the software development and sustainment lifecycles. Provides direction on the development and implementation of test automation and performance testing processes, methods and tools.\\n\\nThe position requires understanding & experience in AWS Data Platform\\n\\nExperience is required in creating test plans/test cases, executing tests for applications & validating data using Tableau.\\nSkills Required:\\nHands-on Engineer with experience in development/testing software with big data components in AWS Cloud infrastructure\\nExperience in testing AWS data pipelines using S3, AWS GLUE, Athena, PySpark, AWS Code Pipeline, Jupyter Notebooks, XML/JSON, Redshift, Tableau, etc.\\nSkills in SQL, Python, pytest, Git, Code deployment & CI/CD practices\\nExperience scripting, running ETL jobs, troubleshooting errors, analyzing data and performance testing.\\nExperience working with Agile SDLC frameworks i.e. SCRUM, Kanban, DevOps.\\nExperience developing or working with commercial or open source automation tools and frameworks\\nDemonstrate knowledge using version control and defect tracking methods, including an understanding of associated tools\\nDemonstrated collaboration working with diverse teams including project managers, business analysts, and Engineers related to quality assurance roles and responsibilities\\nQualifications:\\nBachelor's in Computer Science or related degree\\n3-5 years of experience in Data Engineering in Test\\n3+ years of experience with SQL, Python & Tableau\\nUnderstanding of key QA metrics and defect management                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "162  This position is full time and based at our Seattle, U.S. office.\\n\\n\\nWHO YOU ARE\\nThe ideal candidate has extensive experience designing and building data solutions with the open-source technology stack including: Hadoop, Spark, Hive, Airflow, or any other. Experience developing and maintaining a commercial quality B2B SaaS platform is highly preferred as well as involvement on AWS (Amazon Web Services). You must enjoy collaborating with team members and acting as scrum master to successfully deliver projects using an agile methodology. Even if you do not possess skill in these technologies and architectures, but you are a knowledgeable, experienced data management profession, please do apply!\\n\\n\\nRESPONSIBILITIES\\nDefine, develop, and maintain the TenPoint7 Cloud platform including the technology components that provide: data ingestion, data integration, data modeling, data processing and data visualization\\nProvide data related consulting to clients for the creation of custom analytics apps or the implementation of packaged analytics apps\\nGather, analyze, and document project functional requirements\\nDefine epics, user stories, tasks, and subtasks for projects\\nDesign and develop new features for the SaaS platform\\nMaintain and enhance existing features of the SaaS platform\\nCreate unit tests, perform unit testing and fix bugs\\nAssist in project management responsibilities including scope, schedule, issues and risks\\nHelp validate that solutions meet requirements and service/quality level agreements\\nCommunicate project status to team members\\nMaintain quality standards of excellence and ensure compliance with TenPoint7 delivery standards and best practices\\n\\n\\nTECHNICAL REQUIREMENTS\\nPython, Java or JavaScript, SQL\\nSpark, Spark is nice to have\\nExperience with AWS, EMR or SageMaker is preferred\\n\\n\\nPERSONAL ATTRIBUTES\\nDrive – determined to work hard and get things done\\nIntegrity – always reliable and professional for our clients and our team members\\nTeam Oriented – Collaboratively create productive, cohesive, intercontinental teams\\nInnovative – solve complex problems in new and unique ways\\nAnalytical – Understand data and all its potential\\nSelf-Reliant and self-confident\\nPersistent and fearless\\nPowerfully passionate\\n\\n\\nQUALIFICATIONS\\nBachelor of Science degree in computer science\\nExperience in back-end web application development\\nExperience in managing projects, and providing clarity and transparency on project status to all stakeholders\\nVery good verbal and written English language with strong communication skills\\n\\n\\nABOUT TENPOINT7\\nTenPoint7 is an Analytics Software-as-a-Service company based in Seattle, WA with a global development office located in Ho Chi Minh City, Vietnam. We deliver high value analytics apps hosted in the cloud that are infused with Data Science based algorithms. We are driven by these 3 simple things: Data, People & Value. If you find interest, please send your resume in English to careers@tenpoint7.com.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "163  Join SADA as a Sr. Cloud Solutions Architect!\\n\\nYour Mission\\n\\nAs a Sr. Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.\\n\\nYou will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures that other engineers and architects demur to you for lack of expertise. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.\\n\\nPathway to Success\\n\\n#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.\\n\\nYou will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the solution architecture or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events.\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives\\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Cloud Architect Certified\\n\\n[https://cloud.google.com/certification/cloud-architect] and/or Google\\nProfessional Data Engineer Certified\\n[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.\\n\\nRequired Qualifications:\\n\\nMastery in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio\\nApplication Development: building custom web and mobile applications on top of the GCP stack\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExperience providing oversight and direction of cloud projects\\nExperience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states\\nExperience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP\\nExperience across multiple cloud platforms: GCP, AWS, Azure\\nExperience with container engines: Kubernetes, Docker, AWS Elastic Container Service\\nExperience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation\\nExperience working with engineering and sales teams to elicit customer requirements\\nAbility to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders\\nTime management skills with the ability to manage multiple streams and lead less experienced architects\\nExperience as a technical consultant or another customer-facing technical role\\n\\nUseful Qualifications:\\n\\nHands-on experience designing and recommending elegant solutions that drive business outcomes\\nExperience building, designing and migrating complex cloud architectures\\nStrong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team\\nAbility to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security\\nDeep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed\\nKnowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes\\nHighly self-motivated and able to work independently as well as in a team environment\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.  \n",
       "\n",
       "[164 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_Bellvue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
