{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=San+Francisco%2C+CA&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer, Music Data Experience</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Data Science, Applied Science, Computer Science, Computer Engineering or related technical discipline3+ years of experience as a data/software developer/scientist or related technical jobExperience with SQL and Spark based data pipelinesExperience with traditional and cloud data modeling techniquesA passion for improving customer experienceExcellent verbal and written communication skills and technical writing skills\\n\\nAmazon Music is awash in data! To help make sense of it all, the Music Data Experience team enables repeatable, easy, in depth analysis of music customer behaviors. We reduce the cost in time and effort of analysis, data set building, model building, and user segmentation. Our goal is to empower all teams at Amazon Music to make data driven decisions and effectively measure their results by providing high quality, high availability data, and democratized data access through self-service tools.\\n\\nIf you love the challenges that come with big data then this role is for you. We collect billions of events a day, manage petabyte scale data on Redshift and S3, and develop data pipelines using Spark/Scala EMR, SQL based ETL, and Java services.\\n\\nYou are a talented, seasoned, and detail-oriented Data Engineer, BI Engineer, or Data Scientist who wants to take on big data challenges in an agile way. Duties include designing events and signals, building big data pipelines, creating efficient data models, performing analysis, and statistical/ML modeling. We manage Amazon Music's most important data pipelines and data sets, and are expanding our self-service data knowledge and capabilities through an Amazon Music data university.\\n\\nThis role requires you to focus on the data end to end, from producers to consumers. You will develop an understanding of our data, analytical techniques, and how to connect insights to the business, and you will gain practical experience in insisting on highest standards on operations in ETL and big data pipelines. With our Amazon Music Unlimited and Prime Music services, and our top music provider spot on the Alexa platform, providing high quality, high availability data to our internal customers is critical to our customer experiences.\\n\\nMusic Data Experience team develops data specifically for a set of key business domains like personalization and marketing and provides and protects a robust self-service core data experience for all internal customers. We deal in AWS technologies like Redshift, S3, EMR, EC2, DynamoDB, Kinesis Firehose, and Lambda. In 2019 this team will migrate Amazon Music's information model and data pipelines to a data lake storage and EMR/Spark processing layer. You'll build our data university and partner with Product, Marketing, BI, and ML teams to build new behavioral events, pipelines, datasets, models, and reporting to support their initiatives. You'll also continue to develop our offline analytics capabilities in Tableau and build out our real time dashboarding capabilities.\\n\\nAmazon Music\\n\\nImagine being a part of an agile team where your ideas have the potential to reach millions. Picture working on cutting-edge consumer-facing products, where every single team member is a critical voice in the decision-making process. Envision being able to leverage the resources of a Fortune-500 company within the atmosphere of a start-up. Welcome to Amazon Music, where ideas are born and come to life as Amazon Music Unlimited, Prime Music, and so much more.\\n\\nEveryone on our team has a meaningful impact on product features, new directions in music streaming, and customer engagement. We are looking for new team members across a variety of job functions including software engineering/development, marketing, design, ops and more. Come join us as we make history by launching exciting new projects in the coming year.\\n\\nOur team is focused on building a personalized, curated, and seamless music experience. We want to help our customers discover up-and-coming artists, while also having access to their favorite established musicians. We build systems that are distributed on a large scale, spanning our music apps, web player, and voice-forward audio engagement on mobile and Amazon Echo devices, powered by Alexa to support our customer base. Amazon Music offerings are available in countries around the world, and our applications support our mission of delivering music to customers in new and exciting ways that enhance their day-to-day lives.\\n\\nCome innovate with the Amazon Music team!\\n\\nGraduate degree in a related technical field5+ years of experience as a data/software developer/scientist or related technical jobExperience with Agile DevelopmentExperience with statistical modeling or machine learning (Classification, Collaborative Filtering)Experience with AWS servicesA love of music!\\nAmazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.\\n\\n#MusicJobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer - Sunnyvale, California</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Working at the clients site in Sunnyvale\\nFlexible work hours, full benefits, 401K\\nA day off for your birthday\\nCompetitive total compensation\\nSo, are you ready for a challenge, work in a multi disciplinary team and have an immediate impact on our projects? Well, this role is for you.\\nDon't forget to follow Averna's journey to becoming the best in its field, on YouTube and LinkedIn.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMinimum of 3 years of relevant experience in building and architecting data solutions\\nDeep understanding of distributed systems\\nExpert in SQL and high-level languages such as Python, Java, or Scala\\nBuilt and maintained data warehouses and ETL pipelines\\nExperience with Data modeling\\nYou have worked with big data solutions like Redshift, Snowflake, Hadoop or Hive\\nExperience with realtime data streaming infrastructure like AWS Kinesis, Spark or Kafka\\nWorked with Cloud-based architecture such as AWS or Google Cloud</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBuild and maintain our data warehouse and data pipelines\\nScaling up our data infrastructure to meet business needs\\nDeploy sophisticated analytics programs, machine learning and statistical methods\\nWork cross-functionally with our product, business, finance and engineering teams</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Spin\\n\\nSpin operates electric scooters in cities and campuses nationwide, bringing sustainable last-mile mobility solutions to diverse communities. Recognized for its consistent cooperation and collaboration with cities, Spin partners closely with transportation planners, elected officials, community groups, and university administrators to bring stationless mobility options to streets in a responsible and carefully orchestrated manner.\\n\\nBased in San Francisco, Spin is a diverse team of engineers, designers, urban planners, policymakers, lawyers and operators with experience from Y Combinator, Lyft, Uber, local and federal government, and the transportation advocacy world. Spin was known for launching the first stationless mobility program in Seattle, and has since expanded to become the exclusive electric scooter partner in mid-sized cities like Coral Gables, Florida and Lexington, Kentucky, and one of a few permitted scooter operators in large cities like Denver, Detroit, and Washington, D.C. The team embeds in cities and neighborhoods to understand their specific transportation needs, and hires locally from the community.\\n\\nSpin is expanding quickly and looking for top-tier talent to help us bring affordable and accessible transportation options to cities and define what future safe streets will look like.\\n\\nAbout the Role\\n\\nBeing a data informed company, data helps us create exceptional experience for our customers and provide insights into the effectiveness of our product.\\n\\nWe are looking for Data Engineers that will build and maintain our data warehouse and data pipelines, collect data from multiple sources, and expose services that make data a first class citizen at Spin. You will be building, architecting and launching highly reliable and scalable data pipelines to support data processing and analytics needs. Your efforts will allow access to business and user behavior insights.\\n\\nThe Team\\n\\nOur engineering team consists engineers that are passionate about creating finely polished and intuitive experiences and, at the same time, obsess over performance and reliability of what we build. We challenge the status quo and strive towards finding the best way to solve problems.\\n\\nWe promote being a more well rounded engineer by working on different parts of the engineering stack. We also work in very small groups to keep processes and overhead low, so we have a lot of trust and accountability to perform the work required to build the best product.\\nResponsibilities\\nBuild and maintain our data warehouse and data pipelines\\nScaling up our data infrastructure to meet business needs\\nDeploy sophisticated analytics programs, machine learning and statistical methods\\nWork cross-functionally with our product, business, finance and engineering teams\\nQualifications\\nMinimum of 3 years of relevant experience in building and architecting data solutions\\nDeep understanding of distributed systems\\nExpert in SQL and high-level languages such as Python, Java, or Scala\\nBuilt and maintained data warehouses and ETL pipelines\\nExperience with Data modeling\\nYou have worked with big data solutions like Redshift, Snowflake, Hadoop or Hive\\nExperience with realtime data streaming infrastructure like AWS Kinesis, Spark or Kafka\\nWorked with Cloud-based architecture such as AWS or Google Cloud\\nBenefits &amp; Perks\\nOpportunity to join a fast-growing startup and help shape and establish the company’s industry leadershipCompetitive health benefitsDaily catered lunch in our SF officeUnlimited PTO for salaried rolesCommuter stipend plus pre-tax benefitsMonthly cell phone bill stipendWellness perk for salaried roles\\n\\nSpin is an equal opportunity employer and will not discriminate against any employee or applicant for employment in an unlawful matter. We celebrate diversity and are committed to creating an inclusive environment for all individuals. Spin treats all employees and job applicants on the basis of merit, qualifications, and competence without regard to any qualified individuals' sex, race, color, religion, national origin, ancestry, gender (including pregnancy, breastfeeding, or related medical condition), sexual orientation, gender identity, gender expression, age, physical or mental disability, medical condition, genetic characteristic or information, marital status, military and veteran status, or any other characteristic protected by state or federal law. Spin also considers qualified applicants with criminal histories, consistent with applicable local, state, and federal law.\\n\\nSpin is committed to providing reasonable accommodations for qualified individuals with disabilities in its job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at job_accommodations@spin.pm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Principal Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Varo is on a mission to redefine banking so it's easy for everyone to make smart choices with their money. Our app offers bank accounts and high-yield savings accounts that don’t cost a thing, tools to help you manage your money and save automatically, and invitation-only personal loans at competitive rates. On the contrary, traditional banks charge fees, offer next-to-nothing savings rates, and don’t work with their customer’s best interests in mind.\\n\\nVaro is distinct from other fintechs: With preliminary approval for a bank charter from the Office of the Comptroller of the Currency (OCC), we're on our way to becoming the first mobile-centric national bank in the country. Our unique team combines the best people in tech and banking, and we’re wildly passionate about keeping our customers happy by helping them manage and grow their money. Based in San Francisco and privately held, Varo has raised $178M to date, led by Warburg Pincus and The Rise Fund / TPG Growth.\\n\\nDATA ENGINEERING AT VARO\\n\\nAs a Principal Big Data Engineer, you will play a senior role in implementing a variety of solutions to ingest data into, process data within, and expose data from, a Data Lake that enables our data warehouse, data mart, reporting and data analysts and scientists to use and explore data in an automated or self-service fashion.\\n\\nAs a technical leader, you will take ownership of the data architecture for processing and analyzing data across the Platform.\\nWHAT YOU'LL DO\\nDevelop and maintain data strategy for Varo in terms of capabilities, architecture, and control mechanisms that support company intentions to be a bank\\nDesign, build and maintain Big Data workflows/pipelines to process records into and out of Varo’s lake\\nProvide technical leadership in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics\\nCollaborate to actively gain buy-in from stakeholders at all levels on technology direction\\nWork with business partners on requirements clarification and results from rollout efforts\\nParticipate in developing and enforcing data security &amp; access control policies\\nArchitect effective controls for a resilient data ingestion process\\nSupport application data integration design and build efforts, including real-time capabilities\\nProficiency in Amazon AWS big data technologies including S3, RDS, RedShift, Elasticsearch, Lambda, AWS Glue\\nConduct code reviews in accordance with team processes and standards\\nPREVIOUS EXPERIENCES THAT'LL HELP YOU BE GREAT\\nBachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience\\n10+ years of ETL, data modeling, warehouse and data pipelines experience.\\n5+ years’ experience working within the AWS Big Data/Hadoop Ecosystem (EMR is preferred)\\nExperience developing extract load transform tooling\\nExperience with downstream consumption patterns is a plus (reports, dashboards, API)AWS Glue, Redshift, RDS is a plus\\nExperience in Hadoop, HDFS, Hive, Python, REST API/ SOAP API, Spark2, Oozie WFs is a plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Hinge Health’s mission is to improve the lives of people suffering from chronic conditions by digitizing the delivery of care - starting with musculoskeletal health. Our vision is to be the world’s most patient-centered Digital Hospital. We’re already achieving remarkable outcomes - helping people overcome chronic pain, avoid surgeries, return to work, and get back to doing the things they love. We've raised close to $37M and our growth shows no sign of slowing.\\n\\nWe are looking for an experienced Data Engineer to manage and scale our growing data assets while maintaining a high level of integrity and precision.\\n\\nOur ideal candidate has an extensive understanding of how data is used for both business analysis and data science. You will work with a wide variety of data sources and storage systems to enable R&amp;D, commercial, and clinical teams to solve problems.\\n\\nOur tech stack: AWS, Aptible, Postgres, Redis, Rails, Python, Airflow, Mode Analytics, Android, React, and React Native.\\nRESPONSIBILITIES\\nMaintain our current ETL-lite while scaling it for the future\\nCreate and maintain views and expand use of rollup tables\\nIdentify opportunities to improve the integrity of our datasets and implement the fixes\\nAssist in building out our payments platform for managing medical claims\\nHelp explore options for delivering data to clients, including possible API access\\nInform our 2020 objectives and key results around scaling and data needs\\nREQUIREMENTS\\nBachelor’s degree in C.S. or comparable degree preferred\\nMinimum of 3 years relevant experience in data engineering\\nAbility to collaborate and problem solve across teams\\nExcellent communication skills, both written and verbal\\nPython: using community-standards, linting, and testing at all appropriate levels.\\nSQL: comfort with joins, unions, views, rollups, windowing functions, testing\\nJSON parsing and fluency with RESTful APIs\\nOperational competency with cloud-hosted systems such as AWS, Aptible, or Heroku\\nAbility to correlate data across multiple sources: RDBs, csv, json\\nUnderstands how to write efficient code and can optimize existing software and queries\\nBONUS POINTS\\nPrior experience with healthcare data (PHI/PII/HIPAA requirements)\\nExperience developing software in Ruby on Rails\\nUnderstanding of user experience principles\\nHistory of technical writing\\nWHAT YOU'LL LOVE ABOUT US\\nCompetitive compensation with meaningful stock options\\nMedical, dental, vision\\n401K match\\n3 months paid parental leave\\nDaily lunch\\nProfessional Development budget\\nMonthly wellness benefit\\nNoise-cancelling headphones\\nWork from home policy\\nOpportunity to join a fantastically talented, diverse, and passionate team at a pivotal time in the company’s lifecycle\\nNo recruiters, please.\\n\\nHinge Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.\\nWe celebrate diversity and are committed to creating an inclusive environment for all employees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>San Francisco, CA 94143</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94143</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experience with building scalable and reliable data pipelines using Data engine technologies like Matillion (Informatica), Python, and SQL based programming.8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.4+ years' experience designing and developing complex ETL/ELT programs with the following Matillion (Informatica), Python etc.8+ years' experience developing complex SQLExperience using Cloud database technologies such as RedShift, Snowflake3+ years' experience programming in Python, and/or Java2+ years' experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)Experience with integration of data from multiple data sourcesExperience developing and implementing streaming data ingestion solutionsExperience in Agile methodology (2+ years)\\n</td>\n",
       "      <td>Role : Sr. Data Engineer 6+ month contract\\nSan Francisco location preferred but San Jose location is okay also.\\nLocal candidates only they must interview onsite in San Francisco with the team for the final round.\\nTop 3 skill sets: SQL, Python, ETL development, AWS Redshift\\nRequirements:Experience with building scalable and reliable data pipelines using Data engine technologies like Matillion (Informatica), Python, and SQL based programming.8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.4+ years' experience designing and developing complex ETL/ELT programs with the following Matillion (Informatica), Python etc.8+ years' experience developing complex SQLExperience using Cloud database technologies such as RedShift, Snowflake3+ years' experience programming in Python, and/or Java2+ years' experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)Experience with integration of data from multiple data sourcesExperience developing and implementing streaming data ingestion solutionsExperience in Agile methodology (2+ years)\\n\\n4v1tQw9tmw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Genomics Data Engineer</td>\n",
       "      <td>Oakland, CA</td>\n",
       "      <td>Oakland</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nThis position requires a B.S. (M.S. or Ph.D. preferred) with 3 + years of experience in computer science, specializing in high-performance/distributed computing, data mining, machine learning, or bioinformatics.\\n3+ years of software engineering experience with proficiency in Python, Scala, Java or C/C++, programming languages.\\nKnowledge of distributed compute technologies, such as Spark, Hadoop, map-reduce, MPI, or other parallel computing frameworks is essential.\\nStrong foundation in data engineering, data science, and/or machine learning, with demonstrated experience applying these technologies at scale on real-world data sets.\\nKnowledge of database technologies, indexing/partitioning, and SQL.\\nExperience with cloud computing (AWS preferred).\\nExperience engineering high volume data and scientific dataflows.\\nBackground of bioinformatics, life sciences, genomics or biology is highly preferred.\\nTeam player with excellent communication skills to effectively collaborate with multiple cross-functional teams of scientists, clinicians, and engineers.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBuild out a big data distributed architecture capable of efficiently processing large-scale genomics data\\nDevelop and deploy bioinformatics/AI analysis algorithms at scale\\nBuild automated and production-quality data processing systems\\nInteract and collaborate with scientists to clearly define and iterate on requirements\\nKeep abreast of new state-of-the-art software data engineering and data science technologies\\nAggregate and analyze genomic and other types of clinical data to find novel insights.\\nDevelop code to implement analysis workflows in a robust and reproducible fashion.\\nFollow processes to improve transparency and reliability of applications, reducing project risks for on-time milestones.\\nEducate other scientists, engineers and management on the methods developed and how they apply to the subject domain and customer needs.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>You will work at the interface of genomics, big data engineering, and advanced analytics. The candidate will contribute to the expansion of our Apache Spark-based distributed analytics platform, building production-quality data processing infrastructure and developing scalable algorithms to analyze genomic and health data for diagnostics of rare genetic disease. This role encompasses engineering of end-to-end solutions that unify and structure diverse data sets efficiently and implement AI algorithms at scale to derive genomic insights in support of clinical applications.\\n\\nThe ideal candidate will have a strong background in computer science, data mining, machine learning, or a related field, with demonstrated experience in engineering scalable and performant data processing software in Spark or another distributed compute environment. In addition, previous experience in a life sciences domain or biotech is essential.\\nResponsibilities\\nBuild out a big data distributed architecture capable of efficiently processing large-scale genomics data\\nDevelop and deploy bioinformatics/AI analysis algorithms at scale\\nBuild automated and production-quality data processing systems\\nInteract and collaborate with scientists to clearly define and iterate on requirements\\nKeep abreast of new state-of-the-art software data engineering and data science technologies\\nAggregate and analyze genomic and other types of clinical data to find novel insights.\\nDevelop code to implement analysis workflows in a robust and reproducible fashion.\\nFollow processes to improve transparency and reliability of applications, reducing project risks for on-time milestones.\\nEducate other scientists, engineers and management on the methods developed and how they apply to the subject domain and customer needs.\\nQualifications\\nThis position requires a B.S. (M.S. or Ph.D. preferred) with 3 + years of experience in computer science, specializing in high-performance/distributed computing, data mining, machine learning, or bioinformatics.\\n3+ years of software engineering experience with proficiency in Python, Scala, Java or C/C++, programming languages.\\nKnowledge of distributed compute technologies, such as Spark, Hadoop, map-reduce, MPI, or other parallel computing frameworks is essential.\\nStrong foundation in data engineering, data science, and/or machine learning, with demonstrated experience applying these technologies at scale on real-world data sets.\\nKnowledge of database technologies, indexing/partitioning, and SQL.\\nExperience with cloud computing (AWS preferred).\\nExperience engineering high volume data and scientific dataflows.\\nBackground of bioinformatics, life sciences, genomics or biology is highly preferred.\\nTeam player with excellent communication skills to effectively collaborate with multiple cross-functional teams of scientists, clinicians, and engineers.\\nCandidates must have pre-existing US work authorization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Novato, CA</td>\n",
       "      <td>Novato</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who We Are:\\n\\n2K publishes some of the most popular video game franchises on the planet including Mafia, Borderlands, BioShock, NBA 2K, WWE 2K, Evolve, XCOM, and Sid Meier’s Civilization. The analytics group is responsible for collecting, processing and utilizing the data in the right way to identify problems and opportunities across the company and to build solutions for them. As part of this, the analytics team works with the studios and other partners to build reliable, scalable, and high-performance data pipelines to help inform all aspects of our business. From data designed to improve our development processes to data designed to drive critical business decisions, our group is constantly facing fun and challenging problems in the big data space.\\n\\n\\nWhat We Need:\\n\\nWe are looking for a Data Engineer to be part of our growing data engineering team within our analytics group. Collaborate with cross-functional teams, studios, external data providers to architect, design and develop a metadata driven data pipeline framework focusing on reusability, scalability, and productivity. S/he will work with data analysts and data scientists to understand data requirements, design and develop data pipelines to ingest data from multiple disparate sources. It’s a once in a lifetime opportunity to be part of a great team chartered to define the future of data and analytics platform for 2k. This position will be reporting directly to the Director of Data Engineering, located at our Novato office.\\n\\n\\nWhat You Will Do:\\nActively contribute to the architecture and design of data platform and data engineering practice\\nDesign and develop a data pipeline framework for ingesting structured and unstructured data\\nWork with the game, marketing, and analytics teams to gather requirements, build, test, and deploy new data pipelines based on business requirements\\nTranslate data and BI requirements into technical design documents and data mapping documents\\nMentor junior engineers and be part of their career growth\\n\\n\\nWho We Think Will Be a Great Fit:\\nIf you have experience building large scale data solutions, are interested in taking up another challenge to shape the future of data and platform capabilities in a fast-paced environment, we think you will be a great fit and we’d love to hear from you!\\n\\n5+ Years of experience in developing near real-time data pipelines\\nStrong hands-on experience with object-oriented/object function scripting languages: Python[Preferred], Java, Scala, etc.\\nExpert knowledge in Data warehouse concepts and implementation of Dimensional and star models\\nExperience with data pipeline and workflow management tools: Luigi, Airflow, etc.\\nExperience with AWS cloud services: EC2, EMR, Kinesis, RDS\\nPrior Implementation Experience with stream-processing systems: Storm, Kafka, Spark-Streaming etc.\\nProficient using Source Control, build and deploy tools like perforce/Git and Jenkins\\nStrong project management, organizational and interpersonal skills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>San Francisco Bay Area, CA</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Frontend Arts brings together the brightest minds to create breakthrough technology solutions, helping our customers gain competitive advantage. At Frontend Arts, we are continuously evolving how we work and how we look at the business challenges, so we can continue to deliver measurable, sustainable solutions to our clients.\\nWe are looking for self-motivated \"Big Data Engineer\" with excellent communication and customer service skills\\nData Engineer.\\nPython.\\nHadoop Stack\\nData Pipeline Using ETL or other tool\\nHandling High Volume Data\\nAWS ( S3 or lambdas)\\nMinimum Experience: 7 Yrs\\nEducation:\\nBachelor's Degree in Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA 94104</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94104</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nWork with analysts and data scientists to define processes and standards to inform system design, transform their needs into streaming or batch processing\\nDesign infrastructure and systems to scale easily as data ingest grows\\nImplement new data pipeline features with verified high quality from unit test coverage and production monitoring\\nFocus on data quality! Detect data/analytics quality issues and implement bug fixes and data validation for prevention\\nHelp understand our day to day operations for continuous improvement of production systems</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Senior Data Engineer on the Analytics Systems team, you’ll work with a talented team of engineers to improve Credit Karma’s data pipeline that powers our recommendation systems, enterprise tools, and data warehousing. You’ll help to build a general, secure, scalable, fast, and high throughput data pipeline to process many terabytes of data a day.\\n\\nWe are very passionate about performance, correctness, and data quality. The team spends their time day to day developing new pipelines or features that make it seamless for data to be moved throughout our infrastructure.This includes working with cutting edge tools such as Scala, Kafka, Spark, Akka and Google Cloud. If you enjoy working in a collaborative environment where everyone can have their say while still being able to set and hit deadlines, then this is the role for you.\\nResponsibilities\\nWork with analysts and data scientists to define processes and standards to inform system design, transform their needs into streaming or batch processing\\nDesign infrastructure and systems to scale easily as data ingest grows\\nImplement new data pipeline features with verified high quality from unit test coverage and production monitoring\\nFocus on data quality! Detect data/analytics quality issues and implement bug fixes and data validation for prevention\\nHelp understand our day to day operations for continuous improvement of production systems\\nOur Ideal Candidate\\n5+ years experience with Big Data technologies\\nExperience with Scala/Java, Spark, Kafka, or demonstrated ability to pick up new technology quickly\\nFundamental knowledge about databases and strong SQL skills\\nFamiliar with Google Cloud ecosystem such as BigQuery, GCS, DataProc, etc\\nEnjoys working collaboratively; CK’s values include empathy and helpfulness\\nAble to estimate and meet deadlines\\nExcellent verbal and written communication skills\\nNice to have\\nExperience working with cloud technologies\\nExperience scaling data throughput or building low latency streaming pipelines\\nExperience solving for data quality\\nCredit Karma is committed to a diverse and inclusive work environment. We believe that such an environment advances long-term professional growth, creates a robust business, and supports our mission of championing financial progress for everyone. We offer generous benefits and perks with an eye single to fostering an inclusive environment that recognizes the contributions of all. We’ve worked hard to build an intensely collaborative and creative environment, a diverse and inclusive employee culture, and the opportunity for professional growth. As part of the Credit Karma team, your voice will be heard, your contributions will matter, and your unique background and experiences will be celebrated.\\n\\nCredit Karma is also proud to be an Equal Opportunity Employer. We welcome all candidates without regard to race, color, religion, age, sex (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity or expression, marital status, national origin, disability, genetic information, status as a protected veteran, or any other protected characteristic. We prohibit discrimination of any kind and will also consider qualified applicants with arrest and conviction records in a manner consistent with applicable federal, state, and local law.\\n\\nOur people are everything, our core values are real, and our guiding mission is strong. Join us!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BIG DATA ENGINEER</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for amazing people diverse in thought, perspective, and culture to join our team. We check our egos at the door, roll up our sleeves, work hard, move fast, and support each other. If that sounds like fun to you, please apply!\\n\\nDATA ENGINEER, BIG DATA\\n\\nWe are looking for engineers who are excited about building distributed data pipelines. We want you to help us shape our internal and external brand-new data warehouses, leveraging the latest advances of big data processing: a combination of Kafka Streams, Hadoop, and traditional RDBMS.\\n\\nYOU WILL\\nBe a part of our local San Francisco (Financial District) big data team and be a part of an overall Data Organization spanning multiple offices.\\nMaintain and incrementally improve existing solutions.\\nGet to build brand new pipelines with the technology stack including Spark, Spark Structured Streaming, Kafka, Hadoop, MySql, Python.\\nWork with senior engineers who can help you learn and grow with the focus on Big Data technologies in a small dynamic team\\nWHAT WE ARE LOOKING FOR\\nUnderstanding of distributed system fundamentals.\\nExperience in developing data pipelines that are used in production environments.\\nDemonstrated professional experience working with various components of Big Data ecosystem: Spark/Spark Streaming, Hive, Kafka/KSQL, Hadoop (or similar NoSQL ecosystem), et. al, in a production system.\\nStrong software engineering skills with Python.\\nKnowledge of some flavor of SQL (MySQL, Oracle, Hive, Impala), including the fundamentals of data modeling and performance.\\nEVEN BETTER IF YOU HAVE\\nSkills in real-time streaming applications.\\nKnowledge of Scala.\\nA development workflow using Docker containers.\\nCompulsion for automating your day-to-day processes.\\nWHAT WE OFFER\\nAbility to see your direct impact on a high visibility project.\\nAn opportunity to both create new projects and help improve the existing big data pipelines.\\nWork in a mature, private, nationally-known company with a CEO approval of 85% on Glassdoor and a positive atmosphere in our San Francisco tech team.\\n401K plan with employer matching.\\nCommuter pre-tax contributions.\\nFlexible working hours and work-from-home days.\\nHealth plan.\\nIn-office snacks.\\nOrganized team events.\\nCentro is an Equal Opportunity Employer. We respect and support an inclusive workplace diverse in thought, perspective and culture. We celebrate all team members regardless of gender/identity, sexual orientation, race or cultural background, religion, physical disability and age. We are better together.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Science Engineer</td>\n",
       "      <td>San Francisco, CA 94102</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94102</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join our team @ Eskalera! A super{set} venture studio company\\nDATA SCIENCE ENGINEER\\nLOCATION: San Francisco or New York City\\nAt Eskalera, our ultimate vision is an environment where every worker, regardless of background, pedigree, race, geography etc, gets a fair shot, and where AI and data-driven methods measurably improve results for progressive businesses that recognize people and talent as their most critical asset. The key to our and our clients’ success is rooted in constructing inclusive cultures.\\nAbout the Role\\nSo, you can query large amounts of data using your favorite big-data toolkits, analyze it using established statistical and machine learning techniques, and communicate the derived insights through clear and concise charts and reports. You are a technologist at heart always seeking to push the current boundaries to process more data and run more sophisticated machine learning algorithms. In your past, you may have doubled as a data scientist, a data engineer, or perhaps a machine learning engineer, but deep inside, all you really care about is building best in class products, applications, and systems that extract knowledge from data at any scale and deliver value to the business.\\nGreat, we are excited to talk to you! We're looking for people who get things done by using their smarts and whatever tools get the job done. Are you at the beginning of your career and this is where you see yourself in the future? Let us know; we love to work with bright people looking to grow.\\nYour Role:\\nR&amp;D of cutting edge algorithmic solutions to real-world problems producing a shippable product as well as intellectual property (papers and patents).\\nUsing statistical and machine learning principles to discover hidden patterns, perform predictive analysis and build models that drive insights.\\nClean, transform and validate data for uniformity and accuracy.\\nDevise and utilize algorithms and statistical approaches to mine data stores, perform data analysis and improve model performance.\\nCommunicate findings internally and externally, generating reports and dashboards, building narratives that resonate with clients and stakeholders.\\nScale efforts to democratize data internally and externally, be an ambassador for data-driven culture.\\nBecome and stay an expert in current and emerging technologies, techniques, and tools.\\nYour Skills and Qualities:*\\n3+ years of professional data science related work\\nUnderstanding of key machine learning and data mining approaches.\\nA bayesian at heart but can report significance if asked.\\nUnderstand the fundamentals of computer science including programming principles, design patterns, database fundamentals, and distributed systems.\\nMake things work and get things done using the programming language of your choice (Python/Scala among others).\\nAre a great communicator, able to articulate complex concepts in easy to understand language.\\nLove to learn new things and can do so quickly.\\nLike working in, and being part of, interdisciplinary teams.\\nAbout Eskalera\\nEskalera enables large and medium-sized companies to transform their HR operations by improving employee engagement, productivity, and growth. Our end-to-end platform arms HR professionals with the most modern applications of AI, data science, and evidence-based findings on implicit bias and D&amp;I. By capturing, processing, and analyzing data from easy-to-use experiences and integrating other available employee data, companies gain a real-time view of the zeitgeist of their employee base to drive measurable business results.\\nEskalera is proud to be an equal opportunity workplace. Individuals seeking employment at Eskalera are considered without regard to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, or sexual orientation.\\nWe do not accept resumes from headhunters, placement agencies, or other suppliers that have not signed a formal agreement with us.\\nThis is Eskalera: https://eskalera.com/about-us/\\nHow we are different?: https://eskalera.com/solutions/\\nJob Type: Full-time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA 94103</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94103</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBuild &amp; maintain low-latency, high-scalability data pipelines in service of our human-in-the-loop machine learning workflows platform.\\nBuild &amp; maintain adapter services for ingesting data from a wide variety of streaming and batch-based sources.\\nBuild &amp; maintain services for throttling, backpressure, schema management, and normalization.\\nImplement QA and testing strategies. Promote best practices for writing maintainable code.\\nParticipate in selecting tools and setting development standards at Figure Eight.\\nAbility/readiness to develop excellent working relationships with a diverse team of peers across organizations (Engineering, QA, DevOps, Product, Design, et al).\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Us\\n\\nFigure Eight is the essential Human-in-the-Loop Machine Learning platform for data science and machine learning teams. The Figure Eight platform transforms unstructured text, image, audio, and video data into customized high-quality training data to make AI work in the real world. Figure Eight's technology and expertise supports a wide range of use cases including autonomous vehicles, intelligent personal assistants, medical image labeling, consumer product identification, content categorization, customer support ticket classification, social data insight, CRM data enrichment, product categorization, and search relevance.\\n\\nHeadquartered in San Francisco and backed by Canvas Ventures, Trinity Ventures, Industry Ventures, Microsoft Ventures, and Salesforce Ventures, Figure Eight serves Fortune 500 and fast-growing data-driven organizations across a wide variety of industries. For more information, visit www.figure-eight.com.\\n\\nAbout the Role\\n\\nFigure Eight users range from engineers and data scientists to subject matter experts creating training data for machine learning. You will be working on the most important problems in technology today: how can humans and AI collaborate to solve important and sometimes complicated tasks?\\n\\nAs a member of our core team, you will design, build, and improve on tools used by many of the most widely-known tech companies with large-scale machine learning initiatives active today. This may include collecting and managing training data for AI models, evaluating the performance of the machine learning models used by that data, or building infrastructure and managing data pipelines. Specifically, you will work on a generalized annotation API that consists of both automated and human-driven annotation tools for 2D and 3D images, video, text, and audio data. The platform will combine human input (eg: bounding boxes on objects) and Machine Learning input (eg: automatic object tracking in videos) for maximum efficiency and effectiveness. You will be on a cross-functional team collaborating with members of the Product, Machine Learning, Dev Ops, and Backend Engineering teams.\\n\\nYour work will consist of implementing new features and services, maintaining infrastructure, and migrating existing services to a SOA/microservice-based architecture. You’ll mentor less experienced developers and constantly work on improving your own skills and the quality of our code-base. For more about what we build, please visit www.figure-eight.com/overview\\n\\nThe Ideal Candidate:\\nYou enjoy thinking about and working on enterprise-level data management systems. You are looking for a company at the epicenter of a rapidly-developing machine learning industry and are driven by a hunger to learn and develop your skills. You are passionate about working on a project that contributes meaningfully to the further development of technology and to humanity as a whole. You care about best practices and you choose the tools you work with judiciously and deliberately. You have strong analytical skills, an unwavering commitment to quality, an open-minded and collaborative work ethic, and cutting-edge coding skills.\\n\\nResponsibilities / Opportunities:\\n\\nBuild &amp; maintain low-latency, high-scalability data pipelines in service of our human-in-the-loop machine learning workflows platform.\\nBuild &amp; maintain adapter services for ingesting data from a wide variety of streaming and batch-based sources.\\nBuild &amp; maintain services for throttling, backpressure, schema management, and normalization.\\nImplement QA and testing strategies. Promote best practices for writing maintainable code.\\nParticipate in selecting tools and setting development standards at Figure Eight.\\nAbility/readiness to develop excellent working relationships with a diverse team of peers across organizations (Engineering, QA, DevOps, Product, Design, et al).\\n\\nCompetencies:\\n\\n5+ years of software development experience in cloud-based, multi-tiered, enterprise application systems.\\n5+ years managing data platforms/engineering using enterprise service bus or message-based architectures, such as Kafka, Redis, RabbitMQ, or similar.\\n3+ years production environment-level experience with Ruby on Rails application development.\\nHands-on experience with developing microservices and successfully building products using SOA.\\nHands-on experience with event-sourcing and functional programming patterns.\\nHands-on experience with AWS, Git, Docker, Gradle, Jenkins, Jira, and Confluence.\\n\\nNice-to-have Competencies:\\n\\nFamiliarity with batch processing and workflow tools such as Airflow, Luigi, Celery, or others\\nPrior production experience with Python, Java, and/or Scala.\\nFamiliarity with basic machine learning concepts.\\n\\nFigure Eight offers an attractive total compensation package including outstanding benefits and stock options. Learn more about our culture at https://www.figure-eight.com/company/careers/.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Business Intelligence Engineer Manager</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Womply is leveling the playing field for American small businesses with proprietary data and next-generation SaaS that helps SMBs get more customers, keep them coming back, and save time every day. We’re one of the fastest growing software companies in the country, serving over 100,000 small business merchants across 400+ business verticals in every corner of America.\\n\\nThe Data Team at Womply advances the state of our data, and empowers the company to make better decisions from our data. We’re seeking a talented and motivated Data Engineer to lead our team. As the Business Intelligence Engineer Manager, you will hold the keys to the infrastructure that powers our current and future Data Products. Your team of data engineers are helping us build and leverage the latest technologies to tap into our firehose of data to open up new paths for analysis and discovery. You will have the opportunity to make a big impact, and work with extremely talented peers on a fast paced, high energy team.\\n\\nYou will lead your team to deliver large-scale projects, set and drive roadmap execution through resource planning and allocation. Your focus is to help us evolve our data-driven philosophy and become a world-class data organization. Your team owns the design, execution, and ongoing support of critical data warehousing projects enabling accurate reporting and advanced analytics for all of Womply’s internal business units.\\n\\nYou will have to be self-sufficient - we are a startup, so everyone might do a bit of everything to get things done. We look for people who take pride in their work, execute on it, and deliver phenomenal results.\\n\\nIn order to be successful in this role, you will be responsible for:\\n\\nVision - Your and your team will leverage our data foundation to design and implement innovative solutions to our hardest data problems\\nExecution - Building and maintaining the data pipelines from various data sources, while maintaining high accuracy, consistency, and reliability\\nFeedback &amp; Development - You see people’s strengths and weaknesses and are both gifted and courageous in talking with them about it; you motivate and train your team to be the best version of themselves\\nPartnership - You establish strong relationships with business and technical leaders across the organization\\nYou must have:\\n\\nMust have \"Hands-On\" coding experience as a Lead or Manager\\n3-8 years in software engineering\\nExperience with Data Warehousing, Architecting Pipelines, and Data Modeling\\nTeam-oriented, self-motivated, success-driven, roll-up-your-sleeves attitude\\nStrong intellectual curiosity and demonstrated ability to understand and question the data\\nHealthy Skepticism to challenge the status quo so we can improve\\nTechnically proficient in:\\nLanguages - Python / Scala / SQL / Bash\\nTechnologies - Snowflake, AWS, Airflow\\nNice to Have:\\n\\nSpark\\nCome build something amazing at Womply\\n\\nWe’re a fanatically values-based company with $50 million raised to accelerate our growth. We work hard and push each other to be the best, but we also have fun and don’t take ourselves too seriously. If you want to win and make a big impact, let’s talk. We’re hiring in San Francisco and Lehi, Utah for engineering, DevOps, design, data science, sales, marketing, business development, account management, and more.\\n\\nPLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Engineer - Data Modeling Platform</td>\n",
       "      <td>San Francisco, CA 94105</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94105</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At Uber, we ignite opportunity by setting the world in motion. We take on big problems to help drivers, riders, delivery partners, and eaters get moving in more than 600 cities around the world.\\n\\nWe welcome people from all backgrounds who seek the opportunity to help build a future where everyone and everything can move independently. If you have the curiosity, passion, and collaborative spirit, work with us, and let’s move the world forward, together.\\n\\nThe Global Data Warehouse Team (GDW) powers analytics for many of Uber’s businesses. Want to know how many users joined Uber as Riders and subsequently decided to become Drivers on our platform? The Global Data Warehouse team maintains the data objects which answer this question. Need to analyze how the wait times shown in the Rider app correlate with Rider and Driver ratings? We have the data at the ready. We model tables and build data pipelines for the core of our business including Driver, Rider and Trip analytics. We collaborate with teams including Eats, Fraud, Ops, Finance, and Marketing to support domain specific needs. We ingest truly massive volumes of data generated from our globally distributed users and structure this data in an analytics-friendly way while guaranteeing highest fidelity of historical data and low latency - questions at Uber don’t wait for an answer for a very long time.\\n\\nAs a Senior Software Engineer in Data at Uber you will play a leading role in scaling the global data warehouse to power analytics for teams across Uber. You are a self-starter with extensive industrial experience in SQL, Data Modeling, and ETL pipeline design. You have deep experience implementing ETL pipelines in Hive or another MPP database architecture. You are comfortable with Spark and Presto having used one or both frequently to process very large volumes of data. You possess at least a working knowledge on a platform for streaming analytics. You are comfortable coding in Python, Java, or Scala. You have demonstrated strong competency in reliably operating 100s of ETL pipelines with adherence to strict SLAs and quickly root-causing and correcting complex data problems. Peers describe you as the go-to person for the most challenging data ingestion and modeling problems. You actively mentor junior team members and attract others inside and outside your company to join your team. Detail-orientation, thoroughly tested code, and great documentation are the hallmarks of your work but you excel equally well at explaining concepts in “big picture” terms to a less technical audience. If this describes you and you tick off the boxes below, we would love to hear from you.\\nRequired skills:\\n5+ years expertise creating and evolving dimensional data models &amp; schema designs to structure data for business-relevant analytics.\\n5+ years hands-on experience using SQL to build and deploy production-quality ETL pipelines.\\n3+ years experience ingesting and transforming structured and unstructured data from internal and third party sources into dimensional models.\\n3+ years experience writing and deploying Python, Scala, or Java code.\\n3+ years hands-on experience using Hadoop, Hive, Vertica or another MPP database system like AWS Redshift or Teradata.\\nTrack record of successful partnerships with product and engineering teams resulting in on-time delivery of impactful data products.\\nDemonstrated ability to think strategically about business, product, and technical challenges and implement data solutions which scale to meet future needs.\\nExperience developing scripts and tools to enable faster data consumption.\\n\\nPreferred skills:\\n2+ years experience building and operating realtime streaming data pipelines using Spark Streaming, or Flink\\nIn-depth understanding of with Kimball’s data warehouse lifecycle.\\nExtensive experience with real-time data ingestion and stream processing.\\nDemonstrated familiarity with industry-leading Big Data ETL practices.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Mateo, CA</td>\n",
       "      <td>San Mateo</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Brief Description\\nJob Advertisement\\nData Engineer\\n\\nTen-X Commercial is the CRE marketplace that is a force multiplier for sellers, buyers and brokers. Ten-X precision-matches assets, accelerates close rates, and streamlines the entire transaction process with more than $55 billion in sales and increasing daily. Leveraging desktop and mobile technology, Ten-X allows people to safely and easily complete real estate transactions entirely online. We bring quality assets to the market and attract prospective investors from around the world. By virtue of our best-in-class marketing and scalable technology platform, buyers and seller are able to conduct transactions in an efficient manner.\\nTen-X empowers consumers, investors and real estate professionals with unprecedented levels of flexibility, control and simplicity – and the convenience of transacting properties whenever and wherever they want. As real estate continues to move online, Ten-X is uniquely positioned at the forefront of this dramatic industry evolution.\\n\\nhttps://www.ten-x.com/\\n\\nThe Role:\\nData and our ability to leverage it is seen and championed as a key competitive advantage from our CEO on down. We are looking for a top tier data engineer to work with our data science team on building out proprietary tools and models around our customer and asset data (both internal and external sets). You will be working on key projects that have board level visibility.\\n\\nResponsibilities\\n\\nPlay a leading role in designing, developing and implementing Big Data databases (Hadoop, Graph, MySQL, NoSQL, MongoDB) that contains multiple data sets from both internal and external sources\\nLead the setup of data pipelines of new internal and external data sets into the database\\nWork with Data Scientists to help dedupe and fuzzy match data\\nWork with software engineers on developing APIs\\n\\nExperience\\n\\nUndergraduate degree (ideally a Masters) in a relevant quantitative subject (Math, Statistics, Computer Science, Engineering, Economics, etc.)\\n5+ Years’ Experience in data engineering, including: 2+ years in a modern data stack environment, specifically the Hadoop stack, 3+ Years' Python experience relating to data engineering\\nExperience with iterative Agile methodologies and use of supporting tools like JIRA, Confluence and Git\\nExperience in the following will be a plus:\\nSpark\\nKafka\\nClickstream data\\nMachine Learning\\nStreaming Data\\nElastic Search\\nContainers (Docker)\\nFuzzy Matching / NLP\\nAbility to understand business problems and translate them into data science requirements\\nUnderstanding and Familiarity with:\\nHadoop and all the related stack (Pig, Hive, HBase, etc.)\\nSQL skills and SQL Databases\\nStrong oral and written communication skills and be able to communicate complex technical knowledge in meaning terms\\nAbility to work in a fast-paced environment and fluidly adapt to changing priorities\\nMust be passionate about getting to the root cause of issues and driving to whys\\nProven ability to obtain buy-in/ partner with the data science team, including demonstrated ability to partner with functional leaders toward common goals\\nWell-developed analytical and interpersonal skills with ability to draw conclusions and communicate/present them confidently and effectively to broad audiences, including senior leadership\\nHigh energy and passion about solving business needs through data\\nOrganized, structured thinker with ability to handle multiple assignments, remain calm under pressure, and digest information from multiple, disparate parts\\nContinuous improvement mindset\\nNot afraid to challenge conventional thinking or analyses\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA 94107</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94107</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.\\n\\nAs a Senior Data Engineer, here's what we'll be looking for you to bring:\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Kafka, Hadoop, ecosystem technologies, and others\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nDeep understanding of relational database technologies and database development techniques\\nUnderstanding of how to architect solutions for data science and analytics\\nData management for reporting and BI experience is a plus\\nUnderstanding of “Agility”, including core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different levels\\nAny experience building and leading an offshore/outsourcing function would be highly beneficial.\\nThere's no typical day or engagement for our Senior Engineers. Here’s what you’ll do:\\n\\nBe the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\nRegardless of what you do at ThoughtWorks, you’ll always have the opportunity to:\\n\\nThink through hard problems, and work with a team to make them reality.\\nLearn something new every day.\\nWork in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title\\nTravel the world.\\nSpeak at conferences.\\nWrite blogs and books.\\nDevelop your career outside of the confinements of a traditional career path by focusing on what you’re passionate about rather than a predetermined one-size-fits-all plan\\nBe part of a company with Social and Economic Justice at the heart of its mission.\\nA few important things to know:\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>San Francisco, CA 94107</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94107</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation &amp; engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment—one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.\\nThis role requires a wide variety of strengths and capabilities, including:\\n\\nBS/BA degree or equivalent experience\\nAdvanced knowledge of application, data and infrastructure architecture disciplines\\nUnderstanding of architecture and design across all systems\\nWorking proficiency in developmental toolsets\\nKnowledge of industry wide technology trends and best practices\\nAbility to work in large, collaborative teams to achieve organizational goals, and passionate about building an innovative culture\\nUnderstanding of software skills such as business analysis, development, maintenance and software improvement\\nTechnical proficiency in Java, Spring, and Spring Boot\\nHands on experience with web technologies (e.g. HTTP, XML, REST, HTML, etc.)\\nExperience with NoSQL databases (Cassandra, MongoDB, etc.).\\nExperience with distributed streaming platform (Kafka), Data protection, replication, reconciliation, and distribution\\nExperience with Spark/Spark Streaming (Big Data)\\nExperience with web-based version control tools (GIT, Bitbucket)\\nExperience in DevOps - build, deployment, integration, code management and similar tools like Jenkins, Maven, automated deployment etc.\\nUnderstanding of design patterns and their application\\nGood to have Hands on experience developing and deploying applications to cloud platforms namely AWS &amp; Cloud Foundry\\nAbility to work collaboratively in teams and develop meaningful relationships to achieve common goals\\nExtensive experience with horizontally scalable and highly available system design and implementation, with focus on performance and resiliency and extensive experience profiling, debugging, and performance tuning complex distributed systems\\nExcellent logical reasoning and analytical skills\\nDemonstrated professional writing/communication skills\\nStrong organization, interpersonal and management skills\\nPassionate about the digital landscape with desire for continuous learning and development\\nTeam player that is able to work with diverse groups across an organization\\nOur Consumer &amp; Community Banking Group depends on innovators like you to serve nearly 66 million consumers and over 4 million small businesses, municipalities and non-profits. You’ll support the delivery of award winning tools and services that cover everything from personal and small business banking as well as lending, mortgages, credit cards, payments, auto finance and investment advice. This group is also focused on developing and delivering cutting edged mobile applications, digital experiences and next generation banking technology solutions to better serve our clients and customers.\\n\\nWhen you work at JPMorgan Chase &amp; Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech companies. In 15 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $11B annual investment in technology enables us to hire people to create innovative solutions that are transforming the financial services industry.\\n\\nAt JPMorgan Chase &amp; Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Senior Cloud Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDesign and develop framework to automate data ingestion and integration of structured data from a wide variety of enterprise data sources, at scale.\\nDesign and develop data pipeline components and integrate them with the Splunk and other ETL Platforms.\\nDesign data quality monitoring and automated data cleaning.\\nAssist the business liaison and ETL function with data related issues such as assessing data quality, data consolidation, evaluating existing data sources, etc.\\nExperience with handling large data infrastructure platform and driving stability through automated monitoring, alerting, and actions.\\nExperience developing for, configuring, and supporting Cloud computing solutions</td>\n",
       "      <td>\\nBachelor degree in Computer Science or related field</td>\n",
       "      <td>\\nExperience with building scalable and reliable data pipelines using Data engine technologies like APIs, AWS Redshift, Snowflake, Talend.\\n8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.\\n4+ years experience designing and developing complex ETL/ELT programs with Python, Visual ETL Tools etc\\n3+ years experience developing complex SQL\\nExperience using Cloud Storage and computing technologies such as RedShift, Snowflake\\n3+ years experience programming in Python\\n2+ years experience with Bitbucket\\n2+ years experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues\\n2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)\\nExperience with API based integration from multiple SaaS data sources\\nExperience developing and implementing streaming data ingestion solutions\\nExperience in Agile methodology (2+ years)</td>\n",
       "      <td>Job Description: That’s a cool job - I want it!\\nReady to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year.\\nAs a Cloud Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data lake solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions.\\nResponsibilities: I want to and can do that!\\nDesign and develop framework to automate data ingestion and integration of structured data from a wide variety of enterprise data sources, at scale.\\nDesign and develop data pipeline components and integrate them with the Splunk and other ETL Platforms.\\nDesign data quality monitoring and automated data cleaning.\\nAssist the business liaison and ETL function with data related issues such as assessing data quality, data consolidation, evaluating existing data sources, etc.\\nExperience with handling large data infrastructure platform and driving stability through automated monitoring, alerting, and actions.\\nExperience developing for, configuring, and supporting Cloud computing solutions\\nRequirements: I’ve already done that or have that!\\nExperience with building scalable and reliable data pipelines using Data engine technologies like APIs, AWS Redshift, Snowflake, Talend.\\n8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.\\n4+ years experience designing and developing complex ETL/ELT programs with Python, Visual ETL Tools etc\\n3+ years experience developing complex SQL\\nExperience using Cloud Storage and computing technologies such as RedShift, Snowflake\\n3+ years experience programming in Python\\n2+ years experience with Bitbucket\\n2+ years experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues\\n2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)\\nExperience with API based integration from multiple SaaS data sources\\nExperience developing and implementing streaming data ingestion solutions\\nExperience in Agile methodology (2+ years)\\nEducation: Got it!\\nBachelor degree in Computer Science or related field\\nWe value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which you are applying.\\nFor job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Womply helps small businesses thrive in a digital world. Our software makes it easy for small businesses to boost their online reputations, engage their customers, and monitor the health of their businesses with data and technology they can't get anywhere else. We're one of the fastest growing software companies in the country, serving more than 100,000 small businesses across 400+ business verticals in every corner of America.\\n\\nIn your new role at Womply you will be part of a team of engineers working to manage our ever­-growing collection of payment and other merchant data from across the USA. You'll work primarily with Java and Apache Spark and a variety of data stores including Cassandra, PostgreSQL, and Aurora. You'll work with the rest of the engineering and product teams to design and optimize the schemas needed to support the products. Going forward, you'll support our evaluations of new tools and technologies to scale and analyze our data.\\n\\nYou Must Have:\\n\\n5+ years of experience in software engineering with experience as a senior contributor or team lead.\\nMUST HAVE - Good programming skills in Java or Scala.\\nMUST HAVE - Experience delivering Spark-based data consumption to consumer facing products / systems.\\nMUST HAVE - Data Platform and Pipeline experience\\nExperience with Cassandra, Mongo, or similar data stores.\\nStrong background in SQL, Data Modeling, and Performance Tuning in both relational and noSQL databases.\\nExperience with distributed and federated systems and data processing pipelines\\nFamiliarity with monitoring, backup, and disaster recovery of data systems\\nExperience building POCs, architecting new systems and improving existing systems to solve business problems and support scaling\\n\\nNice to have:\\n\\nAWS experience\\nDatabase Administration Experience.\\nExperience with Python / Pyspark\\nExperience generating and evaluating data quality metrics\\nExperience mentoring engineers in best practices and methods.\\nExperience with PCI data practices\\n\\nCome build something amazing at Womply\\n\\nWe're a fanatically values-based company with $50 million raised to accelerate our growth. We work hard and push each other to be the best, but we also have fun and don't take ourselves too seriously. If you want to win and make a big impact, let's talk. We're hiring in San Francisco and Lehi, Utah for engineering, DevOps, design, data science, sales, marketing, business development, account management, and more.\\n\\nPLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift.\\n\\nMore:\\nWork at Womply ( https://womply.com/jobs/ )\\n\\nLife at Womply ( https://womply.com/life-at-womply/ )\\n\\nHow we work ( https://womply.com/how-we-work )\\n\\nOur values ( https://womply.com/values/ )\\n\\nBenefits ( https://womply.com/benefits )\\n\\nDiversity ( http://womply.com/diversity/ )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Senior Data Engineer, Batch Recommendation Systems</td>\n",
       "      <td>San Francisco, CA 94104</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94104</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Credit Karma is a mission-driven company, focused on championing financial progress for our more than 100 million members in the U.S., Canada and U.K. While we're best known for pioneering free credit scores, our members turn to us for everything related to their financial goals, including helping them with improving their credit, identity monitoring, applying for credit cards, and shopping for loans (car, home and personal) - all for free. Credit Karma has grown significantly through the years: we've added more than 70 million members in the last five years alone and now have more than 1,000 employees across our offices in San Francisco, Charlotte, Los Angeles, Leeds, London and soon Oakland.\\n\\nOur mission is to show the most appropriate notification at the most appropriate time for all of our 100MM+ members. This includes powering the financial products marketplace and data driven personalized content. We do this by using several different recommendation strategies, historical member behavior and understanding customer financial goals. Apart from the machine learning challenges on rich data sets, we also deal with core engineering problems of delivering high throughput data pipeline to process all Credit Karma members everyday, to provide a personalized experience.\\nWhat will you do?\\nYou will work closely with marketing and data science team to provide solutions for various product requirements and excellent personalization experience for our Credit Karma members\\nYou will also engage heavily in design and discovery to ensure excellent stakeholder experience.\\nDesign infrastructure and systems to scale easily as data ingest grows\\nTake a metrics first approach when creating new systems\\nImplement new data pipeline features with verified high quality from unit test coverage and production monitoring\\nHelp understand our day to day operations for continuous improvement of production systems\\nDo you have the skills needed for success?\\n5+ years experience with Big Data technologies\\nExperience with Scala/Java, Spark, Apache Beam, Kafka, or demonstrated ability to pick up new technology quickly\\nFundamental knowledge about databases and strong SQL skills\\nFamiliar with Google Cloud ecosystem such as BigQuery, GCS, DataFlow, etc\\nEnjoys working collaboratively; CK’s values include empathy and helpfulness\\nAble to estimate and meet deadlines\\nExcellent verbal and written communication skills\\n\\nIf you are ready to make an impact on a product that is used by millions of people around the world, including your own friends and family, join us.\\nEqual Employment Opportunity\\n\\nCredit Karma is committed to a diverse and inclusive work environment. We believe that such an environment advances long-term professional growth, creates a robust business, and supports our mission of championing financial progress for everyone. We offer generous benefits and perks with a single eye to nourishing an inclusive environment that recognizes the contributions of all and fosters diversity by supporting our internal Employee Resource Groups. We’ve worked hard to build an intensely collaborative and creative environment, a diverse and inclusive employee culture, and the opportunity for professional growth. As part of the Credit Karma team, your voice will be heard, your contributions will matter, and your unique background and experiences will be celebrated.\\n\\nCredit Karma is also proud to be an Equal Opportunity Employer. We welcome all candidates without regard to race, color, religion, age, marital status, sex (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity or gender expression, national origin, veteran or military status, disability (physical or mental), genetic information, or any other protected characteristic. We prohibit discrimination of any kind and operate in compliance with the San Francisco Fair Chance Ordinance.\\n\\nLearn more about Credit Karma at creditkarma.com/careers\\n#LI-AG1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>goPuff is a high-growth startup that is revolutionizing hyper-local e-commerce. We warehouse and deliver thousands of products to your door super fast. We're currently operating in over 70 markets within the US and expanding quickly.\\n\\nLove data? Love to build stuff? As a Data Engineer at goPuff, you’ll provide hands-on expertise in building the foundation for data-driven decision-making across the company. This role sits within our full-stack data &amp; analytics team that is designed to move fast. You’ll have the opportunity to make a huge impact as the future success of goPuff will largely depend on the business and customer insights we derive from our data.\\n\\nResponsibilities\\n\\n\\nWork on data processing and data warehouse projects for the entire organization ( Marketing, Operations, Merchandising, Supply Chain, Finance, and Business Strategy)\\nDesign/code event, data and ML pipeline features, build data tables and data tools that enable robust business/customer analytics, KPI monitoring, operational insights, predictive analytics and personalization\\nCollaborate with data analysts and other functions to gain a deep understanding of how data impacts decision-making\\nBe the expert in how data flows throughout the business\\nIdentify, design, and implement internal data process improvements including automating manual processes, optimizing data delivery and re-designing infrastructure for greater scalability\\nOwn data availability and integrity\\n\\nRequirements\\n\\n\\nBachelors in Computer Science, Engineering or other quantitative field\\n3+ years of relevant work experience in data engineering\\nExperience with object-oriented/object function scripting languages: Python (pandas/numpy), node JS, Java/Scala, etc.\\nAdvanced working SQL knowledge and experience working with a variety of databases (e.g. Postgres, Redshift/Vertica, Hive, Presto)\\nExtensive experience with ETL pipelines and stream processing architectures, particularly implementing them in a fast-paced startup environment\\nExperience with data pipeline and workflow management tools: Airflow, Luigi, etc.\\nExperience with big data and stream-processing technologies: Spark, Kafka, SQS, Kinesis, Spark-Streaming, Flink etc.\\nExperience with AWS and/or Azure cloud services: EC2, Lambda, SageMaker, EMR, RDS, Redshift, Azure Function apps, HDInsight is a plus\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement\\nExperience with BI platforms like Looker and Tableau\\nExcellent communication and presentation skills\\nTop notch organizational skills and ability to manage multiple projects in a fast-paced environment\\nMove fast, be a team player, always be learning and give back\\n\\nFor the people who have better things to do than go out of their way to stop at the store (again), goPuff is the largest digital convenience retailer delivering thousands of products ranging from snacks, drinks, and ice cream to alcohol, home essentials, and personal care items directly from centrally located warehouses to our customers’ doors.\\n\\nWe’re currently in 80+ markets and growing fast, so we're looking for the most motivated and passionate talent to be a part of our team, grow with us, and join in our mission of delivering the moments that matter most. Note: must love snacks to work at goPuff.\\n\\nThe goPuff Fam is committed to an inclusive workplace that does not discriminate against race, nationality, religion, age, marital status, physical or mental disability, sexual orientation, gender, or gender identity. We believe in diversity and encourage any qualified individual to apply. We are an EEOC Employer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Python Engineer</td>\n",
       "      <td>San Francisco, CA 94105</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94105</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are:\\n\\nApplied Intelligence, the people who love using data to tell a story. We’re also the world’s largest team of data scientists, data engineers, and experts in machine learning and AI. A great day for us? Solving big problems using the latest tech, serious brain power, and deep knowledge of just about every industry. We believe a mix of data, analytics, automation, and responsible AI can do almost anything—spark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services. Want to join our crew of sharp analytical minds? Visit us here to find out more about Applied Intelligence.\\n\\nYou are:\\n\\nA data expert with serious analytical and statistical chops. You know how to take massive amounts of data and find the insights our clients need to help their companies do more.\\n\\nThe work:\\n\\nSolve complex analytic challenges using analytic algorithms and AI\\nDesign, build and deploy predictive and prescriptive models using statistical modeling, machine learning, and optimization\\nFind ways to turn goals into products that use specific analytic tools to help clients\\nUse structured decision-making to complete projects.\\nWork out the best way to complete Enterprise Data &amp; Analytics projects\\n\\nHere’s what you need:\\nStrong quantitative and analytical skills with minimum of 2 years of experience in data science tools, including Python, R, Scala, Julia, or SAS\\nAt least 2 years of implementing and delivering projects using CI/CD rigor and tools such as git, Jenkins, docker, Kubernetes, Kubeflow Pipelines, etc.\\nAt least 2 years of experience in data science and use of statistical methodologies\\nAt least 2 years of experience in machine learning methods, including familiarity with techniques in clustering, regression, optimization, recommendation, neural networks, and other\\nBonus points if:\\n\\nAt least 1 year of designing and implementing data engineering, ingestion and curation functions on GCP cloud using GCP native or custom programming\\nGoogle Cloud Platform data engineer certification a big plus\\nAt least 1 year of experience in architecting large-scale data solutions, performing architectural assessments, crafting architectural options and analysis, finalizing preferred solution alternative working with IT and business stakeholders\\nBachelor's degree in data science or related disciplines such as mathematics, statistics, computer science, physics, or related fields. Master's degree preferred\\n\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\n\\nAccenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Business Intelligence - Data Analyst</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBA/BS in a quantitative field\\n1-2 years of work experience as a data analyst, data engineer, or in a highly analytical role\\nProficiency in writing SQL queries and using a BI tool\\nExperience using the command line and git\\nStrong grasp of statistics and experience conducting rigorous data analyses\\nExperience with a scripting language (preferably Python) for data processing and analysis a plus\\nExperience developing models and visualizations in Looker a plus\\nExperience at an e-commerce or fintech company a plus\\nProficiency in Excel and and a strong familiarity with advanced functions\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Business Intelligence-Data Analyst\\n\\nWho is Credible?\\n\\nWe believe life's changes create financial needs for people and that the traditional financial system often puts up unnecessary obstacles. People celebrate major milestones like going to college, getting married, and buying a home. And most of the time, these milestones come with financial implications.\\n\\nAt Credible, we have built a company with the mission of bringing transparency, choice, simple processes and savings to accessing credit for life's important moments. What you see is what you get. We are committed to being upfront, honest, and clear about your options. There are no mysteries, no hidden fees, and no secret clauses.\\n\\nCredible is a fast-growing Australian Securities Exchange (ASX) listed Fintech company that has world class management, has raised multiple rounds of funding, is generating significant revenue and is disrupting the lending market and helping people save money and get out of debt faster.\\n\\nAbout the role\\n\\nOur Business Intelligence team is looking for a Business Intelligence Analyst who is passionate about data, analytics, and business strategy. You will help the team learn more about our business, teach others in the company about analytics, and improve the use of our data. You'll be an integral part of providing data-driven insights that inform significant company decisions.\\n\\nYou Will:\\n\\nPartner with teams across the organization to understand their analytics needs and create dashboards and reporting that allow them to execute more effectively\\nWork with business leaders to define key metrics and build reporting to monitor and understand performance along those metrics\\nConduct in-depth data analyses that lead to actionable insights, owning the entire process from ideation to execution to presentation of findings to stakeholders\\nDevelop data models in our data warehouse that enable performant, intuitive analysis\\nBuild data pipelines and python-based ETL tools for getting, processing, and delivering data\\nBecome an expert on all aspects of Credible's data and analytics infrastructure\\nBe the driving force behind the adoption and effective use of our BI tool within every team at Credible\\n\\nEducation and Experience:\\n\\nBA/BS in a quantitative field\\n1-2 years of work experience as a data analyst, data engineer, or in a highly analytical role\\nProficiency in writing SQL queries and using a BI tool\\nExperience using the command line and git\\nStrong grasp of statistics and experience conducting rigorous data analyses\\nExperience with a scripting language (preferably Python) for data processing and analysis a plus\\nExperience developing models and visualizations in Looker a plus\\nExperience at an e-commerce or fintech company a plus\\nProficiency in Excel and and a strong familiarity with advanced functions\\n\\nEducation and Experience:\\n\\nThe capacity to juggle multiple priorities effectively within a fast-paced environment is critical\\nYou're a highly motivated self-starter with the ability to work efficiently with minimal supervision.\\nAnticipate business needs and think with a business owner mindset – think critically about analyses, don't just complete them\\nPassion for spreading the value of data throughout the company and communicating insights to a broad audience with varying levels of technical expertise\\n\\nWhy work at Credible:\\nWe are a fast moving, fun-loving, seriously smart group of people who really care about impacting the lives of our customers. We empower our employees to make decisions, take risks, drive our business and make changes when we don't get it right. These are our values:\\n\\n\\nExceed Customer Expectations: We provide an exceptional experience to each and every customer that compels them to share it with others.\\nTake Ownership: We are trusted to make decisions that are in the best interests of our customers and our business. We think and act like owners. We care – and that makes all the difference.\\nBe Curious: We are curious, ask questions, seek to understand and try new things.\\nDo the Right Thing: We earn trust by being transparent, respectful and honest with each person with whom we interact.\\nGet Results: Results fuel our excitement and we know how our personal accomplishments tie to the success of the company\\nBe Bold: We are courageous and take risks that scare us. Our enthusiasm for experimenting is how we will find the next breakthrough.\\n\\nOur benefits: We offer competitive compensation, generous benefits, free food and a flexible vacation policy.\\n\\nBut mainly, you want to work at Credible because you believe in our mission and want to have a major role in delivering on it! We look forward to getting to know you.\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Big Data Developer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Big Data Developer. Scroll down to learn more about the position’s responsibilities and requirements.\\n\\nWe are looking for a Big Data Engineer that will work on collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company.\\n#WestReferralCampaign\\nWhat You’ll Do\\nSelect and integrate any big data tools and frameworks required to provide requested capabilities\\nImplement ETL process\\nMonitor performance and advise on any necessary infrastructure changes\\nDefine data retention policies\\nWhat You Have\\nA degree in an associated field and/or other advanced certification along with significant experience\\nProficient understanding of distributed computing principles\\nManagement of Hadoop cluster, with all included services\\nAbility to solve any ongoing issues with operating the cluster\\nProficiency with Hadoop v2, MapReduce, HDFS\\nExperience with building stream-processing systems, using solutions such as Storm or Spark-Streaming\\nGood knowledge of big data querying tools, such as Pig, Hive, and Impala\\nExperience with Spark\\nExperience with integration of data from multiple data sources\\nExperience with NoSQL databases, such as HBase, Cassandra, MongoDB\\nKnowledge of various ETL techniques and frameworks, such as Flume\\nExperience with various messaging systems, such as Kafka or RabbitMQ\\nExperience with big data ML toolkits, such as Mahout, Spark ML, or H2O (if you are going to integrate machine learning in your big data infrastructure)\\nGood understanding of Lambda Architecture, along with its advantages and drawbacks\\nExperience with Cloudera/MapR/Hortonworks\\nWe offer\\nMedical, Dental and Vision Insurance (Subsidized)\\nHealth Savings Account\\nFlexible Spending Accounts (Healthcare, Dependent Care, Commuter)\\nShort-Term and Long-Term Disability (Company Provided)\\nLife and AD&amp;D Insurance (Company Provided)\\nEmployee Assistance Program\\nUnlimited access to LinkedIn learning solutions\\nMatched 401(k) Retirement Savings Plan\\nPaid Time Off\\nLegal Plan and Identity Theft Protection\\nAccident Insurance\\nEmployee Discounts\\nPet Insurance\\nEPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Womply helps small businesses thrive in a digital world. Our software makes it easy for small businesses to boost their online reputations, engage their customers, and monitor the health of their businesses with data and technology they can’t get anywhere else. We’re one of the fastest growing software companies in the country, serving more than 100,000 small businesses across 400+ business verticals in every corner of America.\\n\\nIn your new role at Womply you will be part of a team of engineers working to manage our ever-growing collection of payment and other merchant data from across the USA. You’ll work primarily with Java and Apache Spark and a variety of data stores including Cassandra, PostgreSQL, and Aurora. You’ll work with the rest of the engineering and product teams to design and optimize the schemas needed to support the products. Going forward, you'll support our evaluations of new tools and technologies to scale and analyze our data.\\n\\nYou Must Have:\\n\\n5+ years of experience in software engineering with experience as a senior contributor or team lead.\\nMUST HAVE - Good programming skills in Java or Scala.\\nMUST HAVE - Experience delivering Spark-based data consumption to consumer facing products / systems.\\nMUST HAVE - Data Platform and Pipeline experience\\nExperience with Cassandra, Mongo, or similar data stores.\\nStrong background in SQL, Data Modeling, and Performance Tuning in both relational and noSQL databases.\\nExperience with distributed and federated systems and data processing pipelines\\nFamiliarity with monitoring, backup, and disaster recovery of data systems\\nExperience building POCs, architecting new systems and improving existing systems to solve business problems and support scaling\\nNice to have:\\n\\nAWS experience\\nDatabase Administration Experience.\\nExperience with Python / Pyspark\\nExperience generating and evaluating data quality metrics\\nExperience mentoring engineers in best practices and methods.\\nExperience with PCI data practices\\nCome build something amazing at Womply\\n\\nWe’re a fanatically values-based company with $50 million raised to accelerate our growth. We work hard and push each other to be the best, but we also have fun and don’t take ourselves too seriously. If you want to win and make a big impact, let’s talk. We’re hiring in San Francisco and Lehi, Utah for engineering, DevOps, design, data science, sales, marketing, business development, account management, and more.\\n\\nPLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift.\\n\\n\\nMore:\\n\\nWork at Womply\\n\\nLife at Womply\\n\\nHow we work\\n\\nOur values\\n\\nBenefits\\n\\nDiversity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Senior Data Engineer - Walmart Media Group</td>\n",
       "      <td>San Bruno, CA</td>\n",
       "      <td>San Bruno</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Walmart Media Group\\n\\nAt Walmart, we enable the connection between supplier brands and retail shoppers at unprecedented scale. As primary stewards of our brand promise, “Save Money. Live Better,” we work alongside some of the most talented people in the world to engage with the more than 150M households who shop with us. This is a unique opportunity to join a small, high-visibility team within the largest company in the world. We believe all digital advertising can be targeted and accountable – and we have Walmart’s sales data to prove it. Walmart Media Group wins when suppliers invest in digital media to drive growth; Walmart and our supplier partners win when your digital expertise helps sell more goods online and offline. Growth in our digital advertising business is key to Walmart’s overall growth strategy.\\n\\nWalmart Media Group (the digital ad sales arm for Walmart.com, Jet, Hayneedle, Online Grocery, Vudu, etc.) is dedicated to driving measurable outcomes for our suppliers, merchants, stores, GMs, brand advertisers, and agencies. Our full funnel ad solutions leverage Walmart’s in-store and online data, extensive reach, and to provide measurable results for our clientele. With a range of flexible pricing and buying models, including self-service; these solutions help businesses build brand awareness, engage with Walmart consumers, and convert Walmart consumers to shoppers.\\n\\nRESPONSIBILITIES\\n\\nJoin a local team of analytical, technical, and operational individual contributors that support our internal tools, systems, and infrastructure.\\nDevelop and maintain a data environment leveraging industry standard technologies at Walmart scale.\\nManage data-set structure and schemas for hundreds of tables and terabytes of data.\\nWork together with infrastructure engineers to build an extensible data streaming platform.\\n\\n\\nMINIMUM QUALIFICATIONS\\n\\nA. or B.S. degree in Computer Science or other technical field, or equivalent experience\\n5+ years of experience in a software development / data engineering role\\nStrong familiarity with cloud and cloud design patterns.\\nExperience working with data at scale, counting in TBs of data.\\nExperience working and coalescing data from a variety of data sources in a single environment.\\nStrong SQL skills and expert knowledge of at least one scripting language (Python, Ruby, etc.). NoSQL experience also considered.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5+ years working as a data engineer\\nExperience working with data ETL pipelines\\nMastery of at least one programming language\\nMastery of SQL, performance tuning, and general database skills\\nExperience leading large data projects through design, implementation, and long term maintenance\\n</td>\n",
       "      <td>Common Networks was founded on the idea that everyone should have a choice for fast, affordable access to broadband internet. Right now, most homes in the U.S. don't. In fact, 62% of homes live in a monopoly broadband market. High-speed access unlocks all the superpowers on the internet. When it works, it can be a great leveling force across the world, giving everyone access to educational tools, entertainment, immediate translations, or even medical care that they wouldn’t otherwise have.\\n\\nCommon Networks provides suburban neighborhoods with internet using wireless technology. We interconnect homes in a neighborhood, creating a mesh network between homes and our fiber internet sources. A whole community can then have fast and reliable internet service with only a few locations needing fiber access.\\n\\nRole\\n\\nAs a Senior Data Engineer, you’ll be responsible for designing and building our data platform. You’ll work closely with Product, Business, Data Science, and the rest of the engineering team to build new data-enabled internet features; ensure robust and timely collection of critical metrics; flow data into performant, user-friendly dashboards; and reinforce a culture that prizes decision-making justified by excellent data.\\nYou will:\\nDesign and implement high performance batch and real-time data pipelines\\nDesign and build visualization and analytics tools to empower internal and external customers with actionable insights\\nInstrument metrics across Common’s wireless mesh network and suite of products\\nTranslate product and business requirements into data models that are maintainable and extensible\\nChampion data quality, retention, security and privacy within the company. Document and promote best practices for working with Common data\\nWho you are\\nYou love to code, and you’re excited to work at a place where you spend 90% of your day heads down coding.\\nYou’re not afraid to get your hands dirty. You’re happy to instrument new metrics in an app or debug broken graphs in a dashboard - whatever it takes to make sure we have the data we need to make the best decisions possible.\\nYou’re relentlessly curious. When something breaks, you’re not satisfied with surface-level explanations and proximate causes, you need to know what the underlying issue was and you’re not afraid to dig in and find out for yourself.\\nYou enjoy working closely with others on a cross-functional team, whether it’s teaching engineers ETL best practices or working with business operators to define critical metrics. You thrive in a highly collaborative environment and embrace diversity of thought and experience when thinking through your designs.\\nYou’ve mastered your craft over years of professional data engineering. You know how to tune, tweak and optimize every kind of query, you know how to fully explore a solution space, and you know the value of good monitoring, alerting, and automated tests.\\nRequirements\\n5+ years working as a data engineer\\nExperience working with data ETL pipelines\\nMastery of at least one programming language\\nMastery of SQL, performance tuning, and general database skills\\nExperience leading large data projects through design, implementation, and long term maintenance\\nNice to have\\nExperience with Google’s suite of data services (BigQuery, Datalab, Dataproc, Cloud Pub/Sub, etc)\\nExperience with golang\\nExperience with linux / the linux networking stack\\nExperience with visualization platforms (e.g. Tableau) and libraries (e.g. D3.js)\\nEqual Employment Opportunity\\n\\nCommon Networks is committed to being an equal opportunity employer – we evaluate all employees and job applicants equally, based on merit, competence, and qualifications. We do not discriminate on the basis of race, religion, color, national origin, gender identity, gender expression, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Senior / Lead / Principal Data Engineer, Machine Learning / Deep Learning (Einstein, Mulesoft)</td>\n",
       "      <td>San Francisco, CA 94105</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94105</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We run on AWS. We dockerize applications. You should have some notion of how to build, test, and deploy code to run on cloud infrastructure.\\nExperience with open source tools for information retrieval (e.g. Solr)\\nSearch, Data Scoring/ Ranking expertise\\nData visualization\\nExperience with Deep Learning for NLP\\nExperience developing in open-source machine-learning libraries such as Apache Mahout or MLLib\\nStrong understanding of security, including threat propagation and malware analysis\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Category\\nProducts and Technology\\nJob Details\\nData Engineer / Machine Learning / Deep Learning\\nTeams include: Sales, Service, Marketing, Security, Analytics, Einstein, IoT, Commerce, Mulesoft\\nLocation: US &amp; Canada (Relocation candidates)\\nThe role:\\n\\nSalesforce is looking for both Senior, Lead and Principal Data / Deep Learning / Machine Learning Engineers with Java, Python, Scala and/or Spark experience, to help us take on one of the world’s most extensive data sets and transform it into amazing products that feel like magic. You will work on cutting-edge AI applications and products. Brainstorming data product ideas with data scientists and engineers to build data products used by hundreds of millions people every day.\\n\\n\\nA typical day for you might include the following:\\nDeveloping data infrastructure that ingest and transforms data from different sources and customers at scale.\\nCreating machine/deep learning infrastructure that generalizes across hundreds of thousands of Salesforce customers, but is expressive enough to generate high lift.\\nPartner end-to-end with Product Managers and Data Scientists to understand customer requirements and design prototypes and bring ideas to production\\nWorking with internal product teams to ingest their data and sprinkle machine/deep learning fairy dust on their products.\\nParticipating in meal conversations with your team members about really important topics, such as: Should the cuteness of panda bears be a factor in their survivability? Is love a decision tree or a regression model? How far ahead would society be today if we had 12 fingers instead of 10?\\n\\nWhat we care about:\\nWe develop real products. You need to be an expert in coding, including Java and Object-Oriented Programming. We also use Scala and Functional Programming principles.\\nWe prioritize professional industry experience; advanced degrees alone do not replace real world experience.\\nWe have massive scale. You need to have experience in distributed, scalable systems. Consistency / availability tradeoffs are made here. You’ve tinkered with modern data storage, messaging, and processing tools (Kafka, Spark, Hadoop, Cassandra, etc.) and demonstrated experience designing and coding in big-data components such as HBase, DynamoDB, or similar.\\nWe’re a growing, diverse team and we work together on projects. We love to collaborate and help each other, and we want someone to share that ideology.\\nYou have to be a very quick learner - we face new challenges every day, anything that ranges between the operating model of a financial services companies, conversation model for chatbots, tinkering with convolutional and recurrent networks, to how to make Spark work with the S3 file system. No school could prepare you for all of these, so you need to be very quick on your feet.\\nSelf-starter who can see the big picture, and prioritize their work to make the largest impact on the business’ and customer’s vision and requirements\\nExcellent communication, leadership, and collaboration skills\\n\\nPreferred Skills: (different teams will care about some of the following over others)\\nWe run on AWS. We dockerize applications. You should have some notion of how to build, test, and deploy code to run on cloud infrastructure.\\nExperience with open source tools for information retrieval (e.g. Solr)\\nSearch, Data Scoring/ Ranking expertise\\nData visualization\\nExperience with Deep Learning for NLP\\nExperience developing in open-source machine-learning libraries such as Apache Mahout or MLLib\\nStrong understanding of security, including threat propagation and malware analysis\\n\\nSalesforce, the Customer Success Platform and world's #1 CRM, empowers companies to connect with their customers in a whole new way. The company was founded on three disruptive ideas: a new technology model in cloud computing, a pay-as-you-go business model, and a new integrated corporate philanthropy model. These founding principles have taken our company to great heights, including being named one of Forbes’s “World’s Most Innovative Company” ten years in a row and one of Fortune’s “100 Best Companies to Work For” nine years in a row. We are the fastest growing of the top 10 enterprise software companies, and this level of growth equals incredible opportunities to grow a career at Salesforce. Together, with our whole Ohana (Hawaiian for \"family\") made up of our employees, customers, partners and communities, we are working to improve the state of the world.\\nLI-Y\\nPosting Statement\\nSalesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay fees to any third-party agency or company that does not have a signed agreement with Salesforce.com or Salesforce.org.\\nPursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>----------\\nWho we are\\n----------\\n\\nOur Company was founded on the idea that there are patterns in people's behavior that, with the right logic, can be used to predict future outcomes. We are a small but rapidly growing organization that works in partnership with our customers to create solutions that are simply not found anywhere else. We work in groups rather than in structured corporate hierarchies; our culture is creative and entrepreneurial where everyone contributes to company goals in very real way. We are a hardworking group, but we have a lot of fun with what we do and are looking for new people with a similar mindset to join the organization.\\n\\n----------\\nWhat we do\\n----------\\n\\nOur proprietary software-as-a-service helps automotive dealerships and sales teams better understand and predict exactly which customers are ready to buy, the reasons why, and the key offers and incentives most likely to close the sale. Its micro-marketing engine then delivers the right message at the right time to those customers, ensuring higher conversion rates and a stronger ROI.\\n\\nWe are looking for talented individuals to join our team in building the core components of our software that at heart of it uses machine learning and data intelligence\\n\\n\\nYou will build large-scale batch and real-time data pipelines with data processing frameworks such as Beam, Google DataFlow, Spark and the Google Cloud Platform.\\nYou will help drive testing and tooling to improve data quality\\nYou will closely work with software engineers, architects, data scientists and stakeholders\\nUse best practices in continuous integration and delivery.\\nYou will work in agile teams to iterate and deliver on new product features.\\n\\n-----------\\nWho you are\\n-----------\\n\\n\\nYou know how to work with data with distributed systems such as Spark, Beam, Dataflow and Cassandra.\\nYou have experience with one or more higher-level JVM-based data processing frameworks such as Beam, Dataflow, Storm, and Spark\\nExperience using SQL like abstractions such as BigQuery, Presto or Hive\\nYou are knowledgeable about data modeling, data access, and data storage techniques.\\nYou understand agile software development processes, data-driven development, and reliability\\nStrong communication skills required to be able effectively communicate with both technical and non-technical teammates and stakeholders\\nAbility to collaborate in multi-functional agile teams\\n\\n----------------------\\nExpected Hours of Work\\n----------------------\\n\\nThis is a full-time position. Generally, work is performed Monday through Friday, though holidays and weekends may be required.\\n\\nTravel: This role may require travel to our New York office several times per year.\\n\\n--------------------------------------------\\nWe believe in equal employment opportunities\\n--------------------------------------------\\n\\nThe company provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, the company complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.\\n\\nThe company expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of the company's employees to perform their job duties may result in disciplinary actions up to and including discharge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Us:\\nLive experiences help people cross today’s digital divide and focus on what truly connects us - the here, the now, the once-in-a-lifetime moment that brings us together. To fulfill Gametime’s vision to unite the world through shared experiences, we deliver fans an extraordinary experience for discovering and purchasing last-minute tickets to live events.\\n\\nWith platforms on iOS, Android, mobile web and desktop supporting more than 100,000 events across the US and Canada, we are reimagining the event ticket experience in a mobile-first world.\\n\\nThe Role:\\nAs a Senior Data Engineer at Gametime, you will have the opportunity to work not only within the data team but across the entire business. You’ll be at the forefront of building scalable systems for product, marketing, engineering, finance, and customer support to handle the high volume of data we collect as one of the fastest-growing startups in the Bay Area.\\nWhat you'll do/own:\\nDesign and implement scalable data pipelines and data storage on AWS using Kinesis, Redshift, S3, and a Spark based streaming architecture\\nCreate scalable and low latency code for data products using MongoDB, Redis, Elasticsearch or similar\\nScale and maintain our Analytics Databases to power our dashboards\\nCollaborate with our data scientists to productionize their models\\nImplement Comprehensive Testing and Continuous Integration frameworks for schema, data, and functional processes/pipelines\\nOur ideal candidate has:\\nBS in Computer Science or equivalent experience or field\\nAt least 4 years experience using Redshift or a similar Data Warehouse\\nAt least 4 years experience programming, preferably with Python or Go\\nHands-on experience with SQL, ETL, Data Warehousing and Data Orchestration\\nFamiliarity with scheduling frameworks, preferably Airflow\\nFamiliarity with real-time/batch distributed systems like Kinesis, Kafka, Spark, professional experience with Redis and Elastic search\\nFamiliarity with business intelligence/analytics tools like Tableau or Periscope Data\\nWhat we can offer you:\\nCompetitive SF Bay Area total comp package\\nFully paid medical, dental and vision insurance\\nMonthly commuter, cell phone, and Friday lunch stipends\\nMonthly credits for events on Gametime ($1,200/yr)\\nNew equipment and multi-monitor setup provided\\nCompany-paid lunches (M-Thu)\\nUnlimited snacks, drinks\\nCompany swag\\nCompany happy-hours, events and outings\\nWellness programs\\nTenure recognition\\nApply for this job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Principal Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Mission\\n\\nAs a Principal Data Engineer, you will have the opportunity to shape the future big data solution landscape for leading Fortune 500 organizations. This position is a senior level customer-facing role that needs deep expertise in Apache Spark along with breadth of big data solution architecture experience. On a weekly basis, you will guide customers through architecture, design and implementation activities while strategically aligning their technical roadmap for expanding the usage of the Databricks platform.\\n\\nAt Databricks we work on some of the most complex distributed processing systems and our customers challenge us with interesting new big data and AI requirements. \"Teamwork makes the dream work\" is a fundamental growth value at Databricks, so this role will work internally within a multi-functional team including Account Executives and Customer Success Engineers, all while having a direct channel to the original creators of Apache Spark in Engineering.\\n\\nOutcomes\\n\\n\\nGuide strategic customers as they design and implement Big Data projects ranging from transformations to data science and AI through on-site and remote engagements\\nProvide technical leadership in a post-sales capacity for customers to support successful understanding, evaluation and adoption of Databricks\\nIdentify and drive new initiatives that enable customers to succeed in turning their data into value\\nBuild reference architectures, frameworks, solutions, how-to's, and prototypes for customers\\nProvide escalated level of support for critical customer operational issues\\nArchitect, implement, and/or validate migration of workloads from 3rd party databases and data platforms to Apache Spark.\\nPlan and coordinate with Account Executives, Customer Success Engineers and Solution Architects for expanding the use of Databricks platform within strategic enterprise customers on a weekly basis\\n\\nCompetencies\\n\\n\\nDeep hands-on technical expertise with Apache Spark\\nMinimum 5+ years of design and implementation experience in Big Data technologies (Hadoop ecosystem, Kafka, NoSQL databases)\\n3-5 years in customer-facing pre-sales, technical architecture or consulting role\\nOpen to travel up to 30% per month\\nFamiliarity with data architecture patterns (data warehouse, data lake, streaming, Lambda/Kappa architecture)\\nOutstanding verbal and written communication skills; Comfortable with talking up and down the IT chain of command including directors, managers, architects and developers\\nPassionate about learning new technologies and making customers successful\\nExcellent presentation and whiteboarding skills\\nComfortable coding Python, Scala or Java\\nFamiliarity with AWS/EC2 cloud deployment models (Public vs. VPC)\\n\\nPreferred Competencies\\n\\n\\nBS / MS in Computer Science or equivalent\\nProven track record within a data platform software vendor in a consulting/services function\\nExperience working as or with Data Scientists\\nExperienced with performance tuning, troubleshooting, and debugging Spark and/or other big data solutions\\nFamiliarity with database and analytics technologies in the industry including Data Warehousing/ETL, Relational Databases, or MPP\\nLocated in any of the following metro areas: San Francisco, New York, Boston, or DC\\n\\nBenefits\\n\\n\\nMedical, dental, vision\\n401k Retirement Plan\\nUnlimited Paid Time Off\\nCatered lunch (everyday), snacks, and drinks\\nGym reimbursement\\nEmployee referral bonus program\\nAwesome coworkers\\nMaternity and paternity plans\\n\\nAbout Databricks\\n\\nDatabricks' mission is to accelerate innovation for its customers by unifying Data Science, Engineering and Business. Founded by the original creators of Apache Spark™, Databricks provides a Unified Analytics Platform for data science teams to collaborate with data engineering and lines of business to build data products. Users achieve faster time-to-value with Databricks by creating analytic workflows that go from ETL and interactive exploration to production. The company also makes it easier for its users to focus on their data by providing a fully managed, scalable, and secure cloud infrastructure that reduces operational complexity and total cost of ownership. Databricks, venture-backed by Andreessen Horowitz, NEA and Battery Ventures, among others, has a global customer base that includes Salesforce, Viacom, Shell, and HP. For more information, visit www.databricks.com.\\n\\nApache, Apache Spark and Spark are trademarks of theApache Software Foundation ( http://www.apache.org/ ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Senior Big Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent\\n4+ years of hands-on experience in Java and Spring frame work\\nMinimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment\\n4+ years of hands-on experience in building data pipeline with Java and Spark\\n4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation\\n2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL\\nExperience working in an Agile/Scrum environment\\nNeed someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders\\nProficient understanding of distributed computing principles\\nStrong written and verbal communications</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nParticipate in technical planning &amp; requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications\\nBe Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus\\nDesign, build and launch extremely efficient &amp; reliable data pipelines to move data (both large and small amounts) to our data warehouses\\nDesign, build and launch new data extraction, transformation and loading processes in production\\nCreate new systems and tools to enable the customer to consume and understand data faster\\nBuild, implement and support the data infrastructure; ingest and transform data (ETL/ELT process)\\nImplements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems\\nDefine and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business\\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale\\nStudy data, identify patterns, make sense out of it and convert it to algorithms\\nDesigns and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions\\nKeep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>12 Feb 2019\\nDB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them.\\nWe have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system.\\nSo, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team.\\nActually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure.\\nThis huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be…\\nSeeking Big Data Superhero\\nAs a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark &amp; Scala development experience with the desire and passion to continue to learn big data technologies.\\nResponsibilities\\nThe exciting things you will get to do once employed as a Senior Big Data Engineer:\\nParticipate in technical planning &amp; requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications\\nBe Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus\\nDesign, build and launch extremely efficient &amp; reliable data pipelines to move data (both large and small amounts) to our data warehouses\\nDesign, build and launch new data extraction, transformation and loading processes in production\\nCreate new systems and tools to enable the customer to consume and understand data faster\\nBuild, implement and support the data infrastructure; ingest and transform data (ETL/ELT process)\\nImplements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems\\nDefine and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business\\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale\\nStudy data, identify patterns, make sense out of it and convert it to algorithms\\nDesigns and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions\\nKeep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance\\nRequired Qualifications\\nThe experience and qualifications we hope you bring to the Senior Big Data Engineer position:\\nBachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent\\n4+ years of hands-on experience in Java and Spring frame work\\nMinimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment\\n4+ years of hands-on experience in building data pipeline with Java and Spark\\n4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation\\n2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL\\nExperience working in an Agile/Scrum environment\\nNeed someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders\\nProficient understanding of distributed computing principles\\nStrong written and verbal communications\\nPreferred Qualifications\\nHands-on experience working with Business Intelligence and Reporting\\nHands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions.\\nExperience with integration of data from multiple data sources\\nHands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices\\nIntelligence &amp; Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.)\\nPassion for working with open-source technologies as well as commercial platforms\\nBenefits\\nAs the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with:\\nWorking with the industry-leading partners and customers — you will not be limited with just the one project we covered here\\nFlexible work hours, up to 32 days off annually\\nFriendly teams, experienced colleagues, and perfect work equipment\\nOpportunities for career growth and raising professional skills\\nTravel opportunities\\nCertainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>EasyPost is a San Francisco based start-up that provides an API to revolutionize the entire shipping process for e-commerce companies. Any online merchant can plug in our state of the art technology into their existing framework to provide a world-class shipping experience for consumers. Today, we help hundreds of leading e-commerce companies ship and track packages. We also have a fulfillment business that offers end to end shipping solutions.\\n\\nWe are looking for a Senior Data Engineers with experience in or who are comfortable in a polyglot environment to join the easypost team. You will be a key member of our small but growing engineering team making important technical decisions that will shape the company's future. If you love to code and work on unique challenges within a collaborative team of developers to build meaningful products, then we'd love to meet you!\\n\\nAbout You:\\n\\n5+ years of professional software development experience\\nExpert in SQL and deep understanding of relational databases\\nStrong experience in performance tuning SQL and ETL pipelines in OLTP, OLAP, and Data Warehouse environments\\nExpert knowledge of data architecture, data modeling, and building data tools for ETL\\nExperience with modern big data technologies - e.g. Hadoop, Hive, Spark\\nIntegrations experience with REST, HTTP/HTTPS protocols, and 3rd party APIs a plus\\nStrong desire to work in a fast-paced, start-up environment with multiple releases a day\\nA passion for working as part of a team - you love connecting and collaborating with others\\n\\nWhat We Offer:\\n\\nCompetitive salary and equity\\nComprehensive medical, dental, and vision and commuter benefits\\nFlexible work schedule and paid time off\\nCollaborative culture with a supportive team\\nThe opportunity to make massive technical contributions at a fast-growing start-up\\nA great place to work with unlimited growth opportunities\\n\\nDo you want to come join us? You'd be a critical member of our growing engineering team to solve some of the hardest problems out there. Please send in an application and we will contact you.\\n\\nEasyPost is proud to be an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Data Engineer - Bioinformatics</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Ancestry:\\nWhen you join Ancestry, you join our family tree. Backed by history, science, and technology, we’re creating a new world of connection, innovation, and understanding. Whether it’s reuniting long-lost relatives through DNA or unearthing new family stories from historical records, Ancestry empowers life-changing experiences. With over 20 billion digitized historical records, 100 million family trees, and 15+ million DNA kits sold, Ancestry is bringing the power of personal discovery to people around the world.\\n\\nThe Opportunity:\\nThe DNA Science team’s goal is to deliver the next generation of consumer genomic tests to shed light on human diversity, origins, relationships, and health. As a Scientific Data Engineer, you will help deliver on this mission by using your data engineering and scientific skills to support research and new product development for Ancestry’s direct-to-consumer DNA test. In this critical role, you will work on fast paced multidisciplinary teams to build a scalable data platform used by Ancestry’s R&amp;D team.\\nWhat You Will Do\\nBring understanding of genomics, product development, and machine learning to build end-to-end cloud-based data ecosystem for Ancestry’s R&amp;D scientific and genealogy data.\\nDevelop tools to locate, manipulate, QC and ensure interoperability of large diverse datasets\\nCollaborate with engineers, scientists, and stakeholders to define technical and business requirements\\nDesign and own solutions to maintain high quality standards\\nInterface with data customers to continuously improve data processes\\nDevelop and enforce policies for proper use of sensitive datasets\\nCollaborate with team members across the company to integrate with existing data sources\\nProactively communicate plans and updates to colleagues and stakeholders\\n\\nWho You Are\\n4+ years of experience building data systems and managing datasets with Master’s degree in Computer Science, Genetics, Physics, Engineering or other quantitative field, or an equivalent combination of education and experience.\\nExpertise in several of the following: Python, R, SQL, git, and Hadoop/Spark in a distributed Unix environment, AWS,\\nAbility to design and implement complex data management systems\\nExperience with biomedical ontologies and controlled vocabularies (such as Gene Ontology, Human Phenotype Ontology, etc) for metadata management and data integration.\\nExperience with public databases and tools commonly used in genomics, such as dbGAP, NCBI, EBI, ENSEMBL.\\nFamiliarity with methods used to analyze genomic data, such as ancestry inference, GWAS, NGS, and machine learning toolkits\\nSelf-starter with strong communication skills and sense of ownership to complete projects independently while thriving in a team environment\\nAble to apply feedback in a professional manner\\nScientific or genomics data management experience in pharma, clinical diagnostics, or biotech\\nExperience designing, building, and supporting complex software and distributed compute systems\\nExperience working with HPC job schedulers (ex. SLURM, SGE), and public cloud experience\\n\\nGD-Sponsored\\nIND1\\n#LI-HK1\\n\\nAdditional Information:\\nAncestry is an Equal Opportunity Employer that makes employment decisions without regard to race, color, religious creed, national origin, ancestry, sex, pregnancy, sexual orientation, gender, gender identity, gender expression, age, mental or physical disability, medical condition, military or veteran status, citizenship, marital status, genetic information, or any other characteristic protected by applicable law. In addition, Ancestry will provide reasonable accommodations for qualified individuals with disabilities.\\n\\nAll job offers are contingent on a background check screen that complies with applicable law. For San Francisco office candidates, pursuant to the San Francisco Fair Chance Ordinance, Ancestry will consider for employment qualified applicants with arrest and conviction records.\\n\\nAncestry is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at Ancestry via-email, the Internet or in any form and/or method without a valid written search agreement in place for this position will be deemed the sole property of Ancestry. No fee will be paid in the event the candidate is hired by Ancestry as a result of the referral or through other means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Data Engineer- Python</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of experience in core JAVA and SQL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Senior Consultant, you will focus on managing the information supply chain from acquisition to ingestion, storage and the provisioning of data to points of impact by modernizing and enabling new capabilities. Information value is enhanced through enterprise-scale applications that enable visualization, consumption and monetization of both structured and unstructured data. Big data is becoming one of the most important technology trends that has the potential for dramatically changing the way organizations use information to enhance the customer experience and transform their business models.\\nWork you'll do\\n\\nSenior Consultants work within an engagement team. Key responsibilities will include:\\n Function as integrators between business needs and technology solutions, helping to create technology solutions to meet clients’ business needs.\\n Identifying business requirements, requirements management, functional design, prototyping, process design (including scenario design, flow mapping), testing, training, defining support procedures and supporting implementations.\\n\\nThe Team\\n\\nAnalytics &amp; Cognitive\\n\\nIn this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.\\n\\n\\nThe Analytics &amp; Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy &amp; Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.\\n\\n\\nAnalytics &amp; Cognitive will work with our clients to:\\n Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms\\n Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions\\n Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements\\n\\n\\nQualifications\\n\\nRequired:\\n\\n 5+ years of experience in core JAVA and SQL\\n 3+ years of experience in Python&amp; Unix Shell Scripting\\n 3+ years of experience in building scalable and high performance data pipelines using Apache Hadoop, Map Reduce, Pig &amp; Hive\\n Experience with bigdata cross platform compatible file formats like Apache Avro &amp; Apache Parquet\\n Experience in Apache Spark is a plus\\n 1+ years of experience with data lake implementations, core modernizations and data ingestion\\n\\n 1 or more years of hands on experience designing and implementing data ingestion techniques for real time and batch processes for video, voice, weblog, sensor, machine and social media data into Hadoop ecosystems and HDFS clusters.\\n 2+ years of experience leading workstreams or small teams\\n Willingness for weekly client-based travel, up to 80-100% (Monday — Thursday/Friday)\\n Bachelor’s Degree or equivalent professional experience\\n\\n Preferred:\\n\\nAWS Certification, Hadoop Certification or Spark Certification\\nExperience with Cloud using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)\\nExperience with data integration products like Informatica Power Center Big Data Edition (BDE), IBM BigInsights, Talend etc.\\nExperience designing and implementing reporting and visualization for unstructured and structured data sets\\nExperience in designing and implementing scalable, distributed systems leveraging cloud computing technologies like AWS EC2, AWS Elastic Map Reduce and Microsoft Azure\\nExperience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching\\nKnowledge of data, master data and metadata related standards, processes and technology\\nExperience working with multi-Terabyte data sets\\nExperience with Data Integration on traditional and Hadoop environments\\nAbility to work independently, manage small engagements or parts of large engagements.\\nStrong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint).\\nStrong problem solving and troubleshooting skills with the ability to exercise mature judgment.\\nEagerness to mentor junior staff.\\nAn advanced degree in the area of specialization is preferred.\\n\\nHow you’ll grow\\n\\nAt Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.\\n\\n\\nBenefits\\n\\nAt Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.\\nDeloitte’s culture\\n\\nOur positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.\\n\\n\\nCorporate citizenship\\n\\nDeloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.\\n\\n\\nRecruiter tips\\n\\nWe want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.\\n\\n#LI:PTY\\n#IND:PTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nThrives in a fast-paced, startup environment, is adaptable and versatile\\n3+ years of python\\nExperience in python data libraries (pandas, luigi, dask, etc)\\nSolid understanding of data structures and algorithms\\nBuilding distributed data systems\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nSan Francisco, CA\\nData department\\n\\nAlice helps small business owners launch and grow. Since launching in May 2017, our goal is to help 6 million owners succeed through our free digital platform,\\nhelloalice.com [http://www.helloalice.com]. Alice serves all entrepreneurs and we wholeheartedly believe that if underserved business owners -- women, people of color, veterans, the LGBTQ+ community, entrepreneurs with disabilities, people in small towns, and immigrants -- are provided better access to resources, they can change the world. Our vision is to be financially successful while making a positive impact on economies and job creation.\\n\\nAlice is growing a diverse team, and we’re looking for a Data Engineer to help us fulfill our mission of connecting all entrepreneurs to the resources they need to grow and scale their businesses. We’re leading a movement to connect every founder, regardless of geography, capitalization, prior experience, or cultural constraints, to the experts, tools, knowledge, and communities that will propel their companies forward. Alice is a Series A-backed company with teams in Houston, TX and San Francisco, CA. We are helping hundreds of thousands of owners a week in all fifty states. Led by co-founders Elizabeth Gore and Carolyn Rodz, we are hiring self-starters who can build the plane while flying it.\\n\\nEngineering is key to scaling our mission, and we’re growing our data engineering team to build new features, improve our AI, and better serve entrepreneurs from around the world. We believe in working fast, but smart, and work toward measurable results. As the first member of the data team you will be able to have a huge impact on the technologies, architecture, and tools we use. Data is key to Alice’s success and this position will have big visibility throughout the organization.\\n\\nRequired Skills/Experiences:\\n\\nThrives in a fast-paced, startup environment, is adaptable and versatile\\n3+ years of python\\nExperience in python data libraries (pandas, luigi, dask, etc)\\nSolid understanding of data structures and algorithms\\nBuilding distributed data systems\\n\\nDesired Skills / Experiences\\n\\nAWS and Google Cloud management\\nAnsible, Terraform or other cloud orchestration libraries\\nRabbitMQ, Kafka or other queuing systems\\ngRPC or other RPC libraries\\nProtobufs\\nMachine learning and statistics libraries\\nScraping websites for relevant information\\n\\nOur company values are important to us. We thought we would share them with you as someone interested in joining our team.\\n\\nINNOVATE ALWAYS. Seek out unique perspectives, diverse experiences, and disconnected dots. These are the seeds of big ideas and exceeded expectations, made even better through constant collaboration.\\nEMBRACE FAILURE. Learn from the inevitable failures that result from innovation, and move forward quickly with a pioneering spirit. Champion the doers, celebrate their contributions to the team, and don't shy away from difficult conversations.\\nDRIVE THE MISSION FORWARD. Everything we do is through the inclusive lens of helping all business owners launch and grow, regardless of who they are or where they come from. Hold yourself to the highest standards of quality and equality in everything you do.\\nEVERYONE TAKES OUT THE TRASH. No task is too small for the success of our company or our owners. Always be thoughtful and practice extreme kindness toward our team, partners, and owners.\\nSIMPLIFY AND COMMIT. Maintain a bias toward efficiency, acting quickly and testing often. Recognize the opportunity cost of every decision, commit to deadlines, be on time, and search for simple, smart solutions.\\n\\nAlice is an Equal Employment Opportunity employer that will consider all qualified applicants, regardless of race, color, religion, gender, sexual orientation, marital status, gender identity or expression, national origin, genetics, age, disability status, protected veteran status, or any other characteristic protected by applicable law.\\n\\nwww.helloalice.com [http://www.helloalice.com/] // Twitter\\n[https://twitter.com/HelloAlice] // Facebook\\n[http://www.facebook.com/aliceconnects] // Instagram\\n[https://www.instagram.com/helloalice_com/] // LinkedIn\\n[https://www.linkedin.com/company/25067438/]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>San Francisco, CA 94107</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94107</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.\\n\\nAs a Lead Data Engineer, here's what we'll be looking for you to bring:\\n\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others\\nDeep understanding of streaming data architectures and technologies for real-time and low-latency data processing\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nUnderstanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists\\nUnderstanding of agile development methods including: core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftsmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different level\\nThere's no typical day or engagement for our Lead Data Engineers. Here’s what you’ll do:\\n\\nBe the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\nA few important things to know:\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Data Engineer - Consultant - San Francisco, CA</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.\\n\\nWe are...\\n\\n • The Industry-recognized data and analytics leaders\\n • Passionate problem solvers across a broad spectrum of technologies and industries\\n • Value seekers for measurable business outcomes\\n • Continuous learners through training and education\\n • Focused on a work-life balance with an unlimited paid time off policy\\n\\nData engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.\\nData engineers deliver value through....\\nHands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools\\nDemonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights\\nCollaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights\\nInternal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO\\nA data engineer’s skills include, but are not limited to...\\nBachelor degree AND 3+ years of professional work experience\\nSQL, SQL, SQL!\\nProgramming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)\\nLinux / Windows (Command line)\\nBig Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop, Spark)\\nCloud Platforms (AWS, Azure, Google Cloud Platform)\\nData Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)\\nData Integration Tools (Ab Initio, DataStage, Informatica, SSIS, Talend)\\nDatabases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)\\nMarkup Languages (JSON, XML, YAML)\\nCode Management Tools (Git/GitHub, SVN, TFS)\\nDevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)\\nTesting / Data Quality (TDD, unit, regression, automation)\\nSolving complex data and technology problems\\nLeading technical teams of 2+ consultants\\nAbility to design components of a larger implementation\\nExcellent communication to narrate data driven insights and technical approach\\nMust reside in the San Francisco, CA bay area\\n\\nIf this sounds like you, let’s talk!\\n\\nWe prefer candidates who are open to a national travel model (M-TH weekly); however, we will consider candidates that do not prefer to travel, but they must be willing to commute anywhere within the bay area.\\n\\nClarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.\\n\\n\\nGLDR\\n#LI-NT1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Data Architect</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nAuthority in everything data related. Know exactly how data is collected, analyzed, and delivered. Also familiar with both the technical and user-facing sides of databases.\\n5+ years work experience as a Data Architect or Data Engineer or similar role\\nIn-depth understanding of database structure principles used for analytics such as star schemas and SCD\\nExpertise in database performance optimization\\nExpertise in building processes supporting data transformation, data structures, metadata, dependency and workload management.\\nA successful history of manipulating, processing and extracting value from large disconnected datasets.\\nExperience using the following software/tools:\\nExpertise in Redshift, Snowflake, or similar cloud databases\\nExperience with object-oriented/object function scripting languages\\nExperience with ETL tools: Alooma, Matillion, Fivetran, Talend, etc.\\nExperience with big data tools: Hadoop, Spark, Kafka, etc.\\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\\nExperience with data pipeline and workflow management tools: Airflow, Data Factory, etc.\\nExperience with stream-processing systems: Storm, Spark-Streaming, etc.\\nFamiliarity with data visualization tools (e.g. Tableau, Looker, Power BI)\\nDemonstrated analytical skills\\nProblem-solving attitude\\nB.S. in Computer Science, Information Systems or another quantitative field</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Your Opportunity\\nWe are looking for a Data Architect to run and evolve our data practice within the Go-To-Market Data &amp; Analytics Team. We use data to (i) define and measure performance and productivity metrics across Go-To-Market Functions (ii) identify and monitor leading indicators and predictive models, and (iii) deliver insights to Go-To-Market teams to ultimately improve core business processes and outcomes. The team partners with Executives and their teams within Sales, Marketing, Pre-and-Post Technical Sales, Customer Success, and Alliances &amp; Channels. We also partner on multi-functional projects with Product and G&amp;A.\\n\\nWho You Are:\\nA creative and forward-thinking data professional\\nAn experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up\\nYou thrive in a fast-paced growth environment\\nResults and Team oriented\\nWhat You'll Do\\nDesign, develop and build database design and architecture to power Go-To-Market analytics products\\nBuild, optimize and maintain conceptual and logical database models\\nDesign data pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other technologies.\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nCreate data monitoring models for each product and work with business and analytics teams to create models ahead of new releases\\nExamine and identify database structural necessities by evaluating operations, applications, and programming.\\nMonitor the system performance by performing regular tests, fixing performance issues, and integrating new features.\\nWork with business partners to assist with data-related technical issues and support their data infrastructure needs\\nRecommend solutions to improve new and existing database systems.\\nEducate staff members through training and individual support.\\nYour Qualifications\\nAuthority in everything data related. Know exactly how data is collected, analyzed, and delivered. Also familiar with both the technical and user-facing sides of databases.\\n5+ years work experience as a Data Architect or Data Engineer or similar role\\nIn-depth understanding of database structure principles used for analytics such as star schemas and SCD\\nExpertise in database performance optimization\\nExpertise in building processes supporting data transformation, data structures, metadata, dependency and workload management.\\nA successful history of manipulating, processing and extracting value from large disconnected datasets.\\nExperience using the following software/tools:\\nExpertise in Redshift, Snowflake, or similar cloud databases\\nExperience with object-oriented/object function scripting languages\\nExperience with ETL tools: Alooma, Matillion, Fivetran, Talend, etc.\\nExperience with big data tools: Hadoop, Spark, Kafka, etc.\\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\\nExperience with data pipeline and workflow management tools: Airflow, Data Factory, etc.\\nExperience with stream-processing systems: Storm, Spark-Streaming, etc.\\nFamiliarity with data visualization tools (e.g. Tableau, Looker, Power BI)\\nDemonstrated analytical skills\\nProblem-solving attitude\\nB.S. in Computer Science, Information Systems or another quantitative field\\nPlease note that visa sponsorship is not available for this position.\\nOur Office\\nOur office is in the tech-rich urban center of San Francisco, with easy commute access and a plethora of good eats. We provide competitive compensation, equity and big-company benefits (medical, dental, etc.)—all while maintaining the energy, agility, and fun of a start-up.\\nAbout Us\\nNew Relic (NYSE: NEWR) is the industry’s largest and most comprehensive cloud-based instrumentation platform built to create more perfect software. The world’s best software and DevOps teams rely on New Relic to move faster, make better decisions and create best-in-class digital experiences. If you run software, you need to run New Relic. We’re proudly trusted by more than 50% of the Fortune 100.\\nFounded in 2008, we’re a global company focused on building a culture where all employees feel a deep sense of belonging, where every ‘Relic’ can bring their whole self to work and feel supported and empowered to thrive. We’re consistently recognized as a distinguished employer and are committed to building world-class products and an award winning culture. For more information, visit newrelic.com.\\nOur Hiring Process\\nIn compliance with applicable law, all persons hired will be required to verify identity and eligibility to work and to complete employment eligibility verification. Note: Our stewardship of the data of thousands of customers’ means that a criminal background check is required to join New Relic.\\nWe will consider qualified applicants with arrest and conviction records based on individual circumstances and in accordance with applicable law including, but not limited to, the San Francisco Fair Chance Ordinance.\\nHeadhunters and recruitment agencies may not submit resumes/CVs through this website or directly to managers. New Relic does not accept unsolicited headhunter and agency resumes, and will not pay fees to any third-party agency or company that does not have a signed agreement with New Relic.\\nNew Relic is an equal opportunity employer. We eagerly seek applicants of diverse background and hire without regard to race, color, gender identity, religion, national origin, ancestry, citizenship, physical abilities (or disability), age, sexual orientation, veteran status, or any other characteristic protected by law.\\n\\nInterested in the details of our privacy policy? Read more here: https://newrelic.com/termsandconditions/applicant-privacy-policy\\n\\n#LI-SP2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Francisco, CA 94102</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94102</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>----------------\\nRole Description\\n----------------\\n\\nIn this role you will build very large, scalable platforms using cutting edge data technologies. This is not a \"maintain existing platform\" or \"make minor tweaks to current code base\" kind of role. We are effectively building from the ground up and plan to leverage the most recent Big Data technologies. If you enjoy building new things without being constrained by technical debt, this is the job for you!\\n\\n----------------\\nResponsibilities\\n----------------\\n\\n\\nYou will help define company data assets (data model), spark, sparkSQL and hiveSQL jobs to populate data models\\nYou will help define/design data integrations, data quality frameworks and design/evaluate open source/vendor tools for data lineage\\nYou will work closely with Dropbox business units and engineering teams to develop strategy for long term Data Platform architecture\\n\\n------------\\nRequirements\\n------------\\n\\n\\nBS or MS degree in Computer Science or a related technical field\\n4+ years of Python or Java development experience\\n4+ years of SQL experience (No-SQL experience is a plus)\\n4+ years of experience with schema design and dimensional data modeling\\nAbility in managing and communicating data warehouse plans to internal clients.\\nExperience designing, building and maintaining data processing systems\\nExperience working with either a Map Reduce or a MPP system on any size/scale\\n\\n------------------\\nBenefits and Perks\\n------------------\\n\\n\\n100% company paid individual medical, dental, &amp; vision insurance coverage\\n401k + company match\\nMarket competitive total compensation package\\nFree Dropbox space for your friends and family\\nWellness Reimbursement\\nGenerous vacation policy\\n10 company paid holidays\\nVolunteer time off\\nCompany sponsored tech talks (technology and other relevant professional topics)\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Senior Data Engineer Sandipan</td>\n",
       "      <td>San Mateo, CA</td>\n",
       "      <td>San Mateo</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nProven expertise in production software development\\n7+ years of experience programming in Java, Python, SQL, or C/C++\\nProficient in SQL, NoSQL, relational database design and methods for efficiently storing &amp; retrieving data\\nStrong analytical and science skills\\nCreative problem solver\\nExcellent verbal and written communications skills\\nStrong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..\\nExperience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies &amp; frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)\\n</td>\n",
       "      <td>\\nPrior Data Platform Engineering experience\\nExperience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.\\nDynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)\\nExperience designing and tuning high performance systems\\nPrior experience with data warehousing and business intelligence systems\\nExperience with Elasticsearch, SolrWeb, and Lucene\\nExperience with Star Schema, fact vs dimensions, updates/restatements and views\\nProfessional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform\\nLinux expertise\\nPrior work and/or research experience with unstructured data and data modeling\\nFamiliarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)\\nDemonstrate understanding of \"var\" vs. \"val\", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem\\nFirm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants\\nUnderstanding of various analytic and visualization utilities available in R\\nConfigure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build\\nAbility to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access\\nUnderstanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.\\nAble to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output\\nAble to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar\\nAble to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue\\nImplementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data\\nImplement a graph (line or pie etc.) backed by a live (changing) data set, something like \"requests per minute\" or similar\\nUnderstand basic modeling techniques, tools sets. Implement simple Python or R analytic routines\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Data Engineer, you will provide technical leadership to the team that designs and develops path-breaking large scale cluster data processing systems.\\n\\nDesign and develop code, scripts and data pipelines that leverage structured and unstructured data integrated from multiple sources. Software installation and configuration. Participate in and help lead requirements and design workshops with our clients. Develop project deliverable documentation. Mentor junior members of the team in software development best practices. Other duties as assigned.\\n\\nAdditionally, as a senior member of our development &amp; Ops team, you will help establish thought leadership in the big data space by contributing best practices, white papers, technical commentary and representing our section as one Leads.\\n\\nJob Qualifications:\\n\\nProven expertise in production software development\\n7+ years of experience programming in Java, Python, SQL, or C/C++\\nProficient in SQL, NoSQL, relational database design and methods for efficiently storing &amp; retrieving data\\nStrong analytical and science skills\\nCreative problem solver\\nExcellent verbal and written communications skills\\nStrong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..\\nExperience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies &amp; frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)\\n\\nPreferred Knowledge, Skills and Abilities:\\n\\nPrior Data Platform Engineering experience\\nExperience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.\\nDynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)\\nExperience designing and tuning high performance systems\\nPrior experience with data warehousing and business intelligence systems\\nExperience with Elasticsearch, SolrWeb, and Lucene\\nExperience with Star Schema, fact vs dimensions, updates/restatements and views\\nProfessional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform\\nLinux expertise\\nPrior work and/or research experience with unstructured data and data modeling\\nFamiliarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)\\nDemonstrate understanding of \"var\" vs. \"val\", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem\\nFirm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants\\nUnderstanding of various analytic and visualization utilities available in R\\nConfigure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build\\nAbility to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access\\nUnderstanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.\\nAble to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output\\nAble to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar\\nAble to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue\\nImplementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data\\nImplement a graph (line or pie etc.) backed by a live (changing) data set, something like \"requests per minute\" or similar\\nUnderstand basic modeling techniques, tools sets. Implement simple Python or R analytic routines\\n\\nJob Abilities:\\nMust be able to sit for long periods of time working on computers. Must be able to interact and communicate with the senior management in meetings. Must be able to write programming code in applicable languages and write project documentation in English.\\n\\nEducation:\\nBachelor's Degree or foreign equivalent in Computer Science or related technical field followed by six (6-8) years of progressively responsible professional experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).\\n\\nOR\\n\\nMaster's Degree or foreign equivalent in Computer Science or related technical field. Four (4-5) years of experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).\\n\\nEmployer will accept any suitable combination of education, training, or experience.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Emeryville, CA</td>\n",
       "      <td>Emeryville</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCollaborate with architects to implement data solutions that solve business problems\\nDesign conceptual, logical and physical data models\\nImplement effective and scale-able end-to-end data pipeline solutions\\nImport data from a multitude of external systems\\nBuild metrics and reports</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor's degree in Computer Science/Engineering\\n2+ years professional development experience\\nFluency in SQL, preferably MySQL\\nFamiliar with data storage mechanisms, e.g. Hadoop, NoSQL, etc.\\nExperienced in ETL development\\nUnderstanding of dimensional and normalized database models and their applications\\nGood knowledge of the Linux operating system, networking, and toolset (bash, ssh, vim, etc.), especially text processing commands (sed, awk, etc.)\\nExperience in OOP (e.g. C++, Python, etc.)\\nStrong software engineering best practices (unit testing, code reviews, design documentation)\\nGood verbal and written communication skills</td>\n",
       "      <td>We combine the power of Big Data, technology, and lean economics. We discover the information people are searching for and provide it. We help transform lives.\\n\\nCallisto Media will be unmatched in providing products, services, and experiences to a diverse universe. From mainstream populations to groups that traditional companies believe are too small or economically unfeasible to address, we will meet their needs.\\n\\nToday, we're the fastest growing company in the $140 billion global publishing industry, and our primary method for meeting peoples' needs is through books. But creating books for them is only the beginning.\\n\\nData Engineer to join our small engineering team. Design, code, test, and support projects to manage the company’s data that will directly impact publishing efforts.\\n\\nResponsibilities\\nCollaborate with architects to implement data solutions that solve business problems\\nDesign conceptual, logical and physical data models\\nImplement effective and scale-able end-to-end data pipeline solutions\\nImport data from a multitude of external systems\\nBuild metrics and reports\\nRequirements\\nBachelor's degree in Computer Science/Engineering\\n2+ years professional development experience\\nFluency in SQL, preferably MySQL\\nFamiliar with data storage mechanisms, e.g. Hadoop, NoSQL, etc.\\nExperienced in ETL development\\nUnderstanding of dimensional and normalized database models and their applications\\nGood knowledge of the Linux operating system, networking, and toolset (bash, ssh, vim, etc.), especially text processing commands (sed, awk, etc.)\\nExperience in OOP (e.g. C++, Python, etc.)\\nStrong software engineering best practices (unit testing, code reviews, design documentation)\\nGood verbal and written communication skills\\nOther useful skills/experience:\\nExperience using Git\\nKnowledge of JavaScript\\nKnowledge of Business Intelligence (BI) / Data Warehousing (DW) principles and software (e.g. Tableau, Looker, etc.)\\nExperience with cloud based SaaS platforms\\nExperience with clustered and/or distributed systems\\nExperience developing within large codebases\\nExperience with web development: CGI, HTML, Javascript, Apache\\nExperience with REST APIs\\nJoin us! We are a team where data drives our decisions. Where culture matters. We put our customers first and they embody our core values. We’re entrepreneurial and focused in our approach. We challenge each employee to experiment and drive results while feeling empowered to do their best work each and every day.\\n\\nCallisto Media offers a competitive salary, full benefits, 401k, stock options, for full-time employees, as well as a friendly working environment. This is a full-time, onsite position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Kiva:\\nKiva is an international nonprofit working in more than 80 countries, with a mission of expanding financial access to help underserved communities thrive. We do this by crowdfunding loans for entrepreneurs, women and students on our kiva.org lending platform, and by addressing the underlying barriers to financial access around the world through innovative projects and partnerships. Our organization combines the culture and technological passion of an internet start-up with the compassion and heart of a nonprofit to create impact and opportunity at global scale. With offices in San Francisco, Portland, New York, Nairobi and Bangkok, Kiva's team includes 100+ employees and 400+ volunteers worldwide. Our team is growing as we pursue exciting new opportunities to create a financially inclusive world.\\n\\nAbout Data Science @Kiva\\n\\nThe newly formed Data Science team at Kiva is tasked with creating impact by using data to solve problems around financial inclusion. This translates to a host of interesting and challenging technical problems spanning analytics, machine learning, data engineering and infrastructure. The team is currently building out algorithms to power personalization on kiva.org and is looking to scale data science for other kinds of problems. Our current ecosystem of infrastructure, tools and skill sets includes AWS, GCP, Kubernetes, Snowflake, Fivetran, Looker, Google Cloud Composer (Airflow), Snowplow, Python, Kotlin, R, PHP, and Hyperledger. We are looking for smart and motivated individuals to join our growing team.\\n\\nAbout the role\\n\\nThe team is looking for our first data engineer to help architect, design, build, maintain and evolve our data platform &amp; machine learning infrastructure. This is an opportunity to be an early influencer and a significant contributor to the direction and growth of the team. As a Senior Data Engineer, you will:\\n\\n\\nBuild a machine learning and data science platform that will influence the experience of millions of lenders and borrowers around the world.\\nMake decisions and trade-offs that will drive the data engineering roadmap.\\nCollaborate with data scientists &amp; machine learning engineers to build new data driven products.\\nBuild an ecosystem of tools to scale data science at Kiva.\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nWork with various parts of the organization (Engineering, Product, Operations etc.) on solving data related technical issues.\\nHelp foster a spirit of innovation and collaboration both within the engineering team and across the organization.\\nWork to create impactful and sustainable solutions to complex problems by taking bold and measured risks.\\nBalance your technical excellence with a high E.Q., showing up with a sense of empathy, awareness, and responsibility.\\nShare the knowledge you gain generously with your peers to perpetuate a culture of engineering excellence.\\n\\nAbout You\\n\\n\\nYou have a BS in a quantitative discipline (computer science, engineering, physics, mathematics or a related field) or comparable work experience.\\nYou have the ability to communicate findings and recommendations to technical and non-technical audiences.\\nYou love solving complex and challenging data and ML infrastructure problems.\\nYou have built data platforms to enable machine learning driven products.\\nYou have 3+ years of experience deploying and scaling machine learning models.\\nYou have 5+ years of experience implementing complex data pipelines that support analytics &amp; machine learning.\\nYou have coding (Python &amp; SQL) and design skills.\\nYou have experience in distributed systems and architectures.\\nYou are willing to learn, take initiative and wear multiple hats as required.\\n\\nThis role will be based in San Francisco or Portland, and will report to the Director of Data Science based in San Francisco. At this time, we can only consider applicants with authorization to work in the United States on a permanent, full-time basis; unfortunately, we cannot provide visa sponsorship.\\n\\nWhat We Offer\\n\\n\\nAn opportunity to improve real lives, solve hard problems, and change the world\\nFriendly, supportive, and adventurous environment with a team of engaged colleagues\\nA comprehensive, industry-leading benefits package\\nOpportunities to connect with and learn from colleagues and partners around the world\\n\\nA diverse and inclusive workplace where we learn from each other is an integral part of Kiva's culture. We actively welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a great place to work. Join us and help us achieve our mission!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Senior Business Intelligence Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Data Team at Womply advances the state of our data, and empowers the company to make better decisions from our data. We're seeking a talented and motivated Sr. BI Engineer to join our team. As a Sr. BI Data Engineer, you will hold the keys to the infrastructure that powers our current and future Data Products. We expect you to help us build and leverage the latest technologies to tap into our firehose of data, and your systems will open up new paths for analysis and discovery. You will have the opportunity to make a big impact, and work with extremely talented peers on a fast paced, high energy team.\\n\\nYou will develop ETL and data modeling solutions from our Data Lake into our Data Warehouse, to help us evolve our data-driven philosophy and become a world-class data organization. You will own the design, execution, and ongoing support of critical data warehousing projects enabling accurate reporting and advanced analytics for all of Womply's internal business units.\\n\\nYou will have to be self-sufficient - we are a startup, so everyone might do a bit of everything to get things done. We look for people who take pride in their work, execute on it, and deliver phenomenal results.\\n\\nIn order to be successful in this role, you will be responsible for:\\n\\n\\nBuilding and maintaining the data pipelines from various data sources, while maintaining high accuracy, consistency, and reliability\\nLeveraging our data foundation to design and implement innovative solutions to our hardest data problems, and ensure their quality and effectiveness with robust verification process\\nDriving innovation by recommending and adopting new tools and technologies that provide competitive data advantages for the company\\nDeveloping, documenting, and implementing data models for analytics\\nPartnering with business stakeholders, product managers, business intelligence analysts, engineering, and data scientists to understand reporting requirements and bring ideas to production\\n\\nYou must have:\\n\\n3-8 years in software engineering\\nExperience with Data Warehousing, Architecting Pipelines, and Data Modeling\\nTeam-oriented, self-motivated, success-driven, roll-up-your-sleeves attitude\\nStrong intellectual curiosity and demonstrated ability to understand and question the data\\nHealthy Skepticism to challenge the status quo so we can improve\\nTechnically proficient in:\\nLanguages - Python / Scala / Ruby / SQL / Bash\\nTechnologies - Snowflake/Athena, AWS, Airflow\\n\\nNice to have:\\n\\nSpark\\n\\nCome build something amazing at Womply:\\nWomply helps small businesses thrive in a digital world. Our software makes it easy for small businesses to boost their online reputations, engage their customers, and monitor the health of their businesses with data and technology they can't get anywhere else. We're one of the fastest growing software companies in the country, serving more than 100,000 small businesses across 400+ business verticals in every corner of America.\\n\\nWe're a fanatically values-based company with $50 million raised to accelerate our growth. We work hard and push each other to be the best, but we also have fun and don't take ourselves too seriously. If you want to win and make a big impact, let's talk. We're hiring in the Bay Area and Lehi, Utah for engineering, DevOps, design, data science, sales, marketing, business development, account management, and more.\\n\\nPLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift.\\n\\nMore:\\nWork at Womply ( https://womply.com/jobs/ )\\n\\nLife at Womply ( https://womply.com/life-at-womply/ )\\n\\nHow we work ( https://womply.com/how-we-work )\\n\\nOur values ( https://womply.com/values/ )\\n\\nBenefits ( https://womply.com/benefits )\\n\\nDiversity ( http://womply.com/diversity/ )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Google Cloud Data Engineer Trainer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience producing applications/products ideally cloud based products\\nComfortable presenting and delivering multiple-day-long training session both in the classroom and online\\nComfortable in training enterprise level clients on how to develop or diversify their current skill set\\nBasic proficiency with command-line tools and Linux operating system environments\\nSystems Operations experience, including deploying and managing applications, either on-premises or in a public cloud environment\\nComfortable and confident in at least one of the following languages and technologies:\\nPython\\nSQL\\nC++\\nJava\\nPHP\\nRuby\\nGO\\nNode.JS\\n.NET\\nProficient in building data pipelines using programming models like Apache Beam\\nExperience working with HADOOP\\nFamiliar with numerical libraries and popular machine learning frameworks such as scikit-learn or tensorflow\\nCertified on one of the following:\\nGoogle Cloud Platform\\nAmazon Web Services\\nMicrosoft Azure</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDeliver Google Cloud Certification courses both in the classroom and online\\nHelp to aggressively grow the range of related courses.\\nMaintain and evolve existing classroom course content and respond to post-training questions\\nThrough the delivery of great training, encourages repeat bookings onto other courses\\nWork with Jellyfish Agency to guide our clients and internal teams on cloud best practices\\nSupport new business opportunities\\nHelp create marketing materials, assets and other promotional material</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\nThe Role\\nWe are looking for a Google Cloud Engineer guru to join the team and help us build our training portfolio within Google Cloud Platform as well as Amazon Web Services.\\nWe require an experienced Cloud Engineer professional who has a proven track-record for delivering high quality products using GCP / AWS technologies.\\nYou will be instrumental in helping the team identify opportunities for new Cloud Engineer and Data Engineer courses to add to the portfolio and take the lead in delivering the current Google Cloud Certification courses offered by the agency in partnership with Google, this should be as a minimum the Cloud engineer track courses (https://cloud.google.com/training/data-ml).\\nYou will need to have in-depth knowledge and strong practical working experience of the Google Cloud Platform, plus ability to teach other cloud products.\\nIdeally you will also have a good understanding of the competitors on the landscape who compete directly with the Google Cloud, including other providers such as Amazon &amp; Microsoft Azure.\\nA key part of this role will involve leading the creation of the Google Cloud Engineer Platform training material being used for both classroom and online courses. You will also be responsible for identifying opportunities for new courses to add to the portfolio and for obtaining the up-to-date knowledge and certifications required to deliver the existing Google Cloud courses..\\nYou will be delivering face-to-face training to clients in the US, but also expect regular travel to deliver courses overseas.\\n\\nThis is an excellent opportunity for the right candidate to build their own profile within the industry, we will be positioning and promoting you as a thought leader and providing time for you to create both written / video content for publication online, creating compelling case studies of work carried out by the agency, leading round-table events, public speaking engagements, hosting events at our training venue and being proactive in sourcing good opportunities for increasing the awareness of the training proposition.\\nTo help keep your skills sharp there will also be the chance to work alongside the agency’s Cloud practitioners. This opportunity can help guide our trainers on product developments and initiatives.\\n\\nThis is an excellent opportunity for the individual to build their own profile within the industry and to refine their public speaking skills. The ideal candidate should be comfortable at collaborating with an integrated team to develop outstanding training solutions.\\n\\nCandidates must hold relevant cloud certifications, or be in a position to have passed them prior to joining the team.\\nResponsibilities\\nResponsibilities\\nDeliver Google Cloud Certification courses both in the classroom and online\\nHelp to aggressively grow the range of related courses.\\nMaintain and evolve existing classroom course content and respond to post-training questions\\nThrough the delivery of great training, encourages repeat bookings onto other courses\\nWork with Jellyfish Agency to guide our clients and internal teams on cloud best practices\\nSupport new business opportunities\\nHelp create marketing materials, assets and other promotional material\\nQualifications\\nKnowledge, Skills &amp; Experience Requirements\\nExperience producing applications/products ideally cloud based products\\nComfortable presenting and delivering multiple-day-long training session both in the classroom and online\\nComfortable in training enterprise level clients on how to develop or diversify their current skill set\\nBasic proficiency with command-line tools and Linux operating system environments\\nSystems Operations experience, including deploying and managing applications, either on-premises or in a public cloud environment\\nComfortable and confident in at least one of the following languages and technologies:\\nPython\\nSQL\\nC++\\nJava\\nPHP\\nRuby\\nGO\\nNode.JS\\n.NET\\nProficient in building data pipelines using programming models like Apache Beam\\nExperience working with HADOOP\\nFamiliar with numerical libraries and popular machine learning frameworks such as scikit-learn or tensorflow\\nCertified on one of the following:\\nGoogle Cloud Platform\\nAmazon Web Services\\nMicrosoft Azure\\nSkill Requirements\\nCan clearly communicate processes, strategies and general requirements to both internal and external stakeholders.\\nCapable of explaining complex concepts in simple terms\\nGather delegate requirements and integrating them with the course learning outcomes\\nFacilitate user led learning through practical workshops\\nAbility to promote user interactions and engagement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nAt least 4-5 years of experience in Scala/Java or Python programming\\nAWS data products (Data pipelines, Redshift, Pinpoint, S3, etc)\\nExperience with recognized industry patterns, methodologies, and techniques\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Skillz:\\nSkillz is driving the future of entertainment by accelerating the convergence of sports, video games and media for an exploding mobile-first audience worldwide. The company's platform empowers mobile game developers and players with democratized access to fun, fair and skill-based competition for real prizes ( https://venturebeat.com/2019/01/31/skillz-top-10-mobile-esports-players-won-8-million-in-2018/ ), shifting the paradigm to make eSports accessible to anyone, anywhere with a mobile device.\\n\\nSkillz helps developers build multi-million dollar game franchises by turning content into competitive social gaming properties for the world's 2.6 billion gamers. The company has already worked with 13,000 game developers, leveraging its patented technology to host over 800 million tournaments for 18 million players worldwide.\\n\\nThis year, Skillz was recognized as one of Fast Company's Most Innovative Companies ( https://www.entrepreneur.com/slideshow/313595 ) and CNBC Disruptor 50 ( https://www.cnbc.com/2019/05/14/skillz-2019-disruptor-50.html ) (for the second time ( https://www.cnbc.com/2017/05/16/skillz-2017-disruptor-50.html )). In 2018, Skillz was listed as one of Forbes' Next Billion-Dollar Startups ( https://www.forbes.com/next-billion-dollar-startups/#6334ab494441 ) and Entrepreneur Magazine's 100 Brilliant Companies ( https://www.entrepreneur.com/slideshow/313595 ). In 2017, Inc. Magazine ranked Skillz the No. 1 fastest-growing private company in America ( https://www.entrepreneur.com/slideshow/313595 ).\\n\\nThe company is backed by leading venture capitalists, media companies, and professional sports luminaries, ranging from Liberty Global, Accomplice, Wildcat Capital, Telstra Ventures, and a founder of Great Hill Partners to the owners of the New England Patriots, Milwaukee Bucks, New York Mets, and Sacramento Kings.\\n\\nWho we're looking for:\\nYou're ready to take the next step in your Data Engineering career - to a fast-moving, successful company building out their next-generation streaming analytics infrastructure! You love data consistency and integrity. You consider yourself scrappy and a technologist, passionate about data infrastructure... with your attention to detail and insistence on doing things correctly, you know you can make a big impact on a small team! You're an excellent communicator and know that you grow faster from being able to mentor others.\\n\\nWhat You'll Do:\\n\\nBuild new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture\\nBuild enterprise grade data lake to support both business analytical needs and next generation data infrastructure\\nBuilding data integration toolkit for backend services\\nSupport our data science team in deploying new algorithms for matchmaking, fraud and cheat detection\\nFind better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people\\nImprove monitoring and alarms that impact data integrity replication lag\\nSupport our product development team in creating new events to measure/track\\n\\nYour Skillz:\\nBasic Qualifications:\\n\\nAt least 4-5 years of experience in Scala/Java or Python programming\\nAWS data products (Data pipelines, Redshift, Pinpoint, S3, etc)\\nExperience with recognized industry patterns, methodologies, and techniques\\n\\nBonus\\n\\n\\nFamiliarity with Agile engineering practices\\n4+ years of experience with Spark, Scala and/or Akka\\n4+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies\\n2+ years of experience working with Kafka or similar data pipeline backbone\\n4+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python\\n3+ years' experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus)\\nAt least 4-5 years of experience with Unix/Linux systems with scripting experience\\nFamiliarity with Alooma, Snowflakes\\nFamiliarity with Kinesis, Lamda\\nPrior experience in gaming\\nPrior experience in finance\\n\\nSkillz embraces diversity and is proud to be an equal opportunity employer. As part of our commitment to diversifying our workforce, we do not discriminate on the basis of age, race, sex, gender, gender identity, color, religion, national origin, sexual orientation, marital status, citizenship, veteran status, or disability status, and we operate in compliance with the San Francisco Fair Chance Ordinance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Oakland, CA</td>\n",
       "      <td>Oakland</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Recent years have made us all too familiar with the destruction that natural disasters can bring about, and the increasing frequency and intensity with which they are occurring. Every year we are faced with the devastation and harsh impact that wildfires, hurricanes, and floods have on millions of lives. Despite mounting losses, conventional methods of risk modeling at best paint an incomplete picture of these threats, and at worst, get them completely wrong. Better outcomes require better preparedness. Better preparedness requires better modeling, which requires better data and better approaches to working with that data.\\n\\nZesty.ai uses novel data gathering and machine learning approaches to produce higher quality information about the risks to property from catastrophes like floods and wildfires. While AI alone may not be able to thwart these disasters, it can help us be more prepared for them, and ultimately that will lead to better outcomes.\\n\\nWe have partnered with leading insurance carriers and reinsurers to underwrite risk more accurately, provide customers a smoother purchasing experience and manage inspections more cost-effectively.\\n\\nAs a Senior Data Engineer, you have strong interest/experience working with remote sensing and imagery. You will be building the platform that delivers processed data and imagery to our machine learning systems. You will be doing advanced imagery manipulation and GIS processing. You thrive in a collaborative, creative environment that moves fast and are comfortable learning new skills and working with unfamiliar technologies.\\n\\nAre you ready to help us change the world?\\n\\nWhat You’ll Do:\\nWorking on data cleansing, batch processing, data transformations, and other data manipulations to enable our data science efforts\\nAdvanced image manipulation\\nGeometry/geographic calculations\\nServing as part of the core team for the technology stack\\nPartnering closely with the Founders to bring a disruptive AI based technology platform to the insurance and real estate markets\\nInvestigating and prototyping new technologies\\nWhat You Bring to the zesty.ai Team:\\nBachelor’s Degree is required\\nAt least 2 years of employment as a data engineer in a professional setting, or other relevant development experience\\nExpert in python\\nExpert working with cloud platforms (AWS, Google Cloud, etc)\\nExperience with Airflow or other workflow management software\\nAbility to define data model and data storage strategies, including knowledge of distributed data systems\\nAbility to manage multiple/competing priorities and make the right tradeoffs and timely delivery of features\\nExperience or familiarity with geography, geometry and GIS systems\\nExperience working with satellite/remote imagery\\nRelevant education (Coding Bootcamp, and/or Bachelors in Computer Science) or equivalent experience\\nMust be legally eligible to work in the U.S.\\nWhy Zesty.ai:\\n\\nBe part of a well-funded early-stage start-up\\nMarket competitive comp and equity incentives to give you a stake in our future\\nMedical, Vision and Dental for you and your dependents\\nPre-tax commuter &amp; parking benefits\\nFlexible Time Off\\nAn upbeat and collaborative work culture\\nCompany-sponsored outings\\nFree One Medical membership\\n\\n\\n\\nAll your information will be kept confidential according to EEO guidelines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Consulting Data Engineer</td>\n",
       "      <td>San Francisco, CA 94105</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94105</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At Ravel we develop the legal profession’s most innovative products for legal analytics and research, using the latest in visualization and text mining. Lawyers use our products to forecast how judges will rule, find critical cases, and make data-driven decisions. We're working with great technology and the challenges that get us fired up involve large-scale heterogeneous text and data mining, beautiful UI, and machine learning. Industry leader LexisNexis acquired Ravel in 2017.\\n\\n\\nWhat you will be doing:\\n\\nBuild and scale data infrastructure that powers batch and real-time data processing of billions of records\\nDesign and build scalable data ingestion and enrichment pipelines (machine learning inference)\\nAutomate and handle life-cycle of the systems and platforms that process our data.\\nProvide visibility into the health of our data platform (comprehensive view of data flow, resources usage, data lineage, etc)\\nEvolve maturity of our data quality and monitoring systems\\nMentor fellow teammates on algorithms, data structures, design patterns, and best practices\\n\\nWhat Are We Looking For:\\n7+ years of Software Engineering experience\\nBachelor’s degree in computer science or equivalent practical experience\\nPassion for big data and creative ideas for what to do with it\\nExpertise in any of the following programming language: Java, Scala, C++, Python\\nExperience working with data technologies that power analytics (e.g. Hadoop, Spark, Kafka, etc). Experience with SQL and/or No-SQL Experience with any cloud solutions (AWS, GCP, Azure)\\nBonus:\\nExperience working with Elasticsearch\\nExperience working with Neptune DB\\nUnderstanding of machine learning\\n\\nLexisNexis Legal &amp; Professional (www.lexisnexis.com) is a leading global provider of content and technology solutions that enable professionals in legal, corporate, tax, government, academic and non-profit organizations to make informed decisions and achieve better business outcomes. As a digital pioneer, the company was the first to bring legal and business information online with its Lexis Nexis services. Today, LexisNexis Legal &amp; Professional harnesses leading-edge technology and world-class content, to help professionals work in faster, easier and more effective ways. Through close collaboration with its customers, the company ensures organizations can leverage its solutions to reduce risk, improve productivity, increase profitability and grow their business. Part of RELX Group plc, LexisNexis Legal &amp; Professional serves customers in more than 100 countries with 10,000 employees worldwide. LexisNexis, a division of RELX Group, is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact 1.877.734.1938 or accommodations@relx.com.\\nRSRLNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Data Strategy Specialist - Business &amp; Data Analysis, Cloud, AWS, Azure, Big Data</td>\n",
       "      <td>San Francisco, CA 94105</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94105</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe North America Data Strategy &amp; Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.\\n\\n Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model\\n\\nRequired Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:\\no Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling\\no Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.\\no Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel\\n Able to travel up to 100% (Mon-Thu)\\n\\nOptional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model\\n Experience in compiling business cases and roadmaps for data, analytics and technology investments\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Staff Big Data Engineer (Java, Spark, Kafka, AWS) - Data Science team</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Ancestry:\\nWhen you join Ancestry, you join our family tree. Backed by history, science, and technology, we’re creating a new world of connection, innovation, and understanding. Whether it’s reuniting long-lost relatives through DNA or unearthing new family stories from historical records, Ancestry empowers life-changing experiences. With over 20 billion digitized historical records, 100 million family trees, and 15+ million DNA kits sold, Ancestry is bringing the power of personal discovery to people around the world.\\n\\nAncestry is seeking a Staff level Software Developer to join our Big Data as a Service team. In this role, you will deliver products and services to help our Data Science and Product Analytics teams to facilitate research to improve our Ancestry customers experience. As such you will play a key role in engineering a secure, scalable and highly available platform used by our internal scientists using a variety of technologies such as AWS EMR, Spark, Kafka, Kinesis, Kubernetes, Docker and many more.\\n\\nWho you are (at a minimum):\\n7+ years of experience in software development (at a minimum, in Java &amp; preferred to also have Scala)\\n2+ years of experience working with large scale distributed frameworks (Spark and/or MapReduce)\\n2+ years of experience with cloud (AWS and cloud provisioning frameworks such as Terraform and orchestration tools such as Airflow will be given priority)\\nHas demonstrated understanding of SOLID principles, and can work effectively in an Agile environment\\nHas demonstrated technical leadership abilities, leading the team to increase quality, and helps promote team discussions for maximum results\\nOne who has been a lead or who has taken the lead for a concept-through completion system/platform build.\\nTakes responsibility for scrum team results\\nGuides the team to solve highly scalable issues, and guides team to maximize productivity/risk reductions\\nProvides guidance for the team, and has ability to interact with product owner and other technical leaders through entire product lifecycle\\nBachelor's in Computer Science or Computer Engineering is required. Master's degree in computer science, computer engineering, or other technical disciplines, or equivalent work experience, is preferred.\\n\\nWill be given priority to those who have any &amp; all of the following:\\nWorking knowledge of streaming engines, such as Kafka or Kinesis.\\nFamiliarity with contemporary data science workflows and techniques is a huge plus.\\n\\n#bigdata #kafka #mapreduce #streamingdata #scala #java #search #terraform #distributedsystems #kinesis #aws\\n\\nGD-Sponsored\\nIND1\\n#LI-AH1\\n\\nAdditional Information:\\nAncestry is an Equal Opportunity Employer that makes employment decisions without regard to race, color, religious creed, national origin, ancestry, sex, pregnancy, sexual orientation, gender, gender identity, gender expression, age, mental or physical disability, medical condition, military or veteran status, citizenship, marital status, genetic information, or any other characteristic protected by applicable law. In addition, Ancestry will provide reasonable accommodations for qualified individuals with disabilities.\\n\\nAll job offers are contingent on a background check screen that complies with applicable law. For San Francisco office candidates, pursuant to the San Francisco Fair Chance Ordinance, Ancestry will consider for employment qualified applicants with arrest and conviction records.\\n\\nAncestry is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at Ancestry via-email, the Internet or in any form and/or method without a valid written search agreement in place for this position will be deemed the sole property of Ancestry. No fee will be paid in the event the candidate is hired by Ancestry as a result of the referral or through other means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Visualization &amp; Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Samsara\\n-------------\\n\\nSamsara, founded in 2015, is a leader in Industrial IoT and our mission is to increase the efficiency, safety, and sustainability of the operations that power our economy. Our solutions combine hardware and software to bring real-time visibility, analytics, and AI to operations across various industries. Samsara's fast-growing team is headquartered in San Francisco, with offices in San Jose, Atlanta, and London. Our team has raised over $530M from Andreessen Horowitz, General Catalyst, Tiger Global, and Dragoneer.\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nSamsara is growing fast… like, really fast! To support our growth, we are standing up a new analytics &amp; operations capability to solve some of Samsara's most pressing business problems with a data-first mindset. The team will tackle cross-functional projects across People Ops, Customer Success, Recruiting, Engineering, Sales, and more with the core mission of unlocking real value through business acumen, data, and a bias toward action. Examples of a few cross-functional strategic projects you will work on include: how can Samsara optimize its recruiting operations as we hire another 1000 employees? Where should Samsara open its next office? How can we enable our customer success team to manage 10K+ customer accounts at scale?\\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nAbout the role\\n--------------\\n\\nAre you an entrepreneurially-minded analytics expert looking to unlock business value through data? Can you manipulate large, disparate datasets and package up your output in easy-to-digest BI dashboards? We are looking for a talented Visualization &amp; Data Engineer to solve Samsara's most pressing business problems as part of a newly formed analytics &amp; operations capability.\\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nThe Visualization &amp; Data Engineer will be a core technical contributor to the team with deep expertise in combining disparate data sources (e.g., via SQL and API calls) and building accessible dashboards (e.g., in Tableau) to drive business outcomes.\\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nThe Visualization &amp; Data Engineer will work on:\\n---------------------------------------------------\\n\\n\\nWriting SQL queries and API calls to merge cross-functional datasets\\n\\n--------------------------------------------------------------------------\\n\\n\\nDeveloping automated processes to clean, structure, and store data\\n\\n------------------------------------------------------------------\\n\\n\\nConducting quantitative analyses to solve business problems (e.g., likelihood to churn models to prioritize customer account management)\\n\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\\n\\nCreating dashboards for management / operational teams (e.g., Tableau visualizations)\\n\\n-------------------------------------------------------------------------------------\\n\\nAn ideal candidate has:\\n-----------------------\\n\\n\\nStrong proficiency in SQL and accessing data via API calls\\n\\n----------------------------------------------------------\\n\\n\\nExperience with data manipulation/processing, preferably in Python (e.g., with Pandas)\\n\\n--------------------------------------------------------------------------------------\\n\\n\\nExperience with data visualization, preferably in Tableau\\n\\n---------------------------------------------------------\\n\\n\\n2+ years experience as a data scientist, data engineer, or related role\\n\\n-----------------------------------------------------------------------\\n\\n\\nPreferred: Experience with statistical analysis and machine learning\\n\\n--------------------------------------------------------------------\\n\\n\\nPreferred: BA / MS degree in Computer Science, Statistics, or related discipled\\n\\n-------------------------------------------------------------------------------\\n\\n------\\n\\nAt Samsara, we welcome all. All sizes, colors, cultures, sexes, beliefs, religions, ages, people. We depend on the unique approaches of our team members to help us solve complex problems. We are committed to increasing diversity across our team and ensuring that Samsara is a place where people from all backgrounds can make an impact.\\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nThe position would primarily work with the statistics and engineering teams, along with day to day communication with the product and marketing team.\\nDesign and develop AWS systems to collect and manage business events at scale.\\nRespond to request from the product, engineering and marketing team for timely data analysis.\\nParticipate in multi-team planning for future data collection needs.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nStrives to deliver clean, maintainable and testable code\\nHas a passion for user privacy\\nIs open to learning new languages and frameworks\\nHas a soft spot for JavaScript\\n</td>\n",
       "      <td>We are looking for an experienced data engineer to be responsible for the design, development and maintenance of large data collection and processing systems at Brave. This position will focus on scaling our backend systems to handle the increased load of data collection, processing and presentation.\\n\\n-----------------\\n\\nResponsibilities:\\n-----------------\\n\\n\\nThe position would primarily work with the statistics and engineering teams, along with day to day communication with the product and marketing team.\\nDesign and develop AWS systems to collect and manage business events at scale.\\nRespond to request from the product, engineering and marketing team for timely data analysis.\\nParticipate in multi-team planning for future data collection needs.\\n\\n-------------\\n\\nRequirements:\\n-------------\\n\\nStrong AWS skills with extensive knowledge of the data collection, storage and processing systems that AWS provides. Experience building ETL pipelines at scale.\\n\\nFamiliarity with Redshift, Kinesis, Athena, Glue, Data Pipeline, MongoDB, Postgresql.\\n\\nStrong JavaScript and SQL skills.\\n\\nExtensive knowledge of modern open source software develop practices and tools i.e. Git, Github, Unit and integration testing, pull requests, code review etc…\\n\\nAdditional qualities:\\n\\nStrives to deliver clean, maintainable and testable code\\nHas a passion for user privacy\\nIs open to learning new languages and frameworks\\nHas a soft spot for JavaScript\\n\\nBenefits\\n\\n\\nCompetitive salary\\n4 weeks (20 days) of paid vacation per year\\nExcellent medical coverage\\nGenerous 401k plan\\nStock option grant\\nTravel and conference budgets\\nCommuters benefit (On­site only)\\nHip office in the SoMA neighborhood of SF\\n\\nCandidates must be legally authorized to work in the United States or Canada.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Data Engineer, Data Solutions</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Affirm is reinventing credit to make it more honest and friendly, giving consumers the flexibility to buy now and pay later without any hidden fees or compounding interest.\\n\\nAs a Data Engineer, you will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data model. You will architect, design, implement data pipelines enabling insights from our product for partners, data scientists, analysts and decision across Engineering, Product, Marketing, Product and Finance organizations.\\n\\nData engineers also need to guarantee compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data models and pipelines. This role will require collaboration across the company.\\nThis is a fun role for problem solvers, who can intuitively anticipate problems and can also look beyond immediate issues. They will be a self-starter, detail-oriented and quality-oriented and passionate about taking initiatives.\\nWhat You'll Do\\nDesign, implement and build data models and ETL pipelines that deliver quality data meeting SLA requirements.\\nPartner with product, data analyst, engineering teams and other data engineers to build foundational, trusted and documented datasets enabling self-service and Single Source of Truth.\\nBe an advocate for data governance, security, privacy, quality and retention.\\nOwn and document data pipelines and data lineage.\\nIdentify, document and promote best practices.\\nIdentify and work with Infrastructure teams to address Data Platform gaps.\\nWhat We Look For\\nBS, MS or PhD in Computer Science, Engineering or a related technical field.\\nExperience in data management disciplines including data modeling, data quality and other areas directly relevant to data engineering.\\nExperience working with cross-functional teams and collaborating with business or product or engineering stakeholders in data management and analytics initiative.\\nAbility to work on multiple areas like ETLs, Data modeling &amp; design, writing complex SQL queries etc.\\nPassionate about various technologies including but not limited to SQL/No SQL/MPP databases etc.\\nHands-on experience with Data Warehouse technologies (Snowflake, Redshift) and Big Data technologies (e.g Spark)\\nProficiency with programming languages is a big plus (e.g. Python)\\nExcellent written and verbal communication and interpersonal skills; able to effectively collaborate with technical and business partners.\\nAt Affirm, \"People Come First\" is a core value and that’s why diversity and inclusion are vital to our priorities as an equal opportunity employer. You can learn more about our D&amp;I efforts here.\\n\\nWe also consider qualified applicants with arrest and conviction records for positions in accordance with applicable laws, including the San Francisco Fair Chance Ordinance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "      <td>Redwood City</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.\\n\\nSenior Data Engineer\\n\\nPosition Summary\\nAt Auction.com, we are embarking on a journey to transform the real estate market with technological innovations. A critical prerequisite for this transformation is a robust data infrastructure. As a senior data engineer, you will help us design and implement our big data environment that is real-time, stable and scalable. You will work with a talented data engineering team to improve our data processing pipeline and developing new capabilities to support mission-critical initiatives. You will mentor junior engineers and help evaluating new technology along the way. You impact will be felt across the team as well as the entire Auction.com organization.\\n\\nResponsibilities/Duties\\n\\nMake major contribution to the implementation of our real time big data initiative\\nBuild and automate productized data processing pipelines in AWS big data platform\\nHelp improve our development process and standards through mentoring and leading-by-example\\nHelp define and implement data ingestion contracts\\nCreate data environment to support our data analytics, reporting and data science teams\\n\\nKnowledge, Skills and Abilities\\n\\nIn-depth understanding of modern big data technology, including Hadoop and Spark\\nKnowledge of real time data streaming and aggregation architectural patterns and practice\\nProficient in programming languages such as Python, Scala and Java\\nFamiliarity with NoSQL as well as SQL databases\\nData modeling and machine learning skill is a plus\\n\\nEducation/Experience\\n\\nBachelor's Degree in computer science, data science or related fields\\nFamiliarity with agile developmental process\\nPrevious experience developing data product required\\nHands-on experience with real time data streaming, aggregation and presentation strongly preferred\\nPrevious experience with production ETL pipeline development required\\nAt least a years’ experience with AWS cloud or another cloud platform\\n\\nTo all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Staff Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Komodo Health is addressing the global burden of disease through the world's most actionable healthcare map. Our solutions drive a more transparent, efficient and productive healthcare ecosystem. We value our culture of encouraging growth, collaboration, and constructive debate as well as delivering innovative solutions that \"wow\" our customers.\\n\\nThe Data Engineering (DE) team is looking for Staff Data Engineers to help us lead projects that will further our tech stack related to data, helping us develop data processing that is scalable, reliable and automated.\\n\\nThis is an opportunity to join a growing company, and be a part of a team of folks accomplished in diverse Engineering disciplines; focused on using the best of what lies at the forefront of technology and skills to address complex, real-world problems in the Healthcare and Life Science space. Some of the tools we use are: Python, Spark, AWS, Kubernetes, Docker, Postgres, Git, Airflow and Flask.\\n\\nRESPONSIBILITIES\\n----------------\\n\\n\\nDesign, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources.\\nCreate automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.\\nEvaluate new technologies for continuous improvements in Data Engineering.\\nCollaborate closely with the product team to build out new data features.\\nWork with the data scientists to implement descriptive, forecasting and predictive algorithms and models using latest technologies.\\n\\nREQUIREMENTS\\n------------\\n\\n\\nBS, MS in CS or related degrees.\\n5+ years of relevant experience with data engineering or similar experience.\\nExperience with...\\nBuilding and deploying large-scale data processing pipelines.\\nPython, Scala or Java coding experience, proficiency with at least one of them is required.\\nWorkflow &amp; pipeline systems, like Airflow, Luigi, etc.\\nDistributed systems for data processing tools such as Spark, Hadoop, Kubernetes, etc.\\nSQL and Relational Databases, like PostgreSQL.\\nContinuous integration and automation tools and processes, like Jenkins.\\nYou've been through the planning, launching and refactoring phases of code you wrote.\\nA serious passion for data.\\nHistory of excellence and responsibility in previous engineering positions.\\nAbility to work as part of a collaborative team in a fast-paced environment.\\nSincere interest in working at a startup and scaling with the company as we grow.\\n\\nNICE TO HAVE\\n\\n\\nExperience or interest in the life sciences industry.\\nBasic knowledge (college level or hands on) of machine learning and statistics.\\nExperience with graph databases.\\n\\nBENEFITS\\n--------\\n\\n\\nCompetitive salary and equity compensation\\nFull Medical, Dental, and Vision benefits\\n401k plan with employer match\\nFlexible hours and a \"use as you need\" vacation policy\\nOpportunities to attend industry conferences and events\\nGreat office location(s) in SF/SOMA and NY/FLAT IRON\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Senior Data Engineer, you will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data Warehouse model. You will be working in our Business Technology Data and Analytics function and have an opportunity to build a sound data foundation and processes that will scale with the company’s growth. You will architect, design, implement data pipelines enabling insights from our Product and Corporate Systems for key partners, data scientists and decision makers at Slack. You would also be responsible for enhancing and maintaining our data warehouse.\\n\\nData engineers also need to guarantee compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines. This would enable faster data access, integrated data reuse and vastly improved time-to-solution for Slack’s data and analytics initiatives. The newly hired data engineer will be the key interface in operationalizing data and analytics on behalf of the Finance, Sales, Product and organizational outcomes. This role will require both creative and collaborative working with IT and the wider business.\\n\\nThis is a fun role for problem solvers, who can intuitively anticipate problems and can also look beyond immediate issues. In short, we look for people who take pride in the craft and want to be part of creating and defining the teams operating model and contribution to the company. They will be a self-starter, detail and quality oriented, and passionate about having a huge impact at Slack. If this role has your name written all over it, please contact us with a resume so that we explore further.\\n\\nResponsibilities\\n\\nDesign, implement and build pipelines that deliver data with measurable quality under the SLA\\nPartner with Data Engineers, Data architects, domain experts, data analysts and other teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service\\nBe a champion of the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements\\nOwn and document data pipelines and data lineage\\nIdentify, document and promote best practices\\nSupport and Maintain analytics tech ecosystem (data warehouse, ETL and BI tools)\\nRequirements\\n\\nBS or MS degree in Computer Science or Engineering discipline.\\nAt least 8 years or more of work experience in data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.\\nAt least 4 years of experience working in cross-functional teams and collaborating with business stakeholders in Sales or Finance in support of a departmental and/or multi-departmental data management and analytics initiative.\\nVery strong experience in dimensional modeling, supporting data warehouse, scaling and optimizing, performance tuning and ETL pipelines\\nDeep understanding of relational as well as big data setup. Preferred: Prior experience with Snowflake, ETL tools (eg Informatica, Matillion, Snaplogic), Hive, Presto, Dimensional Modeling.\\nProblem solver with excellent interpersonal skills with ability to make sound complex decisions in a fast-paced, technical environment.\\nAbility to work on multiple areas like Data pipeline ETL, Data modeling &amp; design, writing complex SQL queries etc.\\nCapable of planning and executing on both short-term and long-term goals individually and with the team.\\nPassionate about various technologies including but not limited to SQL/No SQL/MPP databases etc.\\nHands-on experience with Data Warehouse technologies (Snowflake, Redshift) and Big Data technologies (e.g Hadoop, Hive, Spark)\\nProficiency with programming languages is a big plus (e.g. Python)\\nExcellent written and verbal communication and interpersonal skills, able to effectively collaborate with technical and business partners\\nExcellent understanding of trade-offs\\nDemonstrated ability to navigate between big-picture and implementation details\\nSlack is a layer of the business technology stack that brings together people, data, and applications – a single place where people can effectively work together, find important information, and access hundreds of thousands of critical applications and services to do their best work. From global Fortune 100 companies to corner markets, businesses and teams of all kinds use Slack to bring the right people together with all the right information. Slack is headquartered in San Francisco, CA and has ten offices around the world. For more information on how Slack makes teams better connected, visit slack.com.\\nEnsuring a diverse and inclusive workplace where we learn from each other is core to Slack’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a pleasant and supportive place to work.\\nCome do the best work of your life here at Slack.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience building and maintaining large-scale analytics systems;\\nExperience in software development and delivery;\\nExpertise in one or more programming languages such as Python, Scala, Java, Javascript, etc. and related frameworks (Flask, Django, Spring, AngularJS, React).\\nGood knowledge of architecting, developing, and maintaining of cloud technologies (AWS, Azure, GCP)\\nExperience with Big Data stack (Kafka, Hadoop, Storm, Spark, Hive, Airflow, ElasticSearch, TensorFlow);\\nExperience with relational (Postgres, MySQL) and/or non-relational (Cassandra, MongoDB) databases;\\nStrong machine learning or stats knowledge a plus;\\nExperience in Supply Chain / Retail / eCommerce;\\nExcellent communication skill and a strong team player.\\nBS/MS in Computer Science, Computer Engineering or related technical discipline;\\nStrong analytical and quantitative problem-solving ability.\\n</td>\n",
       "      <td>\\nExperience building and maintaining large-scale analytics systems;\\nExperience in software development and delivery;\\nExpertise in one or more programming languages such as Python, Scala, Java, Javascript, etc. and related frameworks (Flask, Django, Spring, AngularJS, React).\\nGood knowledge of architecting, developing, and maintaining of cloud technologies (AWS, Azure, GCP)\\nExperience with Big Data stack (Kafka, Hadoop, Storm, Spark, Hive, Airflow, ElasticSearch, TensorFlow);\\nExperience with relational (Postgres, MySQL) and/or non-relational (Cassandra, MongoDB) databases;\\nStrong machine learning or stats knowledge a plus;\\nExperience in Supply Chain / Retail / eCommerce;\\nExcellent communication skill and a strong team player.\\nBS/MS in Computer Science, Computer Engineering or related technical discipline;\\nStrong analytical and quantitative problem-solving ability.\\n</td>\n",
       "      <td>\\nPartner in building the infrastructure required for optimal extraction, transformation, visualization, and loading of data from a wide variety of data sources using SQL and big data technologies;\\nImprove our Machine Learning systems by contributing to all phases of algorithm development including ideation, prototyping, design and production;\\nScale our data processing pipelines to handle and maintain complex processes in an efficient and reliable way that are available, scalable, and fault tolerant';\\nBuild scalable production systems for data collection, data transformation, feature extraction, model training, and scoring, using distributed software tools;\\nWork with data science to define data ingestion standards and assist with data-related technical issues;\\nPartner with product development team to understand various opportunities and use cases;\\nMaintain specifications and metadata; create and maintain documentation;\\nBuild Communities-of-Practice in key data technologies.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Avenue Code is the leading software consultancy focused on delivering end-to-end development solutions for digital transformation across every vertical. We’re privately held, profitable, and have been on a solid growth trajectory since day one. We care deeply about our clients, our partners, and our people. We prefer the word ‘partner’ over ‘vendor’, and our investment in professional relationships is a reflection of that philosophy. We pride ourselves on our technical acumen, our collaborative problem-solving ability, and the warm professionalism of our teams.\\n\\nAbout the Opportunity:\\nWe are seeking an energetic and talented Data Scientist Engineer to deliver high value, high-quality business capabilities to our data technology platform. With a background in both software development and machine learning, you will collaborate with software engineers, data scientists and domain experts in Supply Chain and Inventory management to develop data and machine learning products to support our company. You will be an integral member engineering team delivering across multiple business functional areas. You will build data analysis infrastructure for effective prototyping and visualization of various data-driven approaches.\\n\\nKey Responsibilities:\\n\\nPartner in building the infrastructure required for optimal extraction, transformation, visualization, and loading of data from a wide variety of data sources using SQL and big data technologies;\\nImprove our Machine Learning systems by contributing to all phases of algorithm development including ideation, prototyping, design and production;\\nScale our data processing pipelines to handle and maintain complex processes in an efficient and reliable way that are available, scalable, and fault tolerant';\\nBuild scalable production systems for data collection, data transformation, feature extraction, model training, and scoring, using distributed software tools;\\nWork with data science to define data ingestion standards and assist with data-related technical issues;\\nPartner with product development team to understand various opportunities and use cases;\\nMaintain specifications and metadata; create and maintain documentation;\\nBuild Communities-of-Practice in key data technologies.\\n\\nTechnical Skills/Qualifications:\\n\\nExperience building and maintaining large-scale analytics systems;\\nExperience in software development and delivery;\\nExpertise in one or more programming languages such as Python, Scala, Java, Javascript, etc. and related frameworks (Flask, Django, Spring, AngularJS, React).\\nGood knowledge of architecting, developing, and maintaining of cloud technologies (AWS, Azure, GCP)\\nExperience with Big Data stack (Kafka, Hadoop, Storm, Spark, Hive, Airflow, ElasticSearch, TensorFlow);\\nExperience with relational (Postgres, MySQL) and/or non-relational (Cassandra, MongoDB) databases;\\nStrong machine learning or stats knowledge a plus;\\nExperience in Supply Chain / Retail / eCommerce;\\nExcellent communication skill and a strong team player.\\nBS/MS in Computer Science, Computer Engineering or related technical discipline;\\nStrong analytical and quantitative problem-solving ability.\\n\\nPursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records\\n\\nDoes this sound like you?\\nApply now to become an Avenue Coder!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco Bay Area, CA</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Location: Santa Clara or San Francisco\\n\\nUpwork ($UPWK) is the world's largest freelancing website. Each year $1.7 billion of work happens through Upwork, allowing businesses to get more done and helping professionals break free of traditional time and place boundaries and work anytime, anywhere on projects they love. At Upwork, you'll help build on this momentum. Together, we'll create economic and social value on a global scale, providing a trusted online workplace for businesses to connect with extraordinary talent and work without limits.\\n\\nOur Data Management team is tasked with building and supporting the business analytics infrastructure at Upwork that analysts and partners use on a daily basis to make critical business decisions. We are looking for a passionate engineer who has a love of data technologies to empower our teams in leveraging data to drive and measure impact. In this role, you will develop our next generation data infrastructure to support our future business growth.\\n\\nYour responsibilities:\\nBased on Upwork’s unique ecosystem and data needs, partner with our architects to research new cloud-based data technologies and conduct PoC to make recommendations suitable for our overall blueprint of data infrastructure\\nDevelop and maintain our end-to-end data platform, from data acquisition, real-time data stream, to data lake, enterprise data warehouse and front-end data tools and applications.\\nCreate and evolve a data platform that enables your peer data management engineering teams to create data products that serve both internal business units and external clients and partners to maximize the value generated from our data assets in a user-friendly, self-serve, robust, efficient and scalable way.\\nPartner with cross-functional engineering peers to streamline the flow of data in the whole company\\nProvide guidance on development and implementation of data quality rules and validation processes, leading engineering excellence initiatives to improve data quality and issue resolution\\nWhat it takes to catch our eye:\\n8+ years combined experience in DW/BI and Big Data engineering role in medium and large size companies\\nA proven record of building complex data platform from ideation to implementation\\nStrong knowledge and experience of the Big Data ecosystem and Cloud-based solutions\\nExpert in designing, implementing, and operating efficient, scalable, and reliable data transformation pipelines\\nSolid experience in database modeling, architecture, design, and implementation\\nFamiliar with data visualization tools such as Looker, Domo, or similar\\nExperience with marketplace businesses, especially as it pertains to working with large datasets inherent to two-sided marketplaces\\nAble to influence, lead, and communicate effectively across engineering teams, business units and other partners to negotiate priority, scope, and design solution\\nExcellent oral, written, and presentation skills, including the ability to work in person, virtually, and in a globally-staffed environment\\nFluent and effective in working with a globally distributed team\\nDemonstrate a strong understanding of development processes and agile methodologies\\nCome change how the world works.\\n\\nAt Upwork you’ll help shape the future of work. From our offices in San Francisco, Mountain View and Chicago, together we’re creating exciting new opportunities for a world of professionals. You’ll be part of a vibrant culture built on shared values: Inspire a boundless future of work, Put our community first, Have a bias towards action, and Build amazing teams. Along the way you’ll have fun and enjoy the perks of a people-first company: Work from Home Wednesday's, daily breakfast and lunch, regular in-office happy hours, top-notch benefits … and more. Check out Upwork’s spotlight on The Muse for a glimpse of our daily work/life balance.\\n\\nUpwork is proudly committed to recruiting and retaining a diverse and inclusive workforce. As an Equal Opportunity Employer, we never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At Vonage, Business Intelligence is the team working on everything related to data, from ETL and data warehouses all the way to the reporting platform where other teams can build reports and get answers they need from our data.\\n\\nCurrently, we're looking for an experienced Data Engineer, to help us improve data processing systems that handle hundreds of GBs of data every day.\\n\\nWhat will be your role\\n\\nAs a Senior Data Engineer, you'll become a senior member of the Business intelligence team, responsible for designing and building systems for processing the ~500GB of logs generated by the Nexmo API platform every day and loading it into our data warehouse and other systems which consume aggregated data.\\n\\nAs a Senior Data Engineer, you will:\\n\\nDesign and implement data loading and aggregation frameworks and jobs that will be able to handle hundreds of GBs of json files, using Python, Airflow and Snowflake\\nBuild best practice ETLs to transform raw data into easy to use dimensional data for self service reporting\\nImprove our deployment and testing infrastructure within AWS, using tools like Jenkins, Puppet and Docker\\nWork closely with the Product, Infrastructure and Core teams, to make sure data needs are considered during product development and to guide decisions related to data\\nMentor junior team members\\n\\n----------------------------\\nWhat we're looking for\\n----------------------------\\n\\nWe're looking to strengthen our team by adding an experienced ETL / Data engineering professional, who has practical experience building performant data pipelines at scale, modeling data for self-serve use and working in a dynamic, startup-like environment.\\n\\nIdeally, you'll have:\\n\\n5 years+working experience building data processing, ETL and DWH solutions\\nExperience with software development using Python(preferred), Java or Scala\\nExperience working with relational databases (MySQL or Postgres preferred) and DWH technologies (Snowflake and Redshift preferred)\\nExperience working as part of a larger engineering team, within an agile framework and using version control systems\\nFamiliarity with cloud technologies and working with cloud infrastructure (AWS preferred) and exposure to deployment and infrastructure automation would be highly beneficial\\nExposure to distributed data processing systems with tools like Kubernetes, Hadoop, Spark, Kafka or ELK a large plus\\n\\n----------\\nWhy Vonage\\n----------\\n\\nNow acquired and part of Vonage, Nexmo is a global API platform for cloud communications that handles over 90 million requests every day. Customers like Airbnb, Viber, Whatsapp, Snapchat, and many others depend on our APIs and SDKs to connect with their customers all over the world.\\n\\nCompany built by engineers\\n--------------------------\\n\\n\\nNexmo was a tech company, built by engineers at the intersection of cloud and telecommunications and this is reflected everywhere in our culture.\\nEven though we've grown a lot in recent years, we've managed to maintain a startup-like working atmosphere, with very flat hierarchies and a very pragmatic approach to getting things done\\n\\nInteresting engineering challenge\\n---------------------------------\\n\\n\\nWe're in the process of migrating our DWH from Redshift to Snowflake, with a lot of room for re-design and refactoring along the way\\nWe appreciate initiative and ownership, so you'll be encouraged to design, implement and own parts of our data processing system, end to end\\n\\nAgile development using modern technologies\\n-------------------------------------------\\n\\n\\nWe iterate quickly, use the scrum framework, do frequent code reviews and have a very open and pragmatic work culture within the team\\nOur data platform platform is built in the cloud, using technologies like Airflow, Snowflake, Kafka, S3, GCDS, ELK stack and Tableau\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title                    Location  \\\n",
       "0    Data Engineer, Music Data Experience   San Francisco, CA            \n",
       "1    Data Engineer - Sunnyvale, California  San Francisco, CA            \n",
       "2    Data Engineer                          San Francisco, CA            \n",
       "3    Principal Data Engineer                San Francisco, CA            \n",
       "4    Data Engineer                          San Francisco, CA            \n",
       "..             ...                                        ...            \n",
       "198  Staff Data Engineer                    San Francisco, CA            \n",
       "199  Sr. Data Engineer                      San Francisco, CA            \n",
       "200  Data Engineer                          San Francisco, CA            \n",
       "201  Senior Data Engineer                   San Francisco Bay Area, CA   \n",
       "202  Senior Data Engineer                   San Francisco, CA            \n",
       "\n",
       "                       City State         Zip     Country  \\\n",
       "0    San Francisco           CA    None Found  None Found   \n",
       "1    San Francisco           CA    None Found  None Found   \n",
       "2    San Francisco           CA    None Found  None Found   \n",
       "3    San Francisco           CA    None Found  None Found   \n",
       "4    San Francisco           CA    None Found  None Found   \n",
       "..             ...           ..           ...         ...   \n",
       "198  San Francisco           CA    None Found  None Found   \n",
       "199  San Francisco           CA    None Found  None Found   \n",
       "200  San Francisco           CA    None Found  None Found   \n",
       "201  San Francisco Bay Area  CA    None Found  None Found   \n",
       "202  San Francisco           CA    None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Qualifications  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2    \\nMinimum of 3 years of relevant experience in building and architecting data solutions\\nDeep understanding of distributed systems\\nExpert in SQL and high-level languages such as Python, Java, or Scala\\nBuilt and maintained data warehouses and ETL pipelines\\nExperience with Data modeling\\nYou have worked with big data solutions like Redshift, Snowflake, Hadoop or Hive\\nExperience with realtime data streaming infrastructure like AWS Kinesis, Spark or Kafka\\nWorked with Cloud-based architecture such as AWS or Google Cloud                                                                                                                                                                                                                                                                                                                                                                              \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "198  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "199  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "200  \\nExperience building and maintaining large-scale analytics systems;\\nExperience in software development and delivery;\\nExpertise in one or more programming languages such as Python, Scala, Java, Javascript, etc. and related frameworks (Flask, Django, Spring, AngularJS, React).\\nGood knowledge of architecting, developing, and maintaining of cloud technologies (AWS, Azure, GCP)\\nExperience with Big Data stack (Kafka, Hadoop, Storm, Spark, Hive, Airflow, ElasticSearch, TensorFlow);\\nExperience with relational (Postgres, MySQL) and/or non-relational (Cassandra, MongoDB) databases;\\nStrong machine learning or stats knowledge a plus;\\nExperience in Supply Chain / Retail / eCommerce;\\nExcellent communication skill and a strong team player.\\nBS/MS in Computer Science, Computer Engineering or related technical discipline;\\nStrong analytical and quantitative problem-solving ability.\\n   \n",
       "201  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "202  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Skills  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "198  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "199  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "200  \\nExperience building and maintaining large-scale analytics systems;\\nExperience in software development and delivery;\\nExpertise in one or more programming languages such as Python, Scala, Java, Javascript, etc. and related frameworks (Flask, Django, Spring, AngularJS, React).\\nGood knowledge of architecting, developing, and maintaining of cloud technologies (AWS, Azure, GCP)\\nExperience with Big Data stack (Kafka, Hadoop, Storm, Spark, Hive, Airflow, ElasticSearch, TensorFlow);\\nExperience with relational (Postgres, MySQL) and/or non-relational (Cassandra, MongoDB) databases;\\nStrong machine learning or stats knowledge a plus;\\nExperience in Supply Chain / Retail / eCommerce;\\nExcellent communication skill and a strong team player.\\nBS/MS in Computer Science, Computer Engineering or related technical discipline;\\nStrong analytical and quantitative problem-solving ability.\\n   \n",
       "201  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "202  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Responsibilities  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2    \\nBuild and maintain our data warehouse and data pipelines\\nScaling up our data infrastructure to meet business needs\\nDeploy sophisticated analytics programs, machine learning and statistical methods\\nWork cross-functionally with our product, business, finance and engineering teams                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "198  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "199  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "200  \\nPartner in building the infrastructure required for optimal extraction, transformation, visualization, and loading of data from a wide variety of data sources using SQL and big data technologies;\\nImprove our Machine Learning systems by contributing to all phases of algorithm development including ideation, prototyping, design and production;\\nScale our data processing pipelines to handle and maintain complex processes in an efficient and reliable way that are available, scalable, and fault tolerant';\\nBuild scalable production systems for data collection, data transformation, feature extraction, model training, and scoring, using distributed software tools;\\nWork with data science to define data ingestion standards and assist with data-related technical issues;\\nPartner with product development team to understand various opportunities and use cases;\\nMaintain specifications and metadata; create and maintain documentation;\\nBuild Communities-of-Practice in key data technologies.\\n   \n",
       "201  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "202  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "      Education Requirement  \\\n",
       "0    None Found  None Found   \n",
       "1    None Found  None Found   \n",
       "2    None Found  None Found   \n",
       "3    None Found  None Found   \n",
       "4    None Found  None Found   \n",
       "..          ...         ...   \n",
       "198  None Found  None Found   \n",
       "199  None Found  None Found   \n",
       "200  None Found  None Found   \n",
       "201  None Found  None Found   \n",
       "202  None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  FullDescriptions  \n",
       "0    Bachelor’s degree in Data Science, Applied Science, Computer Science, Computer Engineering or related technical discipline3+ years of experience as a data/software developer/scientist or related technical jobExperience with SQL and Spark based data pipelinesExperience with traditional and cloud data modeling techniquesA passion for improving customer experienceExcellent verbal and written communication skills and technical writing skills\\n\\nAmazon Music is awash in data! To help make sense of it all, the Music Data Experience team enables repeatable, easy, in depth analysis of music customer behaviors. We reduce the cost in time and effort of analysis, data set building, model building, and user segmentation. Our goal is to empower all teams at Amazon Music to make data driven decisions and effectively measure their results by providing high quality, high availability data, and democratized data access through self-service tools.\\n\\nIf you love the challenges that come with big data then this role is for you. We collect billions of events a day, manage petabyte scale data on Redshift and S3, and develop data pipelines using Spark/Scala EMR, SQL based ETL, and Java services.\\n\\nYou are a talented, seasoned, and detail-oriented Data Engineer, BI Engineer, or Data Scientist who wants to take on big data challenges in an agile way. Duties include designing events and signals, building big data pipelines, creating efficient data models, performing analysis, and statistical/ML modeling. We manage Amazon Music's most important data pipelines and data sets, and are expanding our self-service data knowledge and capabilities through an Amazon Music data university.\\n\\nThis role requires you to focus on the data end to end, from producers to consumers. You will develop an understanding of our data, analytical techniques, and how to connect insights to the business, and you will gain practical experience in insisting on highest standards on operations in ETL and big data pipelines. With our Amazon Music Unlimited and Prime Music services, and our top music provider spot on the Alexa platform, providing high quality, high availability data to our internal customers is critical to our customer experiences.\\n\\nMusic Data Experience team develops data specifically for a set of key business domains like personalization and marketing and provides and protects a robust self-service core data experience for all internal customers. We deal in AWS technologies like Redshift, S3, EMR, EC2, DynamoDB, Kinesis Firehose, and Lambda. In 2019 this team will migrate Amazon Music's information model and data pipelines to a data lake storage and EMR/Spark processing layer. You'll build our data university and partner with Product, Marketing, BI, and ML teams to build new behavioral events, pipelines, datasets, models, and reporting to support their initiatives. You'll also continue to develop our offline analytics capabilities in Tableau and build out our real time dashboarding capabilities.\\n\\nAmazon Music\\n\\nImagine being a part of an agile team where your ideas have the potential to reach millions. Picture working on cutting-edge consumer-facing products, where every single team member is a critical voice in the decision-making process. Envision being able to leverage the resources of a Fortune-500 company within the atmosphere of a start-up. Welcome to Amazon Music, where ideas are born and come to life as Amazon Music Unlimited, Prime Music, and so much more.\\n\\nEveryone on our team has a meaningful impact on product features, new directions in music streaming, and customer engagement. We are looking for new team members across a variety of job functions including software engineering/development, marketing, design, ops and more. Come join us as we make history by launching exciting new projects in the coming year.\\n\\nOur team is focused on building a personalized, curated, and seamless music experience. We want to help our customers discover up-and-coming artists, while also having access to their favorite established musicians. We build systems that are distributed on a large scale, spanning our music apps, web player, and voice-forward audio engagement on mobile and Amazon Echo devices, powered by Alexa to support our customer base. Amazon Music offerings are available in countries around the world, and our applications support our mission of delivering music to customers in new and exciting ways that enhance their day-to-day lives.\\n\\nCome innovate with the Amazon Music team!\\n\\nGraduate degree in a related technical field5+ years of experience as a data/software developer/scientist or related technical jobExperience with Agile DevelopmentExperience with statistical modeling or machine learning (Classification, Collaborative Filtering)Experience with AWS servicesA love of music!\\nAmazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.\\n\\n#MusicJobs  \n",
       "1    Working at the clients site in Sunnyvale\\nFlexible work hours, full benefits, 401K\\nA day off for your birthday\\nCompetitive total compensation\\nSo, are you ready for a challenge, work in a multi disciplinary team and have an immediate impact on our projects? Well, this role is for you.\\nDon't forget to follow Averna's journey to becoming the best in its field, on YouTube and LinkedIn.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2    About Spin\\n\\nSpin operates electric scooters in cities and campuses nationwide, bringing sustainable last-mile mobility solutions to diverse communities. Recognized for its consistent cooperation and collaboration with cities, Spin partners closely with transportation planners, elected officials, community groups, and university administrators to bring stationless mobility options to streets in a responsible and carefully orchestrated manner.\\n\\nBased in San Francisco, Spin is a diverse team of engineers, designers, urban planners, policymakers, lawyers and operators with experience from Y Combinator, Lyft, Uber, local and federal government, and the transportation advocacy world. Spin was known for launching the first stationless mobility program in Seattle, and has since expanded to become the exclusive electric scooter partner in mid-sized cities like Coral Gables, Florida and Lexington, Kentucky, and one of a few permitted scooter operators in large cities like Denver, Detroit, and Washington, D.C. The team embeds in cities and neighborhoods to understand their specific transportation needs, and hires locally from the community.\\n\\nSpin is expanding quickly and looking for top-tier talent to help us bring affordable and accessible transportation options to cities and define what future safe streets will look like.\\n\\nAbout the Role\\n\\nBeing a data informed company, data helps us create exceptional experience for our customers and provide insights into the effectiveness of our product.\\n\\nWe are looking for Data Engineers that will build and maintain our data warehouse and data pipelines, collect data from multiple sources, and expose services that make data a first class citizen at Spin. You will be building, architecting and launching highly reliable and scalable data pipelines to support data processing and analytics needs. Your efforts will allow access to business and user behavior insights.\\n\\nThe Team\\n\\nOur engineering team consists engineers that are passionate about creating finely polished and intuitive experiences and, at the same time, obsess over performance and reliability of what we build. We challenge the status quo and strive towards finding the best way to solve problems.\\n\\nWe promote being a more well rounded engineer by working on different parts of the engineering stack. We also work in very small groups to keep processes and overhead low, so we have a lot of trust and accountability to perform the work required to build the best product.\\nResponsibilities\\nBuild and maintain our data warehouse and data pipelines\\nScaling up our data infrastructure to meet business needs\\nDeploy sophisticated analytics programs, machine learning and statistical methods\\nWork cross-functionally with our product, business, finance and engineering teams\\nQualifications\\nMinimum of 3 years of relevant experience in building and architecting data solutions\\nDeep understanding of distributed systems\\nExpert in SQL and high-level languages such as Python, Java, or Scala\\nBuilt and maintained data warehouses and ETL pipelines\\nExperience with Data modeling\\nYou have worked with big data solutions like Redshift, Snowflake, Hadoop or Hive\\nExperience with realtime data streaming infrastructure like AWS Kinesis, Spark or Kafka\\nWorked with Cloud-based architecture such as AWS or Google Cloud\\nBenefits & Perks\\nOpportunity to join a fast-growing startup and help shape and establish the company’s industry leadershipCompetitive health benefitsDaily catered lunch in our SF officeUnlimited PTO for salaried rolesCommuter stipend plus pre-tax benefitsMonthly cell phone bill stipendWellness perk for salaried roles\\n\\nSpin is an equal opportunity employer and will not discriminate against any employee or applicant for employment in an unlawful matter. We celebrate diversity and are committed to creating an inclusive environment for all individuals. Spin treats all employees and job applicants on the basis of merit, qualifications, and competence without regard to any qualified individuals' sex, race, color, religion, national origin, ancestry, gender (including pregnancy, breastfeeding, or related medical condition), sexual orientation, gender identity, gender expression, age, physical or mental disability, medical condition, genetic characteristic or information, marital status, military and veteran status, or any other characteristic protected by state or federal law. Spin also considers qualified applicants with criminal histories, consistent with applicable local, state, and federal law.\\n\\nSpin is committed to providing reasonable accommodations for qualified individuals with disabilities in its job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at job_accommodations@spin.pm.                                                                                                                                                                                                                                                                                                                                            \n",
       "3    Varo is on a mission to redefine banking so it's easy for everyone to make smart choices with their money. Our app offers bank accounts and high-yield savings accounts that don’t cost a thing, tools to help you manage your money and save automatically, and invitation-only personal loans at competitive rates. On the contrary, traditional banks charge fees, offer next-to-nothing savings rates, and don’t work with their customer’s best interests in mind.\\n\\nVaro is distinct from other fintechs: With preliminary approval for a bank charter from the Office of the Comptroller of the Currency (OCC), we're on our way to becoming the first mobile-centric national bank in the country. Our unique team combines the best people in tech and banking, and we’re wildly passionate about keeping our customers happy by helping them manage and grow their money. Based in San Francisco and privately held, Varo has raised $178M to date, led by Warburg Pincus and The Rise Fund / TPG Growth.\\n\\nDATA ENGINEERING AT VARO\\n\\nAs a Principal Big Data Engineer, you will play a senior role in implementing a variety of solutions to ingest data into, process data within, and expose data from, a Data Lake that enables our data warehouse, data mart, reporting and data analysts and scientists to use and explore data in an automated or self-service fashion.\\n\\nAs a technical leader, you will take ownership of the data architecture for processing and analyzing data across the Platform.\\nWHAT YOU'LL DO\\nDevelop and maintain data strategy for Varo in terms of capabilities, architecture, and control mechanisms that support company intentions to be a bank\\nDesign, build and maintain Big Data workflows/pipelines to process records into and out of Varo’s lake\\nProvide technical leadership in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics\\nCollaborate to actively gain buy-in from stakeholders at all levels on technology direction\\nWork with business partners on requirements clarification and results from rollout efforts\\nParticipate in developing and enforcing data security & access control policies\\nArchitect effective controls for a resilient data ingestion process\\nSupport application data integration design and build efforts, including real-time capabilities\\nProficiency in Amazon AWS big data technologies including S3, RDS, RedShift, Elasticsearch, Lambda, AWS Glue\\nConduct code reviews in accordance with team processes and standards\\nPREVIOUS EXPERIENCES THAT'LL HELP YOU BE GREAT\\nBachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience\\n10+ years of ETL, data modeling, warehouse and data pipelines experience.\\n5+ years’ experience working within the AWS Big Data/Hadoop Ecosystem (EMR is preferred)\\nExperience developing extract load transform tooling\\nExperience with downstream consumption patterns is a plus (reports, dashboards, API)AWS Glue, Redshift, RDS is a plus\\nExperience in Hadoop, HDFS, Hive, Python, REST API/ SOAP API, Spark2, Oozie WFs is a plus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "4    Hinge Health’s mission is to improve the lives of people suffering from chronic conditions by digitizing the delivery of care - starting with musculoskeletal health. Our vision is to be the world’s most patient-centered Digital Hospital. We’re already achieving remarkable outcomes - helping people overcome chronic pain, avoid surgeries, return to work, and get back to doing the things they love. We've raised close to $37M and our growth shows no sign of slowing.\\n\\nWe are looking for an experienced Data Engineer to manage and scale our growing data assets while maintaining a high level of integrity and precision.\\n\\nOur ideal candidate has an extensive understanding of how data is used for both business analysis and data science. You will work with a wide variety of data sources and storage systems to enable R&D, commercial, and clinical teams to solve problems.\\n\\nOur tech stack: AWS, Aptible, Postgres, Redis, Rails, Python, Airflow, Mode Analytics, Android, React, and React Native.\\nRESPONSIBILITIES\\nMaintain our current ETL-lite while scaling it for the future\\nCreate and maintain views and expand use of rollup tables\\nIdentify opportunities to improve the integrity of our datasets and implement the fixes\\nAssist in building out our payments platform for managing medical claims\\nHelp explore options for delivering data to clients, including possible API access\\nInform our 2020 objectives and key results around scaling and data needs\\nREQUIREMENTS\\nBachelor’s degree in C.S. or comparable degree preferred\\nMinimum of 3 years relevant experience in data engineering\\nAbility to collaborate and problem solve across teams\\nExcellent communication skills, both written and verbal\\nPython: using community-standards, linting, and testing at all appropriate levels.\\nSQL: comfort with joins, unions, views, rollups, windowing functions, testing\\nJSON parsing and fluency with RESTful APIs\\nOperational competency with cloud-hosted systems such as AWS, Aptible, or Heroku\\nAbility to correlate data across multiple sources: RDBs, csv, json\\nUnderstands how to write efficient code and can optimize existing software and queries\\nBONUS POINTS\\nPrior experience with healthcare data (PHI/PII/HIPAA requirements)\\nExperience developing software in Ruby on Rails\\nUnderstanding of user experience principles\\nHistory of technical writing\\nWHAT YOU'LL LOVE ABOUT US\\nCompetitive compensation with meaningful stock options\\nMedical, dental, vision\\n401K match\\n3 months paid parental leave\\nDaily lunch\\nProfessional Development budget\\nMonthly wellness benefit\\nNoise-cancelling headphones\\nWork from home policy\\nOpportunity to join a fantastically talented, diverse, and passionate team at a pivotal time in the company’s lifecycle\\nNo recruiters, please.\\n\\nHinge Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.\\nWe celebrate diversity and are committed to creating an inclusive environment for all employees.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "198  Komodo Health is addressing the global burden of disease through the world's most actionable healthcare map. Our solutions drive a more transparent, efficient and productive healthcare ecosystem. We value our culture of encouraging growth, collaboration, and constructive debate as well as delivering innovative solutions that \"wow\" our customers.\\n\\nThe Data Engineering (DE) team is looking for Staff Data Engineers to help us lead projects that will further our tech stack related to data, helping us develop data processing that is scalable, reliable and automated.\\n\\nThis is an opportunity to join a growing company, and be a part of a team of folks accomplished in diverse Engineering disciplines; focused on using the best of what lies at the forefront of technology and skills to address complex, real-world problems in the Healthcare and Life Science space. Some of the tools we use are: Python, Spark, AWS, Kubernetes, Docker, Postgres, Git, Airflow and Flask.\\n\\nRESPONSIBILITIES\\n----------------\\n\\n\\nDesign, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources.\\nCreate automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.\\nEvaluate new technologies for continuous improvements in Data Engineering.\\nCollaborate closely with the product team to build out new data features.\\nWork with the data scientists to implement descriptive, forecasting and predictive algorithms and models using latest technologies.\\n\\nREQUIREMENTS\\n------------\\n\\n\\nBS, MS in CS or related degrees.\\n5+ years of relevant experience with data engineering or similar experience.\\nExperience with...\\nBuilding and deploying large-scale data processing pipelines.\\nPython, Scala or Java coding experience, proficiency with at least one of them is required.\\nWorkflow & pipeline systems, like Airflow, Luigi, etc.\\nDistributed systems for data processing tools such as Spark, Hadoop, Kubernetes, etc.\\nSQL and Relational Databases, like PostgreSQL.\\nContinuous integration and automation tools and processes, like Jenkins.\\nYou've been through the planning, launching and refactoring phases of code you wrote.\\nA serious passion for data.\\nHistory of excellence and responsibility in previous engineering positions.\\nAbility to work as part of a collaborative team in a fast-paced environment.\\nSincere interest in working at a startup and scaling with the company as we grow.\\n\\nNICE TO HAVE\\n\\n\\nExperience or interest in the life sciences industry.\\nBasic knowledge (college level or hands on) of machine learning and statistics.\\nExperience with graph databases.\\n\\nBENEFITS\\n--------\\n\\n\\nCompetitive salary and equity compensation\\nFull Medical, Dental, and Vision benefits\\n401k plan with employer match\\nFlexible hours and a \"use as you need\" vacation policy\\nOpportunities to attend industry conferences and events\\nGreat office location(s) in SF/SOMA and NY/FLAT IRON\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "199  As a Senior Data Engineer, you will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data Warehouse model. You will be working in our Business Technology Data and Analytics function and have an opportunity to build a sound data foundation and processes that will scale with the company’s growth. You will architect, design, implement data pipelines enabling insights from our Product and Corporate Systems for key partners, data scientists and decision makers at Slack. You would also be responsible for enhancing and maintaining our data warehouse.\\n\\nData engineers also need to guarantee compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines. This would enable faster data access, integrated data reuse and vastly improved time-to-solution for Slack’s data and analytics initiatives. The newly hired data engineer will be the key interface in operationalizing data and analytics on behalf of the Finance, Sales, Product and organizational outcomes. This role will require both creative and collaborative working with IT and the wider business.\\n\\nThis is a fun role for problem solvers, who can intuitively anticipate problems and can also look beyond immediate issues. In short, we look for people who take pride in the craft and want to be part of creating and defining the teams operating model and contribution to the company. They will be a self-starter, detail and quality oriented, and passionate about having a huge impact at Slack. If this role has your name written all over it, please contact us with a resume so that we explore further.\\n\\nResponsibilities\\n\\nDesign, implement and build pipelines that deliver data with measurable quality under the SLA\\nPartner with Data Engineers, Data architects, domain experts, data analysts and other teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service\\nBe a champion of the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements\\nOwn and document data pipelines and data lineage\\nIdentify, document and promote best practices\\nSupport and Maintain analytics tech ecosystem (data warehouse, ETL and BI tools)\\nRequirements\\n\\nBS or MS degree in Computer Science or Engineering discipline.\\nAt least 8 years or more of work experience in data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.\\nAt least 4 years of experience working in cross-functional teams and collaborating with business stakeholders in Sales or Finance in support of a departmental and/or multi-departmental data management and analytics initiative.\\nVery strong experience in dimensional modeling, supporting data warehouse, scaling and optimizing, performance tuning and ETL pipelines\\nDeep understanding of relational as well as big data setup. Preferred: Prior experience with Snowflake, ETL tools (eg Informatica, Matillion, Snaplogic), Hive, Presto, Dimensional Modeling.\\nProblem solver with excellent interpersonal skills with ability to make sound complex decisions in a fast-paced, technical environment.\\nAbility to work on multiple areas like Data pipeline ETL, Data modeling & design, writing complex SQL queries etc.\\nCapable of planning and executing on both short-term and long-term goals individually and with the team.\\nPassionate about various technologies including but not limited to SQL/No SQL/MPP databases etc.\\nHands-on experience with Data Warehouse technologies (Snowflake, Redshift) and Big Data technologies (e.g Hadoop, Hive, Spark)\\nProficiency with programming languages is a big plus (e.g. Python)\\nExcellent written and verbal communication and interpersonal skills, able to effectively collaborate with technical and business partners\\nExcellent understanding of trade-offs\\nDemonstrated ability to navigate between big-picture and implementation details\\nSlack is a layer of the business technology stack that brings together people, data, and applications – a single place where people can effectively work together, find important information, and access hundreds of thousands of critical applications and services to do their best work. From global Fortune 100 companies to corner markets, businesses and teams of all kinds use Slack to bring the right people together with all the right information. Slack is headquartered in San Francisco, CA and has ten offices around the world. For more information on how Slack makes teams better connected, visit slack.com.\\nEnsuring a diverse and inclusive workplace where we learn from each other is core to Slack’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a pleasant and supportive place to work.\\nCome do the best work of your life here at Slack.                                                   \n",
       "200  Avenue Code is the leading software consultancy focused on delivering end-to-end development solutions for digital transformation across every vertical. We’re privately held, profitable, and have been on a solid growth trajectory since day one. We care deeply about our clients, our partners, and our people. We prefer the word ‘partner’ over ‘vendor’, and our investment in professional relationships is a reflection of that philosophy. We pride ourselves on our technical acumen, our collaborative problem-solving ability, and the warm professionalism of our teams.\\n\\nAbout the Opportunity:\\nWe are seeking an energetic and talented Data Scientist Engineer to deliver high value, high-quality business capabilities to our data technology platform. With a background in both software development and machine learning, you will collaborate with software engineers, data scientists and domain experts in Supply Chain and Inventory management to develop data and machine learning products to support our company. You will be an integral member engineering team delivering across multiple business functional areas. You will build data analysis infrastructure for effective prototyping and visualization of various data-driven approaches.\\n\\nKey Responsibilities:\\n\\nPartner in building the infrastructure required for optimal extraction, transformation, visualization, and loading of data from a wide variety of data sources using SQL and big data technologies;\\nImprove our Machine Learning systems by contributing to all phases of algorithm development including ideation, prototyping, design and production;\\nScale our data processing pipelines to handle and maintain complex processes in an efficient and reliable way that are available, scalable, and fault tolerant';\\nBuild scalable production systems for data collection, data transformation, feature extraction, model training, and scoring, using distributed software tools;\\nWork with data science to define data ingestion standards and assist with data-related technical issues;\\nPartner with product development team to understand various opportunities and use cases;\\nMaintain specifications and metadata; create and maintain documentation;\\nBuild Communities-of-Practice in key data technologies.\\n\\nTechnical Skills/Qualifications:\\n\\nExperience building and maintaining large-scale analytics systems;\\nExperience in software development and delivery;\\nExpertise in one or more programming languages such as Python, Scala, Java, Javascript, etc. and related frameworks (Flask, Django, Spring, AngularJS, React).\\nGood knowledge of architecting, developing, and maintaining of cloud technologies (AWS, Azure, GCP)\\nExperience with Big Data stack (Kafka, Hadoop, Storm, Spark, Hive, Airflow, ElasticSearch, TensorFlow);\\nExperience with relational (Postgres, MySQL) and/or non-relational (Cassandra, MongoDB) databases;\\nStrong machine learning or stats knowledge a plus;\\nExperience in Supply Chain / Retail / eCommerce;\\nExcellent communication skill and a strong team player.\\nBS/MS in Computer Science, Computer Engineering or related technical discipline;\\nStrong analytical and quantitative problem-solving ability.\\n\\nPursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records\\n\\nDoes this sound like you?\\nApply now to become an Avenue Coder!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "201  Location: Santa Clara or San Francisco\\n\\nUpwork ($UPWK) is the world's largest freelancing website. Each year $1.7 billion of work happens through Upwork, allowing businesses to get more done and helping professionals break free of traditional time and place boundaries and work anytime, anywhere on projects they love. At Upwork, you'll help build on this momentum. Together, we'll create economic and social value on a global scale, providing a trusted online workplace for businesses to connect with extraordinary talent and work without limits.\\n\\nOur Data Management team is tasked with building and supporting the business analytics infrastructure at Upwork that analysts and partners use on a daily basis to make critical business decisions. We are looking for a passionate engineer who has a love of data technologies to empower our teams in leveraging data to drive and measure impact. In this role, you will develop our next generation data infrastructure to support our future business growth.\\n\\nYour responsibilities:\\nBased on Upwork’s unique ecosystem and data needs, partner with our architects to research new cloud-based data technologies and conduct PoC to make recommendations suitable for our overall blueprint of data infrastructure\\nDevelop and maintain our end-to-end data platform, from data acquisition, real-time data stream, to data lake, enterprise data warehouse and front-end data tools and applications.\\nCreate and evolve a data platform that enables your peer data management engineering teams to create data products that serve both internal business units and external clients and partners to maximize the value generated from our data assets in a user-friendly, self-serve, robust, efficient and scalable way.\\nPartner with cross-functional engineering peers to streamline the flow of data in the whole company\\nProvide guidance on development and implementation of data quality rules and validation processes, leading engineering excellence initiatives to improve data quality and issue resolution\\nWhat it takes to catch our eye:\\n8+ years combined experience in DW/BI and Big Data engineering role in medium and large size companies\\nA proven record of building complex data platform from ideation to implementation\\nStrong knowledge and experience of the Big Data ecosystem and Cloud-based solutions\\nExpert in designing, implementing, and operating efficient, scalable, and reliable data transformation pipelines\\nSolid experience in database modeling, architecture, design, and implementation\\nFamiliar with data visualization tools such as Looker, Domo, or similar\\nExperience with marketplace businesses, especially as it pertains to working with large datasets inherent to two-sided marketplaces\\nAble to influence, lead, and communicate effectively across engineering teams, business units and other partners to negotiate priority, scope, and design solution\\nExcellent oral, written, and presentation skills, including the ability to work in person, virtually, and in a globally-staffed environment\\nFluent and effective in working with a globally distributed team\\nDemonstrate a strong understanding of development processes and agile methodologies\\nCome change how the world works.\\n\\nAt Upwork you’ll help shape the future of work. From our offices in San Francisco, Mountain View and Chicago, together we’re creating exciting new opportunities for a world of professionals. You’ll be part of a vibrant culture built on shared values: Inspire a boundless future of work, Put our community first, Have a bias towards action, and Build amazing teams. Along the way you’ll have fun and enjoy the perks of a people-first company: Work from Home Wednesday's, daily breakfast and lunch, regular in-office happy hours, top-notch benefits … and more. Check out Upwork’s spotlight on The Muse for a glimpse of our daily work/life balance.\\n\\nUpwork is proudly committed to recruiting and retaining a diverse and inclusive workforce. As an Equal Opportunity Employer, we never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "202  At Vonage, Business Intelligence is the team working on everything related to data, from ETL and data warehouses all the way to the reporting platform where other teams can build reports and get answers they need from our data.\\n\\nCurrently, we're looking for an experienced Data Engineer, to help us improve data processing systems that handle hundreds of GBs of data every day.\\n\\nWhat will be your role\\n\\nAs a Senior Data Engineer, you'll become a senior member of the Business intelligence team, responsible for designing and building systems for processing the ~500GB of logs generated by the Nexmo API platform every day and loading it into our data warehouse and other systems which consume aggregated data.\\n\\nAs a Senior Data Engineer, you will:\\n\\nDesign and implement data loading and aggregation frameworks and jobs that will be able to handle hundreds of GBs of json files, using Python, Airflow and Snowflake\\nBuild best practice ETLs to transform raw data into easy to use dimensional data for self service reporting\\nImprove our deployment and testing infrastructure within AWS, using tools like Jenkins, Puppet and Docker\\nWork closely with the Product, Infrastructure and Core teams, to make sure data needs are considered during product development and to guide decisions related to data\\nMentor junior team members\\n\\n----------------------------\\nWhat we're looking for\\n----------------------------\\n\\nWe're looking to strengthen our team by adding an experienced ETL / Data engineering professional, who has practical experience building performant data pipelines at scale, modeling data for self-serve use and working in a dynamic, startup-like environment.\\n\\nIdeally, you'll have:\\n\\n5 years+working experience building data processing, ETL and DWH solutions\\nExperience with software development using Python(preferred), Java or Scala\\nExperience working with relational databases (MySQL or Postgres preferred) and DWH technologies (Snowflake and Redshift preferred)\\nExperience working as part of a larger engineering team, within an agile framework and using version control systems\\nFamiliarity with cloud technologies and working with cloud infrastructure (AWS preferred) and exposure to deployment and infrastructure automation would be highly beneficial\\nExposure to distributed data processing systems with tools like Kubernetes, Hadoop, Spark, Kafka or ELK a large plus\\n\\n----------\\nWhy Vonage\\n----------\\n\\nNow acquired and part of Vonage, Nexmo is a global API platform for cloud communications that handles over 90 million requests every day. Customers like Airbnb, Viber, Whatsapp, Snapchat, and many others depend on our APIs and SDKs to connect with their customers all over the world.\\n\\nCompany built by engineers\\n--------------------------\\n\\n\\nNexmo was a tech company, built by engineers at the intersection of cloud and telecommunications and this is reflected everywhere in our culture.\\nEven though we've grown a lot in recent years, we've managed to maintain a startup-like working atmosphere, with very flat hierarchies and a very pragmatic approach to getting things done\\n\\nInteresting engineering challenge\\n---------------------------------\\n\\n\\nWe're in the process of migrating our DWH from Redshift to Snowflake, with a lot of room for re-design and refactoring along the way\\nWe appreciate initiative and ownership, so you'll be encouraged to design, implement and own parts of our data processing system, end to end\\n\\nAgile development using modern technologies\\n-------------------------------------------\\n\\n\\nWe iterate quickly, use the scrum framework, do frequent code reviews and have a very open and pragmatic work culture within the team\\nOur data platform platform is built in the cloud, using technologies like Airflow, Snowflake, Kafka, S3, GCDS, ELK stack and Tableau\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "\n",
       "[203 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_SF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
