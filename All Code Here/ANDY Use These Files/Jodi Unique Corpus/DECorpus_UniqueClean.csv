,Corpus,Clean,UniqueClean
0,"    A Bachelorâs degree in electrical engineering, computer engineering, mathematics or a related discipline may be substituted for four years of general experience.  ",bachelors degree electrical engineering computer engineering mathematics discipline may substituted four general,bachelors degree electrical engineering computer mathematics discipline may substituted four general
1,"Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting Experience with data warehousing tools and technologies Ability to work within UNIX/Linux operating systems AWS experience a plus This position requires U. S.  Citizenship and an active TS/SCI security clearance  Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting Experience with data warehousing tools and technologies Ability to work within UNIX/Linux operating systems AWS experience a plus This position requires U. S.  Citizenship and an active TS/SCI security clearance  ",distributed computing technologies hadoop hbase cassandra elasticsearch apache spark development java c scala groovy python andor shell scripting data warehousing tools technologies within unixlinux operating systems aws plus position requires u citizenship active tssci security clearance distributed computing technologies hadoop hbase cassandra elasticsearch apache spark development java c scala groovy python andor shell scripting data warehousing tools technologies within unixlinux operating systems aws plus position requires u citizenship active tssci security clearance,distributed computing technologies hadoop hbase cassandra elasticsearch apache spark development java c scala groovy python andor shell scripting data warehousing tools within unixlinux operating systems aws plus position requires u citizenship active tssci security clearance
2," Minimum three  3  yearsâ experience in designing, developing, building, and implementing Big Data solutions or developing automated solutions to solve complex problems, a thoughtful ability to solve problems could outweigh years of experience.  Ability to identify and implement a data solution strategy Demonstrates intellectual curiosity in exploring new technologies and finding creative ways to solve data management problems Experience developing solutions with Python/Javascript/PERL Experience/knowledge of Spark, Impala, Hadoop, Streamsets, Kafka, Rest APIs Experience in SQL-based technologies Experience with at least one of the following NoSQL Database technologies  Arrango Mark Logic HBase Impala Parquet RedShift Experience in Linux administration/troubleshooting   Assist in the development and delivering of large scale data pipelines Leverage new database technologies to improve customer data solutions Develop and implement automated tests for data transformations and data migrations Research and apply big data solution technologies to complex datasets; make recommendations to data science team on new technologies   ",minimum three designing developing building implementing big data solutions developing automated solutions solve complex problems thoughtful solve problems could outweigh identify implement data solution strategy demonstrates intellectual curiosity exploring technologies finding creative ways solve data management problems developing solutions pythonjavascriptperl experienceknowledge spark impala hadoop streamsets kafka rest apis sqlbased technologies least one following nosql database technologies arrango mark logic hbase impala parquet redshift linux administrationtroubleshooting assist development delivering scale data pipelines leverage database technologies improve customer data solutions develop implement automated tests data transformations data migrations research apply big data solution technologies complex datasets make recommendations data science team technologies,minimum three designing developing building implementing big data solutions automated solve complex problems thoughtful could outweigh identify implement solution strategy demonstrates intellectual curiosity exploring technologies finding creative ways management pythonjavascriptperl experienceknowledge spark impala hadoop streamsets kafka rest apis sqlbased least one following nosql database arrango mark logic hbase parquet redshift linux administrationtroubleshooting assist development delivering scale pipelines leverage improve customer develop tests transformations migrations research apply datasets make recommendations science team
3," Bachelorâs degree At least 1 year of experience with leading big data technologies such as Apache Spark, Apache Hadoop, or Apache Kafka At least 2 years of professional experience with data engineering concepts     ",bachelors degree least year leading big data technologies apache spark apache hadoop apache kafka least professional data engineering concepts,bachelors degree least year leading big data technologies apache spark hadoop kafka professional engineering concepts
4," 6 or more years of experience in software/system development 2 or more years of experience working with data technologies Technologies  Bachelor's in Computer Engineering, Computer Science, Information Technology, or related field, or equivalent experience Must currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame.  Must be willing and able to pass a counterintelligence  CI  polygraph examination   Interest In  Building data services Scaling systems on AWS - Elastic MapReduce Building and Managing large performant data pipelines Experience with  Working on a cross functional team Delivering big data solutions Amazon Web Services DevOps best practices - Jenkins Debugging data pipeline issues Comfortable with  Interaction with analysts to help drive big data analytics Agile development Hands on system engineering tasks   Manipulate and store large amounts of data with an eye to performance and efficiency Work with technologies in the Hadoop ecosystem such as HBase, Spark, Phoenix Automate deployment and ingestion of data Design and build data services for consumption by application developers   ",softwaresystem development data technologies technologies bachelors computer engineering computer science information technology must currently possess active us government top secret clearance obtain maintain sci access within reasonable customermandated time frame must willing able pass counterintelligence ci polygraph examination interest building data services scaling systems aws elastic mapreduce building managing performant data pipelines cross functional team delivering big data solutions amazon web services devops best practices jenkins debugging data pipeline issues comfortable interaction analysts help drive big data analytics agile development hands engineering tasks manipulate store amounts data eye performance efficiency technologies hadoop ecosystem hbase spark phoenix automate deployment ingestion data design build data services consumption application developers,softwaresystem development data technologies bachelors computer engineering science information technology must currently possess active us government top secret clearance obtain maintain sci access within reasonable customermandated time frame willing able pass counterintelligence ci polygraph examination interest building services scaling systems aws elastic mapreduce managing performant pipelines cross functional team delivering big solutions amazon web devops best practices jenkins debugging pipeline issues comfortable interaction analysts help drive analytics agile hands tasks manipulate store amounts eye performance efficiency hadoop ecosystem hbase spark phoenix automate deployment ingestion design build consumption application developers
5,"  Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI  must to have   and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala   Must  , Shell and Python  Must . Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools. Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State  Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI  must to have   and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala   Must  , Shell and Python  Must . Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools. Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State",bachelors degree higher computer sciences similarminimum software industry development aws services must ec emr redshift data pipeline airflow cloud formation cli must jenkins development apache spark presto sql nosql implementation extensive different programming scala must shell python must proficiency structured semistructured unstructured data sets social web logs real time streaming data feedsable tune big data solutions improve performance enduser experienceknowledge visualization data science tools expert level usage jenkins github preferredspark developer certification plusability eagerness constantly learn teach othersexperience media industry plusmust legal right united state bachelors degree higher computer sciences similarminimum software industry development aws services must ec emr redshift data pipeline airflow cloud formation cli must jenkins development apache spark presto sql nosql implementation extensive different programming scala must shell python must proficiency structured semistructured unstructured data sets social web logs real time streaming data feedsable tune big data solutions improve performance enduser experienceknowledge visualization data science tools expert level usage jenkins github preferredspark developer certification plusability eagerness constantly learn teach othersexperience media industry plusmust legal right united state,bachelors degree higher computer sciences similarminimum software industry development aws services must ec emr redshift data pipeline airflow cloud formation cli jenkins apache spark presto sql nosql implementation extensive different programming scala shell python proficiency structured semistructured unstructured sets social web logs real time streaming feedsable tune big solutions improve performance enduser experienceknowledge visualization science tools expert level usage github preferredspark developer certification plusability eagerness constantly learn teach othersexperience media plusmust legal right united state
6, 5+ years of related experience Active TS/SCI clearance DoD 8570 compliance or information assurance certification.     5+ years of related experience Active TS/SCI clearance DoD 8570 compliance or information assurance certification.  ,active tssci clearance dod compliance information assurance certification active tssci clearance dod compliance information assurance certification,active tssci clearance dod compliance information assurance certification
7," Master's Degree preferred, or a Bachelor's degree and 4 years' experience, or 10 years of specialized experience Minimum 4 years' experience working on complex data/database projects as a data analyst, data architect, or database engineer Top Secret Clearance with ability to obtain an SCI and CI poly   Certified Data Management Professional  CDMP , Microsoft Certified Solutions Associate  Business Intelligence  or equivalent certification s  strongly desired Experience building n-tier web-based applications using SQL and non-SQL back-ends Unix scripting  Ruby, Perl, Python, shell  Experience with Node. js, Spark, Neo4J, Graph Databases, Mule ESB, and Rest API Experience ingesting, analyzing, and visualizing data using Tableau Produce clear and concise documents and diagrams capturing networking and operational procedures and storage topology using MS Visio, MS Project, MS Excel and MS Word.     ",masters degree bachelors degree specialized minimum complex datadatabase projects data analyst data architect database engineer top secret clearance obtain sci ci poly certified data management professional cdmp microsoft certified solutions associate business intelligence certification strongly desired building ntier webbased applications sql nonsql backends unix scripting ruby perl python shell node js spark neoj graph databases mule esb rest api ingesting analyzing visualizing data tableau produce clear concise documents diagrams capturing networking operational procedures storage topology ms visio ms project ms excel ms word,masters degree bachelors specialized minimum complex datadatabase projects data analyst architect database engineer top secret clearance obtain sci ci poly certified management professional cdmp microsoft solutions associate business intelligence certification strongly desired building ntier webbased applications sql nonsql backends unix scripting ruby perl python shell node js spark neoj graph databases mule esb rest api ingesting analyzing visualizing tableau produce clear concise documents diagrams capturing networking operational procedures storage topology ms visio project excel word
8," 5 years of experience COMPTIA Security+ certification or CISSP certification Proficiency in two or more of the following programming languages  C , Java, . NET, Python, Perl, Ruby, or similar Familiarity with current Agile methods   5 years of experience COMPTIA Security+ certification or CISSP certification Proficiency in two or more of the following programming languages  C , Java, . NET, Python, Perl, Ruby, or similar Familiarity with current Agile methods  ",comptia security certification cissp certification proficiency two following programming languages c java net python perl ruby similar familiarity current agile methods comptia security certification cissp certification proficiency two following programming languages c java net python perl ruby similar familiarity current agile methods,comptia security certification cissp proficiency two following programming languages c java net python perl ruby similar familiarity current agile methods
9,"  Expertise in Java or Scala and in-depth knowledge of the JVM Expertise in Apache Spark or expertise in Computer Science fundamentals, such as analysis of algorithms Expertise in enterprise integration patterns and workflow management Experience and practical knowledge of OOP design patterns Distributed System Development for large-scale applications Experience with continuous integration and testing Experience with agile methodologies and short release cycles Strong attention to detail, good work ethic, ability to work on multiple projects simultaneously, and good communication skills  Design and develop data services, as part of an agile/scrum team Apply best practices in continuous integration and delivery Experience in translating high-level, ambiguous business goals into working software solutions.  Design and develop stream and batch processing data pipelines Work with product managers and other engineers to implement and document complex and evolving requirements  Technical Bachelorâs Degree required, e. g.  Comp Sci, Engineering, Math  Technical Bachelorâs Degree required, e. g.  Comp Sci, Engineering, Math",expertise java scala indepth jvm expertise apache spark expertise computer science fundamentals analysis algorithms expertise enterprise integration patterns workflow management practical oop design patterns distributed development largescale applications continuous integration testing agile methodologies short release cycles attention detail good ethic multiple projects simultaneously good communication design develop data services part agilescrum team apply best practices continuous integration delivery translating highlevel ambiguous business goals software solutions design develop stream batch processing data pipelines product managers engineers implement document complex evolving technical bachelors degree e g comp sci engineering math technical bachelors degree e g comp sci engineering math,expertise java scala indepth jvm apache spark computer science fundamentals analysis algorithms enterprise integration patterns workflow management practical oop design distributed development largescale applications continuous testing agile methodologies short release cycles attention detail good ethic multiple projects simultaneously communication develop data services part agilescrum team apply best practices delivery translating highlevel ambiguous business goals software solutions stream batch processing pipelines product managers engineers implement document complex evolving technical bachelors degree e g comp sci engineering math
10,"Works extensively with Cisco Layer 2 and Layer 3 solutions and products.  Cisco CCNA Certification a minimum.  Familiarity and working knowledge of telecommunication circuit types  e. g. , P2P T1/T3, TDM, IP Ethernet  Understand and have the ability to implement Cisco router, switch, and ASA products.  Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.  Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.  Strong interpersonal, written and oral skills.  From time to time, candidate may be asked to present project outline to customer.  Ability to conduct research on networking products with various vendors to accommodate changing customer requirements.  Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.  Works extensively with Cisco Layer 2 and Layer 3 solutions and products.  Cisco CCNA Certification a minimum.  Familiarity and working knowledge of telecommunication circuit types  e. g. , P2P T1/T3, TDM, IP Ethernet  Understand and have the ability to implement Cisco router, switch, and ASA products.  Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.  Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.  Strong interpersonal, written and oral skills.  From time to time, candidate may be asked to present project outline to customer.  Ability to conduct research on networking products with various vendors to accommodate changing customer requirements.  Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.   Works extensively with Cisco Layer 2 and Layer 3 solutions and products.  Cisco CCNA Certification a minimum.  Familiarity and working knowledge of telecommunication circuit types  e. g. , P2P T1/T3, TDM, IP Ethernet  Understand and have the ability to implement Cisco router, switch, and ASA products.  Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.  Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.  Strong interpersonal, written and oral skills.  From time to time, candidate may be asked to present project outline to customer.  Ability to conduct research on networking products with various vendors to accommodate changing customer requirements.  Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.  ",works extensively cisco layer layer solutions products cisco ccna certification minimum familiarity telecommunication circuit types e g pp tt tdm ip ethernet understand implement cisco router switch asa products troubleshooting accesslists ipv ipv issues across varying protocols ospf bgp eigrp static routing understanding implementing various routing protocols include ospf bgp eigrp static routes interpersonal written oral time time candidate may asked present project outline customer conduct research networking products various vendors accommodate changing customer teamoriented collaborative highly motivated take lead projects works extensively cisco layer layer solutions products cisco ccna certification minimum familiarity telecommunication circuit types e g pp tt tdm ip ethernet understand implement cisco router switch asa products troubleshooting accesslists ipv ipv issues across varying protocols ospf bgp eigrp static routing understanding implementing various routing protocols include ospf bgp eigrp static routes interpersonal written oral time time candidate may asked present project outline customer conduct research networking products various vendors accommodate changing customer teamoriented collaborative highly motivated take lead projects works extensively cisco layer layer solutions products cisco ccna certification minimum familiarity telecommunication circuit types e g pp tt tdm ip ethernet understand implement cisco router switch asa products troubleshooting accesslists ipv ipv issues across varying protocols ospf bgp eigrp static routing understanding implementing various routing protocols include ospf bgp eigrp static routes interpersonal written oral time time candidate may asked present project outline customer conduct research networking products various vendors accommodate changing customer teamoriented collaborative highly motivated take lead projects,works extensively cisco layer solutions products ccna certification minimum familiarity telecommunication circuit types e g pp tt tdm ip ethernet understand implement router switch asa troubleshooting accesslists ipv issues across varying protocols ospf bgp eigrp static routing understanding implementing various include routes interpersonal written oral time candidate may asked present project outline customer conduct research networking vendors accommodate changing teamoriented collaborative highly motivated take lead projects
11," Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases.  Experience building and optimizing âBig Dataâ data pipelines, architectures and data sets.  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Strong analytical skills and detailed oriented.  Build processes supporting data transformation, data structures, metadata, dependency and workload management.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Strong project management and organizational skills.  Experience supporting and working with cross-functional teams in a dynamic environment.  Experience with NoSQL solutions like Mongo DB a plus.    Create and maintain optimal data pipeline architecture.  Assemble large, complex data sets that meet functional / non-functional business requirements.  Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.  Work with data and analytics experts to strive for greater functionality in our data systems.   ",advanced sql relational databases query authoring sql well familiarity variety databases building optimizing big data data pipelines architectures data sets performing root cause analysis internal external data processes answer specific business questions identify opportunities improvement analytical detailed oriented build processes supporting data transformation data structures metadata dependency workload management successful history manipulating processing extracting value disconnected datasets project management organizational supporting crossfunctional teams dynamic nosql solutions like mongo db plus create maintain optimal data pipeline architecture assemble complex data sets meet functional nonfunctional business identify design implement internal process improvements automating manual processes optimizing data delivery redesigning infrastructure greater scalability build infrastructure optimal extraction transformation loading data wide variety data sources sql build analytics tools utilize data pipeline actionable insights customer acquisition operational efficiency key business performance metrics stakeholders executive product data design teams assist datarelated technical issues support data infrastructure needs create data tools analytics data scientist team members assist building optimizing product innovative industry leader data analytics experts strive greater functionality data systems,advanced sql relational databases query authoring well familiarity variety building optimizing big data pipelines architectures sets performing root cause analysis internal external processes answer specific business questions identify opportunities improvement analytical detailed oriented build supporting transformation structures metadata dependency workload management successful history manipulating processing extracting value disconnected datasets project organizational crossfunctional teams dynamic nosql solutions like mongo db plus create maintain optimal pipeline architecture assemble complex meet functional nonfunctional design implement process improvements automating manual delivery redesigning infrastructure greater scalability extraction loading wide sources analytics tools utilize actionable insights customer acquisition operational efficiency key performance metrics stakeholders executive product assist datarelated technical issues support needs scientist team members innovative industry leader experts strive functionality systems
12,"1+ year of work experience with ETL and data modeling1+ year of experience with the suite of open source big data technologies and platforms  Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra 1+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale1+ year of experience with Cloud Technologies  AWS, Azure, Google, etc 1+ year of experience with at least one SQL language such as T-SQL or PL/SQL 1+ year of work experience with ETL and data modeling1+ year of experience with the suite of open source big data technologies and platforms  Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra 1+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale1+ year of experience with Cloud Technologies  AWS, Azure, Google, etc 1+ year of experience with at least one SQL language such as T-SQL or PL/SQL   ",year etl data modeling year suite open source big data technologies platforms clouderahortonworks spark kafka presto hive cassandra year architecting building scalable data platforms processing data terabyte petabyte scale year cloud technologies aws azure google year least one sql language tsql plsql year etl data modeling year suite open source big data technologies platforms clouderahortonworks spark kafka presto hive cassandra year architecting building scalable data platforms processing data terabyte petabyte scale year cloud technologies aws azure google year least one sql language tsql plsql,year etl data modeling suite open source big technologies platforms clouderahortonworks spark kafka presto hive cassandra architecting building scalable processing terabyte petabyte scale cloud aws azure google least one sql language tsql plsql
13,"Bachelorsâ Degree and 18+ yearsâ experience.  Direct, relevant experience and familiarization with NASA/NOAA system architecture and organizational structures Ground System Engineering experience Experience in ground system development, installation and test Writing Test Procedures    ",bachelors degree direct relevant familiarization nasanoaa architecture organizational structures ground engineering ground development installation test writing test procedures,bachelors degree direct relevant familiarization nasanoaa architecture organizational structures ground engineering development installation test writing procedures
14," Must be a US Citizen BA/BS or Master's degree with emphasis on coursework of a quantitative nature  e. g. , Statistics, Computer Science, Engineering, Mathematics, Data Sciences .  Experience in SQL or PL/SQL, ETL   batch and stream processing  and data modeling Experience in Open source technologies  Spark, Kafka, Hive  Experience in Architecting big data Experience in Processing large volumes of data Experience in Cloud Technologies  AWS  Experience in with Java or Scala programming for data processing Experience supporting projects with Machine learning  Create, design and maintain reusable datasets for analysis by data scientists.  Assess new data sources to better understand availability and quality of data.  Provide governance and best practices of data structures, data integrity, and querying.  Interpret business needs from requests, and rapidly implement effective technical solutions.  Design, implement and enhance ETL  extract, transform and load  processes.  Write SQL queries to answer questions from stakeholders.  Maintain source code repository of scripts  SQL, Python, R  and other data products  dashboards, reports, etc.  .  Work with technology teams  BA,QA, Dev and Admin  to understand data capture and testing needs.  Automate and improve creation/maintenance of reports and dashboards .    ",must us citizen babs masters degree emphasis coursework quantitative nature e g statistics computer science engineering mathematics data sciences sql plsql etl batch stream processing data modeling open source technologies spark kafka hive architecting big data processing volumes data cloud technologies aws java scala programming data processing supporting projects machine create design maintain reusable datasets analysis data scientists assess data sources better understand availability data governance best practices data structures data integrity querying interpret business needs requests rapidly implement effective technical solutions design implement enhance etl extract transform load processes write sql queries answer questions stakeholders maintain source code repository scripts sql python r data products dashboards reports technology teams baqa dev admin understand data capture testing needs automate improve creationmaintenance reports dashboards,must us citizen babs masters degree emphasis coursework quantitative nature e g statistics computer science engineering mathematics data sciences sql plsql etl batch stream processing modeling open source technologies spark kafka hive architecting big volumes cloud aws java scala programming supporting projects machine create design maintain reusable datasets analysis scientists assess sources better understand availability governance best practices structures integrity querying interpret business needs requests rapidly implement effective technical solutions enhance extract transform load processes write queries answer questions stakeholders code repository scripts python r products dashboards reports technology teams baqa dev admin capture testing automate improve creationmaintenance
15,"    Preeminent expert in MySQL with at least 5 years demonstrated experience working with large databases Technical Bachelorâs degree and 8+ years professional experience Expert level ability writing advanced SQL queries and extensive experience in query optimization.  Advanced experience in scalable data and full text indexing solutions such as Apache Solr, or Elastic Search/Logstash/Kibana  ELK stack  Experienced Linux user and comfortable administrating databases from the Linux command line.  Strong initiative and self âmotivated to work independently.  Experience with database backup/restoration and disaster recovery.  ",preeminent expert mysql least demonstrated databases technical bachelors degree professional expert level writing advanced sql queries extensive query optimization advanced scalable data full text indexing solutions apache solr elastic searchlogstashkibana elk stack experienced linux user comfortable administrating databases linux command line initiative self motivated independently database backuprestoration disaster recovery,preeminent expert mysql least demonstrated databases technical bachelors degree professional level writing advanced sql queries extensive query optimization scalable data full text indexing solutions apache solr elastic searchlogstashkibana elk stack experienced linux user comfortable administrating command line initiative self motivated independently database backuprestoration disaster recovery
16,"1 - 2 yearsâ experience Azure SQL, MySQL and other Cloud Data ServicesExperience with one or more of the following technologies is required   Support Encryption of Data at Rest with Keys managed in Azure Key VaultImplement highly available Business Intelligence, BigData and Integration systemsCreate development standards, that comply with enterprise architecture guidelines and InfoSec rulesWork with internal partners and external vendors in order to test and validate infrastructure in the cloud environmentCreate database backup and archival strategies for Cloud native dataElaborate and propose on data integration methodologies between cloud-to-cloud and on-premises-to-cloud systemsDeploy integration systems leveraging high security standardsConfigure integration, database and BigData systems with key management and encryption systemsCollaborate with members of teams on different streams of the project, such as  but not limited to   Solutions development, DevOps, Network, Infrastructure  ",azure sql mysql cloud data servicesexperience one following technologies support encryption data rest keys managed azure key vaultimplement highly available business intelligence bigdata integration systemscreate development standards comply enterprise architecture guidelines infosec ruleswork internal partners external vendors order test validate infrastructure cloud environmentcreate database backup archival strategies cloud native dataelaborate propose data integration methodologies cloudtocloud onpremisestocloud systemsdeploy integration systems leveraging security standardsconfigure integration database bigdata systems key management encryption systemscollaborate members teams different streams project limited solutions development devops network infrastructure,azure sql mysql cloud data servicesexperience one following technologies support encryption rest keys managed key vaultimplement highly available business intelligence bigdata integration systemscreate development standards comply enterprise architecture guidelines infosec ruleswork internal partners external vendors order test validate infrastructure environmentcreate database backup archival strategies native dataelaborate propose methodologies cloudtocloud onpremisestocloud systemsdeploy systems leveraging security standardsconfigure management systemscollaborate members teams different streams project limited solutions devops network
17," 5+ years of work experience with ETL and data modeling 5+ years of work experience with architecting and solutioning Experience in estimating work and defining an implementation plan 3+ year of experience with the suite of open source big data technologies and platforms  Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra  3+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale 3+ year of experience with Cloud Technologies  AWS, Azure, Google, etc.   5+ years of experience with at least one SQL language such as T-SQL or PL/SQL  5+ years of work experience with ETL and data modeling 5+ years of work experience with architecting and solutioning Experience in estimating work and defining an implementation plan 3+ year of experience with the suite of open source big data technologies and platforms  Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra  3+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale 3+ year of experience with Cloud Technologies  AWS, Azure, Google, etc.   5+ years of experience with at least one SQL language such as T-SQL or PL/SQL   ",etl data modeling architecting solutioning estimating defining implementation plan year suite open source big data technologies platforms clouderahortonworks spark kafka presto hive cassandra year architecting building scalable data platforms processing data terabyte petabyte scale year cloud technologies aws azure google least one sql language tsql plsql etl data modeling architecting solutioning estimating defining implementation plan year suite open source big data technologies platforms clouderahortonworks spark kafka presto hive cassandra year architecting building scalable data platforms processing data terabyte petabyte scale year cloud technologies aws azure google least one sql language tsql plsql,etl data modeling architecting solutioning estimating defining implementation plan year suite open source big technologies platforms clouderahortonworks spark kafka presto hive cassandra building scalable processing terabyte petabyte scale cloud aws azure google least one sql language tsql plsql
18," BS in technical field and proficiency in at least programming language  Java, Python, etc.   Demonstrable experience with Google Cloud Platform  GCP  technologies, to include tools like  BigQuery, BigTable, Cloud Spanner, KubeFlow, Kubernetes Engine, ML Engine, Compute Engine, DataFlow, etc.  Minimum of three years of experience architecting data analytics and ETL pipelines in cloud environments Experience cleaning and wrangling data, and working with complex data models   BS in technical field and proficiency in at least programming language  Java, Python, etc.   Demonstrable experience with Google Cloud Platform  GCP  technologies, to include tools like  BigQuery, BigTable, Cloud Spanner, KubeFlow, Kubernetes Engine, ML Engine, Compute Engine, DataFlow, etc.  Minimum of three years of experience architecting data analytics and ETL pipelines in cloud environments Experience cleaning and wrangling data, and working with complex data models  ",bs technical proficiency least programming language java python demonstrable google cloud platform gcp technologies include tools like bigquery bigtable cloud spanner kubeflow kubernetes engine ml engine compute engine dataflow minimum three architecting data analytics etl pipelines cloud environments cleaning wrangling data complex data models bs technical proficiency least programming language java python demonstrable google cloud platform gcp technologies include tools like bigquery bigtable cloud spanner kubeflow kubernetes engine ml engine compute engine dataflow minimum three architecting data analytics etl pipelines cloud environments cleaning wrangling data complex data models,bs technical proficiency least programming language java python demonstrable google cloud platform gcp technologies include tools like bigquery bigtable spanner kubeflow kubernetes engine ml compute dataflow minimum three architecting data analytics etl pipelines environments cleaning wrangling complex models
19," BS/MS/PhD in Computer Science or a related field.  7+ years of relevant engineering work experience and 2+ hands-on technical management experience.  Experience designing and delivering frameworks to facilitate the data development lifecycle  e. g. , mainly testing and deploying pipeline code .  Demonstrated experience working with internal customers and/or end users.  Experience in data integration, ETL, or pipeline design and implementation.  Use and development of open source technologies such as Hadoop, Kafka, or Spark.  Solid understanding of data modeling best practices.  Experience designing and deploying high performance systems with robust monitoring and logging practices.  Knowledge of core ML concepts  e. g. , feature discovery and engineering, model validation, retraining strategies .  Knowledge of relational databases and query authoring.  Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level  Develop and automate large scale, high-performance data processing systems and tools to improve data quality and volume, accelerate iteration cycles for anyone working with data in DoD, and improve our ability to deliver AI-enabled capabilities as a Department.  Understand the data needs of our internal teams and abstract problems and requests to build data engineering solutions along with your partners in engineering and design.  Be the champion for the correct use of data and help establish Department-wide best practices.  Write technical papers and blog posts.   BS/MS/PhD in Computer Science or a related field.  7+ years of relevant engineering work experience and 2+ hands-on technical management experience.  Experience designing and delivering frameworks to facilitate the data development lifecycle  e. g. , mainly testing and deploying pipeline code .  Demonstrated experience working with internal customers and/or end users.  Experience in data integration, ETL, or pipeline design and implementation.  Use and development of open source technologies such as Hadoop, Kafka, or Spark.  Solid understanding of data modeling best practices.  Experience designing and deploying high performance systems with robust monitoring and logging practices.  Knowledge of core ML concepts  e. g. , feature discovery and engineering, model validation, retraining strategies .  Knowledge of relational databases and query authoring.  Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level  BS/MS/PhD in Computer Science or a related field.  7+ years of relevant engineering work experience and 2+ hands-on technical management experience.  Experience designing and delivering frameworks to facilitate the data development lifecycle  e. g. , mainly testing and deploying pipeline code .  Demonstrated experience working with internal customers and/or end users.  Experience in data integration, ETL, or pipeline design and implementation.  Use and development of open source technologies such as Hadoop, Kafka, or Spark.  Solid understanding of data modeling best practices.  Experience designing and deploying high performance systems with robust monitoring and logging practices.  Knowledge of core ML concepts  e. g. , feature discovery and engineering, model validation, retraining strategies .  Knowledge of relational databases and query authoring.  Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level ",bsmsphd computer science relevant engineering handson technical management designing delivering frameworks facilitate data development lifecycle e g mainly testing deploying pipeline code demonstrated internal customers andor end users data integration etl pipeline design implementation use development open source technologies hadoop kafka spark solid understanding data modeling best practices designing deploying performance systems robust monitoring logging practices core ml concepts e g feature discovery engineering model validation retraining strategies relational databases query authoring obtain department defense security clearance top secret sci level develop automate scale highperformance data processing systems tools improve data volume accelerate iteration cycles anyone data dod improve deliver aienabled capabilities department understand data needs internal teams abstract problems requests build data engineering solutions along partners engineering design champion correct use data help establish departmentwide best practices write technical papers blog posts bsmsphd computer science relevant engineering handson technical management designing delivering frameworks facilitate data development lifecycle e g mainly testing deploying pipeline code demonstrated internal customers andor end users data integration etl pipeline design implementation use development open source technologies hadoop kafka spark solid understanding data modeling best practices designing deploying performance systems robust monitoring logging practices core ml concepts e g feature discovery engineering model validation retraining strategies relational databases query authoring obtain department defense security clearance top secret sci level bsmsphd computer science relevant engineering handson technical management designing delivering frameworks facilitate data development lifecycle e g mainly testing deploying pipeline code demonstrated internal customers andor end users data integration etl pipeline design implementation use development open source technologies hadoop kafka spark solid understanding data modeling best practices designing deploying performance systems robust monitoring logging practices core ml concepts e g feature discovery engineering model validation retraining strategies relational databases query authoring obtain department defense security clearance top secret sci level,bsmsphd computer science relevant engineering handson technical management designing delivering frameworks facilitate data development lifecycle e g mainly testing deploying pipeline code demonstrated internal customers andor end users integration etl design implementation use open source technologies hadoop kafka spark solid understanding modeling best practices performance systems robust monitoring logging core ml concepts feature discovery model validation retraining strategies relational databases query authoring obtain department defense security clearance top secret sci level develop automate scale highperformance processing tools improve volume accelerate iteration cycles anyone dod deliver aienabled capabilities understand needs teams abstract problems requests build solutions along partners champion correct help establish departmentwide write papers blog posts
20,"  B. S.  or equivalent degree in computer science, mathematics or other relevant fields.  3-7 years of hands-on experience in ETL, Data warehouse, Data Marts, Visualization and/or building data pipelines, modeling and designing schema for data lakes or for data platforms.  Must have previous experience with Java Experience implementing and resolving dependency issues from common Java build tools  Maven  preferred , Gradle, Ant, or equivilent Experience with Agile implementation methodologies.  Experience using common CI/CD tools, such as Jenkins Experience working with NoSQL databases in both the extract and load aspects of the ETL process.  Strong programming and scripting skills experience and expertise in two or more of the following  XML/XSLT, Python, Perl, Shell Scala, C.  Proficient in big data/distributed computing frameworks such as Spring, Hadoop, Apache Hive, Spark, Kafka, etc.  Familiarity and/or a strong willingness to learn ApacheNiFi and NiFi Registry.  Practice working with, processing, and managing large data sets  multi TB/PB scale .  Similar roles  Database Administrator  DBA , Database Operator  DBO , Data Architect, Data Manager, Data Analyst, Data Integrator, Systems Integrator, Systems Engineer.   Lead efforts to create standard documentation for the various software development life cycle processes ensuring uniformity across the office.  Conduct research and apply best practices and ensures that they are approved and documented.  Critically analyzes processes/systems/practices to identify current gaps and opportunities for improvement.  Facilitate business process reengineering documenting the ""As Is"" processes, identify recommended process improvements and document approved changes in To-Be process documents and diagrams.  Conduct interviews and evaluate current documentation to gain insight.  Develop process functional flow diagrams  MS Visio  in support of requirements capture and development based on existing policy, instructions and interviews.  Provide business process improvement recommendations and briefs to ADO3 leadership.  Propose and create standard operating procedures and diagrams to provide direction to ADO3 leadership, system development team members, Requirements Analysts and others.  Collaborate and recommend improvements to JIRA with JIRA SMEs and JIRA Administrators.  Attend Change Control Board  CCB  & Requirements Working Group Meetings.  Capture notes, disseminate for client review and further dissemination ensuring continuity throughout the CCB activities, prepare and submit after action reports  AAR .  Your primary role will be to design and build data pipelines to meet overall architecture requirements.  You will be focused on helping client projects on data collection, integration, prep/transformation, and implementing services such as data processing and/or machine learning on datasets.  In this role, you will work on both traditional and on some of the latest technologies, collaborate with teams for data pipelines and delivery, interact daily with management, and help build a great program of operations.   ",b degree computer science mathematics relevant fields handson etl data warehouse data marts visualization andor building data pipelines modeling designing schema data lakes data platforms must previous java implementing resolving dependency issues common java build tools maven gradle ant equivilent agile implementation methodologies common cicd tools jenkins nosql databases extract load aspects etl process programming scripting expertise two following xmlxslt python perl shell scala c proficient big datadistributed computing frameworks spring hadoop apache hive spark kafka familiarity andor willingness learn apachenifi nifi registry practice processing managing data sets multi tbpb scale similar roles database administrator dba database operator dbo data architect data manager data analyst data integrator systems integrator systems engineer lead efforts create standard documentation various software development life cycle processes ensuring uniformity across office conduct research apply best practices ensures approved documented critically analyzes processessystemspractices identify current gaps opportunities improvement facilitate business process reengineering documenting processes identify recommended process improvements document approved changes tobe process documents diagrams conduct interviews evaluate current documentation gain insight develop process functional flow diagrams ms visio support capture development based existing policy instructions interviews business process improvement recommendations briefs ado leadership propose create standard operating procedures diagrams direction ado leadership development team members analysts others collaborate recommend improvements jira jira smes jira administrators attend change control board ccb group meetings capture notes disseminate client review dissemination ensuring continuity throughout ccb activities prepare submit action reports aar primary role design build data pipelines meet overall architecture focused helping client projects data collection integration preptransformation implementing services data processing andor machine datasets role traditional latest technologies collaborate teams data pipelines delivery interact daily management help build great program operations,b degree computer science mathematics relevant fields handson etl data warehouse marts visualization andor building pipelines modeling designing schema lakes platforms must previous java implementing resolving dependency issues common build tools maven gradle ant equivilent agile implementation methodologies cicd jenkins nosql databases extract load aspects process programming scripting expertise two following xmlxslt python perl shell scala c proficient big datadistributed computing frameworks spring hadoop apache hive spark kafka familiarity willingness learn apachenifi nifi registry practice processing managing sets multi tbpb scale similar roles database administrator dba operator dbo architect manager analyst integrator systems engineer lead efforts create standard documentation various software development life cycle processes ensuring uniformity across office conduct research apply best practices ensures approved documented critically analyzes processessystemspractices identify current gaps opportunities improvement facilitate business reengineering documenting recommended improvements document changes tobe documents diagrams interviews evaluate gain insight develop functional flow ms visio support capture based existing policy instructions recommendations briefs ado leadership propose operating procedures direction team members analysts others collaborate recommend jira smes administrators attend change control board ccb group meetings notes disseminate client review dissemination continuity throughout activities prepare submit action reports aar primary role design meet overall architecture focused helping projects collection integration preptransformation services machine datasets traditional latest technologies teams delivery interact daily management help great program operations
21,"   US Citizen Current TS/SCI CI poly security clearance is required for consideration 10+ years of experience with database and data aggregation Strong knowledge of big data analysis and storage Big data experience  Hadoop  Experience with Structured Query Language Understanding of JDBC Understanding of multiple database vendors  Microsoft, Oracle, SAP, SAS, PeopleSoft  Familiarity with database vendors and offerings  ",us citizen current tssci ci poly security clearance consideration database data aggregation big data analysis storage big data hadoop structured query language understanding jdbc understanding multiple database vendors microsoft oracle sap sas peoplesoft familiarity database vendors offerings,us citizen current tssci ci poly security clearance consideration database data aggregation big analysis storage hadoop structured query language understanding jdbc multiple vendors microsoft oracle sap sas peoplesoft familiarity offerings
22,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform.  Multi-cloud experience a plus.    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",minimum previous consulting client service delivery google gcp devops gcp platform multicloud plus proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,minimum previous consulting client service delivery google gcp devops platform multicloud plus proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
23,"At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ",least consulting client service delivery azure devops azure platform proven build manage foster teamoriented,least consulting client service delivery azure devops platform proven build manage foster teamoriented
24," Experience working with relational or multi-dimensional databases Experience developing logical data models within a data warehouse Experience developing ETL processes Demonstrated mastery in one or more SQL variants  PostgreSQL, MySQL, Oracle, SQL Server, or DB2 Demonstrated mastery in database concepts and large-scale database implementations and design patterns Proven ability to work with users to define requirements and business issues Excellent analytic and troubleshooting skills Strong written and oral communication skills    Responsible for data modeling and schema design that will range across multiple business domains within higher education Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas Work with clients to research and conduct business information flow studies Codify high-performing SQL for efficient data transformation Coordinate work with external teams to ensure a smooth development process Support operations by identifying, researching and resolving performance and production issues   ",relational multidimensional databases developing logical data models within data warehouse developing etl processes demonstrated mastery one sql variants postgresql mysql oracle sql server db demonstrated mastery database concepts largescale database implementations design patterns proven users define business issues analytic troubleshooting written oral communication responsible data modeling schema design range across multiple business domains within higher education partner multiple stakeholders clients product development bi engineers develop scalable standard schemas clients research conduct business information flow codify highperforming sql efficient data transformation coordinate external teams smooth development process support operations identifying researching resolving performance production issues,relational multidimensional databases developing logical data models within warehouse etl processes demonstrated mastery one sql variants postgresql mysql oracle server db database concepts largescale implementations design patterns proven users define business issues analytic troubleshooting written oral communication responsible modeling schema range across multiple domains higher education partner stakeholders clients product development bi engineers develop scalable standard schemas research conduct information flow codify highperforming efficient transformation coordinate external teams smooth process support operations identifying researching resolving performance production
25,"Experience managing streaming data in a data management practice or similar experience managing big data.  Specific data management such as Kafka Administrator for Kafka jobs and Kafka integration.  Experience with the following tools, languages, and applications is beneficial  Apache, Hadoop, Java, Flume, Spark, Zookeeper, SQL    ",managing streaming data data management practice similar managing big data specific data management kafka administrator kafka jobs kafka integration following tools languages applications beneficial apache hadoop java flume spark zookeeper sql,managing streaming data management practice similar big specific kafka administrator jobs integration following tools languages applications beneficial apache hadoop java flume spark zookeeper sql
26,"     Bachelorâs Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience Mid level roles available requiring 3-5 years of experience and senior to SME roles available requiring 8-15+ years of related software engineering and ETL experience Experience building and maintaining data flows in NiFi or Pentaho Excellent organizational, coordination, interpersonal and team building skills.  ",bachelors degree computer science electrical computer engineering technical discipline combination education technical training workmilitary mid level roles available requiring senior sme roles available requiring software engineering etl building maintaining data flows nifi pentaho organizational coordination interpersonal team building,bachelors degree computer science electrical engineering technical discipline combination education training workmilitary mid level roles available requiring senior sme software etl building maintaining data flows nifi pentaho organizational coordination interpersonal team
27,"  3+ years of Python Development, with emphasis in ETL Development 5+ years of SQL experience, with emphasis in Data Analysis Proficiency in relational database design and development Experienced building and scaling batch/asynchronous systems Hands-on development using and migrating data to cloud platforms, AWS Analytical approach to problem-solving; ability to use technology to solve business problems Display passion for delivering high quality products that meet customers' needs Solving data-oriented problems in an analytical and iterative fashion Perform analysis, architecture, design, and development of cloud data solutions Working with various kinds of data  structured, unstructured, metrics, logs, json, xml, etc.   Working in various agile methodologies  Scrum, Kanban, SAFe   ",python development emphasis etl development sql emphasis data analysis proficiency relational database design development experienced building scaling batchasynchronous systems handson development migrating data cloud platforms aws analytical approach problemsolving use technology solve business problems display passion delivering products meet customers needs solving dataoriented problems analytical iterative fashion perform analysis architecture design development cloud data solutions various kinds data structured unstructured metrics logs json xml various agile methodologies scrum kanban safe,python development emphasis etl sql data analysis proficiency relational database design experienced building scaling batchasynchronous systems handson migrating cloud platforms aws analytical approach problemsolving use technology solve business problems display passion delivering products meet customers needs solving dataoriented iterative fashion perform architecture solutions various kinds structured unstructured metrics logs json xml agile methodologies scrum kanban safe
28," Databases/Data Stores  Oracle, MySQL, HIVE, HBASE, and HDFS Frameworks  Hadoop, Rails, JavaScript Frameworks, SOA/WebServices, JSP Indexing  SOLR and Lucine Development/Scripting Languages  JAVA  J2EE , Python, Ruby, JavaScript, MapReduce, Pig, XML, SQL, JAQL, HTML, CSS, XML, BASH, ANT, and Perl   Design and develop methods, processes, and systems to consolidate and analyze structured and unstructured data from diverse sources including ""big data"" sources.  Develop and use advanced software programs, algorithms, query techniques, model complex business problems, and automated processes to cleanse, integrate, and evaluate datasets.  Analyze the requirements and evaluate technologies for data science capabilities including one or more of the following  Natural Language Processing, Machine Learning, predictive modeling, statistical analysis and hypothesis testing.  Develop information tools, algorithms, dashboards, and queries to monitor and improve business performance.  Maintain awareness of emerging analytics and big-data technologies.  Designs, implement, and maintain standard data interfaces for data ingest including Extract/Transform/Load  ETL  methodology and implementation, APIs, RESTful Web Services, data quality, and data cleansing.  Provide data services, data administration, data management, and ""Big Data"" support in client/server, virtual machine, Hadoop, and cloud infrastructure environment and/or migrations between these environments.  Database installation, configuration, and the upgrading of database server software and related products, backup and recovery policies and procedures, database implementation, security, optimization, multi-domain operation, and performance management.  Hadoop, cloud, and other technologies associated with data storage, processing, management, and use.  The migration/transition of database capability into cloud based technologies and/or creation of interfaces between classic relational databases and key indexes to cloud based columnar databases and map reduce index capabilities.    ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED.  MUST BE A U. S.  CITIZEN. ",databasesdata stores oracle mysql hive hbase hdfs frameworks hadoop rails javascript frameworks soawebservices jsp indexing solr lucine developmentscripting languages java jee python ruby javascript mapreduce pig xml sql jaql html css xml bash ant perl design develop methods processes systems consolidate analyze structured unstructured data diverse sources big data sources develop use advanced software programs algorithms query techniques model complex business problems automated processes cleanse integrate evaluate datasets analyze evaluate technologies data science capabilities one following natural language processing machine predictive modeling statistical analysis hypothesis testing develop information tools algorithms dashboards queries monitor improve business performance maintain awareness emerging analytics bigdata technologies designs implement maintain standard data interfaces data ingest extracttransformload etl methodology implementation apis restful web services data data cleansing data services data administration data management big data support clientserver virtual machine hadoop cloud infrastructure andor migrations environments database installation configuration upgrading database server software products backup recovery policies procedures database implementation security optimization multidomain operation performance management hadoop cloud technologies associated data storage processing management use migrationtransition database capability cloud based technologies andor creation interfaces classic relational databases key indexes cloud based columnar databases map reduce index capabilities candidates active government security clearances appropriate poly considered must u citizen,databasesdata stores oracle mysql hive hbase hdfs frameworks hadoop rails javascript soawebservices jsp indexing solr lucine developmentscripting languages java jee python ruby mapreduce pig xml sql jaql html css bash ant perl design develop methods processes systems consolidate analyze structured unstructured data diverse sources big use advanced software programs algorithms query techniques model complex business problems automated cleanse integrate evaluate datasets technologies science capabilities one following natural language processing machine predictive modeling statistical analysis hypothesis testing information tools dashboards queries monitor improve performance maintain awareness emerging analytics bigdata designs implement standard interfaces ingest extracttransformload etl methodology implementation apis restful web services cleansing administration management support clientserver virtual cloud infrastructure andor migrations environments database installation configuration upgrading server products backup recovery policies procedures security optimization multidomain operation associated storage migrationtransition capability based creation classic relational databases key indexes columnar map reduce index candidates active government clearances appropriate poly considered must u citizen
29,"Bachelor degree in computer science, information systems or related field or equivalent experience 5 years of experience with developing, installing, configuring, tuning, and supporting large dataset environments 5 years of experience with various database platforms, including Oracle, Oracle RAC 5 years of experience with database development languages, including SQL, PL/SQL and Linux scripting 5 years of experience with database design applications, including SQL Developer, DB Visualizer, and Embarcadero ER Studio 2+ years of experience in installing and upgrading Tableau server and server performance tuning Experience with the design, development and delivery of Tableau visualization solutions Experience with creation of users, groups, projects, workbooks and the appropriate permission sets for Tableau server logons and security checks 5 years of experience in handling large transaction datasets   Assist in implementing long-term strategic goals for BI database development in conjunction with end users, managers, clients, and other stakeholders Analyze user requirements and, based on findings, collaborate with DS and other functions to design functional specifications for databases and database applications following database standards Assist in planning and implementing capacity and resource expansion to ensure scalability of BI databases in close collaboration with DS function Assist with the design of redundant systems, policies, and procedures for disaster recovery to ensure effective availability, protection, and integrity of data assets Conduct research and make recommendations on database products, services, protocols, and standards in support of procurement and development efforts Develop automated database applications, where necessary, using applicable database techniques Work with agile teams and product owners to ensure database design and performance meet business requirements Identify inefficiencies in current databases and implement improved solutions Assist in planning and performing database upgrades and migrations Assist in evaluating and selecting database components, including hardware, relational database management systems, ETL software, metadata management tools, and database design solutions.  As a subject matter expert, the Data Architect will define feasibility and address all data related questions and challenges to support dashboards and tracking tools from all functions across the organization including Product Innovation, Compliance, Finance and CPI.    Bachelor degree in computer science, information systems or related field or equivalent experience 5 years of experience with developing, installing, configuring, tuning, and supporting large dataset environments 5 years of experience with various database platforms, including Oracle, Oracle RAC 5 years of experience with database development languages, including SQL, PL/SQL and Linux scripting 5 years of experience with database design applications, including SQL Developer, DB Visualizer, and Embarcadero ER Studio 2+ years of experience in installing and upgrading Tableau server and server performance tuning Experience with the design, development and delivery of Tableau visualization solutions Experience with creation of users, groups, projects, workbooks and the appropriate permission sets for Tableau server logons and security checks 5 years of experience in handling large transaction datasets ",bachelor degree computer science information systems developing installing configuring tuning supporting dataset environments various database platforms oracle oracle rac database development languages sql plsql linux scripting database design applications sql developer db visualizer embarcadero er studio installing upgrading tableau server server performance tuning design development delivery tableau visualization solutions creation users groups projects workbooks appropriate permission sets tableau server logons security checks handling transaction datasets assist implementing longterm strategic goals bi database development conjunction end users managers clients stakeholders analyze user based findings collaborate ds functions design functional specifications databases database applications following database standards assist planning implementing capacity resource expansion scalability bi databases close collaboration ds function assist design redundant systems policies procedures disaster recovery effective availability protection integrity data assets conduct research make recommendations database products services protocols standards support procurement development efforts develop automated database applications necessary applicable database techniques agile teams product owners database design performance meet business identify inefficiencies current databases implement improved solutions assist planning performing database upgrades migrations assist evaluating selecting database components hardware relational database management systems etl software metadata management tools database design solutions subject matter expert data architect define feasibility address data questions challenges support dashboards tracking tools functions across organization product innovation compliance finance cpi bachelor degree computer science information systems developing installing configuring tuning supporting dataset environments various database platforms oracle oracle rac database development languages sql plsql linux scripting database design applications sql developer db visualizer embarcadero er studio installing upgrading tableau server server performance tuning design development delivery tableau visualization solutions creation users groups projects workbooks appropriate permission sets tableau server logons security checks handling transaction datasets,bachelor degree computer science information systems developing installing configuring tuning supporting dataset environments various database platforms oracle rac development languages sql plsql linux scripting design applications developer db visualizer embarcadero er studio upgrading tableau server performance delivery visualization solutions creation users groups projects workbooks appropriate permission sets logons security checks handling transaction datasets assist implementing longterm strategic goals bi conjunction end managers clients stakeholders analyze user based findings collaborate ds functions functional specifications databases following standards planning capacity resource expansion scalability close collaboration function redundant policies procedures disaster recovery effective availability protection integrity data assets conduct research make recommendations products services protocols support procurement efforts develop automated necessary applicable techniques agile teams product owners meet business identify inefficiencies current implement improved performing upgrades migrations evaluating selecting components hardware relational management etl software metadata tools subject matter expert architect define feasibility address questions challenges dashboards tracking across organization innovation compliance finance cpi
30,Bachelor's degree in Computer Science or related degree.  At least a 3. 0 overall undergraduate GPA Strong knowledge of at least one programming language SQL and C  experience highly desired Experience working in a cloud environment Experience with the Hadoop ecosystem highly preferred A thorough understanding of the Systems Development Life Cycle  SDLC  Excellent interpersonal and teamwork skills Solid verbal and written communication skills Excellent analytical and problem solving skills Willingness to learn new technologies and programming languages   Bachelor's degree in Computer Science or related degree.  At least a 3. 0 overall undergraduate GPA Strong knowledge of at least one programming language SQL and C  experience highly desired Experience working in a cloud environment Experience with the Hadoop ecosystem highly preferred A thorough understanding of the Systems Development Life Cycle  SDLC  Excellent interpersonal and teamwork skills Solid verbal and written communication skills Excellent analytical and problem solving skills Willingness to learn new technologies and programming languages   ,bachelors degree computer science degree least overall undergraduate gpa least one programming language sql c highly desired cloud hadoop ecosystem highly thorough understanding systems development life cycle sdlc interpersonal teamwork solid verbal written communication analytical problem solving willingness learn technologies programming languages bachelors degree computer science degree least overall undergraduate gpa least one programming language sql c highly desired cloud hadoop ecosystem highly thorough understanding systems development life cycle sdlc interpersonal teamwork solid verbal written communication analytical problem solving willingness learn technologies programming languages,bachelors degree computer science least overall undergraduate gpa one programming language sql c highly desired cloud hadoop ecosystem thorough understanding systems development life cycle sdlc interpersonal teamwork solid verbal written communication analytical problem solving willingness learn technologies languages
31,"   Maintain and enhance existing custom solutions built in SharePoint, including but not limited to farm solutions deployed on the SharePoint platform.  Plan, lead, and execute SharePoint 2010/2013/2016 tasks; custom master pages, layouts, and templates; custom workflows; implementing permissions structures; working with timer jobs and event handlers; as well as implementation, integration, and maintenance of existing solutions.  Gather user requirements, analyze business processes, and work with functional areas to define and scope projects, document requirements, and application functionality.  Provide day-to-day operations support and configuration management, serve as the POC for differentiating functional issues from technical issues, and resolve technical issues.  Operate both independently and as part of a Scrum team across multiple projects.  Provide guidance in pursuing innovative solutions to achieve client goals and objectives.      Bachelor's Degree and 4+ years of SharePoint application development, using Visual Studio and related code management practices.  2+ years of SharePoint application development using client-side code  including CSOM  and server-side code on multiple SharePoint platforms  2010 to 2016 .  2+ years of experience operating as part of an Agile development team Experience with SharePoint Designer, InfoPath, Web Parts, and workflow creation.  Experience with jQuery, CSOM, and front-end UI design a plus.  Experience supporting migration between SharePoint versions  i. e. , 2010-2016 is preferred.  Experience with Business Intelligence dashboards a plus.  Experience developing new software and web applications, as well experience operating and maintaining existing applications.  Experience developing cloud-based solutions, cloud-based training and certifications a plus.  Familiarity with DevOps and Site Reliability Engineering principles.  Proficiency in time management, attention to detail, and adaptability depending on circumstances.  Proven ability to work with remote teams; Capable of critical thinking for problem resolution.  US Citizen, ability to obtain Secret Clearance  Candidates with existing Secret or Top Secret, have recently worked with the intelligence community, or have recently held a DHS HQ EOD is a plus ",maintain enhance existing custom solutions built sharepoint limited farm solutions deployed sharepoint platform plan lead execute sharepoint tasks custom master pages layouts templates custom workflows implementing permissions structures timer jobs event handlers well implementation integration maintenance existing solutions gather user analyze business processes functional areas define scope projects document application functionality daytoday operations support configuration management serve poc differentiating functional issues technical issues resolve technical issues operate independently part scrum team across multiple projects guidance pursuing innovative solutions achieve client goals objectives bachelors degree sharepoint application development visual studio code management practices sharepoint application development clientside code csom serverside code multiple sharepoint platforms operating part agile development team sharepoint designer infopath web parts workflow creation jquery csom frontend ui design plus supporting migration sharepoint versions e business intelligence dashboards plus developing software web applications well operating maintaining existing applications developing cloudbased solutions cloudbased training certifications plus familiarity devops site reliability engineering principles proficiency time management attention detail adaptability depending circumstances proven remote teams capable critical thinking problem resolution us citizen obtain secret clearance candidates existing secret top secret recently worked intelligence community recently held dhs hq eod plus,maintain enhance existing custom solutions built sharepoint limited farm deployed platform plan lead execute tasks master pages layouts templates workflows implementing permissions structures timer jobs event handlers well implementation integration maintenance gather user analyze business processes functional areas define scope projects document application functionality daytoday operations support configuration management serve poc differentiating issues technical resolve operate independently part scrum team across multiple guidance pursuing innovative achieve client goals objectives bachelors degree development visual studio code practices clientside csom serverside platforms operating agile designer infopath web parts workflow creation jquery frontend ui design plus supporting migration versions e intelligence dashboards developing software applications maintaining cloudbased training certifications familiarity devops site reliability engineering principles proficiency time attention detail adaptability depending circumstances proven remote teams capable critical thinking problem resolution us citizen obtain secret clearance candidates top recently worked community held dhs hq eod
32,"  5+ yearsâ experience developing in PHP 2+ yearsâ experience working with the Drupal Content Management system, version 8 preferred Solid understanding of object-oriented programming Familiar with various design and architectural patterns Experience working with ElasticSearch and MySQL database management systems Demonstrated experience with REST API integrations Experience working with NoSQL data architectures is a huge plus Understanding fundamental design principles behind a scalable application Creating database schemas that represent and support business processes Ability to work independently, prioritize tasks and hit deadlines in a fast-paced work environment.  Demonstrates good judgment, excellent planning, problem-solving, troubleshooting, management, and communication  verbal and written  skills with the ability to think strategically, act quickly, multi-task, and work collaboratively in an environment that values creativity and flexibility to make things happen.   Perform ETL processing.  Deal with raw data that contains human, machine or instrument errors, may be un-validated, unformatted or contain suspect records or system-specific codes.  Recommend and sometimes implement ways to improve data reliability, efficiency and quality.  Design, build, and maintain efficient database structures Develop functions and scripts to import data into the designed database structure, via RESTful APIs, manual uploads, and other methods.  Develop functionality to query data for use in web applications and visualizations.   ",developing php drupal content management version solid understanding objectoriented programming familiar various design architectural patterns elasticsearch mysql database management systems demonstrated rest api integrations nosql data architectures huge plus understanding fundamental design principles behind scalable application creating database schemas represent support business processes independently prioritize tasks hit deadlines fastpaced demonstrates good judgment planning problemsolving troubleshooting management communication verbal written think strategically act quickly multitask collaboratively values creativity flexibility make things happen perform etl processing deal raw data contains human machine instrument errors may unvalidated unformatted contain suspect records systemspecific codes recommend sometimes implement ways improve data reliability efficiency design build maintain efficient database structures develop functions scripts import data designed database structure via restful apis manual uploads methods develop functionality query data use web applications visualizations,developing php drupal content management version solid understanding objectoriented programming familiar various design architectural patterns elasticsearch mysql database systems demonstrated rest api integrations nosql data architectures huge plus fundamental principles behind scalable application creating schemas represent support business processes independently prioritize tasks hit deadlines fastpaced demonstrates good judgment planning problemsolving troubleshooting communication verbal written think strategically act quickly multitask collaboratively values creativity flexibility make things happen perform etl processing deal raw contains human machine instrument errors may unvalidated unformatted contain suspect records systemspecific codes recommend sometimes implement ways improve reliability efficiency build maintain efficient structures develop functions scripts import designed structure via restful apis manual uploads methods functionality query use web applications visualizations
33,"  Bachelor's or Master's degree in computer science or software engineering preferred Experience with object-oriented design, coding and testing patterns as well as experience in engineering  commercial or open source  software platforms and large-scale data infrastructures.  Ability to architect highly scalable distributed systems, using different open source tools.  Experience building high-performance algorithms.  Extensive knowledge of different programming or scripting languages such as Java, Linux, C++, PHP, Ruby, Phyton and/or R.  Experience with different  NoSQL or RDBMS  databases such as MongoDB needed.  Experience building data processing systems with Hadoop and Hive using Java or Python   ",bachelors masters degree computer science software engineering objectoriented design coding testing patterns well engineering commercial open source software platforms largescale data infrastructures architect highly scalable distributed systems different open source tools building highperformance algorithms extensive different programming scripting languages java linux c php ruby phyton andor r different nosql rdbms databases mongodb needed building data processing systems hadoop hive java python,bachelors masters degree computer science software engineering objectoriented design coding testing patterns well commercial open source platforms largescale data infrastructures architect highly scalable distributed systems different tools building highperformance algorithms extensive programming scripting languages java linux c php ruby phyton andor r nosql rdbms databases mongodb needed processing hadoop hive python
34,"     Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting Experience with data warehousing tools and technologies Ability to work within UNIX/Linux operating systems AWS experience a plus ",distributed computing technologies hadoop hbase cassandra elasticsearch apache spark development java c scala groovy python andor shell scripting data warehousing tools technologies within unixlinux operating systems aws plus,distributed computing technologies hadoop hbase cassandra elasticsearch apache spark development java c scala groovy python andor shell scripting data warehousing tools within unixlinux operating systems aws plus
35," Must have an active Top Secret  TS  clearance.  Must be able to obtain a TS/SCI clearance Must be able to obtain DHS Suitability 10+ years of relevant database experience Demonstrated ability to lead teams with diverse skill sets  e. g.  data architects, data scientists, software developers  Demonstrated experience mentoring junior to mid-level data professionals Able to effectively work as a leader, in a group, or as an individual contributor Excellent understanding of big data and data analytics Experience working with large structured and unstructured data sets Development experience building ETL pipelines at scale Solid SQL development skills Experience with Linux/Unix tools and shell scripts Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing and encryption Good communication skills, both oral and written Must work well in a team environment as well as independently Must exhibit good time management skills, independent decision making capability, and a focus on customer service.   Using database expertise to lead teams with diverse skill sets  e. g.  data architects, data scientists, software developers  in support of a large, agile-based, cybersecurity system Working with large structured and unstructured data sets Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment Design, setup, administer, and tune NoSQL databases in the AWS cloud Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations Write and refine code to ensure quality and reliability of data extraction and processing Analyze and resolve data performance and quality issues Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.  Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages Maintain current industry knowledge of relevant concepts, practices and procedures  IBM Certified Data Engineer - Big Data Google Cloud Certified Professional - Data Engineer Cloudera Certified Professional - Data Engineer CCDH  Cloudera Certified Developer for Apache Hadoop CCAH  Cloudera Certified Administrator for Apache Hadoop CCSHB  Cloudera Certified Specialist in Apache HBase CSSLP Certified Secure Software Lifecycle Professional Certifications related to Scaled Agile Framework  SAFe  such as SAFe Practitioner  SP  or SAFe Program Consultant  SPC  DoD 8570. 1 IAT Level I  ",must active top secret ts clearance must able obtain tssci clearance must able obtain dhs suitability relevant database demonstrated lead teams diverse skill sets e g data architects data scientists software developers demonstrated mentoring junior midlevel data professionals able effectively leader group individual contributor understanding big data data analytics structured unstructured data sets development building etl pipelines scale solid sql development linuxunix tools shell scripts expertise data analysis design data modeling master data management metadata management data warehousing performance tuning data improvement data security auditing encryption good communication oral written must well team well independently must exhibit good time management independent decision making capability focus customer service database expertise lead teams diverse skill sets e g data architects data scientists software developers support agilebased cybersecurity structured unstructured data sets implementing data management systems conception stages design development deployment cicd agile design setup administer tune nosql databases aws cloud design implement technical architecture necessary support analytic statistical processing based tradeoff performance performing data transformations aggregations joins data cleaning support analytic applications visualizations write refine code reliability data extraction processing analyze resolve data performance issues make data available data scientists programmers users programming scripting languages r python java javascript perform data analysis design data modeling business intelligence management master data management metadata management data management data security auditing functions data management governance generate data estimates performed compare estimates actuals continue refine estimates collaboratively agile development teams attending daily scrums providing data solutions development team develop maintain database code libraries data models sops documentation team collaboration migrate data legacy rdbmss data sources nosql database aws cloud complex etl processes tools programming languages maintain current industry relevant concepts practices procedures ibm certified data engineer big data google cloud certified professional data engineer cloudera certified professional data engineer ccdh cloudera certified developer apache hadoop ccah cloudera certified administrator apache hadoop ccshb cloudera certified specialist apache hbase csslp certified secure software lifecycle professional certifications scaled agile framework safe safe practitioner sp safe program consultant spc dod iat level,must active top secret ts clearance able obtain tssci dhs suitability relevant database demonstrated lead teams diverse skill sets e g data architects scientists software developers mentoring junior midlevel professionals effectively leader group individual contributor understanding big analytics structured unstructured development building etl pipelines scale solid sql linuxunix tools shell scripts expertise analysis design modeling master management metadata warehousing performance tuning improvement security auditing encryption good communication oral written well team independently exhibit time independent decision making capability focus customer service support agilebased cybersecurity implementing systems conception stages deployment cicd agile setup administer tune nosql databases aws cloud implement technical architecture necessary analytic statistical processing based tradeoff performing transformations aggregations joins cleaning applications visualizations write refine code reliability extraction analyze resolve issues make available programmers users programming scripting languages r python java javascript perform business intelligence functions governance generate estimates performed compare actuals continue collaboratively attending daily scrums providing solutions develop maintain libraries models sops documentation collaboration migrate legacy rdbmss sources complex processes current industry concepts practices procedures ibm certified engineer google professional cloudera ccdh developer apache hadoop ccah administrator ccshb specialist hbase csslp secure lifecycle certifications scaled framework safe practitioner sp program consultant spc dod iat level
36,"TS/SCI with Polygraph required Bachelor's and fourteen  14  years of related experience.  Experience may be substituted in lieu of degree Expertise developing and implementing robust data schemas across heterogeneous datasets Expertise with data security, and data tagging; experience with customer policies and systems is preferred; understanding of relevant privacy and data deconstruction policies or ability to develop expertise quickly Expertise processing structured and unstructured data Experience using python and/or R is beneficial Experience leveraging APIs for data ingest and sharing    ",tssci polygraph bachelors fourteen may substituted lieu degree expertise developing implementing robust data schemas across heterogeneous datasets expertise data security data tagging customer policies systems understanding relevant privacy data deconstruction policies develop expertise quickly expertise processing structured unstructured data python andor r beneficial leveraging apis data ingest sharing,tssci polygraph bachelors fourteen may substituted lieu degree expertise developing implementing robust data schemas across heterogeneous datasets security tagging customer policies systems understanding relevant privacy deconstruction develop quickly processing structured unstructured python andor r beneficial leveraging apis ingest sharing
37,"    Using database expertise to lead teams with diverse skill sets  e. g.  data architects, data scientists, software developers  in support of a large, agile-based, cybersecurity system.  Working with large structured and unstructured data sets.  Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment.  Design, setup, administer, and tune NoSQL databases in the AWS cloud.  Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality.  Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations.  Write and refine code to ensure quality and reliability of data extraction and processing.  Analyze and resolve data performance and quality issues.  Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.  Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance.  Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates.  Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team.  Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment.  Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages.  Maintain current industry knowledge of relevant concepts, practices and procedures ",database expertise lead teams diverse skill sets e g data architects data scientists software developers support agilebased cybersecurity structured unstructured data sets implementing data management systems conception stages design development deployment cicd agile design setup administer tune nosql databases aws cloud design implement technical architecture necessary support analytic statistical processing based tradeoff performance performing data transformations aggregations joins data cleaning support analytic applications visualizations write refine code reliability data extraction processing analyze resolve data performance issues make data available data scientists programmers users programming scripting languages r python java javascript perform data analysis design data modeling business intelligence management master data management metadata management data management data security auditing functions data management governance generate data estimates performed compare estimates actuals continue refine estimates collaboratively agile development teams attending daily scrums providing data solutions development team develop maintain database code libraries data models sops documentation team collaboration migrate data legacy rdbmss data sources nosql database aws cloud complex etl processes tools programming languages maintain current industry relevant concepts practices procedures,database expertise lead teams diverse skill sets e g data architects scientists software developers support agilebased cybersecurity structured unstructured implementing management systems conception stages design development deployment cicd agile setup administer tune nosql databases aws cloud implement technical architecture necessary analytic statistical processing based tradeoff performance performing transformations aggregations joins cleaning applications visualizations write refine code reliability extraction analyze resolve issues make available programmers users programming scripting languages r python java javascript perform analysis modeling business intelligence master metadata security auditing functions governance generate estimates performed compare actuals continue collaboratively attending daily scrums providing solutions team develop maintain libraries models sops documentation collaboration migrate legacy rdbmss sources complex etl processes tools current industry relevant concepts practices procedures
38," Bachelorâs degree in Computer Engineering, Computer Science, Management Information Systems, or a related business field.  5+ years of development, architecture, and configuration skills in Microsoft SQL Server and extensive experience with SQL Server Reporting Services  SSRS  and SQL Server Integration Services  SSIS .  Should possess expert level competence with Microsoft T-SQL, Microsoft PowerShell, Microsoft Visual Studio, and Microsoft SQL Server Management Studio  SSMS .  Experience with Power BI Experience with BI analytics stack including reports, dashboards, and scorecards.     The Senior BI Intelligence / Data Engineer is responsible for designing enterprise and departmental business intelligence, data warehousing and reporting solutions.  Work with stakeholders at various levels to assist with data-related technical issues and support their data acquisition and infrastructure needs.  Work with analytics to develop greater functionality in data presentation.  Solutions include, end-user reports, data visualizations, ETL systems, master data management and other BI Solutions.  This position will perform requirements analysis, design, and implementation of end-user requested BI solutions.  Translate business reporting and analytic requirements into ETL and report specifications.  Develop and implement ETL processes, reports and queries in support of business analytics.  Develop and implement interactive analytic reports and dashboards.    ",bachelors degree computer engineering computer science management information systems business development architecture configuration microsoft sql server extensive sql server reporting services ssrs sql server integration services ssis possess expert level competence microsoft tsql microsoft powershell microsoft visual studio microsoft sql server management studio ssms power bi bi analytics stack reports dashboards scorecards senior bi intelligence data engineer responsible designing enterprise departmental business intelligence data warehousing reporting solutions stakeholders various levels assist datarelated technical issues support data acquisition infrastructure needs analytics develop greater functionality data presentation solutions include enduser reports data visualizations etl systems master data management bi solutions position perform analysis design implementation enduser requested bi solutions translate business reporting analytic etl report specifications develop implement etl processes reports queries support business analytics develop implement interactive analytic reports dashboards,bachelors degree computer engineering science management information systems business development architecture configuration microsoft sql server extensive reporting services ssrs integration ssis possess expert level competence tsql powershell visual studio ssms power bi analytics stack reports dashboards scorecards senior intelligence data engineer responsible designing enterprise departmental warehousing solutions stakeholders various levels assist datarelated technical issues support acquisition infrastructure needs develop greater functionality presentation include enduser visualizations etl master position perform analysis design implementation requested translate analytic report specifications implement processes queries interactive
39,  Lead the team   8-10  with full stack development for DevOps efforts for a federal customer   ,lead team full stack development devops efforts federal customer,lead team full stack development devops efforts federal customer
40,"Active Top Secret security clearance with SCI eligibility BS degree in a related scientific or engineering discipline from an accredited college or university and ten  10  to fourteen  14  years of progressive experience, or an MS degree in a related scientific or engineering discipline, and eight  8  to twelve  12  years of progressive experience, or a Ph. D.  degree in a related scientific or engineering discipline and four  4  to seven  7  years of progressive experience.  Familiar with ETL technologies, MapReduce, JSON/XML transformations and schemas Familiar with AngularJS Familiar with Apache NiFi and Java  NAR  NiFi Archives Knowledge of Amazon Web Services  AWS Cloud  Programming languages â Java/JEE, Javascript, Python, Groovy, Shell Script HTTP via REST and SOAP Datastores â HDFS, MongoDB, S3, Elastic, NoSQL, RDBMS Build and Configuration Management Tools â Maven, Ansible, Puppet Working knowledge with public keys and digital certificates Linux/Unix server environments  Preferred Qualifications  desired but not required   Masterâs level education Familiarity with  jQuery, XPath, XQuery, Spark, Impala, Sqoop, Hive/Pig, Python, Gradle, Maven, PL/SQL, Unix Shell, C++/C, AngularJS, Spring, JSON, XML/XSLT/HTML, JPA/Hibernate, Spark, Accumulo, MapReduce, Storm/Kafka, HSpace, Pig, Servlet/JSP, LDAP     ",active top secret security clearance sci eligibility bs degree scientific engineering discipline accredited college university ten fourteen progressive ms degree scientific engineering discipline eight twelve progressive ph degree scientific engineering discipline four seven progressive familiar etl technologies mapreduce jsonxml transformations schemas familiar angularjs familiar apache nifi java nar nifi archives amazon web services aws cloud programming languages javajee javascript python groovy shell script http via rest soap datastores hdfs mongodb elastic nosql rdbms build configuration management tools maven ansible puppet public keys digital certificates linuxunix server environments qualifications desired masters level education familiarity jquery xpath xquery spark impala sqoop hivepig python gradle maven plsql unix shell cc angularjs spring json xmlxslthtml jpahibernate spark accumulo mapreduce stormkafka hspace pig servletjsp ldap,active top secret security clearance sci eligibility bs degree scientific engineering discipline accredited college university ten fourteen progressive ms eight twelve ph four seven familiar etl technologies mapreduce jsonxml transformations schemas angularjs apache nifi java nar archives amazon web services aws cloud programming languages javajee javascript python groovy shell script http via rest soap datastores hdfs mongodb elastic nosql rdbms build configuration management tools maven ansible puppet public keys digital certificates linuxunix server environments qualifications desired masters level education familiarity jquery xpath xquery spark impala sqoop hivepig gradle plsql unix cc spring json xmlxslthtml jpahibernate accumulo stormkafka hspace pig servletjsp ldap
41,"TOP SECRET Cleared professional required to strategize and execute on logical and physical aspects of growing Enterprise Data Warehouse.  Expertise in dimensional modeling, snowflake and star schemas, snapshots, facts, etc.  Candidate will assist multiple Scrum teams develop logical and physical models as well as data load strategies.  Other important areas of expertise include  1  entity-relationship modeling techniques 2  translate a logical dimensional model into a star-schema design 3  Client interaction skills 4  Use of tools such as ErWIN 5  Data Security considerations 6  Work well on teams  Fluency in SQL and NoSQL database technologies such as SQL Server, Oracle SQL Fluency in programming languages such as Python and R; and query languages such as SQL Proficiency in data extraction, transformation, and loading to support advanced analytics Familiarity with JSON and XML data formats Experience in web scraping  e. g. , using R or Python  and retrieving data from APIs Experience with data visualization tools, such as Tableau, Qlik, PowerBI, d3. js, or equivalent Ability to identify solutions for communicating across different technologies Creativity and ability to look past existing workflows and identify opportunities for optimization Thrives in fast-paced work environment with multiple stakeholders Ability to decompose a technical problem into its sub-components and build a plan to rigorously tackle the process that is defensible and repeatable Strong strategic communication skills to articulate pipeline flows and actively listening to identify business problems and their causes High-performing team player    ",top secret cleared professional strategize execute logical physical aspects growing enterprise data warehouse expertise dimensional modeling snowflake star schemas snapshots facts candidate assist multiple scrum teams develop logical physical models well data load strategies important areas expertise include entityrelationship modeling techniques translate logical dimensional model starschema design client interaction use tools erwin data security considerations well teams fluency sql nosql database technologies sql server oracle sql fluency programming languages python r query languages sql proficiency data extraction transformation loading support advanced analytics familiarity json xml data formats web scraping e g r python retrieving data apis data visualization tools tableau qlik powerbi js identify solutions communicating across different technologies creativity look past existing workflows identify opportunities optimization thrives fastpaced multiple stakeholders decompose technical problem subcomponents build plan rigorously tackle process defensible repeatable strategic communication articulate pipeline flows actively listening identify business problems causes highperforming team player,top secret cleared professional strategize execute logical physical aspects growing enterprise data warehouse expertise dimensional modeling snowflake star schemas snapshots facts candidate assist multiple scrum teams develop models well load strategies important areas include entityrelationship techniques translate model starschema design client interaction use tools erwin security considerations fluency sql nosql database technologies server oracle programming languages python r query proficiency extraction transformation loading support advanced analytics familiarity json xml formats web scraping e g retrieving apis visualization tableau qlik powerbi js identify solutions communicating across different creativity look past existing workflows opportunities optimization thrives fastpaced stakeholders decompose technical problem subcomponents build plan rigorously tackle process defensible repeatable strategic communication articulate pipeline flows actively listening business problems causes highperforming team player
42," Extensive experience in engineering and designing data management solutions using Hadoop ecosystem tools and technologies with particular emphasis in using Sqoop, Spark, Scala, Python, Java, Shell HIVE and IMPALA for minimum of three  3  years Proficient in the data ingestion pipeline process, exception handling and metadata management on big data platforms Extensive experience in architecting, and designing data architecture solutions using Hadoop ecosystem tools and technologies like, Hive, Impala, HBase, and Solr Extensive experience in GitHub CI/CD Experience providing technical and data leadership to the application development terms, IT and the enterprise.  Experience in designing shared data assets such as data discovery and analytics platforms, metadata, operational data stores, data warehouses and data marts Experience in business intelligence disciplines, and a deep understanding of Data Warehousing BI, and advanced analytics concepts in large organizations   Extensive experience in engineering and designing data management solutions using Hadoop ecosystem tools and technologies with particular emphasis in using Sqoop, Spark, Scala, Python, Java, Shell HIVE and IMPALA for minimum of three  3  years Proficient in the data ingestion pipeline process, exception handling and metadata management on big data platforms Extensive experience in architecting, and designing data architecture solutions using Hadoop ecosystem tools and technologies like, Hive, Impala, HBase, and Solr Extensive experience in GitHub CI/CD Experience providing technical and data leadership to the application development terms, IT and the enterprise.  Experience in designing shared data assets such as data discovery and analytics platforms, metadata, operational data stores, data warehouses and data marts Experience in business intelligence disciplines, and a deep understanding of Data Warehousing BI, and advanced analytics concepts in large organizations  ",extensive engineering designing data management solutions hadoop ecosystem tools technologies particular emphasis sqoop spark scala python java shell hive impala minimum three proficient data ingestion pipeline process exception handling metadata management big data platforms extensive architecting designing data architecture solutions hadoop ecosystem tools technologies like hive impala hbase solr extensive github cicd providing technical data leadership application development terms enterprise designing shared data assets data discovery analytics platforms metadata operational data stores data warehouses data marts business intelligence disciplines deep understanding data warehousing bi advanced analytics concepts organizations extensive engineering designing data management solutions hadoop ecosystem tools technologies particular emphasis sqoop spark scala python java shell hive impala minimum three proficient data ingestion pipeline process exception handling metadata management big data platforms extensive architecting designing data architecture solutions hadoop ecosystem tools technologies like hive impala hbase solr extensive github cicd providing technical data leadership application development terms enterprise designing shared data assets data discovery analytics platforms metadata operational data stores data warehouses data marts business intelligence disciplines deep understanding data warehousing bi advanced analytics concepts organizations,extensive engineering designing data management solutions hadoop ecosystem tools technologies particular emphasis sqoop spark scala python java shell hive impala minimum three proficient ingestion pipeline process exception handling metadata big platforms architecting architecture like hbase solr github cicd providing technical leadership application development terms enterprise shared assets discovery analytics operational stores warehouses marts business intelligence disciplines deep understanding warehousing bi advanced concepts organizations
43," 5 to 7+ years of experience in using SQL and data warehousing in a business environment5+ years of experience in custom ETL design, implementation, and maintenance5+ years of experience with data warehouse schema design and data modelingProduction level experience with Python, Java, SQL, and shell scriptingExperience with cloud databases and managed servicesExperience with batch and stream processingExperience with microservice patterns, API developmentExperience with building large scale data processing systemSolid understanding of data design patterns and best practicesWorking knowledge of data visualization tools such as Tableau, PowerBI is a plusExperience to analyze data to identify deliverables, gaps, and inconsistenciesFamiliarity with agile software development practices and drive to ship quicklyExperience leading change, taking initiative, and driving resultsEffective communication skills and strong problem-solving skillsProven ability and desire to mentor others in a team environmentRetail experience highly desired  5 to 7+ years of experience in using SQL and data warehousing in a business environment5+ years of experience in custom ETL design, implementation, and maintenance5+ years of experience with data warehouse schema design and data modelingProduction level experience with Python, Java, SQL, and shell scriptingExperience with cloud databases and managed servicesExperience with batch and stream processingExperience with microservice patterns, API developmentExperience with building large scale data processing systemSolid understanding of data design patterns and best practicesWorking knowledge of data visualization tools such as Tableau, PowerBI is a plusExperience to analyze data to identify deliverables, gaps, and inconsistenciesFamiliarity with agile software development practices and drive to ship quicklyExperience leading change, taking initiative, and driving resultsEffective communication skills and strong problem-solving skillsProven ability and desire to mentor others in a team environmentRetail experience highly desired ",sql data warehousing business custom etl design implementation maintenance data warehouse schema design data modelingproduction level python java sql shell scriptingexperience cloud databases managed servicesexperience batch stream processingexperience microservice patterns api developmentexperience building scale data processing systemsolid understanding data design patterns best practicesworking data visualization tools tableau powerbi plusexperience analyze data identify deliverables gaps inconsistenciesfamiliarity agile software development practices drive ship quicklyexperience leading change taking initiative driving resultseffective communication problemsolving skillsproven desire mentor others team environmentretail highly desired sql data warehousing business custom etl design implementation maintenance data warehouse schema design data modelingproduction level python java sql shell scriptingexperience cloud databases managed servicesexperience batch stream processingexperience microservice patterns api developmentexperience building scale data processing systemsolid understanding data design patterns best practicesworking data visualization tools tableau powerbi plusexperience analyze data identify deliverables gaps inconsistenciesfamiliarity agile software development practices drive ship quicklyexperience leading change taking initiative driving resultseffective communication problemsolving skillsproven desire mentor others team environmentretail highly desired,sql data warehousing business custom etl design implementation maintenance warehouse schema modelingproduction level python java shell scriptingexperience cloud databases managed servicesexperience batch stream processingexperience microservice patterns api developmentexperience building scale processing systemsolid understanding best practicesworking visualization tools tableau powerbi plusexperience analyze identify deliverables gaps inconsistenciesfamiliarity agile software development practices drive ship quicklyexperience leading change taking initiative driving resultseffective communication problemsolving skillsproven desire mentor others team environmentretail highly desired
44," At least one  1  year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models At least two  2  years of experience in Scala, Python, Apache Spark and SQL Experience working directly with relational database structures and flat files Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.  Good verbal and written communication skills, with both technical and non-technical stakeholders   Build pipelines to ingest and maintain complex data sets into Cerebri AIâs proprietary data stores for use in machine learning modeling Develop and maintain data ontologies for key market segments Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data Write quality documentation on the discovery process and software projects Work equally well in a team environment and on your own.  Communicate complex ideas clearly with both team members and clients Travel up to 25%  ",least one year designing building data processing solutions etl pipelines varied data formats ideally company leverages machine models least two scala python apache spark sql directly relational database structures flat files write efficient database queries functions views include complex joins identification development custom indices professional software engineering practices best practices full software development life cycle coding standards code reviews source control management build processes testing continuous integration development operations good verbal written communication technical nontechnical stakeholders build pipelines ingest maintain complex data sets cerebri ais proprietary data stores use machine modeling develop maintain data ontologies key market segments collaborate data scientists perform exploratory data analysis map data fields proprietary data stores find signals client data collaborate clients develop pipeline infrastructure ask appropriate questions gain deep understanding client data write documentation discovery process software projects equally well team communicate complex ideas clearly team members clients travel,least one year designing building data processing solutions etl pipelines varied formats ideally company leverages machine models two scala python apache spark sql directly relational database structures flat files write efficient queries functions views include complex joins identification development custom indices professional software engineering practices best full life cycle coding standards code reviews source control management build processes testing continuous integration operations good verbal written communication technical nontechnical stakeholders ingest maintain sets cerebri ais proprietary stores use modeling develop ontologies key market segments collaborate scientists perform exploratory analysis map fields find signals client clients pipeline infrastructure ask appropriate questions gain deep understanding documentation discovery process projects equally well team communicate ideas clearly members travel
45,  5+ years of advanced knowledge of PL/SQL development 5+ years of Data Modeling 5+ years of Data Warehousing 5+ years of Oracle Database storage and schema design 1+ years of Salesforce Experience  Must have significant development experience with PL/SQL.  Design and develop a data mart within an Oracle environment.  Experience in mortgage banking or home loans a plus.  Design and develop a data mart within an Oracle environment  Bachelor's degree is required ,advanced plsql development data modeling data warehousing oracle database storage schema design salesforce must significant development plsql design develop data mart within oracle mortgage banking home loans plus design develop data mart within oracle bachelors degree,advanced plsql development data modeling warehousing oracle database storage schema design salesforce must significant develop mart within mortgage banking home loans plus bachelors degree
46," Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.  Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured  Postgres, MySQL  and unstructured data sources  Elastic, Kafka, and the Hadoop ecosystem  as implemented at Internet-scale.  Experience writing and optimizing streaming and batch analytics.  Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.  BS/BA in Computer Science, Engineering, or relevant field experience.     ",handson programming expertise multiple implementation languagesframeworks subset python java scala delivery background middleware backend implementations familiarity largescale big data streaming data technologies well exposure variety structured postgres mysql unstructured data sources elastic kafka hadoop ecosystem implemented internetscale writing optimizing streaming batch analytics agile frameworks secure software design testdriven development modern containerdelivered code deployment cloudbased devops bsba computer science engineering relevant,handson programming expertise multiple implementation languagesframeworks subset python java scala delivery background middleware backend implementations familiarity largescale big data streaming technologies well exposure variety structured postgres mysql unstructured sources elastic kafka hadoop ecosystem implemented internetscale writing optimizing batch analytics agile frameworks secure software design testdriven development modern containerdelivered code deployment cloudbased devops bsba computer science engineering relevant
47,"Bachelorâs degree in a quantitative field  e. g. , engineering, statistics, mathematics, information technology, etc.   is preferred.  Master's degree is desired.  Must have at least 3 years of experience, preferably with a federal government customer.  Experience with big data tools  Hadoop, Spark, Kafka Experience with relational SQL and NoSQL databases  Postgres, Cassandra, MongoDB Experience with data governance tools  Collibra, Immuta Experience with AWS cloud services  EC2, EMR, RDS, Redshift Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala Must possess strong written and verbal communication skills.  DoD Secret Clearance Required  Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.   ",bachelors degree quantitative e g engineering statistics mathematics information technology masters degree desired must least preferably federal government customer big data tools hadoop spark kafka relational sql nosql databases postgres cassandra mongodb data governance tools collibra immuta aws cloud services ec emr rds redshift objectorientedobject function scripting languages python java c scala must possess written verbal communication dod secret clearance develop data structures systems support generation business insightsknowledge overall etl processesmaintain data infrastructure develop scripts regular processesdefine design develop data flow diagrams data dictionaries logical physical modelsdefine data document data elements capture maintain metadetaidentify clean incomplete incorrect inaccurate irrelevant dataidentify opportunities use data improve business performancecommunicate present data developing reports tableau business intelligence toolsadhere compliance audit data storage architecture cybersecurity,bachelors degree quantitative e g engineering statistics mathematics information technology masters desired must least preferably federal government customer big data tools hadoop spark kafka relational sql nosql databases postgres cassandra mongodb governance collibra immuta aws cloud services ec emr rds redshift objectorientedobject function scripting languages python java c scala possess written verbal communication dod secret clearance develop structures systems support generation business insightsknowledge overall etl processesmaintain infrastructure scripts regular processesdefine design flow diagrams dictionaries logical physical modelsdefine document elements capture maintain metadetaidentify clean incomplete incorrect inaccurate irrelevant dataidentify opportunities use improve performancecommunicate present developing reports tableau intelligence toolsadhere compliance audit storage architecture cybersecurity
48," Masterâs degree in engineering, computer science, or other related technical field or Bachelorâs degree in a business or management-related field accompanied by experience managing technical requirements in complex programs.  Experience displaying expert domain knowledge in a technical field.  Experience negotiating complex scenarios and challenges and devising courses of action to resolve situations with predictable outcomes.  Experience leading critical objectives where decision making is of utmost concern to the outcome.  Experience supervising others.      ",masters degree engineering computer science technical bachelors degree business managementrelated accompanied managing technical complex programs displaying expert domain technical negotiating complex scenarios challenges devising courses action resolve situations predictable outcomes leading critical objectives decision making utmost concern outcome supervising others,masters degree engineering computer science technical bachelors business managementrelated accompanied managing complex programs displaying expert domain negotiating scenarios challenges devising courses action resolve situations predictable outcomes leading critical objectives decision making utmost concern outcome supervising others
49,"Candidate shall have demonstrated at least 5 years of experience managing and maintaining structured, semi-structured, and unstructured data, as well as structuring and wrangling data as appropriate for statistical data analysis.  Candidate shall possess the necessary skills to gather, define, document, analyze and implement ETL/data Extensive experience in understanding data warehouse concepts and relational databases.  Candidate shall possess the necessary skills to model database structures, tables, and fields.  Candidate shall possess a basic understanding of statistics as related to their experiences in structuring data for statistical analyses.  Candidate shall possess excellent oral and written communication skills with emphasis on complex technical topics and effectively communicating details with all levels of management.  Candidate shall possess the necessary people skills to identify requirements and deliver results.  All personnel shall have at a minimum an active security clearance of Top Secret, meaning the clearance initially issued or revalidated within the past five years, at the time of the proposal submission and Single Scope Background Investigation.     ",candidate shall demonstrated least managing maintaining structured semistructured unstructured data well structuring wrangling data appropriate statistical data analysis candidate shall possess necessary gather define document analyze implement etldata extensive understanding data warehouse concepts relational databases candidate shall possess necessary model database structures tables fields candidate shall possess basic understanding statistics experiences structuring data statistical analyses candidate shall possess oral written communication emphasis complex technical topics effectively communicating details levels management candidate shall possess necessary people identify deliver results personnel shall minimum active security clearance top secret meaning clearance initially issued revalidated within past five time proposal submission single scope background investigation,candidate shall demonstrated least managing maintaining structured semistructured unstructured data well structuring wrangling appropriate statistical analysis possess necessary gather define document analyze implement etldata extensive understanding warehouse concepts relational databases model database structures tables fields basic statistics experiences analyses oral written communication emphasis complex technical topics effectively communicating details levels management people identify deliver results personnel minimum active security clearance top secret meaning initially issued revalidated within past five time proposal submission single scope background investigation
50," Understanding the basics of distributed systems Knowledge of algorithms and data structures Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases  PostgreSQL, MySQL, etc.  .  4+ Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, etc Demonstrable experience with Kafka or Hadoop Experience with data visualization tools like Tableau and/or ElasticSearch will be a big plus Experience with AWS cloud services  EC2, EMR, RDS, Redshift  Understanding the basics of distributed systems Knowledge of algorithms and data structures Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases  PostgreSQL, MySQL, etc.  .  4+ Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, etc Demonstrable experience with Kafka or Hadoop Experience with data visualization tools like Tableau and/or ElasticSearch will be a big plus Experience with AWS cloud services  EC2, EMR, RDS, Redshift  Assemble large, complex data sets that meet functional / non-functional business requirements.  Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS âbig dataâ technologies.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.  Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.  Work with data and analytics experts to strive for greater functionality in our data systems.  Performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Build processes supporting data transformation, data structures, metadata, dependency and workload management.   ",understanding basics distributed systems algorithms data structures advanced sql relational databases query authoring sql well familiarity variety databases postgresql mysql objectorientedobject function scripting languages python java c scala demonstrable kafka hadoop data visualization tools like tableau andor elasticsearch big plus aws cloud services ec emr rds redshift understanding basics distributed systems algorithms data structures advanced sql relational databases query authoring sql well familiarity variety databases postgresql mysql objectorientedobject function scripting languages python java c scala demonstrable kafka hadoop data visualization tools like tableau andor elasticsearch big plus aws cloud services ec emr rds redshift assemble complex data sets meet functional nonfunctional business identify design implement internal process improvements automating manual processes optimizing data delivery redesigning infrastructure greater scalability build infrastructure optimal extraction transformation loading data wide variety data sources sql aws big data technologies build analytics tools utilize data pipeline actionable insights customer acquisition operational efficiency key business performance metrics stakeholders executive product data design teams assist datarelated technical issues support data infrastructure needs keep data separated secure across national boundaries multiple data centers aws regions create data tools analytics data scientist team members assist building optimizing product innovative industry leader data analytics experts strive greater functionality data systems performing root cause analysis internal external data processes answer specific business questions identify opportunities improvement build processes supporting data transformation data structures metadata dependency workload management,understanding basics distributed systems algorithms data structures advanced sql relational databases query authoring well familiarity variety postgresql mysql objectorientedobject function scripting languages python java c scala demonstrable kafka hadoop visualization tools like tableau andor elasticsearch big plus aws cloud services ec emr rds redshift assemble complex sets meet functional nonfunctional business identify design implement internal process improvements automating manual processes optimizing delivery redesigning infrastructure greater scalability build optimal extraction transformation loading wide sources technologies analytics utilize pipeline actionable insights customer acquisition operational efficiency key performance metrics stakeholders executive product teams assist datarelated technical issues support needs keep separated secure across national boundaries multiple centers regions create scientist team members building innovative industry leader experts strive functionality performing root cause analysis external answer specific questions opportunities improvement supporting metadata dependency workload management
51," Bachelor's Degree in Computer Science or equivalent work experience 2-4 years of experience in Software Development Strong Python skills with experience in Java/C++ or other object oriented language.  Experience with two or more of the following  Code optimization Data structures Elasticsearch SQL Linux / Ubuntu JSON / YAML AWS SQS, S3, EC2 Compilers/Parsers Docker    Assume responsibility for key parts of the Interos Knowledge Graph product Translate Data Science experimental code into tested, production ready modules Create connectors for ingesting data from third party databases, APIs, and web sites Optimize data structures, improve code execution, and reduce memory consumption of existing implementation Develop JSON APIs   ",bachelors degree computer science software development python javac object oriented language two following code optimization data structures elasticsearch sql linux ubuntu json yaml aws sqs ec compilersparsers docker assume responsibility key parts interos graph product translate data science experimental code tested production ready modules create connectors ingesting data third party databases apis web sites optimize data structures improve code execution reduce memory consumption existing implementation develop json apis,bachelors degree computer science software development python javac object oriented language two following code optimization data structures elasticsearch sql linux ubuntu json yaml aws sqs ec compilersparsers docker assume responsibility key parts interos graph product translate experimental tested production ready modules create connectors ingesting third party databases apis web sites optimize improve execution reduce memory consumption existing implementation develop
52,"   Work in a fast-paced agile environment Building efficient storage for structured and unstructured data Developing and deploying distributed computing Big Data applications using Frameworks like Apache Spark, Presto, Apex, Kafka on AWS Cloud using EMR and/or Amazon Athena Utilizing programming languages like Java, Scala, Python, and Cloud-based services such as Athena and Glue Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Maven, Terraform, Git, BitBucket and Docker Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance Write, design, code, test, implement, debug, and validate applications; document design decisions and develop modular software components; monitor system performance metrics, and identify potential risks/issues Collaborate in agile scrum team with product owners and fellow software engineers to deliver upon most important business and technical priorities Provide active mentorship/guidance to fellow members of the agile tech team and participate in internal and external technology conference & communities  ",fastpaced agile building efficient storage structured unstructured data developing deploying distributed computing big data applications frameworks like apache spark presto apex kafka aws cloud emr andor amazon athena utilizing programming languages like java scala python cloudbased services athena glue leveraging devops techniques practices like continuous integration continuous deployment test automation build automation test driven development enable rapid delivery code utilizing tools like jenkins maven terraform git bitbucket docker performing unit tests conducting reviews team members make sure code rigorously designed elegantly coded effectively tuned performance write design code test implement debug validate applications document design decisions develop modular software components monitor performance metrics identify potential risksissues collaborate agile scrum team product owners fellow software engineers deliver upon important business technical priorities active mentorshipguidance fellow members agile tech team participate internal external technology conference communities,fastpaced agile building efficient storage structured unstructured data developing deploying distributed computing big applications frameworks like apache spark presto apex kafka aws cloud emr andor amazon athena utilizing programming languages java scala python cloudbased services glue leveraging devops techniques practices continuous integration deployment test automation build driven development enable rapid delivery code tools jenkins maven terraform git bitbucket docker performing unit tests conducting reviews team members make sure rigorously designed elegantly coded effectively tuned performance write design implement debug validate document decisions develop modular software components monitor metrics identify potential risksissues collaborate scrum product owners fellow engineers deliver upon important business technical priorities active mentorshipguidance tech participate internal external technology conference communities
53,"Bachelorâs degree with a major in a analytical  e. g. , engineering, statistics, mathematics, information technology, etc.   or social science discipline is preferred.  Master's degree is desired.  Must have at least 3 years of experience, preferably with an IC customer.  Must have one or more of the following skillsets through education or work experience  quantitative and qualitative analysis; developing reports; scripting tools; and data warehouses and environments.  Must possess strong written and verbal communication skills.  Must have an active TS/SCI clearance with Polygraph.   Define data requirements and gather and validate information, applying judgment and statistical tests. Develop data structures to support the generation of business insights and strategy. Maintain data infrastructure and develop scripts for regular processes. Design and develop operational data analysis and visualization tools, techniques, metrics, and dashboards to meet business needs. Analyze data analysis requests obtained from management to determine operational problems and define data modeling requirements, validation of content, and problem solving parameters. Identify opportunities to use data to develop new strategies and improve business performance and utilize knowledge of mathematical modeling and other optimization methods to perform quantitative and qualitative data analysis. Communicate and present data to management by developing reports using Tableau or Business Intelligence tools.   ",bachelors degree major analytical e g engineering statistics mathematics information technology social science discipline masters degree desired must least preferably ic customer must one following skillsets education quantitative qualitative analysis developing reports scripting tools data warehouses environments must possess written verbal communication must active tssci clearance polygraph define data gather validate information applying judgment statistical tests develop data structures support generation business insights strategy maintain data infrastructure develop scripts regular processes design develop operational data analysis visualization tools techniques metrics dashboards meet business needs analyze data analysis requests obtained management determine operational problems define data modeling validation content problem solving parameters identify opportunities use data develop strategies improve business performance utilize mathematical modeling optimization methods perform quantitative qualitative data analysis communicate present data management developing reports tableau business intelligence tools,bachelors degree major analytical e g engineering statistics mathematics information technology social science discipline masters desired must least preferably ic customer one following skillsets education quantitative qualitative analysis developing reports scripting tools data warehouses environments possess written verbal communication active tssci clearance polygraph define gather validate applying judgment statistical tests develop structures support generation business insights strategy maintain infrastructure scripts regular processes design operational visualization techniques metrics dashboards meet needs analyze requests obtained management determine problems modeling validation content problem solving parameters identify opportunities use strategies improve performance utilize mathematical optimization methods perform communicate present tableau intelligence
54,"At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ",least consulting client service delivery azure devops azure platform proven build manage foster teamoriented,least consulting client service delivery azure devops platform proven build manage foster teamoriented
55,"   Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five  5  years technology industry or related experience, including items such as Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive  5  years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support. Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.  ",bachelors degree computer science combination education experiencetechnical training demonstrates analytical technical competencyminimum five technology industry items build highly scalable scaledout architectures scale database platformsexperience complex data infrastructure environmentfive data engineering roleextensive depth data pipeline development industry standard data integration toolsexperience sdlc process gathering analysis architecture design implementation testing deployment technical support industry standard tool source control project managementexperience writing test cases test scripts data assuranceexperience creating stored procedures functionsexperience developing dimensional data model industry standard tool,bachelors degree computer science combination education experiencetechnical training demonstrates analytical technical competencyminimum five technology industry items build highly scalable scaledout architectures scale database platformsexperience complex data infrastructure environmentfive engineering roleextensive depth pipeline development standard integration toolsexperience sdlc process gathering analysis architecture design implementation testing deployment support tool source control project managementexperience writing test cases scripts assuranceexperience creating stored procedures functionsexperience developing dimensional model
56,Typically has 7 or more years of consulting and/or industry experience    ,typically consulting andor industry,typically consulting andor industry
57,"     BS in Computer Science, Mathematics, Software Engineering, or other relevant field.  2 or more years of software development and a passion for working with large data sets.  Demonstrated extensive experience working in large-scale data environments which includes real time and batch processing requirements, as well as graph databases.  Experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures.  Hands-on coding experience in programming languages such as Java, Python, Scala.  Experience with Big Data platforms and technologies â AWS, Apache, Hadoop, Azure, and advanced analytics tools.  Experience with ETL processing with large data stores.  Working knowledge of data security management policies and procedures.  Knowledge and experience with Agile, Scrum and DevOps principles and practices and working on collaborative development teams.  ",bs computer science mathematics software engineering relevant software development passion data sets demonstrated extensive largescale data environments includes real time batch processing well graph databases designing delivering scale missioncritical data pipelines features modern big data architectures handson coding programming languages java python scala big data platforms technologies aws apache hadoop azure advanced analytics tools etl processing data stores data security management policies procedures agile scrum devops principles practices collaborative development teams,bs computer science mathematics software engineering relevant development passion data sets demonstrated extensive largescale environments includes real time batch processing well graph databases designing delivering scale missioncritical pipelines features modern big architectures handson coding programming languages java python scala platforms technologies aws apache hadoop azure advanced analytics tools etl stores security management policies procedures agile scrum devops principles practices collaborative teams
58," Active Top-Secret US Government clearance Bachelorâs Degree in Statistics, Mathematics, Computer Science, Management Information Systems, Engineering, Business Analytics disciplines, or related area 3+ years of experience with ETL and data integration, data quality analysis, statistical analysis and/or modeling Advanced working SQL knowledge and experience with RDBMS platforms Experience building and optimizing data pipelines, architectures, and data sets Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Strong business and analytical skills including direct interaction with business end users for requirements analysis    ",active topsecret us government clearance bachelors degree statistics mathematics computer science management information systems engineering business analytics disciplines area etl data integration data analysis statistical analysis andor modeling advanced sql rdbms platforms building optimizing data pipelines architectures data sets performing root cause analysis internal external data processes answer specific business questions identify opportunities improvement business analytical direct interaction business end users analysis,active topsecret us government clearance bachelors degree statistics mathematics computer science management information systems engineering business analytics disciplines area etl data integration analysis statistical andor modeling advanced sql rdbms platforms building optimizing pipelines architectures sets performing root cause internal external processes answer specific questions identify opportunities improvement analytical direct interaction end users
59,"Experienced with Oracle Business Intelligence  OBIEE  is a must.  SQL & PLSQL query development experience.  Python scripting experience.  Proven ability to work independently and as a team member.  Good communication  written and oral  and interpersonal skills.  Good organizational, multi-tasking, and time-management skills.  US Citizen in order to obtain the Public Trust.     Working with the client to define business questions  i. e.  problem statements , associated metrics, data challenges, and scope business objectives.  Ability to manipulate and understand large, complex data sets.  Research/analyze/create data models to drive statistical analysis and results related to confirmed client business questions.  Build analytic models using reinforcement learning techniques to business decisions.  Leverage suite of analytic models  e. g.  decision trees, regression models, Bayesian methods  to solve client pain points.  Leverage defined algorithms or statistical techniques to enhance data analysis and explore new outcomes.  Able to present and simplify complex statistical approaches, findings and concepts to the client.  Understand how to present information visually to help one understand the data story.  Design, develop and implement analytics reporting solutions as needed.   ",experienced oracle business intelligence obiee must sql plsql query development python scripting proven independently team member good communication written oral interpersonal good organizational multitasking timemanagement us citizen order obtain public trust client define business questions e problem statements associated metrics data challenges scope business objectives manipulate understand complex data sets researchanalyzecreate data models drive statistical analysis results confirmed client business questions build analytic models reinforcement techniques business decisions leverage suite analytic models e g decision trees regression models bayesian methods solve client pain points leverage defined algorithms statistical techniques enhance data analysis explore outcomes able present simplify complex statistical approaches findings concepts client understand present information visually help one understand data story design develop implement analytics reporting solutions needed,experienced oracle business intelligence obiee must sql plsql query development python scripting proven independently team member good communication written oral interpersonal organizational multitasking timemanagement us citizen order obtain public trust client define questions e problem statements associated metrics data challenges scope objectives manipulate understand complex sets researchanalyzecreate models drive statistical analysis results confirmed build analytic reinforcement techniques decisions leverage suite g decision trees regression bayesian methods solve pain points defined algorithms enhance explore outcomes able present simplify approaches findings concepts information visually help one story design develop implement analytics reporting solutions needed
60,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
61,"1+ years of experience working with relational or multi-dimensional databases Experience developing logical data models within a data warehouse Experience developing ETL processes Demonstrated mastery in one or more SQL variants  PostgreSQL, MySQL, Oracle, SQL Server, or DB2 Demonstrated mastery in database concepts and large-scale database implementations and design patterns Proven ability to work with users to define requirements and business issues Excellent analytic and troubleshooting skills Strong written and oral communication skills Bachelorâs degree in Computer Science or Computer Engineering   Responsible for data modeling and schema design that will range across multiple business domains within higher education Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas Work with clients to research and conduct business information flow studies Codify high-performing SQL for efficient data transformation Coordinate work with external teams to ensure a smooth development process Support operations by identifying, researching and resolving performance and production issues   ",relational multidimensional databases developing logical data models within data warehouse developing etl processes demonstrated mastery one sql variants postgresql mysql oracle sql server db demonstrated mastery database concepts largescale database implementations design patterns proven users define business issues analytic troubleshooting written oral communication bachelors degree computer science computer engineering responsible data modeling schema design range across multiple business domains within higher education partner multiple stakeholders clients product development bi engineers develop scalable standard schemas clients research conduct business information flow codify highperforming sql efficient data transformation coordinate external teams smooth development process support operations identifying researching resolving performance production issues,relational multidimensional databases developing logical data models within warehouse etl processes demonstrated mastery one sql variants postgresql mysql oracle server db database concepts largescale implementations design patterns proven users define business issues analytic troubleshooting written oral communication bachelors degree computer science engineering responsible modeling schema range across multiple domains higher education partner stakeholders clients product development bi engineers develop scalable standard schemas research conduct information flow codify highperforming efficient transformation coordinate external teams smooth process support operations identifying researching resolving performance production
62,"Bachelorâs degree in a quantitative field  e. g. , engineering, statistics, mathematics, information technology, etc.   is preferred.  Master's degree is desired.  Must have at least 3 years of experience, preferably with a federal government customer.  Experience with relational SQL and NoSQL databases  Postgres, Cassandra, MongoDB Experience with big data tools  Hadoop, Spark, Kafka Experience with data governance tools  Collibra, Immuta Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala Must possess strong written and verbal communication skills.   Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceFamiliarity with enterprise data management, data governance, and data control policiesAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.   ",bachelors degree quantitative e g engineering statistics mathematics information technology masters degree desired must least preferably federal government customer relational sql nosql databases postgres cassandra mongodb big data tools hadoop spark kafka data governance tools collibra immuta objectorientedobject function scripting languages python java c scala must possess written verbal communication develop data structures systems support generation business insightsknowledge overall etl processesmaintain data infrastructure develop scripts regular processesdefine design develop data flow diagrams data dictionaries logical physical modelsdefine data document data elements capture maintain metadetaidentify clean incomplete incorrect inaccurate irrelevant dataidentify opportunities use data improve business performancefamiliarity enterprise data management data governance data control policiesadhere compliance audit data storage architecture cybersecurity,bachelors degree quantitative e g engineering statistics mathematics information technology masters desired must least preferably federal government customer relational sql nosql databases postgres cassandra mongodb big data tools hadoop spark kafka governance collibra immuta objectorientedobject function scripting languages python java c scala possess written verbal communication develop structures systems support generation business insightsknowledge overall etl processesmaintain infrastructure scripts regular processesdefine design flow diagrams dictionaries logical physical modelsdefine document elements capture maintain metadetaidentify clean incomplete incorrect inaccurate irrelevant dataidentify opportunities use improve performancefamiliarity enterprise management control policiesadhere compliance audit storage architecture cybersecurity
63," Must have an active/current TS/SCI and be able to pass a CI Poly.  Must have at least five years' experience in technology consulting.  Bachelor's degree or equivalent training and experience.  Master's degree preferred with advanced training in information technology.  Experience and knowledge of tools associated with counterintelligence and HUMINT collectors.  Experience in cloud technologies, data layers, microservices.  Experience with the following tools  AWS, AWS Cloud and C2S Database experience in SQL/NSQL Experience and knowledge in software coding and unit level testing including  Java, python, Ruby R Knowledge of web services feeds Must be a diverse thinker and be able to work in a large group setting.  Write and edit technical documents and reports; Write, edit, and produce contents for contract deliverables  reports, training materials, presentation slides, letters, fact sheets, diagrams, and capability Effectively communicate project expectations to team members and stakeholders in a timely and clear fashion.  Communicate formally and informally through existing forums to stakeholders at all levels, including senior leadership.     Develop data flow diagrams depicting data movement through data centric architecture.  Provide recommendation to senior leadership to simplify end user processes and create better efficiencies.  Translates customer requirements into formal agreements and plans to culminate in customer acceptance or results.  Execute a wide range of process activities beginning with the request for proposal through development, test and final delivery.  Anticipates future customer, industry and business trends.  Challenges the validity of given procedures and processes with a view toward enhancements or improvement.  Creates innovative solutions to problems involving finance, scheduling, technology, methodology, tools and solution components.  Leads team on large complex projects.  Provide capability to ingest and extract data.  Create repeatable, reusable procedures.  Deliver common services in support of architecture roadmap and Agency direction.  Work with a team to design, implement, and maintain new systems.  Provide guidance to the customer on best-practices.  Perform other duties as assigned.     Travel may be required both inside and outside the Washington National Capital Region  NCR  and worldwide.  ",must activecurrent tssci able pass ci poly must least five technology consulting bachelors degree training masters degree advanced training information technology tools associated counterintelligence humint collectors cloud technologies data layers microservices following tools aws aws cloud cs database sqlnsql software coding unit level testing java python ruby r web services feeds must diverse thinker able group setting write edit technical documents reports write edit produce contents contract deliverables reports training materials presentation slides letters fact sheets diagrams capability effectively communicate project expectations team members stakeholders timely clear fashion communicate formally informally existing forums stakeholders levels senior leadership develop data flow diagrams depicting data movement data centric architecture recommendation senior leadership simplify end user processes create better efficiencies translates customer formal agreements plans culminate customer acceptance results execute wide range process activities beginning request proposal development test final delivery anticipates future customer industry business trends challenges validity given procedures processes view toward enhancements improvement creates innovative solutions problems involving finance scheduling technology methodology tools solution components leads team complex projects capability ingest extract data create repeatable reusable procedures deliver common services support architecture roadmap agency direction team design implement maintain systems guidance customer bestpractices perform duties assigned travel may inside outside washington national capital region ncr worldwide,must activecurrent tssci able pass ci poly least five technology consulting bachelors degree training masters advanced information tools associated counterintelligence humint collectors cloud technologies data layers microservices following aws cs database sqlnsql software coding unit level testing java python ruby r web services feeds diverse thinker group setting write edit technical documents reports produce contents contract deliverables materials presentation slides letters fact sheets diagrams capability effectively communicate project expectations team members stakeholders timely clear fashion formally informally existing forums levels senior leadership develop flow depicting movement centric architecture recommendation simplify end user processes create better efficiencies translates customer formal agreements plans culminate acceptance results execute wide range process activities beginning request proposal development test final delivery anticipates future industry business trends challenges validity given procedures view toward enhancements improvement creates innovative solutions problems involving finance scheduling methodology solution components leads complex projects ingest extract repeatable reusable deliver common support roadmap agency direction design implement maintain systems guidance bestpractices perform duties assigned travel may inside outside washington national capital region ncr worldwide
64,"4+ years of full-time experience in software development including design, coding, testing, and support At least 1 year of Cloud infrastructure experience working with one or more of the following Amazon Web Services  AWS  Cloud services  EC2, EMR, ECS, S3, SNS, SQS, Cloud Formation, Cloud watch, Lambda Hands-on experience with AWS architecture design, Data Management, Big Data, and Data Warehousing Experience ofwith data application and data product development 2+ years of experience with Agile, Kanban, or Scrum methodologies Master's Degree in Computer Science or related fields Proficient with CICD process, Agile and DevOps Software Development Life Cycle including analysis, high level design, coding, testing, and implementation, performance tuning, bug fixing and quality control Experience building data lakes in AWS Cloud, moving Data applications to the Cloud, and developing cloud native Data applications Expertise in creating data models, optimizing data, automating & restructuring data reporting system in financial services domain 2+ years of experience in big data technologies  Spark, Hadoop, HDFS, MongoDB, PostGre SQL  3+ years of experience using Java, Python or Scala Experience leading complex data applications with large volumes of data   ",fulltime software development design coding testing support least year cloud infrastructure one following amazon web services aws cloud services ec emr ecs sns sqs cloud formation cloud watch lambda handson aws architecture design data management big data data warehousing ofwith data application data product development agile kanban scrum methodologies masters degree computer science fields proficient cicd process agile devops software development life cycle analysis level design coding testing implementation performance tuning bug fixing control building data lakes aws cloud moving data applications cloud developing cloud native data applications expertise creating data models optimizing data automating restructuring data reporting financial services domain big data technologies spark hadoop hdfs mongodb postgre sql java python scala leading complex data applications volumes data,fulltime software development design coding testing support least year cloud infrastructure one following amazon web services aws ec emr ecs sns sqs formation watch lambda handson architecture data management big warehousing ofwith application product agile kanban scrum methodologies masters degree computer science fields proficient cicd process devops life cycle analysis level implementation performance tuning bug fixing control building lakes moving applications developing native expertise creating models optimizing automating restructuring reporting financial domain technologies spark hadoop hdfs mongodb postgre sql java python scala leading complex volumes
65," Bachelorâs Degree from an accredited college or university is required; Bachelorâs degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics  STEM  degrees or a Masterâs Degree is highly preferred.  Minimum of 5 years of general experience in the military or intelligence community is required.  Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.  Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level  3/4 star officer or SES-3/4  is preferred; at least 3 years of experience with OUSD I  is highly preferred.  Minimum of 3 years of experience using scripting  e. g.  Python, R, VBA  or programming languages  e. g.  Java, C++, Ruby  to deliver data engineering / software development services is required.  Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.  Machine Learning  e. g.  TensorFlow, PyTorch, Keras Data Visualization  e. g.  Tableau, D3, Kibana Geospatial Analysis  e. g.  ArcGIS, R Web Services  e. g.  SOAP, RESTful Web Development  e. g.  JavaScript, React. js, Node. js, HTML Database Development  e. g.  MongoDB, PostgreSQL, MS-SQL Active TS/SCI security clearance.   Simultaneously support 2-3 ISR Analytic studies with analytic expertise, project-specific web pages, on-demand ETL and data analysis, and the development of web-enabled analytic tools and dashboards.  Continuously supplement, improve, and maintain ISR Data Enrichment and Aggregation  IDEA , a JWICS-based, end-to-end automated ISR data capability, according to OUSD I  and stakeholder priorities and agile development principles.  Continuously supplement, improve, and maintain a classified studies and analysis website including project repositories, analytic apps, and dynamic decision-support dashboards.  Design, build, and/or insert new technology into extant classified development and production architecture baseline to supplement and improve analytic output, algorithmic performance, and user experience across websites and APIs.  Design and develop custom scripts and tools on SIPRNET and JWICS to solve emergent analytic challenges and/or answer quick-turn or recurring senior executive questions about ISR performance and effectiveness.  Develop custom algorithms to ingest and transform SIPRNET- and JWICS-derived data, artifacts, and information into analytic-ready data.  Leverage advanced analytic techniques, including, but not limited to, geospatial analysis, regression analysis, machine learning, and natural language processing, to derive insight about ISR performance and effectiveness.  Perform DevOps across JWICS and SIPR development architectures containing multiple database clusters, web servers, load balancers, and virtual machine instances, and optimize the deployment and maintenance pipelines for all architectural components in support of IDEA and Data Engineering efforts.  Engage with the ISR community to understand analytic needs, raise awareness of ongoing work, seek feedback on in-progress innovations, and develop one-off analytics aids.  Support technical briefings on analytic methodologies, decision-support dashboards, and capability innovations.  The work environment for this position requires an individual to be able to  Work sitting or standing at a desk or conference table for extended periods of time with the ability to shift positions while working  sit, stand, pace, adjust positioning in any of those without issue Walk in the office to collaborate with co-workers, attend meetings or retrieve documents from printer Must be able to lift and carry up to 10 lbs.   Bachelorâs Degree from an accredited college or university is required; Bachelorâs degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics  STEM  degrees or a Masterâs Degree is highly preferred.  Minimum of 5 years of general experience in the military or intelligence community is required.  Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.  Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level  3/4 star officer or SES-3/4  is preferred; at least 3 years of experience with OUSD I  is highly preferred.  Minimum of 3 years of experience using scripting  e. g.  Python, R, VBA  or programming languages  e. g.  Java, C++, Ruby  to deliver data engineering / software development services is required.  Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.  Machine Learning  e. g.  TensorFlow, PyTorch, Keras Data Visualization  e. g.  Tableau, D3, Kibana Geospatial Analysis  e. g.  ArcGIS, R Web Services  e. g.  SOAP, RESTful Web Development  e. g.  JavaScript, React. js, Node. js, HTML Database Development  e. g.  MongoDB, PostgreSQL, MS-SQL Active TS/SCI security clearance.   Bachelorâs Degree from an accredited college or university is required; Bachelorâs degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics  STEM  degrees or a Masterâs Degree is highly preferred.  Minimum of 5 years of general experience in the military or intelligence community is required.  Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.  Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level  3/4 star officer or SES-3/4  is preferred; at least 3 years of experience with OUSD I  is highly preferred.  Minimum of 3 years of experience using scripting  e. g.  Python, R, VBA  or programming languages  e. g.  Java, C++, Ruby  to deliver data engineering / software development services is required.  Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.  Machine Learning  e. g.  TensorFlow, PyTorch, Keras Data Visualization  e. g.  Tableau, D3, Kibana Geospatial Analysis  e. g.  ArcGIS, R Web Services  e. g.  SOAP, RESTful Web Development  e. g.  JavaScript, React. js, Node. js, HTML Database Development  e. g.  MongoDB, PostgreSQL, MS-SQL Active TS/SCI security clearance.  ",bachelors degree accredited college university bachelors degree operations research applicable science technology engineering mathematics stem degrees masters degree highly minimum general military intelligence community minimum applying data engineer software development develop advanced analytic tools deliver mission support services create data visualization capabilities analysis isr intelligence systems minimum intelligence policy oversight organization led senior executivelevel star officer ses least ousd highly minimum scripting e g python r vba programming languages e g java c ruby deliver data engineering software development services minimum least one following technologies deliver data engineering software development services machine e g tensorflow pytorch keras data visualization e g tableau kibana geospatial analysis e g arcgis r web services e g soap restful web development e g javascript react js node js html database development e g mongodb postgresql mssql active tssci security clearance simultaneously support isr analytic analytic expertise projectspecific web pages ondemand etl data analysis development webenabled analytic tools dashboards continuously supplement improve maintain isr data enrichment aggregation idea jwicsbased endtoend automated isr data capability according ousd stakeholder priorities agile development principles continuously supplement improve maintain classified analysis website project repositories analytic apps dynamic decisionsupport dashboards design build andor insert technology extant classified development production architecture baseline supplement improve analytic output algorithmic performance user across websites apis design develop custom scripts tools siprnet jwics solve emergent analytic challenges andor answer quickturn recurring senior executive questions isr performance effectiveness develop custom algorithms ingest transform siprnet jwicsderived data artifacts information analyticready data leverage advanced analytic techniques limited geospatial analysis regression analysis machine natural language processing derive insight isr performance effectiveness perform devops across jwics sipr development architectures containing multiple database clusters web servers load balancers virtual machine instances optimize deployment maintenance pipelines architectural components support idea data engineering efforts engage isr community understand analytic needs raise awareness ongoing seek feedback inprogress innovations develop oneoff analytics aids support technical briefings analytic methodologies decisionsupport dashboards capability innovations position requires individual able sitting standing desk conference table extended periods time shift positions sit stand pace adjust positioning without issue walk office collaborate coworkers attend meetings retrieve documents printer must able lift carry lbs bachelors degree accredited college university bachelors degree operations research applicable science technology engineering mathematics stem degrees masters degree highly minimum general military intelligence community minimum applying data engineer software development develop advanced analytic tools deliver mission support services create data visualization capabilities analysis isr intelligence systems minimum intelligence policy oversight organization led senior executivelevel star officer ses least ousd highly minimum scripting e g python r vba programming languages e g java c ruby deliver data engineering software development services minimum least one following technologies deliver data engineering software development services machine e g tensorflow pytorch keras data visualization e g tableau kibana geospatial analysis e g arcgis r web services e g soap restful web development e g javascript react js node js html database development e g mongodb postgresql mssql active tssci security clearance bachelors degree accredited college university bachelors degree operations research applicable science technology engineering mathematics stem degrees masters degree highly minimum general military intelligence community minimum applying data engineer software development develop advanced analytic tools deliver mission support services create data visualization capabilities analysis isr intelligence systems minimum intelligence policy oversight organization led senior executivelevel star officer ses least ousd highly minimum scripting e g python r vba programming languages e g java c ruby deliver data engineering software development services minimum least one following technologies deliver data engineering software development services machine e g tensorflow pytorch keras data visualization e g tableau kibana geospatial analysis e g arcgis r web services e g soap restful web development e g javascript react js node js html database development e g mongodb postgresql mssql active tssci security clearance,bachelors degree accredited college university operations research applicable science technology engineering mathematics stem degrees masters highly minimum general military intelligence community applying data engineer software development develop advanced analytic tools deliver mission support services create visualization capabilities analysis isr systems policy oversight organization led senior executivelevel star officer ses least ousd scripting e g python r vba programming languages java c ruby one following technologies machine tensorflow pytorch keras tableau kibana geospatial arcgis web soap restful javascript react js node html database mongodb postgresql mssql active tssci security clearance simultaneously expertise projectspecific pages ondemand etl webenabled dashboards continuously supplement improve maintain enrichment aggregation idea jwicsbased endtoend automated capability according stakeholder priorities agile principles classified website project repositories apps dynamic decisionsupport design build andor insert extant production architecture baseline output algorithmic performance user across websites apis custom scripts siprnet jwics solve emergent challenges answer quickturn recurring executive questions effectiveness algorithms ingest transform jwicsderived artifacts information analyticready leverage techniques limited regression natural language processing derive insight perform devops sipr architectures containing multiple clusters servers load balancers virtual instances optimize deployment maintenance pipelines architectural components efforts engage understand needs raise awareness ongoing seek feedback inprogress innovations oneoff analytics aids technical briefings methodologies position requires individual able sitting standing desk conference table extended periods time shift positions sit stand pace adjust positioning without issue walk office collaborate coworkers attend meetings retrieve documents printer must lift carry lbs
66,"  Design and implement ETL strategies for diverse sets of structured and unstructured data.  Manage cloud-based Linux servers.  Analyze data to better understand their features and relationships between variables.  Work in a highly collaborative manner with members of a close-knit team while exhibiting independent and creative thought.    Bachelor's degree in Computer Science or related discipline, e. g.  Information Science, Statistics, or Mathematics.  Master s degree is preferred.  Experience with data integration, harmonization, and curation is desired.  Must be competent with Python, SQL, and Linux.  Programming experience with Apache Spark, Java, C/C++ in an agile development environment is a plus.  Research experience is a plus.  Must have excellent oral and written communication skills and be able to work collaboratively.  Promising applicants will be tested for programming competence during the interview process.  ",design implement etl strategies diverse sets structured unstructured data manage cloudbased linux servers analyze data better understand features relationships variables highly collaborative manner members closeknit team exhibiting independent creative thought bachelors degree computer science discipline e g information science statistics mathematics master degree data integration harmonization curation desired must competent python sql linux programming apache spark java cc agile development plus research plus must oral written communication able collaboratively promising applicants tested programming competence interview process,design implement etl strategies diverse sets structured unstructured data manage cloudbased linux servers analyze better understand features relationships variables highly collaborative manner members closeknit team exhibiting independent creative thought bachelors degree computer science discipline e g information statistics mathematics master integration harmonization curation desired must competent python sql programming apache spark java cc agile development plus research oral written communication able collaboratively promising applicants tested competence interview process
67," U. S.  Citizenship Must have an active Top Secret  TS  clearance.  Must be able to obtain a TS/SCI clearance Must be able to obtain DHS Suitability Demonstrated ability with diverse skill sets  e. g.  data architects, data scientists, software developers  Excellent understanding of big data and data analytics Experience working with large structured and unstructured data sets Development experience building ETL pipelines at scale Solid SQL development skills Experience with Linux/Unix tools and shell scripts Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing, and encryption Good communication skills, both oral and written Must work well in a team environment as well as independently Must exhibit good time management skills, independent decision making capability, and a focus on customer service.   Using database expertise to lead teams with diverse skill sets  e. g.  data architects, data scientists, software developers  in support of a large, agile-based, cybersecurity system Working with large structured and unstructured data sets Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment Design, setup, administer, and tune NoSQL databases in the AWS cloud Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on a tradeoff between performance and quality Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations Write and refine code to ensure the quality and reliability of data extraction and processing Analyze and resolve data performance and quality issues Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.  Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to the development team Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages Maintain current industry knowledge of relevant concepts, practices, and procedures  BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering.  Two years of related work experience may be substituted for each year of degree-level education  ",u citizenship must active top secret ts clearance must able obtain tssci clearance must able obtain dhs suitability demonstrated diverse skill sets e g data architects data scientists software developers understanding big data data analytics structured unstructured data sets development building etl pipelines scale solid sql development linuxunix tools shell scripts expertise data analysis design data modeling master data management metadata management data warehousing performance tuning data improvement data security auditing encryption good communication oral written must well team well independently must exhibit good time management independent decision making capability focus customer service database expertise lead teams diverse skill sets e g data architects data scientists software developers support agilebased cybersecurity structured unstructured data sets implementing data management systems conception stages design development deployment cicd agile design setup administer tune nosql databases aws cloud design implement technical architecture necessary support analytic statistical processing based tradeoff performance performing data transformations aggregations joins data cleaning support analytic applications visualizations write refine code reliability data extraction processing analyze resolve data performance issues make data available data scientists programmers users programming scripting languages r python java javascript perform data analysis design data modeling business intelligence management master data management metadata management data management data security auditing functions data management governance generate data estimates performed compare estimates actuals continue refine estimates collaboratively agile development teams attending daily scrums providing data solutions development team develop maintain database code libraries data models sops documentation team collaboration migrate data legacy rdbmss data sources nosql database aws cloud complex etl processes tools programming languages maintain current industry relevant concepts practices procedures bs computer science computer engineering computer information systems computer systems engineering two may substituted year degreelevel education,u citizenship must active top secret ts clearance able obtain tssci dhs suitability demonstrated diverse skill sets e g data architects scientists software developers understanding big analytics structured unstructured development building etl pipelines scale solid sql linuxunix tools shell scripts expertise analysis design modeling master management metadata warehousing performance tuning improvement security auditing encryption good communication oral written well team independently exhibit time independent decision making capability focus customer service database lead teams support agilebased cybersecurity implementing systems conception stages deployment cicd agile setup administer tune nosql databases aws cloud implement technical architecture necessary analytic statistical processing based tradeoff performing transformations aggregations joins cleaning applications visualizations write refine code reliability extraction analyze resolve issues make available programmers users programming scripting languages r python java javascript perform business intelligence functions governance generate estimates performed compare actuals continue collaboratively attending daily scrums providing solutions develop maintain libraries models sops documentation collaboration migrate legacy rdbmss sources complex processes current industry relevant concepts practices procedures bs computer science engineering information two may substituted year degreelevel education
68,"Experience with Amazon Web Services  AWS , Microsoft Azure, or MilCloud 2. 0   Experience with Amazon Web Services  AWS , Microsoft Azure, or MilCloud 2. 0 ",amazon web services aws microsoft azure milcloud amazon web services aws microsoft azure milcloud,amazon web services aws microsoft azure milcloud
69,  http //www-01. ibm. com/employment/us/benefits/ https //www-03. ibm. com/press/us/en/pressrelease/50744. wss  ,http www ibm comemploymentusbenefits https www ibm compressusenpressrelease wss,http www ibm comemploymentusbenefits https compressusenpressrelease wss
70,"    Minimum Requirements  Bachelorâs Degree in Computer Science, IT or related field & minimum 5-7 years of QA experience Minimum experience of 5 years with relational databases such as Oracle, SQL Server, Sybase, RedShift Minimum 3+ years of experience with ETL technologies ",minimum bachelors degree computer science minimum qa minimum relational databases oracle sql server sybase redshift minimum etl technologies,minimum bachelors degree computer science qa relational databases oracle sql server sybase redshift etl technologies
71," 3+ years of experience with SAS programming in a business environment, particularly with data modeling, or in a âbig dataâ context.  1 year of Java experience Well qualified with SQL.  BS or MS in any quantitative field  computer science, systems engineering, mathematics, economics, statistics, etc.   or equivalent work experience.  Solid experience with SAS data processing, macro programming and running SQL queries within SAS  PROC SQL and SQL pass-through to Vertica.   Our planned Data Lake will make use of new tools including AWS Glue, AWS Athena, Spark, etc.  Knowledge of these tools or the interest and desire to learn them.  Familiarity with Linux, GIT and AWS Console.  Able to implement complex algorithms which transform, cleanse, impute, and mash up data.  Passion for data processing, data modeling, data mining and tackling complex operations.  Professional experience working in an Agile environment.  Professional experience working with a source code version control system.  Proficiency with Microsoft Excel.  English fluency, both spoken and written.  Able to discuss complex technical subjects with clarity and precision.  Maintaining, improving, and executing existing scripts written in SAS, SQL, and Python.  Most of the codebase is currently in SAS but the future will contain more Java, and possibly Python or R.  Designing new scripts using the above tools to ingest, cleanse, consolidate, analyze, and summarize the incoming data.  You will also be implementing and tuning algorithms and business rules, quality-checking the data results, and working iteratively with evolving requirements.  Meeting delivery deadlines for new products, features, and enhancements.  Implementing and delivering large historical data solutions to new and existing customers.  Coding new applications for ad hoc reporting and for data research inquiries.  Investigating issues with data quality and responding to stakeholdersâ technical questions.  Identifying opportunities to complement, enhance and/or optimize our data processing environment with new tools and techniques.  Provide on-call support on rotation basis, occasionally during afterhours and weekends.   ",sas programming business particularly data modeling big data context year java well qualified sql bs ms quantitative computer science systems engineering mathematics economics statistics solid sas data processing macro programming running sql queries within sas proc sql sql passthrough vertica planned data lake make use tools aws glue aws athena spark tools interest desire learn familiarity linux git aws console able implement complex algorithms transform cleanse impute mash data passion data processing data modeling data mining tackling complex operations professional agile professional source code version control proficiency microsoft excel english fluency spoken written able discuss complex technical subjects clarity precision maintaining improving executing existing scripts written sas sql python codebase currently sas future contain java possibly python r designing scripts tools ingest cleanse consolidate analyze summarize incoming data also implementing tuning algorithms business rules qualitychecking data results iteratively evolving meeting delivery deadlines products features enhancements implementing delivering historical data solutions existing customers coding applications ad hoc reporting data research inquiries investigating issues data responding stakeholders technical questions identifying opportunities complement enhance andor optimize data processing tools techniques oncall support rotation basis occasionally afterhours weekends,sas programming business particularly data modeling big context year java well qualified sql bs ms quantitative computer science systems engineering mathematics economics statistics solid processing macro running queries within proc passthrough vertica planned lake make use tools aws glue athena spark interest desire learn familiarity linux git console able implement complex algorithms transform cleanse impute mash passion mining tackling operations professional agile source code version control proficiency microsoft excel english fluency spoken written discuss technical subjects clarity precision maintaining improving executing existing scripts python codebase currently future contain possibly r designing ingest consolidate analyze summarize incoming also implementing tuning rules qualitychecking results iteratively evolving meeting delivery deadlines products features enhancements delivering historical solutions customers coding applications ad hoc reporting research inquiries investigating issues responding stakeholders questions identifying opportunities complement enhance andor optimize techniques oncall support rotation basis occasionally afterhours weekends
72,"   Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five  5  years technology industry or related experience, including items such as Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive  5  years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support. Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.  ",bachelors degree computer science combination education experiencetechnical training demonstrates analytical technical competencyminimum five technology industry items build highly scalable scaledout architectures scale database platformsexperience complex data infrastructure environmentfive data engineering roleextensive depth data pipeline development industry standard data integration toolsexperience sdlc process gathering analysis architecture design implementation testing deployment technical support industry standard tool source control project managementexperience writing test cases test scripts data assuranceexperience creating stored procedures functionsexperience developing dimensional data model industry standard tool,bachelors degree computer science combination education experiencetechnical training demonstrates analytical technical competencyminimum five technology industry items build highly scalable scaledout architectures scale database platformsexperience complex data infrastructure environmentfive engineering roleextensive depth pipeline development standard integration toolsexperience sdlc process gathering analysis architecture design implementation testing deployment support tool source control project managementexperience writing test cases scripts assuranceexperience creating stored procedures functionsexperience developing dimensional model
73," Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Knowledge of SQL as well as NoSQL queries, syntax, and technologies Knowledge of big data requirements, applications, and technologies such as Hadoop Knowledge of ETL methods and approaches including triggers, named views, temporary tables, etc.  Linux expertise  Specific responsibilities include  Help design an architecture for federated data stores and data fusion Help design methods for storing data in a way that facilitates extremely fast data parsing and management Implement ""glue code"" that connects middle tier components with backend components Implement data management and analytics code utilizing data architecture  e. g.  map reduce  Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores Collaborate with data architects to understand the applications we integrate with and the data they produce Review requirements for new approaches to big data storage and analytics Design methods for caching, paging, and integrating real-time data with historical data stores   Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Knowledge of SQL as well as NoSQL queries, syntax, and technologies Knowledge of big data requirements, applications, and technologies such as Hadoop Knowledge of ETL methods and approaches including triggers, named views, temporary tables, etc.  Linux expertise ",development integrating components contributing core code base java solid understanding database data warehousing technologies sql well nosql queries syntax technologies big data applications technologies hadoop etl methods approaches triggers named views temporary tables linux expertise specific responsibilities include help design architecture federated data stores data fusion help design methods storing data way facilitates extremely fast data parsing management implement glue code connects middle tier components backend components implement data management analytics code utilizing data architecture e g map reduce collaborate machine folks determine analyze various data sets set methods querying data stores collaborate data architects understand applications integrate data produce review approaches big data storage analytics design methods caching paging integrating realtime data historical data stores development integrating components contributing core code base java solid understanding database data warehousing technologies sql well nosql queries syntax technologies big data applications technologies hadoop etl methods approaches triggers named views temporary tables linux expertise,development integrating components contributing core code base java solid understanding database data warehousing technologies sql well nosql queries syntax big applications hadoop etl methods approaches triggers named views temporary tables linux expertise specific responsibilities include help design architecture federated stores fusion storing way facilitates extremely fast parsing management implement glue connects middle tier backend analytics utilizing e g map reduce collaborate machine folks determine analyze various sets set querying architects understand integrate produce review storage caching paging realtime historical
74,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform.  Multi-cloud experience a plus.    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",minimum previous consulting client service delivery google gcp devops gcp platform multicloud plus proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,minimum previous consulting client service delivery google gcp devops platform multicloud plus proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
75,"Breadth - Substantial experience as a high-performance Engineer across multiple environments on high-performance data systems that process data across various sources at a near real-time pace At least 1 project where you personally designed, implemented, and operated large scale, high throughput data pipelines with a focus on high data quality At least 1 other project where you built a secure, reliable, scalable system on AWS that saw significant traffic Depth - You can go up and down the stack from deep in the infrastructure up to the data, application, and client layers Speed - Experience with small teams that move fast - all members are expected to be able to achieve maximum results with minimal direction Ownership and accountability - You own the things that you build Drive - When you see a need, you fill a need.  You can step in where you see a need and push us all forward Modern Infra - Hands-on experience with Kubernetes, Airflow, and Kafka Modern CI/CD - Hands-on experience with Codefresh, Spinnaker, Jenkins or similar     ",breadth substantial highperformance engineer across multiple environments highperformance data systems process data across various sources near realtime pace least project personally designed implemented operated scale throughput data pipelines focus data least project built secure reliable scalable aws saw significant traffic depth go stack deep infrastructure data application client layers speed small teams move fast members expected able achieve maximum results minimal direction ownership accountability things build drive see need fill need step see need push us forward modern infra handson kubernetes airflow kafka modern cicd handson codefresh spinnaker jenkins similar,breadth substantial highperformance engineer across multiple environments data systems process various sources near realtime pace least project personally designed implemented operated scale throughput pipelines focus built secure reliable scalable aws saw significant traffic depth go stack deep infrastructure application client layers speed small teams move fast members expected able achieve maximum results minimal direction ownership accountability things build drive see need fill step push us forward modern infra handson kubernetes airflow kafka cicd codefresh spinnaker jenkins similar
76,"   Ingestion of data from multiple, unstructured sources using multiple analytics tools Implementing ETL process Monitoring performance and advising any necessary infrastructure changes Defining data retention policies   Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey. ",ingestion data multiple unstructured sources multiple analytics tools implementing etl process monitoring performance advising necessary infrastructure changes defining data retention policies voted significantly services firms clients business impact execution predictability organizational commitment recent trianz wide client satisfaction survey,ingestion data multiple unstructured sources analytics tools implementing etl process monitoring performance advising necessary infrastructure changes defining retention policies voted significantly services firms clients business impact execution predictability organizational commitment recent trianz wide client satisfaction survey
77," Minimum 3-5 years of experience is required B. S.  in Computer Science or closely related field preferred, but not required.  Real-world experience and proven track records count as much, if not more.  Programming Languages  Python and C   C++ and R are a plus .  Experience with databricks is a plus.  Established expertise developing ETL pipelines on serverless cloud solutions.   AWS Lambda, Azure App Services, etc. .   Linux/Unix Docker Databases  SQL, JSON, object storage, etc.  Experience with graph databases like Neo4j and TigerGraph is a plus.  Strong technical writing skills will be heavily stressed    Build automated ETL pipelines for cloud environments including, AWS, GCP, Azure, and Heroku.  Develop and support data integrity reporting and alerting for ETL pipelines.  Ability to work closely with Engineering, Product and Customer Success Teams.  Build robust and deployable software in Python, C++ or C .  Build, deploy, and maintain RESTful APIs to access datasets.  Parse and extract data from common formats including, XML, JSON, CSV, and Pipe delimited.  Integrate with customer provided APIs.  Build, organize, and maintain datamarts using any of SQL, JSON, Blob, or other databases as needed.  Write and maintain excellent documentation of all work.    ",minimum b computer science closely realworld proven track records count much programming languages python c c r plus databricks plus established expertise developing etl pipelines serverless cloud solutions aws lambda azure app services linuxunix docker databases sql json object storage graph databases like neoj tigergraph plus technical writing heavily stressed build automated etl pipelines cloud environments aws gcp azure heroku develop support data integrity reporting alerting etl pipelines closely engineering product customer success teams build robust deployable software python c c build deploy maintain restful apis access datasets parse extract data common formats xml json csv pipe delimited integrate customer provided apis build organize maintain datamarts sql json blob databases needed write maintain documentation,minimum b computer science closely realworld proven track records count much programming languages python c r plus databricks established expertise developing etl pipelines serverless cloud solutions aws lambda azure app services linuxunix docker databases sql json object storage graph like neoj tigergraph technical writing heavily stressed build automated environments gcp heroku develop support data integrity reporting alerting engineering product customer success teams robust deployable software deploy maintain restful apis access datasets parse extract common formats xml csv pipe delimited integrate provided organize datamarts blob needed write documentation
78," Mastery in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or customer-facing role     ",mastery least one following domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale writing software one languages python java scala go building productiongrade data solutions relational nosql systems monitoringalerting capacity planning performance tuning technical consulting customerfacing role,mastery least one following domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale writing languages python java scala go relational nosql systems monitoringalerting capacity planning performance tuning consulting customerfacing role
79,"  Experience with cloud-based systems like AWS, AZURE or Google Cloud.    ",cloudbased systems like aws azure google cloud,cloudbased systems like aws azure google cloud
80,"   1-3 years of experience in quantitative analysis experience.  Bachelor's degree in Computer Science, Statistics, Math or other technical field required.  Graduate degrees preferred.   ",quantitative analysis bachelors degree computer science statistics math technical graduate degrees,quantitative analysis bachelors degree computer science statistics math technical graduate degrees
81,"Bachelorâs degree in Computer Science, Information Systems, Business Administration, or other related field required Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources Minimum 2 years of experience with PowerBI Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired Experience working with both structured and unstructured data Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types  star, snowflake, etc.   Data modeling experience in building logical and physical data models Applied knowledge of Microsoft Security/Authentication Concepts  Active Directory, IIS, Windows OS  Strong technical planning skills with the ability to prioritize and multitask across a number of work streams Must have a passion for continued improvement, learning, and mentoring Polished presentation skills; experience creating and presenting findings to executive level staff Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders Experience with Data Warehouse is a plus Knowledge of SSRS is a plus Healthcare industry experience a plus    ",bachelors degree computer science information systems business administration minimum relevant data engineering role leveraging sql ssis design support etl routines support import data multiple data sources minimum powerbi minimum data warehousing design development ongoing support star snowflake data schemas support business intelligence applications minimum database administration database development sql mysql microsoft technology stack background azure infrastructure service desired structured unstructured data demonstrated understanding business intelligence data solutions cubes data warehouse data marts supporting schema types star snowflake data modeling building logical physical data models applied microsoft securityauthentication concepts active directory iis windows os technical planning prioritize multitask across number streams must passion continued improvement mentoring polished presentation creating presenting findings executive level staff written verbal interpersonal communication communicate ideas solutions effectively must highly collaborative manage motivate project teams meet deliverables build stakeholder relationships translate complex technical concepts nontechnical stakeholders data warehouse plus ssrs plus healthcare industry plus,bachelors degree computer science information systems business administration minimum relevant data engineering role leveraging sql ssis design support etl routines import multiple sources powerbi warehousing development ongoing star snowflake schemas intelligence applications database mysql microsoft technology stack background azure infrastructure service desired structured unstructured demonstrated understanding solutions cubes warehouse marts supporting schema types modeling building logical physical models applied securityauthentication concepts active directory iis windows os technical planning prioritize multitask across number streams must passion continued improvement mentoring polished presentation creating presenting findings executive level staff written verbal interpersonal communication communicate ideas effectively highly collaborative manage motivate project teams meet deliverables build stakeholder relationships translate complex nontechnical stakeholders plus ssrs healthcare industry
82," Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation Minimum five years experience using one or more data integration tools including Data Quality and ETL Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies Experienced in big data requirements, applications, and technologies such as Hadoop Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc.  Experienced in Linux environments  Identify data warehouse needs and develop strategy for implementing a warehousing solution including investigating data sources, rationalizing information sources, identifying technology components, building roadmaps and reference architecture stacks Work with Business Analysts and other information management professionals through all phases of project development, from envisioning to architecture definition and end solution realization Provide direction and collaborate with Data Engineers to implement enterprise solutions that will support organizational business intelligence and analytics requirements Work with the business to identify opportunities where technology can be leveraged to solve existing problems and can assist with new market opportunities Meet with technology vendors, convey technical requirements and business use cases, develop scorecards, install vendor products in a lab environment, and summarize findings and results of testing Technologies to be used may include some combination of relational databases  PostgreSQL, Teradata, Aster, HANA , NoSQL, Hadoop, Object-based stores, and OLAP.  Specific responsibilities include  Help design an architecture for federated data stores and data fusion Help design methods for storing data in a way that facilitates extremely fast data parsing and management Implement ""glue code"" that connects middle tier components with backend components Implement data management and analytics code utilizing data architecture  e. g.  map reduce  Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores Collaborate with enterprise architects to understand the applications we integrate with and the data they produce Review requirements for new approaches to big data storage and analytics Design methods for caching, paging, and integrating real-time data with historical data stores Desired Skills and Experience Requirements Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation Minimum five years experience using one or more data integration tools including Data Quality and ETL Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies Experienced in big data requirements, applications, and technologies such as Hadoop Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc.  Experienced in Linux environments Bonus Points Experience working with IT strategy teams, business teams and business analysts to define information systems, services and management Experience with RDBMS including SQL Server, Oracle 11g, MySQL; Big Data including SQL Data Warehouse Appliance, Oracle Exadata, Netezza, Greenplum, Vertica, Teradata, Aster Data, SAP HANA, Hadoop a plus; Analytics including SAS, SPSS, Spotfire, Tableau, Qlikview, R, Oracle Endeca; BI Tools including Oracle OBIEE, SAP Business Objects, SAS and other Analytics Vendors with BI components; ETL & MDM including Informatica, SAS Dataflux, IBM, Siperion, Rochade, Map/Reduce for ETL is a plus Java is strongly preferred  e. g.  for working with map reduce  but not ultimately a requirement if you excel in other areas Strong SQL skills are highly desirable OLAP experience  Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation Minimum five years experience using one or more data integration tools including Data Quality and ETL Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies Experienced in big data requirements, applications, and technologies such as Hadoop Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc.  Experienced in Linux environments ",minimum ten least five designing enterprise database warehouses designing robust data models partitioning strategies minimum five data architecture data modeling project teams data governance strategy metadata management architecture design implementation minimum five one data integration tools data etl development integrating components contributing core code base java solid understanding database data warehousing technologies experienced advanced sql well nosql queries syntax technologies experienced big data applications technologies hadoop proficient etl methods approaches triggers named views temporary tables experienced linux environments identify data warehouse needs develop strategy implementing warehousing solution investigating data sources rationalizing information sources identifying technology components building roadmaps reference architecture stacks business analysts information management professionals phases project development envisioning architecture definition end solution realization direction collaborate data engineers implement enterprise solutions support organizational business intelligence analytics business identify opportunities technology leveraged solve existing problems assist market opportunities meet technology vendors convey technical business use cases develop scorecards install vendor products lab summarize findings results testing technologies used may include combination relational databases postgresql teradata aster hana nosql hadoop objectbased stores olap specific responsibilities include help design architecture federated data stores data fusion help design methods storing data way facilitates extremely fast data parsing management implement glue code connects middle tier components backend components implement data management analytics code utilizing data architecture e g map reduce collaborate machine folks determine analyze various data sets set methods querying data stores collaborate enterprise architects understand applications integrate data produce review approaches big data storage analytics design methods caching paging integrating realtime data historical data stores desired minimum ten least five designing enterprise database warehouses designing robust data models partitioning strategies minimum five data architecture data modeling project teams data governance strategy metadata management architecture design implementation minimum five one data integration tools data etl development integrating components contributing core code base java solid understanding database data warehousing technologies experienced advanced sql well nosql queries syntax technologies experienced big data applications technologies hadoop proficient etl methods approaches triggers named views temporary tables experienced linux environments bonus points strategy teams business teams business analysts define information systems services management rdbms sql server oracle g mysql big data sql data warehouse appliance oracle exadata netezza greenplum vertica teradata aster data sap hana hadoop plus analytics sas spss spotfire tableau qlikview r oracle endeca bi tools oracle obiee sap business objects sas analytics vendors bi components etl mdm informatica sas dataflux ibm siperion rochade mapreduce etl plus java strongly e g map reduce ultimately requirement excel areas sql highly desirable olap minimum ten least five designing enterprise database warehouses designing robust data models partitioning strategies minimum five data architecture data modeling project teams data governance strategy metadata management architecture design implementation minimum five one data integration tools data etl development integrating components contributing core code base java solid understanding database data warehousing technologies experienced advanced sql well nosql queries syntax technologies experienced big data applications technologies hadoop proficient etl methods approaches triggers named views temporary tables experienced linux environments,minimum ten least five designing enterprise database warehouses robust data models partitioning strategies architecture modeling project teams governance strategy metadata management design implementation one integration tools etl development integrating components contributing core code base java solid understanding warehousing technologies experienced advanced sql well nosql queries syntax big applications hadoop proficient methods approaches triggers named views temporary tables linux environments identify warehouse needs develop implementing solution investigating sources rationalizing information identifying technology building roadmaps reference stacks business analysts professionals phases envisioning definition end realization direction collaborate engineers implement solutions support organizational intelligence analytics opportunities leveraged solve existing problems assist market meet vendors convey technical use cases scorecards install vendor products lab summarize findings results testing used may include combination relational databases postgresql teradata aster hana objectbased stores olap specific responsibilities help federated fusion storing way facilitates extremely fast parsing glue connects middle tier backend utilizing e g map reduce machine folks determine analyze various sets set querying architects understand integrate produce review storage caching paging realtime historical desired bonus points define systems services rdbms server oracle mysql appliance exadata netezza greenplum vertica sap plus sas spss spotfire tableau qlikview r endeca bi obiee objects mdm informatica dataflux ibm siperion rochade mapreduce strongly ultimately requirement excel areas highly desirable
83," Bachelorâs Degree At least 3 years of SDLC experience using Java technologies At least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper At least 1 years experience in one of the following Cloud technologies  AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform     ",bachelors degree least sdlc java technologies least leading big data technologies like cassandra accumulo python hbase scala hadoop hdfs avro mongodb zookeeper least one following cloud technologies aws azure openstack docker ansible chef terraform,bachelors degree least sdlc java technologies leading big data like cassandra accumulo python hbase scala hadoop hdfs avro mongodb zookeeper one following cloud aws azure openstack docker ansible chef terraform
84,"Bachelorâs degree or equivalent in an engineering or technical field such as Computer Science, Information Systems, Statistics, Engineering, or similar.  5-10 years of quantitative and qualitative experience in building ETL data flows in Big Data Ecosystem.   Designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, S3, EC2, DynamoDB, RedShift, Kinesis, Lambda, Glue, Snowflake etc.  Hands-on experience analyzing, re-architecting and re-platforming on-premise data warehouses to data platforms on AWS cloud using AWS/3rd party services.  Knowledge about other NoSQL databases, such as MongoDB, Cassandra, HBase, etc.  Building and migrating the complex ETL pipelines on Redshift and Elastic Map Reduce to make the system grow elastically Hands-on knowledge in using advanced SQL queries  analytical functions , experience in writing and optimizing highly efficient SQL queries Proficiency with Python scripting Experienced in testing and monitoring data for anomalies and rectifying them.  Advanced communication skills to be able to work with business owners to develop and define key business uses and to build data sets that address them.  Experience in working with Data visualization tools such a Tableau   Responsible for designing, building & managing the advanced analytics platform to support downstream data science and the business intelligence teams Work with the product owner, data scientist and various internal data users to understand the business requirements and implement optimal data solutions Keen eye towards configuration driven approach to automate repeatable processes and tasks Build and operate stable, scalable and highly performant data pipelines that cleanse, structure and integrate disparate big data sets into a readable and accessible format for end user analyses and targeting.  Develop data quality and governance framework that supports data lineage and ensures delivery of high-quality data to internal and external stakeholders Using analytical and problem-solving skills to take complex business requests and transform them into clean, simple data solutions.  Implement/improve version control, deployment strategies, notifications to ensure product quality, agility and recoverability.  Understand and work with technology & IT teams to support database procedures, such as upgrade, backup, recovery, migrations, etc.    ",bachelors degree engineering technical computer science information systems statistics engineering similar quantitative qualitative building etl data flows big data ecosystem designing building operationalizing largescale enterprise data solutions applications one aws data analytics services combination rd parties spark emr ec dynamodb redshift kinesis lambda glue snowflake handson analyzing rearchitecting replatforming onpremise data warehouses data platforms aws cloud awsrd party services nosql databases mongodb cassandra hbase building migrating complex etl pipelines redshift elastic map reduce make grow elastically handson advanced sql queries analytical functions writing optimizing highly efficient sql queries proficiency python scripting experienced testing monitoring data anomalies rectifying advanced communication able business owners develop define key business uses build data sets address data visualization tools tableau responsible designing building managing advanced analytics platform support downstream data science business intelligence teams product owner data scientist various internal data users understand business implement optimal data solutions keen eye towards configuration driven approach automate repeatable processes tasks build operate stable scalable highly performant data pipelines cleanse structure integrate disparate big data sets readable accessible format end user analyses targeting develop data governance framework supports data lineage ensures delivery highquality data internal external stakeholders analytical problemsolving take complex business requests transform clean simple data solutions implementimprove version control deployment strategies notifications product agility recoverability understand technology teams support database procedures upgrade backup recovery migrations,bachelors degree engineering technical computer science information systems statistics similar quantitative qualitative building etl data flows big ecosystem designing operationalizing largescale enterprise solutions applications one aws analytics services combination rd parties spark emr ec dynamodb redshift kinesis lambda glue snowflake handson analyzing rearchitecting replatforming onpremise warehouses platforms cloud awsrd party nosql databases mongodb cassandra hbase migrating complex pipelines elastic map reduce make grow elastically advanced sql queries analytical functions writing optimizing highly efficient proficiency python scripting experienced testing monitoring anomalies rectifying communication able business owners develop define key uses build sets address visualization tools tableau responsible managing platform support downstream intelligence teams product owner scientist various internal users understand implement optimal keen eye towards configuration driven approach automate repeatable processes tasks operate stable scalable performant cleanse structure integrate disparate readable accessible format end user analyses targeting governance framework supports lineage ensures delivery highquality external stakeholders problemsolving take requests transform clean simple implementimprove version control deployment strategies notifications agility recoverability technology database procedures upgrade backup recovery migrations
85," Experience leading, managing and hiring a team of talented engineers Expertise in at least one of the following engineering domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Expertise in at least one of the following data domains    Predictive analytics  e. g. , recommendation systems, predictive maintenance  Natural language processing  e. g. , conversational chatbots  Document understanding Image classification Marketing analytics IoT systems Experience writing software in one or more languages such as Python or Java/Scala Experience in technical consulting or customer-facing role Excellent critical thinking, problem-solving and analytical skills     ",leading managing hiring team talented engineers expertise least one following engineering domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale expertise least one following data domains predictive analytics e g recommendation systems predictive maintenance natural language processing e g conversational chatbots document understanding image classification marketing analytics iot systems writing software one languages python javascala technical consulting customerfacing role critical thinking problemsolving analytical,leading managing hiring team talented engineers expertise least one following engineering domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale domains predictive analytics e g recommendation systems maintenance natural language conversational chatbots document understanding image classification marketing iot writing languages python javascala consulting customerfacing role critical thinking problemsolving analytical
86,"A deep passion for working with data and developing software to address data processing challenges Bachelorâs, Masterâs or PhD degree in Computer Science or equivalent experience Proficiency within one or more programming languages including C, C++, Python, R or JavaScript.  Proficiency with multiple data platforms including RDBMS, NoSQL, MongoDB, Spark, Hadoop Experience with some of the following areas  Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development Strong written and verbal communications skills Ability to manage multiple tasks and thrive in a fast-paced team environment  A deep passion for working with data and developing software to address data processing challenges Bachelorâs, Masterâs or PhD degree in Computer Science or equivalent experience Proficiency within one or more programming languages including C, C++, Python, R or JavaScript.  Proficiency with multiple data platforms including RDBMS, NoSQL, MongoDB, Spark, Hadoop Experience with some of the following areas  Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development Strong written and verbal communications skills Ability to manage multiple tasks and thrive in a fast-paced team environment    ",deep passion data developing software address data processing challenges bachelors masters phd degree computer science proficiency within one programming languages c c python r javascript proficiency multiple data platforms rdbms nosql mongodb spark hadoop following areas distributed computing natural language processing machine cloud platform development networking andor rest service development written verbal communications manage multiple tasks thrive fastpaced team deep passion data developing software address data processing challenges bachelors masters phd degree computer science proficiency within one programming languages c c python r javascript proficiency multiple data platforms rdbms nosql mongodb spark hadoop following areas distributed computing natural language processing machine cloud platform development networking andor rest service development written verbal communications manage multiple tasks thrive fastpaced team,deep passion data developing software address processing challenges bachelors masters phd degree computer science proficiency within one programming languages c python r javascript multiple platforms rdbms nosql mongodb spark hadoop following areas distributed computing natural language machine cloud platform development networking andor rest service written verbal communications manage tasks thrive fastpaced team
87,  Experience with RDBMS applications  SQL Server preferred  Good communication skills and experience working with cross-functional teams Exposure to the concepts of data warehouse design SQL programming familiarity in large RDBMS systems  T-SQL preferred   Troubleshoot and resolve issues as they arise related to all BI Tools Manage iteration and release cycles and deployments Assist in data modeling and design sessions Proactively maintain documentation and training materials  ,rdbms applications sql server good communication crossfunctional teams exposure concepts data warehouse design sql programming familiarity rdbms systems tsql troubleshoot resolve issues arise bi tools manage iteration release cycles deployments assist data modeling design sessions proactively maintain documentation training materials,rdbms applications sql server good communication crossfunctional teams exposure concepts data warehouse design programming familiarity systems tsql troubleshoot resolve issues arise bi tools manage iteration release cycles deployments assist modeling sessions proactively maintain documentation training materials
88," Undergraduate degree in Computer Science, Mathematics, Engineering  or related field  or equivalent experience preferred 5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function Ability to work with broad parameters in complex situations Experience in developing, managing, and manipulating large, complex datasets Expert high-level coding skills such as SQL and Python and/or other scripting languages UNIX  required.  Scala is a plus.    Responsible for design, prototyping and delivery of software solutions within the big data eco-system Leading projects and/or serving as analytics SME to provide new or enhanced data to the business Improving data governance and quality increasing the reliability of our data Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise   ",undergraduate degree computer science mathematics engineering data integration etl andor business intelligenceanalytics function broad parameters complex situations developing managing manipulating complex datasets expert highlevel coding sql python andor scripting languages unix scala plus responsible design prototyping delivery software solutions within big data ecosystem leading projects andor serving analytics sme enhanced data business improving data governance increasing reliability data influencing creation single trusted source key claims business data shared across enterprise,undergraduate degree computer science mathematics engineering data integration etl andor business intelligenceanalytics function broad parameters complex situations developing managing manipulating datasets expert highlevel coding sql python scripting languages unix scala plus responsible design prototyping delivery software solutions within big ecosystem leading projects serving analytics sme enhanced improving governance increasing reliability influencing creation single trusted source key claims shared across enterprise
89,"Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree  preferred  in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.  Understand Hadoop cluster administration concepts.  3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.  Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.  Must have experience with batch and real-time data pipelines.  Must have experience as a Hadoop Technical Lead / Architect Must have experience with design, development and deployment in the specified technologies.  Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.  Writing complex SQL queries, extracting and importing large amounts of data.  Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams.  Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.  Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.  Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.  Excellent written and oral communication skills.  Adept and presenting complex topics, influencing and executing with timely / actionable follow-through   Lead a development team of data engineers Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases  Hbase  and EMR  Hadoop  Responsible for design, development, testing oversight and implementation Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks Coordinate with data engineers and API developers to drive program delivery.  Drive technical development and application standards across enterprise data lake Benchmark and debug critical issues with algorithms and software as they arise.  Lead and assist with the technical design and implementation of the Big Data cluster in various environments.  Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.  Perform other duties and/or special projects as assigned   Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree  preferred  in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.  Understand Hadoop cluster administration concepts.  3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.  Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.  Must have experience with batch and real-time data pipelines.  Must have experience as a Hadoop Technical Lead / Architect Must have experience with design, development and deployment in the specified technologies.  Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.  Writing complex SQL queries, extracting and importing large amounts of data.  Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams.  Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.  Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.  Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.  Excellent written and oral communication skills.  Adept and presenting complex topics, influencing and executing with timely / actionable follow-through ",must bachelors degree computer science engineering plus industry masters degree computer science engineering plus industry big data technologies understand hadoop cluster administration concepts handson scale big data environments hortonworks cloudera must java spark mapreduce rdbms hive pig hbase kafka scala python linuxunix technologies must batch realtime data pipelines must hadoop technical lead architect must design development deployment specified technologies must oops concepts design principles patterns scala java python writing complex sql queries extracting importing amounts data must willing fastpaced shore shore distributed agile teams must technical background handson building enterprisewide data warehouse solutions must develop maintain collaborative relationships levels across business stakeholders must prioritize multiple tasks deal urgent requests fastpaced demanding written oral communication adept presenting complex topics influencing executing timely actionable followthrough lead development team data engineers implement big data enterprise data lake bi analytics hive llap spark kafka sqoop hive sqoop nosql databases hbase emr hadoop responsible design development testing oversight implementation works closely program manager scrum master architects convey technical impacts development timeline risks coordinate data engineers api developers drive program delivery drive technical development application standards across enterprise data lake benchmark debug critical issues algorithms software arise lead assist technical design implementation big data cluster various environments guidementor development team example create custom common utilitieslibraries reused multiple big data development efforts perform duties andor special projects assigned must bachelors degree computer science engineering plus industry masters degree computer science engineering plus industry big data technologies understand hadoop cluster administration concepts handson scale big data environments hortonworks cloudera must java spark mapreduce rdbms hive pig hbase kafka scala python linuxunix technologies must batch realtime data pipelines must hadoop technical lead architect must design development deployment specified technologies must oops concepts design principles patterns scala java python writing complex sql queries extracting importing amounts data must willing fastpaced shore shore distributed agile teams must technical background handson building enterprisewide data warehouse solutions must develop maintain collaborative relationships levels across business stakeholders must prioritize multiple tasks deal urgent requests fastpaced demanding written oral communication adept presenting complex topics influencing executing timely actionable followthrough,must bachelors degree computer science engineering plus industry masters big data technologies understand hadoop cluster administration concepts handson scale environments hortonworks cloudera java spark mapreduce rdbms hive pig hbase kafka scala python linuxunix batch realtime pipelines technical lead architect design development deployment specified oops principles patterns writing complex sql queries extracting importing amounts willing fastpaced shore distributed agile teams background building enterprisewide warehouse solutions develop maintain collaborative relationships levels across business stakeholders prioritize multiple tasks deal urgent requests demanding written oral communication adept presenting topics influencing executing timely actionable followthrough team engineers implement enterprise lake bi analytics llap sqoop nosql databases emr responsible testing oversight implementation works closely program manager scrum master architects convey impacts timeline risks coordinate api developers drive delivery application standards benchmark debug critical issues algorithms software arise assist various guidementor example create custom common utilitieslibraries reused efforts perform duties andor special projects assigned
90,"Bachelorâs degree in Computer Science, Information Systems, Business Administration, or other related field required Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources  Minimum 2 years of experience with PowerBI  Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications  Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired  Experience working with both structured and unstructured data Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types  star, snowflake, etc.   Data modeling experience in building logical and physical data models  Applied knowledge of Microsoft Security/Authentication Concepts  Active Directory, IIS, Windows OS   Strong technical planning skills with the ability to prioritize and multitask across a number of work streams  Must have a passion for continued improvement, learning, and mentoring  Polished presentation skills; experience creating and presenting findings to executive level staff  Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively  Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables  Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders Experience with Data Warehouse is a plus  Knowledge of SSRS is a plus  Healthcare industry experience a plus     ",bachelors degree computer science information systems business administration minimum relevant data engineering role leveraging sql ssis design support etl routines support import data multiple data sources minimum powerbi minimum data warehousing design development ongoing support star snowflake data schemas support business intelligence applications minimum database administration database development sql mysql microsoft technology stack background azure infrastructure service desired structured unstructured data demonstrated understanding business intelligence data solutions cubes data warehouse data marts supporting schema types star snowflake data modeling building logical physical data models applied microsoft securityauthentication concepts active directory iis windows os technical planning prioritize multitask across number streams must passion continued improvement mentoring polished presentation creating presenting findings executive level staff written verbal interpersonal communication communicate ideas solutions effectively must highly collaborative manage motivate project teams meet deliverables build stakeholder relationships translate complex technical concepts nontechnical stakeholders data warehouse plus ssrs plus healthcare industry plus,bachelors degree computer science information systems business administration minimum relevant data engineering role leveraging sql ssis design support etl routines import multiple sources powerbi warehousing development ongoing star snowflake schemas intelligence applications database mysql microsoft technology stack background azure infrastructure service desired structured unstructured demonstrated understanding solutions cubes warehouse marts supporting schema types modeling building logical physical models applied securityauthentication concepts active directory iis windows os technical planning prioritize multitask across number streams must passion continued improvement mentoring polished presentation creating presenting findings executive level staff written verbal interpersonal communication communicate ideas effectively highly collaborative manage motivate project teams meet deliverables build stakeholder relationships translate complex nontechnical stakeholders plus ssrs healthcare industry
91,"  BS in Computer Science or equivalent education/professional experience is required.  5+ years in a data-engineering role with demonstrable experience with data integration and data warehouse projects.  Experience architecting and building data warehouses, customer profile databases, data marts, etc.  Knowledge of Apache Beam and programming languages including Java and Python.  Experience with MPP systems  Google Big Query, AWS Redshift, Azure Datawarehouse .  Experience with data modeling, warehouse design, and fact/dimension concepts.  Experience working with different query languages  i. e.  T-SQL, PostgreSQL, PL-SQL .  Experience in data integration projects and automation via ETL Tools  i. e.  Talend, Informatica, SSIS, etc.  .  Experience in Hadoop  Hive, Spark, Impala, etc.   ecosystem is a plus.  Experience working with code repositories and continuous integration  i. e.  Git, Jenkins, etc.   Understanding of development and project methodologies.  Ability to work collaboratively in teams with other specialized individuals.  Able to work in a fast-paced, technical environment.  Good verbal and written communication skills.   Deliver quality work on defined tasks with little oversight and direction.  Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews.  Participate in integrated test sessions of components and subsystems on test and production servers.  Serve as technical resource during software development life cycle to solve business issues through the process of identifying and analyzing detailed requirements that translate into data integration and database system designs.  Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs.  Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed.  Ability to transform data into actionable information and convert the results of the analysis into a format that is easy to draw insights from and to share with colleagues and peers.   ",bs computer science educationprofessional dataengineering role demonstrable data integration data warehouse projects architecting building data warehouses customer profile databases data marts apache beam programming languages java python mpp systems google big query aws redshift azure datawarehouse data modeling warehouse design factdimension concepts different query languages e tsql postgresql plsql data integration projects automation via etl tools e talend informatica ssis hadoop hive spark impala ecosystem plus code repositories continuous integration e git jenkins understanding development project methodologies collaboratively teams specialized individuals able fastpaced technical good verbal written communication deliver defined tasks little oversight direction deliverables throughout project adhering coding standards best practices participating code reviews participate integrated test sessions components subsystems test production servers serve technical resource software development life cycle solve business issues process identifying analyzing detailed translate data integration database designs solve business issues process identifying analyzing detailed translate business technical designs use information gained prior sharing technology associates education training resolve issues remove project barriers status updates team members regular basis clearly escalate issues risks project management needed transform data actionable information convert results analysis format easy draw insights share colleagues peers,bs computer science educationprofessional dataengineering role demonstrable data integration warehouse projects architecting building warehouses customer profile databases marts apache beam programming languages java python mpp systems google big query aws redshift azure datawarehouse modeling design factdimension concepts different e tsql postgresql plsql automation via etl tools talend informatica ssis hadoop hive spark impala ecosystem plus code repositories continuous git jenkins understanding development project methodologies collaboratively teams specialized individuals able fastpaced technical good verbal written communication deliver defined tasks little oversight direction deliverables throughout adhering coding standards best practices participating reviews participate integrated test sessions components subsystems production servers serve resource software life cycle solve business issues process identifying analyzing detailed translate database designs use information gained prior sharing technology associates education training resolve remove barriers status updates team members regular basis clearly escalate risks management needed transform actionable convert results analysis format easy draw insights share colleagues peers
92," 3+ years of experience in programming languages such as Java or Python.  2+ years of experience in big data engineering.  1+ years of experience as Spark Developer.    Ability to develop spark jobs to cleanse/enrich/process large amounts of data.  Ability to develop spark streaming jobs to read data from Kafka.  Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc.  Good understanding of various file formats and compression techniques.  Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data.  Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level.  Ability to mentor developers and lead projects of medium to high complexity.  Excellent communication and collaboration skills.     ",programming languages java python big data engineering spark developer develop spark jobs cleanseenrichprocess amounts data develop spark streaming jobs read data kafka tuning spark jobs efficient performance execution time job execution memory good understanding various file formats compression techniques source code management systems git developing cicd pipelines tools jenkins data understand deeply entire architecture major part business able articulate scaling reliability limits area design develop debug enterprise level design estimate crossproject level mentor developers lead projects medium complexity communication collaboration,programming languages java python big data engineering spark developer develop jobs cleanseenrichprocess amounts streaming read kafka tuning efficient performance execution time job memory good understanding various file formats compression techniques source code management systems git developing cicd pipelines tools jenkins understand deeply entire architecture major part business able articulate scaling reliability limits area design debug enterprise level estimate crossproject mentor developers lead projects medium complexity communication collaboration
93,"   Design, build and maintain our data pipelines and automate analyses using SQL and Python based ETL framework Develop ETL ecosystem tools Analyze query pattern of internal users and adjust analytics schemas based on those patterns Collaborate with data science and stakeholders across the organization to raise the bar for data best practices and management Demonstrate and communicate a deep understanding of your chosen languages and frameworks to be able to make tradeoffs.  Able to do more with less complexity.  Advocate for internal and external customers to break down problems, set priorities and follow up on performance and functionality Build and maintain internal data processing and visualization tools to ensure that stakeholders have timely access to data.  Participate in pairing sessions, code reviews and take initiative on research projects/ requirements    Professional experience in ETL systems and database architecture Production-level Python experience a must, Scala and AWS experience are pluses as they're part of our ecosystem.  Understanding of different types of data storage and their trade-offs with regards to availability, consistency, read/write throughput and maintenance cost.  Ability to communicate effectively with engineering peers, data analytics, and business stakeholders.  ",design build maintain data pipelines automate analyses sql python based etl framework develop etl ecosystem tools analyze query pattern internal users adjust analytics schemas based patterns collaborate data science stakeholders across organization raise bar data best practices management demonstrate communicate deep understanding chosen languages frameworks able make tradeoffs able less complexity advocate internal external customers break problems set priorities follow performance functionality build maintain internal data processing visualization tools stakeholders timely access data participate pairing sessions code reviews take initiative research projects professional etl systems database architecture productionlevel python must scala aws pluses theyre part ecosystem understanding different types data storage tradeoffs regards availability consistency readwrite throughput maintenance cost communicate effectively engineering peers data analytics business stakeholders,design build maintain data pipelines automate analyses sql python based etl framework develop ecosystem tools analyze query pattern internal users adjust analytics schemas patterns collaborate science stakeholders across organization raise bar best practices management demonstrate communicate deep understanding chosen languages frameworks able make tradeoffs less complexity advocate external customers break problems set priorities follow performance functionality processing visualization timely access participate pairing sessions code reviews take initiative research projects professional systems database architecture productionlevel must scala aws pluses theyre part different types storage regards availability consistency readwrite throughput maintenance cost effectively engineering peers business
94,"You have a deep understanding of either Java or C++ and a practical understanding of the other  C++ is preferred  You have experience writing high-performance feed handlers in a Linux environment You have experience with multi-threaded programming and distributed application architecture You have strong analytic and design capabilities You have the ability to quickly triage issues and drive the resolution effort through completion You prefer simple, cohesive, and practical solutions that are maintainable and extensible You demonstrate a high level of interpersonal skills and are skilled in building collaborative relationships  You design and build Market Data server-based components and frameworks You collaborate with developers on other teams including operations, option pricing, exchange access, inventory and risk management You develop requirements, propose solutions and deliver software into production environment in a timely and robust manner You participate in design and code reviews You partner with QA and support teams to ensure that the software delivered is high quality and easy to manage in a production environment You keep up to date on the latest technologies that could benefit our system   ",deep understanding either java c practical understanding c writing highperformance feed handlers linux multithreaded programming distributed application architecture analytic design capabilities quickly triage issues drive resolution effort completion prefer simple cohesive practical solutions maintainable extensible demonstrate level interpersonal skilled building collaborative relationships design build market data serverbased components frameworks collaborate developers teams operations option pricing exchange access inventory risk management develop propose solutions deliver software production timely robust manner participate design code reviews partner qa support teams software delivered easy manage production keep date latest technologies could benefit,deep understanding either java c practical writing highperformance feed handlers linux multithreaded programming distributed application architecture analytic design capabilities quickly triage issues drive resolution effort completion prefer simple cohesive solutions maintainable extensible demonstrate level interpersonal skilled building collaborative relationships build market data serverbased components frameworks collaborate developers teams operations option pricing exchange access inventory risk management develop propose deliver software production timely robust manner participate code reviews partner qa support delivered easy manage keep date latest technologies could benefit
95,"  Bachelor's Degree in computer science or equivalent experience required.  2+ years of experience in the design and development of data pipelines and tasks.  Strong analytical and problem solving ability with strong attention to detail and accuracy.  Good understanding of data warehousing concepts and dimensional data modeling.  Hands-on experience with troubleshooting performance issues and fine tuning queries.  Proven experience extracting data from structured data sources  SQL, Excel, CSV files  and unstructured data sources  Couchbase, Splunk, log files  both on-premise and in the cloud Experience consuming data from web services, SOAP and REST technologies, HTML, XML and JSON.  Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.  Proficient in at least one programming language  Python, Java, Go, C , Ruby, C, C++.  Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.  Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.    Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.  Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.  Perform unit testing, system integration testing and assist with user acceptance testing.  Adapt data components to accommodate changes in source data and new business requirements.  Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.  Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.  Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.  Ability to work with the rest of the Data Engineering Team to cross-train and provide support for other BI tasks such as Cube Maintenance, Data Analytics and Requirements Gathering.    ",bachelors degree computer science design development data pipelines tasks analytical problem solving attention detail accuracy good understanding data warehousing concepts dimensional data modeling handson troubleshooting performance issues fine tuning queries proven extracting data structured data sources sql excel csv files unstructured data sources couchbase splunk log files onpremise cloud consuming data web services soap rest technologies html xml json version control systems git bitbucket svn team foundation proficient least one programming language python java go c ruby c c microsoft sql server ssis ssrs power bi azure familiar data warehouse platforms like aws redshift aws data pipeline design develop deploy optimal extraction transformation loading data various gohealth external data sources monitor execute report data pipeline tasks appropriate teams take corrective action quickly case issues perform unit testing integration testing assist user acceptance testing adapt data components accommodate changes source data business create maintain documentation technical detail design operational support maintenance procedures data pipeline tasks data compliance development architecture reporting regulatory standards throughout entire data pipeline collaborate rest data engineering team subject matter experts department leaders understand analyze build deliver datarelated processes andor reports rest data engineering team crosstrain support bi tasks cube maintenance data analytics gathering,bachelors degree computer science design development data pipelines tasks analytical problem solving attention detail accuracy good understanding warehousing concepts dimensional modeling handson troubleshooting performance issues fine tuning queries proven extracting structured sources sql excel csv files unstructured couchbase splunk log onpremise cloud consuming web services soap rest technologies html xml json version control systems git bitbucket svn team foundation proficient least one programming language python java go c ruby microsoft server ssis ssrs power bi azure familiar warehouse platforms like aws redshift pipeline develop deploy optimal extraction transformation loading various gohealth external monitor execute report appropriate teams take corrective action quickly case perform unit testing integration assist user acceptance adapt components accommodate changes source business create maintain documentation technical operational support maintenance procedures compliance architecture reporting regulatory standards throughout entire collaborate engineering subject matter experts department leaders understand analyze build deliver datarelated processes andor reports crosstrain cube analytics gathering
96," Strong ELT/ ETL designer/developer Strong SQL Strong Python Structured & unstructured data expertise Cloud environment development & operations experience  e. g.  AWS, GCP  Preference for candidates experienced with  Google Cloud Platform  GCP  and associated services; e. g.  BigQuery, GCS, Cloud Composer, Dataproc, Dataflow, Dataprep, Cloud Pub/Sub, Metadata DB, Data Studio, Datalab, other Other important Zoro tools  Apache Airflow  scheduler , Bitbucket and git  version control , Stackdriver  ops monitoring , Opsgenie  alert notification , Docker Real-time data replication/streaming tools Data Modeling Excellent verbal and written communications Strong team player  Participate in Requirements Gathering  work with key business partner groups  e. g.  Product Mgt  and other Data Engineering personnel to understand department-level data requirements for the ZDP Design Data Pipelines  work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP Build Data Pipelines  leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP Support Data Quality Program  work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data Support Operations  triage alerts channeled to you and remediate as necessary Technical Documentation  leverage templates provided and create clear, simple and comprehensive documentation for your development Key contributor to defining, implementing and supporting  Data Services Data Dictionary Tool Standards Best Practices Data Lineage User Training   ",elt etl designerdeveloper sql python structured unstructured data expertise cloud development operations e g aws gcp preference candidates experienced google cloud platform gcp associated services e g bigquery gcs cloud composer dataproc dataflow dataprep cloud pubsub metadata db data studio datalab important zoro tools apache airflow scheduler bitbucket git version control stackdriver ops monitoring opsgenie alert notification docker realtime data replicationstreaming tools data modeling verbal written communications team player participate gathering key business partner groups e g product mgt data engineering personnel understand departmentlevel data zdp design data pipelines data engineering personnel overall design flowing data various internal external sources zdp build data pipelines leverage standard toolset develop etlelt code move data various internal external sources zdp support data program data qa engineer identify automated qa checks associated monitoring alerting zdp maintains consistently data support operations triage alerts channeled remediate necessary technical documentation leverage templates provided create clear simple comprehensive documentation development key contributor defining implementing supporting data services data dictionary tool standards best practices data lineage user training,elt etl designerdeveloper sql python structured unstructured data expertise cloud development operations e g aws gcp preference candidates experienced google platform associated services bigquery gcs composer dataproc dataflow dataprep pubsub metadata db studio datalab important zoro tools apache airflow scheduler bitbucket git version control stackdriver ops monitoring opsgenie alert notification docker realtime replicationstreaming modeling verbal written communications team player participate gathering key business partner groups product mgt engineering personnel understand departmentlevel zdp design pipelines overall flowing various internal external sources build leverage standard toolset develop etlelt code move support program qa engineer identify automated checks alerting maintains consistently triage alerts channeled remediate necessary technical documentation templates provided create clear simple comprehensive contributor defining implementing supporting dictionary tool standards best practices lineage user training
97," Expert-level data modeler  back-end and semantic layer  Expert-level ETL/ELT designer/developer Strong database administration and operations experience & proficiency Strong SQL Structured & unstructured data expertise Cloud environment development & operations experience  e. g.  Google Cloud Platform/GCP experience a plus  Excellent verbal and written communications Strong team player Working knowledge of eCommerce data a plus Prior experience with Git, Terraform, GCP Deployment Manager, CICD, Docker, Kubernetes, Apache Airflow, Apache Beam, Apache Spark experience is a plus   Primary responsibility for Zoro Data Platform  ZDP   Data model ongoing design & development Conceptual, logical and physical design  database, ODS, aggregates, etc.   Database administration Capacity analysis & management MDM Lead Identify key domains thatâd benefit from an MDM approach  e. g.  Product, Customer , along with best data sources & necessary attributes, and integrate into the ZDP Define governance strategy with associated roles & responsibilities  e. g.  Data Steward, Quality Specialist  Define & implement Policies & SOPs Monitor operations, develop and report quality metrics to key stakeholders Data Pipeline development  Participate in Requirements Gathering  work with key business partner groups  e. g.  Product Mgt  and other Data Engineering personnel to understand department-level data requirements for the ZDP Design Data Pipelines  work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP Build Data Pipelines  leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP Support Data Quality Program  work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data Support Operations  triage alerts channeled to you and remediate as necessary Technical Documentation  leverage templates provided and create clear, simple and comprehensive documentation for your development Key contributor to defining, implementing and supporting  Data Services Data Dictionary Tool Standards Best Practices Data Lineage User Training Define Best Practices and Guidelines for other Data Engineering team members Lead the team in developing new technical skills necessary for cloud-native data engineering platform Explores new tech Shares and documents learnings Productionalizes proof of concepts    ",expertlevel data modeler backend semantic layer expertlevel etlelt designerdeveloper database administration operations proficiency sql structured unstructured data expertise cloud development operations e g google cloud platformgcp plus verbal written communications team player ecommerce data plus prior git terraform gcp deployment manager cicd docker kubernetes apache airflow apache beam apache spark plus primary responsibility zoro data platform zdp data model ongoing design development conceptual logical physical design database ods aggregates database administration capacity analysis management mdm lead identify key domains thatd benefit mdm approach e g product customer along best data sources necessary attributes integrate zdp define governance strategy associated roles responsibilities e g data steward specialist define implement policies sops monitor operations develop report metrics key stakeholders data pipeline development participate gathering key business partner groups e g product mgt data engineering personnel understand departmentlevel data zdp design data pipelines data engineering personnel overall design flowing data various internal external sources zdp build data pipelines leverage standard toolset develop etlelt code move data various internal external sources zdp support data program data qa engineer identify automated qa checks associated monitoring alerting zdp maintains consistently data support operations triage alerts channeled remediate necessary technical documentation leverage templates provided create clear simple comprehensive documentation development key contributor defining implementing supporting data services data dictionary tool standards best practices data lineage user training define best practices guidelines data engineering team members lead team developing technical necessary cloudnative data engineering platform explores tech shares documents learnings productionalizes proof concepts,expertlevel data modeler backend semantic layer etlelt designerdeveloper database administration operations proficiency sql structured unstructured expertise cloud development e g google platformgcp plus verbal written communications team player ecommerce prior git terraform gcp deployment manager cicd docker kubernetes apache airflow beam spark primary responsibility zoro platform zdp model ongoing design conceptual logical physical ods aggregates capacity analysis management mdm lead identify key domains thatd benefit approach product customer along best sources necessary attributes integrate define governance strategy associated roles responsibilities steward specialist implement policies sops monitor develop report metrics stakeholders pipeline participate gathering business partner groups mgt engineering personnel understand departmentlevel pipelines overall flowing various internal external build leverage standard toolset code move support program qa engineer automated checks monitoring alerting maintains consistently triage alerts channeled remediate technical documentation templates provided create clear simple comprehensive contributor defining implementing supporting services dictionary tool standards practices lineage user training guidelines members developing cloudnative explores tech shares documents learnings productionalizes proof concepts
98," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ",expertise least one following domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale writing software one languages python java scala go building productiongrade data solutions relational nosql systems monitoringalerting capacity planning performance tuning technical consulting customerfacing role,expertise least one following domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale writing languages python java scala go relational nosql systems monitoringalerting capacity planning performance tuning consulting customerfacing role
99," Bachelorâs degree in computer science or another technology/business field of study 4+ years of T-SQL experience 4-5 years of progressive experience within application support/IT operations organizations Experience in Microsoft . NET Technologies, C , Windows Forms Analytical thinker with a problem solving and an entrepreneurial mindset Ability to communicate and work with both technical and non-technical audiences  Strong attention to detail, ensuring processes are followed and root cause remediation is planned and executed for each issue, and that actions are fully documented.  Critical thinker with the vision to work both tactically and strategically.  Exceptional verbal and written communication skills, ability to modify communication style to match the appropriate level of the audience targeted, with strong understanding of the impact of a message on the organization or customer.   Provide technical and process leadership support to management in driving issue resolution, escalations, SLA adherence, and support process improvements Collaborate with other team members across IT, Operations, and business functions to troubleshoot and resolve support tickets Intermediate level T-SQL experience in writing/maintaining stored procedures and complex queries Reverse engineer application code and configurations and write custom/ad-hoc T-SQL queries to diagnose critical application issues reported by end users Troubleshoot and provide necessary bug fixes related to client and user reported issues Provide maintenance support to existing applications Conduct presentations of the work where requested and participate in knowledge sharing sessions with others on the team  ",bachelors degree computer science another technologybusiness study tsql progressive within application supportit operations organizations microsoft net technologies c windows forms analytical thinker problem solving entrepreneurial mindset communicate technical nontechnical audiences attention detail ensuring processes followed root cause remediation planned executed issue actions fully documented critical thinker vision tactically strategically exceptional verbal written communication modify communication style match appropriate level audience targeted understanding impact message organization customer technical process leadership support management driving issue resolution escalations sla adherence support process improvements collaborate team members across operations business functions troubleshoot resolve support tickets intermediate level tsql writingmaintaining stored procedures complex queries reverse engineer application code configurations write customadhoc tsql queries diagnose critical application issues reported end users troubleshoot necessary bug fixes client user reported issues maintenance support existing applications conduct presentations requested participate sharing sessions others team,bachelors degree computer science another technologybusiness study tsql progressive within application supportit operations organizations microsoft net technologies c windows forms analytical thinker problem solving entrepreneurial mindset communicate technical nontechnical audiences attention detail ensuring processes followed root cause remediation planned executed issue actions fully documented critical vision tactically strategically exceptional verbal written communication modify style match appropriate level audience targeted understanding impact message organization customer process leadership support management driving resolution escalations sla adherence improvements collaborate team members across business functions troubleshoot resolve tickets intermediate writingmaintaining stored procedures complex queries reverse engineer code configurations write customadhoc diagnose issues reported end users necessary bug fixes client user maintenance existing applications conduct presentations requested participate sharing sessions others
100,"   Work as part of a team to design and develop cloud data solutions at local Chicago clients Deliver on the technical scope of projects & demonstrate thought leadership at clients as well as internally at Slalom Gather technical requirements, assess client capabilities and analyze findings to provide appropriate cloud solution recommendations and adoption strategy Research, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems Assist in designing multi-phased cloud data strategies, including designing multi-phased implementation roadmaps Analyze, architect, design, and actively develop cloud data warehouse, data lakes, and other cloud-based data solutions Design and develop scalable data ingestion frameworks to transform variety of datasets Serve as a subject matter expert in a cloud platform for larger the Slalom practice and contribute back to community  ",part team design develop cloud data solutions local chicago clients deliver technical scope projects demonstrate thought leadership clients well internally slalom gather technical assess client capabilities analyze findings appropriate cloud solution recommendations adoption strategy research analyze recommend select technical approaches solving challenging complex development integration problems assist designing multiphased cloud data strategies designing multiphased implementation roadmaps analyze architect design actively develop cloud data warehouse data lakes cloudbased data solutions design develop scalable data ingestion frameworks transform variety datasets serve subject matter expert cloud platform larger slalom practice contribute back community,part team design develop cloud data solutions local chicago clients deliver technical scope projects demonstrate thought leadership well internally slalom gather assess client capabilities analyze findings appropriate solution recommendations adoption strategy research recommend select approaches solving challenging complex development integration problems assist designing multiphased strategies implementation roadmaps architect actively warehouse lakes cloudbased scalable ingestion frameworks transform variety datasets serve subject matter expert platform larger practice contribute back community
101," Actively pursuing a degree in Information Science & Technology, Computer Science, or related field.  Relevant Computer Science coursework, development bootcamp, or experience on any platform as well as a passion for coding   Helping transform data into usable information based on business needs and requirements Working closely on a cross functional team including other engineers, analysts, and architects Identifying and implementing automation opportunities that streamline and enhance the process Interacting with customers and creating technology solutions to help their business succeed Learning new technologies with an eagerness to learn innovative and unfamiliar languages quickly Participating in information gathering sessions with the clients to understand data requirements and objectives Designing reports, dashboards, and metrics to summarize information and leverage solutions  ",actively pursuing degree information science technology computer science relevant computer science coursework development bootcamp platform well passion coding helping transform data usable information based business needs closely cross functional team engineers analysts architects identifying implementing automation opportunities streamline enhance process interacting customers creating technology solutions help business succeed technologies eagerness learn innovative unfamiliar languages quickly participating information gathering sessions clients understand data objectives designing reports dashboards metrics summarize information leverage solutions,actively pursuing degree information science technology computer relevant coursework development bootcamp platform well passion coding helping transform data usable based business needs closely cross functional team engineers analysts architects identifying implementing automation opportunities streamline enhance process interacting customers creating solutions help succeed technologies eagerness learn innovative unfamiliar languages quickly participating gathering sessions clients understand objectives designing reports dashboards metrics summarize leverage
102,"     Bachelor's degree Developing applications in a linux environment in at least one of the following languages  Python, Scala, Java Experience with file formats JSON, CSV, XML Experience maintaining shared code bases Previous experience using APIs and web services  SOAP, REST Deep understanding of relational and dimensional database designs Experience designing and maintaining relational databases such as MySQL and columnar databases such as Redshift Experience with integrating multiple data sources and managing large data structures  data warehouse, data lake  in cloud architectures  Azure, AWS  Proven ability to communicate effectively with technical and non-technical stakeholders Excellent written and verbal communication skills Exceptional time management skills and ability to prioritize workflow Full-time position, 40+ hour work week Some weekend and holiday work may be required  depending on the project  ",bachelors degree developing applications linux least one following languages python scala java file formats json csv xml maintaining shared code bases previous apis web services soap rest deep understanding relational dimensional database designs designing maintaining relational databases mysql columnar databases redshift integrating multiple data sources managing data structures data warehouse data lake cloud architectures azure aws proven communicate effectively technical nontechnical stakeholders written verbal communication exceptional time management prioritize workflow fulltime position hour week weekend holiday may depending project,bachelors degree developing applications linux least one following languages python scala java file formats json csv xml maintaining shared code bases previous apis web services soap rest deep understanding relational dimensional database designs designing databases mysql columnar redshift integrating multiple data sources managing structures warehouse lake cloud architectures azure aws proven communicate effectively technical nontechnical stakeholders written verbal communication exceptional time management prioritize workflow fulltime position hour week weekend holiday may depending project
103," 5+ years of experience in programming languages such as Java or Python.  3+ years of experience in big data engineering.  2+ years of experience as Spark Developer.    Ability to develop spark jobs to cleanse/enrich/process large amounts of data.  Ability to develop spark streaming jobs to read data from Kafka.  Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc.  Good understanding of various file formats and compression techniques.  Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data.  Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level.  Ability to mentor developers and lead projects of medium to high complexity.  Excellent communication and collaboration skills.     ",programming languages java python big data engineering spark developer develop spark jobs cleanseenrichprocess amounts data develop spark streaming jobs read data kafka tuning spark jobs efficient performance execution time job execution memory good understanding various file formats compression techniques source code management systems git developing cicd pipelines tools jenkins data understand deeply entire architecture major part business able articulate scaling reliability limits area design develop debug enterprise level design estimate crossproject level mentor developers lead projects medium complexity communication collaboration,programming languages java python big data engineering spark developer develop jobs cleanseenrichprocess amounts streaming read kafka tuning efficient performance execution time job memory good understanding various file formats compression techniques source code management systems git developing cicd pipelines tools jenkins understand deeply entire architecture major part business able articulate scaling reliability limits area design debug enterprise level estimate crossproject mentor developers lead projects medium complexity communication collaboration
104," Intermediate to advanced level in Structured Query Language  SQL    Bachelor Level preparation in computer, mathematical, information sciences or equivalent training â Flexibility and ability to work with individuals of diverse backgrounds and educational levels ",intermediate advanced level structured query language sql bachelor level preparation computer mathematical information sciences training flexibility individuals diverse backgrounds educational levels,intermediate advanced level structured query language sql bachelor preparation computer mathematical information sciences training flexibility individuals diverse backgrounds educational levels
105,"Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.  Experience with Data Analytics.  Experience with Web Services and APIs.  Experience in the development of batch and real-time data integration and data consolidation processes.  Experience with machine learning, AI, and data lakes.  Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.  Strong analytical skills with ability for problem-solving.  Understands the importance of data provenance and the ability to demonstrate it to clients.  Detail oriented, organized, self-motivated.     Develop strategy for new multi-platform data integration and analytics.  Develop strategy for new multi-platform-sourced data lake.  Contribute to API strategy to facilitate application connectivity and analytics.  Contribute to the maintenance and evolution of best practices.  Contribute to process documentation.  Perform multiple proofs of concept  POCs .  Contribute to implementation plan for decided-upon solution s .     ",javascriptjava python jitterbit developer languages data analytics web services apis development batch realtime data integration data consolidation processes machine ai data lakes proficiency tsqlplsql querywriting stored procedure development views analytical problemsolving understands importance data provenance demonstrate clients detail oriented organized selfmotivated develop strategy multiplatform data integration analytics develop strategy multiplatformsourced data lake contribute api strategy facilitate application connectivity analytics contribute maintenance evolution best practices contribute process documentation perform multiple proofs concept pocs contribute implementation plan decidedupon solution,javascriptjava python jitterbit developer languages data analytics web services apis development batch realtime integration consolidation processes machine ai lakes proficiency tsqlplsql querywriting stored procedure views analytical problemsolving understands importance provenance demonstrate clients detail oriented organized selfmotivated develop strategy multiplatform multiplatformsourced lake contribute api facilitate application connectivity maintenance evolution best practices process documentation perform multiple proofs concept pocs implementation plan decidedupon solution
106," 4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.  2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.  1+ years of experience on distributed, high throughput and low latency architecture.  1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.  A successful track-record of manipulating, processing and extracting value from large disconnected datasets.     ",software development substantial part gained highthroughput decisionautomation big data technologies like spark kafka flink hadoop nosql datastores distributed throughput low latency architecture deploying managing data pipelines supporting datasciencedriven decisioning scale successful trackrecord manipulating processing extracting value disconnected datasets,software development substantial part gained highthroughput decisionautomation big data technologies like spark kafka flink hadoop nosql datastores distributed throughput low latency architecture deploying managing pipelines supporting datasciencedriven decisioning scale successful trackrecord manipulating processing extracting value disconnected datasets
107," C  programming and . NET platform experience Azure Data Architecture Experience with data lakes, MS SQL, Data Bricks NoSQL environments such as CosmosDB Engineering and/or software development experience and demonstrable architecture experience at enterprise scale History of working successfully with cross-functional engineering teams Structured, Unstructured, Semi-Structured Data techniques and processes Working knowledge of open source Apache products, such as Hadoop  Profile and analyze data in designing scalable solutions Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale.  Analyze, develop, and maintain data pipelines from internal and external sources Ability to adapt to new technologies and learn quickly  ",c programming net platform azure data architecture data lakes ms sql data bricks nosql environments cosmosdb engineering andor software development demonstrable architecture enterprise scale history successfully crossfunctional engineering teams structured unstructured semistructured data techniques processes open source apache products hadoop profile analyze data designing scalable solutions develop highly scalable extensible big data platforms enable collection storage modeling analysis massive data sets streaming data leverage technologies cosmosdb gremlin api graph model host speed applications global scale analyze develop maintain data pipelines internal external sources adapt technologies learn quickly,c programming net platform azure data architecture lakes ms sql bricks nosql environments cosmosdb engineering andor software development demonstrable enterprise scale history successfully crossfunctional teams structured unstructured semistructured techniques processes open source apache products hadoop profile analyze designing scalable solutions develop highly extensible big platforms enable collection storage modeling analysis massive sets streaming leverage technologies gremlin api graph model host speed applications global maintain pipelines internal external sources adapt learn quickly
108," C  programming and . NET platform experience Azure Data Architecture Experience with data lakes, MS SQL, Data Bricks NoSQL environments such as CosmosDB Engineering and/or software development experience and demonstrable architecture experience at enterprise scale History of working successfully with cross-functional engineering teams Structured, Unstructured, Semi-Structured Data techniques and processes Working knowledge of open source Apache products, such as Hadoop  Profile and analyze data in designing scalable solutions Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale.  Analyze, develop, and maintain data pipelines from internal and external sources Ability to adapt to new technologies and learn quickly  ",c programming net platform azure data architecture data lakes ms sql data bricks nosql environments cosmosdb engineering andor software development demonstrable architecture enterprise scale history successfully crossfunctional engineering teams structured unstructured semistructured data techniques processes open source apache products hadoop profile analyze data designing scalable solutions develop highly scalable extensible big data platforms enable collection storage modeling analysis massive data sets streaming data leverage technologies cosmosdb gremlin api graph model host speed applications global scale analyze develop maintain data pipelines internal external sources adapt technologies learn quickly,c programming net platform azure data architecture lakes ms sql bricks nosql environments cosmosdb engineering andor software development demonstrable enterprise scale history successfully crossfunctional teams structured unstructured semistructured techniques processes open source apache products hadoop profile analyze designing scalable solutions develop highly extensible big platforms enable collection storage modeling analysis massive sets streaming leverage technologies gremlin api graph model host speed applications global maintain pipelines internal external sources adapt learn quickly
109,"8+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.  4+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.  3+ years of experience on distributed, high-throughput and low-latency architecture.  1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.  Limited immigration sponsorship may be available.     ",software development substantial part gained highthroughput decisionautomation big data technologies like spark kafka flink hadoop nosql datastores distributed highthroughput lowlatency architecture deploying managing data pipelines supporting datasciencedriven decisioning scale limited immigration sponsorship may available,software development substantial part gained highthroughput decisionautomation big data technologies like spark kafka flink hadoop nosql datastores distributed lowlatency architecture deploying managing pipelines supporting datasciencedriven decisioning scale limited immigration sponsorship may available
110,"  Experience Range  2-5 years  Fresh Graduates must be top-ranked and exceptionally qualified  Bachelor or higher Degree in information systems or computer science Understanding of Data Integration flavors Solid understanding of SQL and good grasp of relational and analytical database management theory and practice.  Good knowledge of software development and architectural patterns.  Technical skills include Java development, JDBC, XML, Web Service related APIs, experience with version control systems  e. g.  SVN, git .  Basic experience in Big Data, NoSQL, and InMemory environments is welcome.  Experience in Windows & Linux  and UNIX  operating systems in server environments.  Personal and Relationship qualities  Professional curiosity and the ability to enable yourself in new technologies and tasks.  Active listener.  Curiosity and continuous learning.  Creativity.  Team worker.  Communications  Good written/verbal communication skills in English  other international languages a plus  are essential for interaction with clients, making presentations, attending meetings and writing technical documentation.  Willingness to travel.   Conception, implementation, and execution of customer-specific integration projects based on the Denodo Platform.  Education, coaching and support during the introduction as well as ongoing projects of the Denodo Platform to achieve high level of client satisfaction.  Diagnose and resolve clients inquiries related to operating Denodo software products in their environment.  Participate in problem escalation and call prevention projects to help clients and other technical specialists increase their efficiency when using Denodo products.  Contribute to knowledge management activities and promote best practices for project execution.  Implement product demos and pilots to showcase Data Virtualization in enterprise scenarios, cloud deployments and Big Data projects.  Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding clientâs business cases, requirements and issues.   ",range fresh graduates must topranked exceptionally qualified bachelor higher degree information systems computer science understanding data integration flavors solid understanding sql good grasp relational analytical database management theory practice good software development architectural patterns technical include java development jdbc xml web service apis version control systems e g svn git basic big data nosql inmemory environments welcome windows linux unix operating systems server environments personal relationship qualities professional curiosity enable technologies tasks active listener curiosity continuous creativity team worker communications good writtenverbal communication english international languages plus essential interaction clients making presentations attending meetings writing technical documentation willingness travel conception implementation execution customerspecific integration projects based denodo platform education coaching support introduction well ongoing projects denodo platform achieve level client satisfaction diagnose resolve clients inquiries operating denodo software products participate problem escalation call prevention projects help clients technical specialists increase efficiency denodo products contribute management activities promote best practices project execution implement product demos pilots showcase data virtualization enterprise scenarios cloud deployments big data projects timely prioritized complete customerbased feedback product management sales support andor development regarding clients business cases issues,range fresh graduates must topranked exceptionally qualified bachelor higher degree information systems computer science understanding data integration flavors solid sql good grasp relational analytical database management theory practice software development architectural patterns technical include java jdbc xml web service apis version control e g svn git basic big nosql inmemory environments welcome windows linux unix operating server personal relationship qualities professional curiosity enable technologies tasks active listener continuous creativity team worker communications writtenverbal communication english international languages plus essential interaction clients making presentations attending meetings writing documentation willingness travel conception implementation execution customerspecific projects based denodo platform education coaching support introduction well ongoing achieve level client satisfaction diagnose resolve inquiries products participate problem escalation call prevention help specialists increase efficiency contribute activities promote best practices project implement product demos pilots showcase virtualization enterprise scenarios cloud deployments timely prioritized complete customerbased feedback sales andor regarding business cases issues
111,"   Serve as a member of a data team that solves complex challenges and builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, and table functions Develop solutions and contributing to development, leveraging Object-Oriented programming techniques  . Net , Software Development Lifecycles, Unit Test Techniques, and Debugging/Analytical Techniques.  Collaborate with the team to develop database structures that fit into the overall architecture of the systems under development.  Code, install, optimize, and debug database queries and stored procedures using appropriate tools and editors.  Perform code reviews and provide feedback in a timely manner.  Promote collective code ownership for everyone to have visibility into the feature codebase.  Present technical ideas and concepts in business-friendly language.  Provide recommendations, analysis, and evaluation of systems improvements, optimization, development, and maintenance efforts, including capacity planning.  Identify and correct performance bottlenecks related to SQL code.  Support timely production releases and adherence to release activities.  Contribute to data retention strategy.    ",serve member data team solves complex challenges builds database solutions sql server tsql ssis stored procedures views userdefined functions table functions develop solutions contributing development leveraging objectoriented programming techniques net software development lifecycles unit test techniques debugginganalytical techniques collaborate team develop database structures fit overall architecture systems development code install optimize debug database queries stored procedures appropriate tools editors perform code reviews feedback timely manner promote collective code ownership everyone visibility feature codebase present technical ideas concepts businessfriendly language recommendations analysis evaluation systems improvements optimization development maintenance efforts capacity planning identify correct performance bottlenecks sql code support timely production releases adherence release activities contribute data retention strategy,serve member data team solves complex challenges builds database solutions sql server tsql ssis stored procedures views userdefined functions table develop contributing development leveraging objectoriented programming techniques net software lifecycles unit test debugginganalytical collaborate structures fit overall architecture systems code install optimize debug queries appropriate tools editors perform reviews feedback timely manner promote collective ownership everyone visibility feature codebase present technical ideas concepts businessfriendly language recommendations analysis evaluation improvements optimization maintenance efforts capacity planning identify correct performance bottlenecks support production releases adherence release activities contribute retention strategy
112,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
113,"2+ years' experience building data solutions including AWS S3, EMR, and Redshift, Tableau and Tableau server Expert SQL scripting skills; R, Python, or SAS preferred but not required Ability to effectively build relationships across the business at all levels Self-starter, entrepreneurial, high-energy who can take initiative in a fast-moving environment Strong technical understanding of current and emerging business intelligence and analytics technologies    Act as data strategist responsible for the short and long term vision of data management, warehouse performance, and data architecture supporting both business intelligence and prescriptive analytics Partner with both internal stakeholders and external vendors involved in project definition, design and planning, and mapping the data journey from source through consumer  data visualization, application, or predictive model  Gather, document, and analyze business requirements; establish and prioritize efforts to deliver data models that support business needs Design, develop, test and deploy data models, data collection, and transformation components.  Determine best point for transformations, calculations, and joins  e. g.  data lake, data warehouse, or Tableau data source  Troubleshoot and support existing data workflow processes; deliver fixes and optimizations where appropriate Work closely with CIO, Chief Architect, and Chief Analytics Officer on data governance and planning - ensure scalability and sustainability of business intelligence data architecture  ",building data solutions aws emr redshift tableau tableau server expert sql scripting r python sas effectively build relationships across business levels selfstarter entrepreneurial highenergy take initiative fastmoving technical understanding current emerging business intelligence analytics technologies act data strategist responsible short long term vision data management warehouse performance data architecture supporting business intelligence prescriptive analytics partner internal stakeholders external vendors involved project definition design planning mapping data journey source consumer data visualization application predictive model gather document analyze business establish prioritize efforts deliver data models support business needs design develop test deploy data models data collection transformation components determine best point transformations calculations joins e g data lake data warehouse tableau data source troubleshoot support existing data workflow processes deliver fixes optimizations appropriate closely cio chief architect chief analytics officer data governance planning scalability sustainability business intelligence data architecture,building data solutions aws emr redshift tableau server expert sql scripting r python sas effectively build relationships across business levels selfstarter entrepreneurial highenergy take initiative fastmoving technical understanding current emerging intelligence analytics technologies act strategist responsible short long term vision management warehouse performance architecture supporting prescriptive partner internal stakeholders external vendors involved project definition design planning mapping journey source consumer visualization application predictive model gather document analyze establish prioritize efforts deliver models support needs develop test deploy collection transformation components determine best point transformations calculations joins e g lake troubleshoot existing workflow processes fixes optimizations appropriate closely cio chief architect officer governance scalability sustainability
114,"At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ",least consulting client service delivery azure devops azure platform proven build manage foster teamoriented,least consulting client service delivery azure devops platform proven build manage foster teamoriented
115,"3+ years experience programming with Python is required 3+ years in an ETL or Data Engineering role, building and implementing data pipelines Strong skills with PySpark and SQL with the ability to write efficient queries Familiarity with AWS big data services  S3, Redshift, Glue, EC2, Lambda, SageMaker, Dynamo Experience working in a highly collaborative environment - we do Agile using sprints, planning, retro, etc.  Experience with Airflow and other open source technologies Excited and willing to learn new things Background in computer science, engineering, mathematics, related field, or equivalent work experience  Collaborate with our reporting, analytics, and data science teams to understand data sources and business requirements Work within a collaborative team, adhering to Agile best practices, documentation, and knowledge sharing Gather, clean, enrich, and transform data to feed internal and external client needs Define, build, test, and implement data pipelines, batch and streaming Monitor pipeline performance and document infrastructure changes Make code decisions and adhere to best practices for ETL and programming Contribute to our overall architecture and pipeline design and make contributions to the product road map  ",programming python etl data engineering role building implementing data pipelines pyspark sql write efficient queries familiarity aws big data services redshift glue ec lambda sagemaker dynamo highly collaborative agile sprints planning retro airflow open source technologies excited willing learn things background computer science engineering mathematics collaborate reporting analytics data science teams understand data sources business within collaborative team adhering agile best practices documentation sharing gather clean enrich transform data feed internal external client needs define build test implement data pipelines batch streaming monitor pipeline performance document infrastructure changes make code decisions adhere best practices etl programming contribute overall architecture pipeline design make contributions product road map,programming python etl data engineering role building implementing pipelines pyspark sql write efficient queries familiarity aws big services redshift glue ec lambda sagemaker dynamo highly collaborative agile sprints planning retro airflow open source technologies excited willing learn things background computer science mathematics collaborate reporting analytics teams understand sources business within team adhering best practices documentation sharing gather clean enrich transform feed internal external client needs define build test implement batch streaming monitor pipeline performance document infrastructure changes make code decisions adhere contribute overall architecture design contributions product road map
116,"Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.  Interest in Machine Learning algorithms, techniques and methodologies and growing in that space Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies Familiarity with one or more general purpose programming language including but not limited to Python and C++.  Demonstrated ability to learn quickly and has a passion for emerging digital technologies Comfortable with communicating clearly and concisely across a variety of audiences including technology and business.  Ability to think strategically as well as tactically in order to drive ideas into action.  Ability to collaborate in a team-oriented, dynamic environment Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.  Interest in Machine Learning algorithms, techniques and methodologies and growing in that space Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies Familiarity with one or more general purpose programming language including but not limited to Python and C++.  Demonstrated ability to learn quickly and has a passion for emerging digital technologies Comfortable with communicating clearly and concisely across a variety of audiences including technology and business.  Ability to think strategically as well as tactically in order to drive ideas into action.  Ability to collaborate in a team-oriented, dynamic environment Conceptual, analytical thinker, with a real passion for data exploration and design Exposure to business intelligence which includes Databases, ETL, Machine Learning, and Data Mining Participate in hands-on technical projects and enthusiastic to tackle problems supporting our team.  Shows great attention to detail  Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.  Interest in Machine Learning algorithms, techniques and methodologies and growing in that space Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies Familiarity with one or more general purpose programming language including but not limited to Python and C++.  Demonstrated ability to learn quickly and has a passion for emerging digital technologies Comfortable with communicating clearly and concisely across a variety of audiences including technology and business.  Ability to think strategically as well as tactically in order to drive ideas into action.  Ability to collaborate in a team-oriented, dynamic environment",pursuing bachelors degree technology focus machine data science interest machine algorithms techniques methodologies growing space familiarity semistructured unstructured databases algorithms techniques methodologies familiarity one general purpose programming language limited python c demonstrated learn quickly passion emerging digital technologies comfortable communicating clearly concisely across variety audiences technology business think strategically well tactically order drive ideas action collaborate teamoriented dynamic pursuing bachelors degree technology focus machine data science interest machine algorithms techniques methodologies growing space familiarity semistructured unstructured databases algorithms techniques methodologies familiarity one general purpose programming language limited python c demonstrated learn quickly passion emerging digital technologies comfortable communicating clearly concisely across variety audiences technology business think strategically well tactically order drive ideas action collaborate teamoriented dynamic conceptual analytical thinker real passion data exploration design exposure business intelligence includes databases etl machine data mining participate handson technical projects enthusiastic tackle problems supporting team shows great attention detail pursuing bachelors degree technology focus machine data science interest machine algorithms techniques methodologies growing space familiarity semistructured unstructured databases algorithms techniques methodologies familiarity one general purpose programming language limited python c demonstrated learn quickly passion emerging digital technologies comfortable communicating clearly concisely across variety audiences technology business think strategically well tactically order drive ideas action collaborate teamoriented dynamic,pursuing bachelors degree technology focus machine data science interest algorithms techniques methodologies growing space familiarity semistructured unstructured databases one general purpose programming language limited python c demonstrated learn quickly passion emerging digital technologies comfortable communicating clearly concisely across variety audiences business think strategically well tactically order drive ideas action collaborate teamoriented dynamic conceptual analytical thinker real exploration design exposure intelligence includes etl mining participate handson technical projects enthusiastic tackle problems supporting team shows great attention detail
117,"  Prioritizes and executes rapid raw data collection from source systems, targets and implements efficient storage of, employs fast and reliable access patterns.  Understands system protocols, how systems operate and data flows.  Aware of current and emerging technology tools and their benefits.  Expected to independently develop a full software stack.  Understands the building blocks, interactions, dependencies, and tools required to complete software and automation work.  Independent study of evolving technology is expected.  Drives engineering projects by developing software solutions; conducting tests and inspections; building reports and calculations.  Strong focus on innovation and enablement, contributes to designs to implement new ideas which improve an existing and new system/process/service.  Understands and can apply new industry perspectives to our existing business and data models.  Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.  Maintains knowledge of existing technology documents.  Writes basic documentation on how technology works using collaboration tools like Confluence.  Creates clear documentation for new code and systems used.  Documenting systems designs, presentations, and business requirements for consumption and consideration at the manager level.  Collaborates with technical teams and utilizes system expertise to deliver technical solutions.  Continuously learns and teaches others existing and new technologies.  Contributes to the development of others through mentoring or in-house workshops and learning sessions.  Drives team practices and procedures to achieve repeatable success and defined expectation of services Provides a significant collaborative role in long-term department planning, with focus on initiatives achieving data empowerment, operational efficiency and sustainability Monitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities.    Bachelorâs degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.  Prior experience in Capital Markets strongly preferred.  5+ years of experience developing software in a professional environment  preferably financial services but not required  3 years of hands on Data Driven Enterprise Application development, preferable in financial industry Strong understanding of Enterprise architecture patterns, Object Oriented & Service Oriented principles, design patterns, industry best practices Foundational knowledge of data structures, algorithms, and designing for performance.  Proficiency in programming in Java, C  or Python and willingness to learn and adopt new languages as necessary Experience in database technology like MSSQL and one of key value and document databases like MongoDb, Dynamo Db, Casandra.  Exposure to containers, microservices, distributed systems architecture, orchestrators and cloud computing.  Comfortable with core programming concepts and techniques  e. g.  concurrency, memory management  Enjoys working with algorithms and data structures  e. g.  trees, hash maps, queues  Data Analytics and Data Science experience will be a plus.  Good sense of user interaction and usability design to provide an intuitive, seamless end user experience.  Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts.  Ability to work and potentially lead in an Agile methodology environment.  ",prioritizes executes rapid raw data collection source systems targets implements efficient storage employs fast reliable access patterns understands protocols systems operate data flows aware current emerging technology tools benefits expected independently develop full software stack understands building blocks interactions dependencies tools complete software automation independent study evolving technology expected drives engineering projects developing software solutions conducting tests inspections building reports calculations focus innovation enablement contributes designs implement ideas improve existing systemprocessservice understands apply industry perspectives existing business data models reviews existing designs processes highlight efficient ways complete existing workload effectively industry perspectives maintains existing technology documents writes basic documentation technology works collaboration tools like confluence creates clear documentation code systems used documenting systems designs presentations business consumption consideration manager level collaborates technical teams utilizes expertise deliver technical solutions continuously learns teaches others existing technologies contributes development others mentoring inhouse workshops sessions drives team practices procedures achieve repeatable success defined expectation services provides significant collaborative role longterm department planning focus initiatives achieving data empowerment operational efficiency sustainability monitors evaluates overall strategic data infrastructure tracks efficiency reliability identifies recommends efficiency improvements mitigates operational vulnerabilities bachelors degree relevant computer science mathematics electrical engineering technical discipline prior capital markets strongly developing software professional preferably financial services hands data driven enterprise application development preferable financial industry understanding enterprise architecture patterns object oriented service oriented principles design patterns industry best practices foundational data structures algorithms designing performance proficiency programming java c python willingness learn adopt languages necessary database technology like mssql one key value document databases like mongodb dynamo db casandra exposure containers microservices distributed systems architecture orchestrators cloud computing comfortable core programming concepts techniques e g concurrency memory management enjoys algorithms data structures e g trees hash maps queues data analytics data science plus good sense user interaction usability design intuitive seamless end user communications subject matter expert extract critical business concepts potentially lead agile methodology,prioritizes executes rapid raw data collection source systems targets implements efficient storage employs fast reliable access patterns understands protocols operate flows aware current emerging technology tools benefits expected independently develop full software stack building blocks interactions dependencies complete automation independent study evolving drives engineering projects developing solutions conducting tests inspections reports calculations focus innovation enablement contributes designs implement ideas improve existing systemprocessservice apply industry perspectives business models reviews processes highlight ways workload effectively maintains documents writes basic documentation works collaboration like confluence creates clear code used documenting presentations consumption consideration manager level collaborates technical teams utilizes expertise deliver continuously learns teaches others technologies development mentoring inhouse workshops sessions team practices procedures achieve repeatable success defined expectation services provides significant collaborative role longterm department planning initiatives achieving empowerment operational efficiency sustainability monitors evaluates overall strategic infrastructure tracks reliability identifies recommends improvements mitigates vulnerabilities bachelors degree relevant computer science mathematics electrical discipline prior capital markets strongly professional preferably financial hands driven enterprise application preferable understanding architecture object oriented service principles design best foundational structures algorithms designing performance proficiency programming java c python willingness learn adopt languages necessary database mssql one key value document databases mongodb dynamo db casandra exposure containers microservices distributed orchestrators cloud computing comfortable core concepts techniques e g concurrency memory management enjoys trees hash maps queues analytics plus good sense user interaction usability intuitive seamless end communications subject matter expert extract critical potentially lead agile methodology
118," Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree  preferred  in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.  Understand Hadoop cluster administration concepts.  3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.  Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.  Must have experience with batch and real-time data pipelines.  Must have experience as a Hadoop Technical Lead / Architect Must have experience with design, development and deployment in the specified technologies.  Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.  Writing complex SQL queries, extracting and importing large amounts of data.  Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams.  Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.  Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.  Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.  Excellent written and oral communication skills.  Adept and presenting complex topics, influencing and executing with timely / actionable follow-through   Lead a development team of data engineers Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases  Hbase  and EMR  Hadoop  Responsible for design, development, testing oversight and implementation Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks Coordinate with data engineers and API developers to drive program delivery.  Drive technical development and application standards across enterprise data lake Benchmark and debug critical issues with algorithms and software as they arise.  Lead and assist with the technical design and implementation of the Big Data cluster in various environments.  Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.  Perform other duties and/or special projects as assigned   Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree  preferred  in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.  Understand Hadoop cluster administration concepts.  3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.  Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.  Must have experience with batch and real-time data pipelines.  Must have experience as a Hadoop Technical Lead / Architect Must have experience with design, development and deployment in the specified technologies.  Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.  Writing complex SQL queries, extracting and importing large amounts of data.  Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams.  Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.  Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.  Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.  Excellent written and oral communication skills.  Adept and presenting complex topics, influencing and executing with timely / actionable follow-through",must bachelors degree computer science engineering plus industry masters degree computer science engineering plus industry big data technologies understand hadoop cluster administration concepts handson scale big data environments hortonworks cloudera must java spark mapreduce rdbms hive pig hbase kafka scala python linuxunix technologies must batch realtime data pipelines must hadoop technical lead architect must design development deployment specified technologies must oops concepts design principles patterns scala java python writing complex sql queries extracting importing amounts data must willing fastpaced shore shore distributed agile teams must technical background handson building enterprisewide data warehouse solutions must develop maintain collaborative relationships levels across business stakeholders must prioritize multiple tasks deal urgent requests fastpaced demanding written oral communication adept presenting complex topics influencing executing timely actionable followthrough lead development team data engineers implement big data enterprise data lake bi analytics hive llap spark kafka sqoop hive sqoop nosql databases hbase emr hadoop responsible design development testing oversight implementation works closely program manager scrum master architects convey technical impacts development timeline risks coordinate data engineers api developers drive program delivery drive technical development application standards across enterprise data lake benchmark debug critical issues algorithms software arise lead assist technical design implementation big data cluster various environments guidementor development team example create custom common utilitieslibraries reused multiple big data development efforts perform duties andor special projects assigned must bachelors degree computer science engineering plus industry masters degree computer science engineering plus industry big data technologies understand hadoop cluster administration concepts handson scale big data environments hortonworks cloudera must java spark mapreduce rdbms hive pig hbase kafka scala python linuxunix technologies must batch realtime data pipelines must hadoop technical lead architect must design development deployment specified technologies must oops concepts design principles patterns scala java python writing complex sql queries extracting importing amounts data must willing fastpaced shore shore distributed agile teams must technical background handson building enterprisewide data warehouse solutions must develop maintain collaborative relationships levels across business stakeholders must prioritize multiple tasks deal urgent requests fastpaced demanding written oral communication adept presenting complex topics influencing executing timely actionable followthrough,must bachelors degree computer science engineering plus industry masters big data technologies understand hadoop cluster administration concepts handson scale environments hortonworks cloudera java spark mapreduce rdbms hive pig hbase kafka scala python linuxunix batch realtime pipelines technical lead architect design development deployment specified oops principles patterns writing complex sql queries extracting importing amounts willing fastpaced shore distributed agile teams background building enterprisewide warehouse solutions develop maintain collaborative relationships levels across business stakeholders prioritize multiple tasks deal urgent requests demanding written oral communication adept presenting topics influencing executing timely actionable followthrough team engineers implement enterprise lake bi analytics llap sqoop nosql databases emr responsible testing oversight implementation works closely program manager scrum master architects convey impacts timeline risks coordinate api developers drive delivery application standards benchmark debug critical issues algorithms software arise assist various guidementor example create custom common utilitieslibraries reused efforts perform duties andor special projects assigned
119,"   Act as a trusted technical advisor to customers and solve complex Big Data challenges.  Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.  Travel up to 30% of the time.  Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.   ",act trusted technical advisor customers solve complex big data challenges create deliver best practices recommendations tutorials blog articles sample code technical presentations adapting different levels key business technical stakeholders travel time communicate effectively via video conferencing meetings technical reviews onsite delivery activities,act trusted technical advisor customers solve complex big data challenges create deliver best practices recommendations tutorials blog articles sample code presentations adapting different levels key business stakeholders travel time communicate effectively via video conferencing meetings reviews onsite delivery activities
120,"  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks.  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.  Craft analytics tools that utilize the data pipeline to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Work with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.  Orchestrate large, complex data sets that meet functional / non-functional business requirements.  Seek out, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.  Partner with data and analytics talent to strive for greater functionality in our data systems.   Interested candidates must submit an application and resume/CV online to be considered Must be 18 years of age or older Must be willing to submit to a background investigation; any offer of employment is conditioned upon the successful completion of a background investigation Must have unrestricted work authorization to work in the United States.  For U. S.  employment opportunities, Gallagher hires U. S.  citizens, permanent residents, asylees, refugees, and temporary residents.  Temporary residence does not include those with non-immigrant work authorization  F, J, H or L visas , such as students in practical training status.  Exceptions to these requirements will be determined based on shortage of qualified candidates with a particular skill.  Gallagher will require proof of work authorization Must be willing to execute Gallagher's Employee Agreement or Confidentiality and Non-Disclosure Agreement, which require, among other things, post-employment obligations relating to non-solicitation, confidentiality and non-disclosure",build infrastructure optimal extraction transformation loading data wide variety data sources microsoft azure technologies azure data factory databricks create data tools analytics data scientist team members assist building optimizing product innovative industry leader craft analytics tools utilize data pipeline deliver actionable insights customer acquisition operational efficiency key business performance metrics partners executive product data design teams assist datarelated technical issues support data infrastructure needs orchestrate complex data sets meet functional nonfunctional business seek design implement internal process improvements automating manual processes optimizing data delivery redesigning infrastructure greater scalability partner data analytics talent strive greater functionality data systems interested candidates must submit application resumecv online considered must age older must willing submit background investigation offer employment conditioned upon successful completion background investigation must unrestricted authorization united states u employment opportunities gallagher hires u citizens permanent residents asylees refugees temporary residents temporary residence include nonimmigrant authorization f j h l visas students practical training status exceptions determined based shortage qualified candidates particular skill gallagher require proof authorization must willing execute gallaghers employee agreement confidentiality nondisclosure agreement require among things postemployment obligations relating nonsolicitation confidentiality nondisclosure,build infrastructure optimal extraction transformation loading data wide variety sources microsoft azure technologies factory databricks create tools analytics scientist team members assist building optimizing product innovative industry leader craft utilize pipeline deliver actionable insights customer acquisition operational efficiency key business performance metrics partners executive design teams datarelated technical issues support needs orchestrate complex sets meet functional nonfunctional seek implement internal process improvements automating manual processes delivery redesigning greater scalability partner talent strive functionality systems interested candidates must submit application resumecv online considered age older willing background investigation offer employment conditioned upon successful completion unrestricted authorization united states u opportunities gallagher hires citizens permanent residents asylees refugees temporary residence include nonimmigrant f j h l visas students practical training status exceptions determined based shortage qualified particular skill require proof execute gallaghers employee agreement confidentiality nondisclosure among things postemployment obligations relating nonsolicitation
121,"  Bachelor's Degree in computer science or equivalent experience required.  2+ years of experience in the design and development of data pipelines and tasks.  Good understanding of data warehousing concepts and dimensional data modeling.  Hands-on experience with troubleshooting performance issues and fine tuning SQL queries.  Experience in Python including in modules/libraries such as pandas, numpy, Flask, scikit-learn, and sci-py.  Proven experience extracting data from structured data sources  SQL, Excel, CSV files, Couchbase  and unstructured data sources  Splunk, log files  both on-premise and in the cloud.  Experience consuming data from web services, REST and SOAP, HTML, XML and JSON.  Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.  Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.  Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.    Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.  Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.  Perform unit testing, system integration testing and assist with user acceptance testing.  Adapt data components to accommodate changes in source data and new business requirements.  Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.  Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.  Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.    ",bachelors degree computer science design development data pipelines tasks good understanding data warehousing concepts dimensional data modeling handson troubleshooting performance issues fine tuning sql queries python moduleslibraries pandas numpy flask scikitlearn scipy proven extracting data structured data sources sql excel csv files couchbase unstructured data sources splunk log files onpremise cloud consuming data web services rest soap html xml json version control systems git bitbucket svn team foundation microsoft sql server ssis ssrs power bi azure familiar data warehouse platforms like aws redshift aws data pipeline design develop deploy optimal extraction transformation loading data various gohealth external data sources monitor execute report data pipeline tasks appropriate teams take corrective action quickly case issues perform unit testing integration testing assist user acceptance testing adapt data components accommodate changes source data business create maintain documentation technical detail design operational support maintenance procedures data pipeline tasks data compliance development architecture reporting regulatory standards throughout entire data pipeline collaborate rest data engineering team subject matter experts department leaders understand analyze build deliver datarelated processes andor reports,bachelors degree computer science design development data pipelines tasks good understanding warehousing concepts dimensional modeling handson troubleshooting performance issues fine tuning sql queries python moduleslibraries pandas numpy flask scikitlearn scipy proven extracting structured sources excel csv files couchbase unstructured splunk log onpremise cloud consuming web services rest soap html xml json version control systems git bitbucket svn team foundation microsoft server ssis ssrs power bi azure familiar warehouse platforms like aws redshift pipeline develop deploy optimal extraction transformation loading various gohealth external monitor execute report appropriate teams take corrective action quickly case perform unit testing integration assist user acceptance adapt components accommodate changes source business create maintain documentation technical detail operational support maintenance procedures compliance architecture reporting regulatory standards throughout entire collaborate engineering subject matter experts department leaders understand analyze build deliver datarelated processes andor reports
122,"    BS degree in a computer discipline or relevant certification 3+ years of database management with expertise in backup, recovery and replication 1+ years of managing an AWS Ecosystem with knowledge in areas of RDS, Dynamo DB, MySQL, Maria DB, and/or AWS Aurora Strong practical knowledge of SQL query performance, resolution and overall tuning Proven knowledge of identifying and setting up system and performance monitoring Understanding of the development lifecycle and service management processes including Code Promotion, Change Control and Incident/Problem Management Hands on Security and Compliance  PCI / SOX  on data infrastructure Backup/recovery, replication, cluster failover and disaster recovery Willingness to participate in On-call rotation and off hour releases Worked on DB Migrations with zero downtime Excellent oral and written communication ",bs degree computer discipline relevant certification database management expertise backup recovery replication managing aws ecosystem areas rds dynamo db mysql maria db andor aws aurora practical sql query performance resolution overall tuning proven identifying setting performance monitoring understanding development lifecycle service management processes code promotion change control incidentproblem management hands security compliance pci sox data infrastructure backuprecovery replication cluster failover disaster recovery willingness participate oncall rotation hour releases worked db migrations zero downtime oral written communication,bs degree computer discipline relevant certification database management expertise backup recovery replication managing aws ecosystem areas rds dynamo db mysql maria andor aurora practical sql query performance resolution overall tuning proven identifying setting monitoring understanding development lifecycle service processes code promotion change control incidentproblem hands security compliance pci sox data infrastructure backuprecovery cluster failover disaster willingness participate oncall rotation hour releases worked migrations zero downtime oral written communication
123," 5 years of working with following technology stack  Core languages are Java and C ; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.  Growing track record of success or the groundwork to be an impactful member of the team.  Weâre looking for candidates that exhibit many of the following skills/attributes  Strong Educational Background Hands-on Engineering experience in Problem solving and debugging skillsWriting and deploying code the Linux, Windows, or cloud environmentsFamiliarity with algorithms and performance analysisWillingness to contribute to the operational responsibility of the teamâs applications Some experience with one or more of the following  Relational Databases & SQL NoSQL databases  Cassandra, Redis, DynamoDB, MongoDB Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase   ",following technology stack core languages java c restful services jquery sql server hadoop hive hbase storm spark aws services kinesis dynamodb redshift lamda sqs growing track record success groundwork impactful member team looking candidates exhibit many following skillsattributes educational background handson engineering problem solving debugging skillswriting deploying code linux windows cloud environmentsfamiliarity algorithms performance analysiswillingness contribute operational responsibility teams applications one following relational databases sql nosql databases cassandra redis dynamodb mongodb big data tools hadoop hive emr storm spark dynamodb hbase,following technology stack core languages java c restful services jquery sql server hadoop hive hbase storm spark aws kinesis dynamodb redshift lamda sqs growing track record success groundwork impactful member team looking candidates exhibit many skillsattributes educational background handson engineering problem solving debugging skillswriting deploying code linux windows cloud environmentsfamiliarity algorithms performance analysiswillingness contribute operational responsibility teams applications one relational databases nosql cassandra redis mongodb big data tools emr
124,7+ years of experience in data analysis. 7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems.  Preferably in a custom software development environment. 7+ years of SQL development skills within a multi-tier environment are required.     ,data analysis integrating data multiple formats volume data exchange transaction processing systems preferably custom software development sql development within multitier,data analysis integrating multiple formats volume exchange transaction processing systems preferably custom software development sql within multitier
125," 5-8 years of Python or Java/J2EE development experience 3+ years of demonstrated technical proficiency with Hadoop and big data projects 5-8 years of demonstrated experience and success in data modeling Fluent in writing shell scripts [bash, korn] Writing high-performance, reliable and maintainable code Ability to write MapReduce jobs Knowledge of database structures, theories, principles, and practices Understand how to develop code in an environment secured using a local KDC and OpenLDAP Familiarity with and implementation knowledge of loading data using Sqoop Knowledge and ability to implement workflow/schedulers within Oozie Experience working with AWS components [EC2, S3, SNS, SQS] Analytical and problem-solving skills, applied to Big Data domain Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark Aptitude in multi-threading and concurrency concepts M. S.  in Computer Science or Engineering   Translate complex functional and technical requirements into detailed design Hadoop technical development and implementation Loading from disparate data sets by leveraging various big data technology e. g.  Kafka Pre-processing using Hive, Impala, Spark, and Pig Design and implement data modeling Maintain security and data privacy in an environment secured using Kerberos and LDAP High-speed querying using in-memory technologies such as Spark Following and contributing best engineering practice for source control, release management, deployment etc Production support, job scheduling/monitoring, ETL data quality, data freshness reporting  ",python javajee development demonstrated technical proficiency hadoop big data projects demonstrated success data modeling fluent writing shell scripts bash korn writing highperformance reliable maintainable code write mapreduce jobs database structures theories principles practices understand develop code secured local kdc openldap familiarity implementation loading data sqoop implement workflowschedulers within oozie aws components ec sns sqs analytical problemsolving applied big data domain proven understanding hands hadoop hive pig impala spark aptitude multithreading concurrency concepts computer science engineering translate complex functional technical detailed design hadoop technical development implementation loading disparate data sets leveraging various big data technology e g kafka preprocessing hive impala spark pig design implement data modeling maintain security data privacy secured kerberos ldap highspeed querying inmemory technologies spark following contributing best engineering practice source control release management deployment production support job schedulingmonitoring etl data data freshness reporting,python javajee development demonstrated technical proficiency hadoop big data projects success modeling fluent writing shell scripts bash korn highperformance reliable maintainable code write mapreduce jobs database structures theories principles practices understand develop secured local kdc openldap familiarity implementation loading sqoop implement workflowschedulers within oozie aws components ec sns sqs analytical problemsolving applied domain proven understanding hands hive pig impala spark aptitude multithreading concurrency concepts computer science engineering translate complex functional detailed design disparate sets leveraging various technology e g kafka preprocessing maintain security privacy kerberos ldap highspeed querying inmemory technologies following contributing best practice source control release management deployment production support job schedulingmonitoring etl freshness reporting
126," Experience on client-facing projects, including working in close-knit teams Experience and interest in Big Data technologies  Hadoop / Spark / NoSQL DBs  Experience or familiarity with real-time ingestion and streaming frameworks is a plus Experience and desire to work with open source and branded open source frameworks Experience working on projects within the cloud ideally AWS or Azure Experience with NLP, Machine Learning, etc.  is a plus Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time Strong development background with experience in at least two scripting, object oriented or functional programming language, etc.  SQL, Python, Java, Scala, C , R Data Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.  A deep personal motivation to always produce outstanding work for your clients and colleagues Excel in team collaboration and working with others from diverse skill-sets and backgrounds    ",clientfacing projects closeknit teams interest big data technologies hadoop spark nosql dbs familiarity realtime ingestion streaming frameworks plus desire open source branded open source frameworks projects within cloud ideally aws azure nlp machine plus lively projects consulting setting often different multiple projects time development background least two scripting object oriented functional programming language sql python java scala c r data warehousing building operational etl data pipelines across number sources constructing relational dimensional data models interpersonal interacting clients clear timely professional manner deep personal motivation always produce outstanding clients colleagues excel team collaboration others diverse skillsets backgrounds,clientfacing projects closeknit teams interest big data technologies hadoop spark nosql dbs familiarity realtime ingestion streaming frameworks plus desire open source branded within cloud ideally aws azure nlp machine lively consulting setting often different multiple time development background least two scripting object oriented functional programming language sql python java scala c r warehousing building operational etl pipelines across number sources constructing relational dimensional models interpersonal interacting clients clear timely professional manner deep personal motivation always produce outstanding colleagues excel team collaboration others diverse skillsets backgrounds
127,"Bachelorâs degree; STEM background preferred5+ yearsâ experience developing scalable big data solutions in cloud and on premise environmentsExperience with big data technologies  Spark, Hadoop ; data lakesExperience developing data processing/ETL/ELT solutions using e. g.  Scala, Python, Java, SQLExperience deploying statistical/machine learning models to productionExperience building cloud-based data solutions; Azure experience preferred  Develop, construct, test and maintain architectures for scalable, timely and efficient big data processingBuild data pipelines in lambda architectures for structured/unstructured, streaming/batch dataDevelop solutions for data modeling, mining and quality monitoringDevelop solutions for deploying models to productionIdentify and make recommendations on ways to improve data quality, reliability and efficiencyDiscover opportunities for data acquisitionMaintain awareness of ongoing developments in big data and data engineering technologies both in the industry in general and within the enterprise  ",bachelors degree stem background developing scalable big data solutions cloud premise environmentsexperience big data technologies spark hadoop data lakesexperience developing data processingetlelt solutions e g scala python java sqlexperience deploying statisticalmachine models productionexperience building cloudbased data solutions azure develop construct test maintain architectures scalable timely efficient big data processingbuild data pipelines lambda architectures structuredunstructured streamingbatch datadevelop solutions data modeling mining monitoringdevelop solutions deploying models productionidentify make recommendations ways improve data reliability efficiencydiscover opportunities data acquisitionmaintain awareness ongoing developments big data data engineering technologies industry general within enterprise,bachelors degree stem background developing scalable big data solutions cloud premise environmentsexperience technologies spark hadoop lakesexperience processingetlelt e g scala python java sqlexperience deploying statisticalmachine models productionexperience building cloudbased azure develop construct test maintain architectures timely efficient processingbuild pipelines lambda structuredunstructured streamingbatch datadevelop modeling mining monitoringdevelop productionidentify make recommendations ways improve reliability efficiencydiscover opportunities acquisitionmaintain awareness ongoing developments engineering industry general within enterprise
128,"4 year College degree in Computer Science, Information Technology or equivalent demonstrated experience.  Masters degree preferred.  Strong SQL development skills using databases like oracle and Vertica.  Experience in cloud databases like Snowflake or Redshift is a plus Experience with AWS technologies such as EC2, S3 and other basic AWS technologies Certification âpreferably AWS Certified Big Data or any other cloud data platforms, big data platforms 4+ years of experience in the data and analytics space Solid Programing experience in Python - needs to be an expert in this 4/5 level.  Experience with workload automation tools such as Airflow, Autosys.  4+ years experience developing and implementing enterprise-level data solutions utilizing Python , Java, Spark, and Scala, Airflow , Hive 3+ years in key aspects of software engineering such as parallel data processing, data flows, REST APIs, JSON, XML, and micro service architectures.  6+ years of RDBMS concepts with Strong Data analysis and SQL experience 3+ years of Linux OS command line tools and bash scripting proficiency Cloud data warehouse experience - Snowflake is a plus Must be an EXPERT with ETL Development on Unix Servers.  Must have demonstratable working knowledge of modern information and delivery practices for on-premises and cloud environments.  Must have demonstratable experience delivering robust information delivery and management solutions as part of a fast paced data platform program.  MUST BE AN EXPERT applying business rules/logic using SQL scripts.  Must have working knowledge of various data modeling techniques  3NF, denormalized, STAR Schema .  Position requires a self-starter, capable of quickly turning around vaguely defined projects with minimal supervision or assistance.  Ability to conduct analysis of source data sets to achieve target data set objectives.  STRONG VERBAL/WRITTEN COMMUNICATION is a MUST.  Interacting with business community/users is a core requirement of the role.  Must be able to provide specialized support for our Legacy platforms, as well as the new Cloud Based Data Platform.     ",year college degree computer science information technology demonstrated masters degree sql development databases like oracle vertica cloud databases like snowflake redshift plus aws technologies ec basic aws technologies certification preferably aws certified big data cloud data platforms big data platforms data analytics space solid programing python needs expert level workload automation tools airflow autosys developing implementing enterpriselevel data solutions utilizing python java spark scala airflow hive key aspects software engineering parallel data processing data flows rest apis json xml micro service architectures rdbms concepts data analysis sql linux os command line tools bash scripting proficiency cloud data warehouse snowflake plus must expert etl development unix servers must demonstratable modern information delivery practices onpremises cloud environments must demonstratable delivering robust information delivery management solutions part fast paced data platform program must expert applying business ruleslogic sql scripts must various data modeling techniques nf denormalized star schema position requires selfstarter capable quickly turning around vaguely defined projects minimal supervision assistance conduct analysis source data sets achieve target data set objectives verbalwritten communication must interacting business communityusers core requirement role must able specialized support legacy platforms well cloud based data platform,year college degree computer science information technology demonstrated masters sql development databases like oracle vertica cloud snowflake redshift plus aws technologies ec basic certification preferably certified big data platforms analytics space solid programing python needs expert level workload automation tools airflow autosys developing implementing enterpriselevel solutions utilizing java spark scala hive key aspects software engineering parallel processing flows rest apis json xml micro service architectures rdbms concepts analysis linux os command line bash scripting proficiency warehouse must etl unix servers demonstratable modern delivery practices onpremises environments delivering robust management part fast paced platform program applying business ruleslogic scripts various modeling techniques nf denormalized star schema position requires selfstarter capable quickly turning around vaguely defined projects minimal supervision assistance conduct source sets achieve target set objectives verbalwritten communication interacting communityusers core requirement role able specialized support legacy well based
129,"     Based on understanding of system upstream & downstream, provide feedback and inputs on gaps in requirements and technical feasibility of requirements.  ",based understanding upstream downstream feedback inputs gaps technical feasibility,based understanding upstream downstream feedback inputs gaps technical feasibility
130,"   Participate in the design and development of a big data analytics application Design, support and continuously enhance the project code base, continuous integration pipeline, etc.  Write complex ETL processes and frameworks for analytics and data management Developing new processes and models Implement large-scale near real-time streaming data processing pipelines Work with a team of industry experts on cutting-edge big data technologies to develop solutions for deployment at massive scale   5+ years of experience in Hadoop ecosystem 3+ years of hands-on experience in architecting, designing, and implementing data ingestion pipes for batch, real-time, and streams.  3+ years of hands-on experience with a proven track record in building data lakes on Public Clouds  preferably GCP and Azure  HDInsight   3+ years of hands-on experience in Bigdata tools such as Sqoop, Hive, Spark, Scala, HBase, MapReduce etc.  2+ years of experience in Python and Scala.  Ready to work onsite I the USA up to 3 months.  Experience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred.  BA/BS required; preferably in Computer Science, Data Analytics, Data Science or Operations Research.  Highly analytical, motivated, decisive thought leader with solid critical thinking able to quickly connect technical and business âdotsâ.  Has strong communication and organizational skills and has the ability to deal with ambiguity while juggling multiple priorities and projects at the same time.  Able to understand statistical solutions and execute similar activities. ",participate design development big data analytics application design support continuously enhance project code base continuous integration pipeline write complex etl processes frameworks analytics data management developing processes models implement largescale near realtime streaming data processing pipelines team industry experts cuttingedge big data technologies develop solutions deployment massive scale hadoop ecosystem handson architecting designing implementing data ingestion pipes batch realtime streams handson proven track record building data lakes public clouds preferably gcp azure hdinsight handson bigdata tools sqoop hive spark scala hbase mapreduce python scala ready onsite usa months data wrangling advanced analytic modeling aiml capabilities babs preferably computer science data analytics data science operations research highly analytical motivated decisive thought leader solid critical thinking able quickly connect technical business dots communication organizational deal ambiguity juggling multiple priorities projects time able understand statistical solutions execute similar activities,participate design development big data analytics application support continuously enhance project code base continuous integration pipeline write complex etl processes frameworks management developing models implement largescale near realtime streaming processing pipelines team industry experts cuttingedge technologies develop solutions deployment massive scale hadoop ecosystem handson architecting designing implementing ingestion pipes batch streams proven track record building lakes public clouds preferably gcp azure hdinsight bigdata tools sqoop hive spark scala hbase mapreduce python ready onsite usa months wrangling advanced analytic modeling aiml capabilities babs computer science operations research highly analytical motivated decisive thought leader solid critical thinking able quickly connect technical business dots communication organizational deal ambiguity juggling multiple priorities projects time understand statistical execute similar activities
131,"Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming Experience in Big Data technologies  Hadoop, Spark, NiFi, Kafka  Ability to demonstrate experience in distributed UNIX environments.  Experience in writing shell scripts Ability to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio.  Ability to multi-task and work under pressure.  Ability to be careful and thorough with detail.  Ability to work both independently and in a collaborative environment.  Ability to analyze information and use logic to address work related issues and problems.  Experience in the Healthcare Industry is a plus.  Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming Experience in Big Data technologies  Hadoop, Spark, NiFi, Kafka  Ability to demonstrate experience in distributed UNIX environments.  Experience in writing shell scripts Ability to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio.  Ability to multi-task and work under pressure.  Ability to be careful and thorough with detail.  Ability to work both independently and in a collaborative environment.  Ability to analyze information and use logic to address work related issues and problems.  Experience in the Healthcare Industry is a plus.  Design, implement, and test major subsystems of AWS or AZURE cloud platform and core service offerings using the Scrum agile framework Develop and follow best practices relative to design, implementation, and testing Prototype new ideas or technologies to prove efficacy and usefulness in production Build a service structure on AWS or AZURE capable of being deployed and scaled to run a variety of platform components dynamically Build a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS or AZURE cloud Construct a state-of-the-art data lake using Hadoop or Cassandra, Apache Spark, NiFi, and Kafka Design & develop data pipelines for batch & streaming data sets using ETL and Data Integration tools, open source, AWS & Azure tech stack Mentor junior team members The knowledge typically acquired during the course of attaining a Bachelorâs degree in Computer Science, Mathematics, or related discipline is required.  A combination of education and experience may be used in lieu of a diploma.  ",commandlevel java python programming fundamentals computer science data structures programming big data technologies hadoop spark nifi kafka demonstrate distributed unix environments writing shell scripts demonstrate proficiency microsoft access excel word powerpoint visio multitask pressure careful thorough detail independently collaborative analyze information use logic address issues problems healthcare industry plus commandlevel java python programming fundamentals computer science data structures programming big data technologies hadoop spark nifi kafka demonstrate distributed unix environments writing shell scripts demonstrate proficiency microsoft access excel word powerpoint visio multitask pressure careful thorough detail independently collaborative analyze information use logic address issues problems healthcare industry plus design implement test major subsystems aws azure cloud platform core service offerings scrum agile framework develop follow best practices relative design implementation testing prototype ideas technologies prove efficacy usefulness production build service structure aws azure capable deployed scaled run variety platform components dynamically build nextgeneration tools platform creating managing deploying multichannel outreach campaigns aws azure cloud construct stateoftheart data lake hadoop cassandra apache spark nifi kafka design develop data pipelines batch streaming data sets etl data integration tools open source aws azure tech stack mentor junior team members typically acquired course attaining bachelors degree computer science mathematics discipline combination education may used lieu diploma,commandlevel java python programming fundamentals computer science data structures big technologies hadoop spark nifi kafka demonstrate distributed unix environments writing shell scripts proficiency microsoft access excel word powerpoint visio multitask pressure careful thorough detail independently collaborative analyze information use logic address issues problems healthcare industry plus design implement test major subsystems aws azure cloud platform core service offerings scrum agile framework develop follow best practices relative implementation testing prototype ideas prove efficacy usefulness production build structure capable deployed scaled run variety components dynamically nextgeneration tools creating managing deploying multichannel outreach campaigns construct stateoftheart lake cassandra apache pipelines batch streaming sets etl integration open source tech stack mentor junior team members typically acquired course attaining bachelors degree mathematics discipline combination education may used lieu diploma
132,"Financial Services industry experiencePrevious work experience in Cyber Security field is a plus. Experience with the Hadoop eco-system  HDFS, Spark Experience with cloud based big data platforms such as AWS or Google a plus.     ",financial services industry experienceprevious cyber security plus hadoop ecosystem hdfs spark cloud based big data platforms aws google plus,financial services industry experienceprevious cyber security plus hadoop ecosystem hdfs spark cloud based big data platforms aws google
133," Undergraduate degree in Computer Science, Mathematics, Engineering  or related field  or equivalent experience preferred 5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function Ability to work with broad parameters in complex situations Experience in developing, managing, and manipulating large, complex datasets Expert high-level coding skills such as SQL and Python and/or other scripting languages UNIX  required.  Scala is a plus.    Responsible for design, prototyping and delivery of software solutions within the big data eco-system Leading projects and/or serving as analytics SME to provide new or enhanced data to the business Improving data governance and quality increasing the reliability of our data Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise   ",undergraduate degree computer science mathematics engineering data integration etl andor business intelligenceanalytics function broad parameters complex situations developing managing manipulating complex datasets expert highlevel coding sql python andor scripting languages unix scala plus responsible design prototyping delivery software solutions within big data ecosystem leading projects andor serving analytics sme enhanced data business improving data governance increasing reliability data influencing creation single trusted source key claims business data shared across enterprise,undergraduate degree computer science mathematics engineering data integration etl andor business intelligenceanalytics function broad parameters complex situations developing managing manipulating datasets expert highlevel coding sql python scripting languages unix scala plus responsible design prototyping delivery software solutions within big ecosystem leading projects serving analytics sme enhanced improving governance increasing reliability influencing creation single trusted source key claims shared across enterprise
134,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform.  Multi-cloud experience a plus.    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",minimum previous consulting client service delivery google gcp devops gcp platform multicloud plus proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,minimum previous consulting client service delivery google gcp devops platform multicloud plus proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
135,"Bachelors/4 Year Degree.  7-10 years of experience.  Expert knowledge of Big Data technologies including but not limited to Python and/or Databricks.  Strong Analytical and problem-solving skills.  Knowledgeable in cloud platforms  preferable AWS  both traditional EC2 and serverless ambda , micro-services architecture, CI/CD solutions  including Docker , DevOps principles, message queue system.  Proficiency in API security frameworks, token management and user access control including OAuth, JWT, etc.  Solid foundation and understanding of relational and NoSQL database principles.    Development of custom APIs for inbound/outbound data integrations, Spark jobs for ETL development and data integrations to internal and external systems.  Participating in designing new data applications.  Code deployments, Dev Ops, and Reviews.   ",bachelors year degree expert big data technologies limited python andor databricks analytical problemsolving knowledgeable cloud platforms preferable aws traditional ec serverless ambda microservices architecture cicd solutions docker devops principles message queue proficiency api security frameworks token management user access control oauth jwt solid foundation understanding relational nosql database principles development custom apis inboundoutbound data integrations spark jobs etl development data integrations internal external systems participating designing data applications code deployments dev ops reviews,bachelors year degree expert big data technologies limited python andor databricks analytical problemsolving knowledgeable cloud platforms preferable aws traditional ec serverless ambda microservices architecture cicd solutions docker devops principles message queue proficiency api security frameworks token management user access control oauth jwt solid foundation understanding relational nosql database development custom apis inboundoutbound integrations spark jobs etl internal external systems participating designing applications code deployments dev ops reviews
136,"  Design and develop data models, ETL mappings and associated database objects for analytical solutions using ERWin, ETL tools like Informatica, Oracle DB.  Create/review technical documentation for all new and modified data model.  Review solution design and ensure that the defined EDW standards and framework are followed.  Review and validate logical and physical design to ensure alignment with the defined solution architecture.  Ensure quality assurance plan and cases are comprehensive to validate the solution thoroughly.  Experience developing Packages, Procedures and Functions with PL/SQL to support ETL processes.  Experience in performance analysis and optimization of reports, dashboards, ETL and other components.  Tableau or equivalent reporting platformâs Infrastructure experience, including tuning performance issues, monitoring applications, analyzing logs and performing system operations   ",design develop data models etl mappings associated database objects analytical solutions erwin etl tools like informatica oracle db createreview technical documentation modified data model review solution design defined edw standards framework followed review validate logical physical design alignment defined solution architecture assurance plan cases comprehensive validate solution thoroughly developing packages procedures functions plsql support etl processes performance analysis optimization reports dashboards etl components tableau reporting platforms infrastructure tuning performance issues monitoring applications analyzing logs performing operations,design develop data models etl mappings associated database objects analytical solutions erwin tools like informatica oracle db createreview technical documentation modified model review solution defined edw standards framework followed validate logical physical alignment architecture assurance plan cases comprehensive thoroughly developing packages procedures functions plsql support processes performance analysis optimization reports dashboards components tableau reporting platforms infrastructure tuning issues monitoring applications analyzing logs performing operations
137,"     3+ years of data engineering experience Robust experience with Python, Spark, and Jenkins Experience writing and executing complex SQL queries Experience building data pipelines and ETL design  implementation and maintenance  Experience with AWS or other cloud provider Scrum/Agile software development process. ",data engineering robust python spark jenkins writing executing complex sql queries building data pipelines etl design implementation maintenance aws cloud provider scrumagile software development process,data engineering robust python spark jenkins writing executing complex sql queries building pipelines etl design implementation maintenance aws cloud provider scrumagile software development process
138,"  Hands-on experience with the Hadoop eco-system - HDFS, MapReduce, HBase, Hive, Impala, Spark, Kafka Experience in implementing Hadoop Data Lakes - Data storage, partitioning, splitting, file types  Parquet, Avro, ORC  for specific use cases etc.  Experience with Query languages â SQL, Hive, Impala, Drill etc.  Experience with NoSQL databases â MapR-DB, HBase, MongoDB, Cassandra etc.  Experience in agile scrum  development methodology Exposure to Data ingestion frameworks such as Kafka, Sqoop, Storm, Nifi, Spring Cloud, etc.  Experience with building stream-processing systems using solutions such as Kafka, MapR-Streams, Spark-Streaming etc.  Experience with development/automation skills.  Must be very comfortable with reading and writing Scala, Python or Java code Experience with one of the Hadoop open source distributions - MapR and Cloudera Bachelor or masterâs Degree in engineering in Computer Science or Information Technology  Partner with data analyst, product owners and data scientists, to better understand requirements, solution designs, finding bottlenecks, resolutions, etc.  Support/Enhance data pipelines and ETL using heterogeneous sources Transform data using data mapping and data processing capabilities like Spark, Spark SQL, HiveQL etc.  Expands and grows data platform capabilities to solve new data problems and challenges Ability to dynamically adapt to conventional big-data frameworks and tools with the use-cases required by the project  10+ years Enterprise Development 5+ years of experience with the Hadoop and Big Data technologies 2+ Years Design with Big Data Strategies. ",handson hadoop ecosystem hdfs mapreduce hbase hive impala spark kafka implementing hadoop data lakes data storage partitioning splitting file types parquet avro orc specific use cases query languages sql hive impala drill nosql databases maprdb hbase mongodb cassandra agile scrum development methodology exposure data ingestion frameworks kafka sqoop storm nifi spring cloud building streamprocessing systems solutions kafka maprstreams sparkstreaming developmentautomation must comfortable reading writing scala python java code one hadoop open source distributions mapr cloudera bachelor masters degree engineering computer science information technology partner data analyst product owners data scientists better understand solution designs finding bottlenecks resolutions supportenhance data pipelines etl heterogeneous sources transform data data mapping data processing capabilities like spark spark sql hiveql expands grows data platform capabilities solve data problems challenges dynamically adapt conventional bigdata frameworks tools usecases project enterprise development hadoop big data technologies design big data strategies,handson hadoop ecosystem hdfs mapreduce hbase hive impala spark kafka implementing data lakes storage partitioning splitting file types parquet avro orc specific use cases query languages sql drill nosql databases maprdb mongodb cassandra agile scrum development methodology exposure ingestion frameworks sqoop storm nifi spring cloud building streamprocessing systems solutions maprstreams sparkstreaming developmentautomation must comfortable reading writing scala python java code one open source distributions mapr cloudera bachelor masters degree engineering computer science information technology partner analyst product owners scientists better understand solution designs finding bottlenecks resolutions supportenhance pipelines etl heterogeneous sources transform mapping processing capabilities like hiveql expands grows platform solve problems challenges dynamically adapt conventional bigdata tools usecases project enterprise big technologies design strategies
139," Extensive experience in all phases of Software development life cycle  SDLC  is a must. Excellent skills in analyzing system architecture usage, defining and implementing procedures. Hands on experience in programming and implementation of Java, Scala and Python codes with strong knowledge in Object Oriented Concepts.    ",extensive phases software development life cycle sdlc must analyzing architecture usage defining implementing procedures hands programming implementation java scala python codes object oriented concepts,extensive phases software development life cycle sdlc must analyzing architecture usage defining implementing procedures hands programming implementation java scala python codes object oriented concepts
140," Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases and CDC processes Experience building and optimizing âbig dataâ data pipelines, architectures and data sets Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Strong analytical skills related to working with unstructured datasets Build processes supporting data transformation, data structures, metadata, dependency and workload management A successful history of manipulating, processing and extracting value from large disconnected datasets Working knowledge of message queuing, stream processing, and highly scalable âbig dataâ data stores Strong project management and organizational skills Experience supporting and working with cross-functional teams in a dynamic environment 5+ years of experience in a Data Engineer role Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field Experience using the following software/tools  Spark, Kafka, Kinesis, etc.  Relational SQL and NoSQL databases, including Postgres, MSSQL, Redis Data pipeline and workflow management tools  Airflow, AWS Glue, Step functions etc.  AWS cloud services  EC2, EMR, RDS, Redshift, Athena, Lambda Stream-processing systems  Storm, Spark-Streaming, etc.  Languages  Python, Java, Golang   Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of structured and unstructured data sources using âbig dataâ technologies preferably using AWS services Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs Keep our data separated and secure through multiple AWS regions Create data tools for analytics and data science team members and assist them in building and optimizing our product into an innovative industry leader Assemble large, complex data sets that meet functional / non-functional business requirements Create and maintain optimal data pipeline architecture Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Work with data and analytics experts to strive for greater functionality in our data systems   ",advanced sql relational databases query authoring sql well familiarity variety databases cdc processes building optimizing big data data pipelines architectures data sets performing root cause analysis internal external data processes answer specific business questions identify opportunities improvement analytical unstructured datasets build processes supporting data transformation data structures metadata dependency workload management successful history manipulating processing extracting value disconnected datasets message queuing stream processing highly scalable big data data stores project management organizational supporting crossfunctional teams dynamic data engineer role graduate degree computer science statistics informatics information systems another quantitative following softwaretools spark kafka kinesis relational sql nosql databases postgres mssql redis data pipeline workflow management tools airflow aws glue step functions aws cloud services ec emr rds redshift athena lambda streamprocessing systems storm sparkstreaming languages python java golang build infrastructure optimal extraction transformation loading data wide variety structured unstructured data sources big data technologies preferably aws services build analytics tools utilize data pipeline actionable insights customer acquisition operational efficiency key business performance metrics stakeholders product data design teams assist datarelated technical issues support data infrastructure needs keep data separated secure multiple aws regions create data tools analytics data science team members assist building optimizing product innovative industry leader assemble complex data sets meet functional nonfunctional business create maintain optimal data pipeline architecture identify design implement internal process improvements automating manual processes optimizing data delivery redesigning infrastructure greater scalability data analytics experts strive greater functionality data systems,advanced sql relational databases query authoring well familiarity variety cdc processes building optimizing big data pipelines architectures sets performing root cause analysis internal external answer specific business questions identify opportunities improvement analytical unstructured datasets build supporting transformation structures metadata dependency workload management successful history manipulating processing extracting value disconnected message queuing stream highly scalable stores project organizational crossfunctional teams dynamic engineer role graduate degree computer science statistics informatics information systems another quantitative following softwaretools spark kafka kinesis nosql postgres mssql redis pipeline workflow tools airflow aws glue step functions cloud services ec emr rds redshift athena lambda streamprocessing storm sparkstreaming languages python java golang infrastructure optimal extraction loading wide structured sources technologies preferably analytics utilize actionable insights customer acquisition operational efficiency key performance metrics stakeholders product design assist datarelated technical issues support needs keep separated secure multiple regions create team members innovative industry leader assemble complex meet functional nonfunctional maintain architecture implement process improvements automating manual delivery redesigning greater scalability experts strive functionality
141," 3+ years of experience in SQL and developing SQL server objects e. g. , store procedures, tables, triggers, views and functions.  At least 2 years of experience with Big Data technologies.  At least 2 years of coding experience in data environments.  3+ years design & implementation experience with distributed applications.  2+ years of experience in database architectures and data pipeline development.  Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.  Experience in manipulating multiple, complex and large data sources.  Experienced in Data Science, statistical models as a plus.  Experience in Designing, implementing and maintaining SQL Server databases.  Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.  Experience in SQL query tuning and optimization.  Experience working in SaaS, IaaS, and PaaS.    Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organizationResponsible for design, development and implementation of optimal solutions to integrate, store, process and analyze huge data setsRecommend and implement strategies for bi-directional synchronization between sourcing data repositories and the central normalized data repositoryBuild data pipeline frameworks to automate high-volume and real-time data deliveryBuild data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partnersWork on multiple projects/tasks simultaneously to meet project deadlines for self and others as required. Establish and maintain positive working relationships with other employeesAll other duties as assigned.   ",sql developing sql server objects e g store procedures tables triggers views functions least big data technologies least coding data environments design implementation distributed applications database architectures data pipeline development conceptual reporting visualization tools ssrs powerbi tableau business intelligence tools manipulating multiple complex data sources experienced data science statistical models plus designing implementing maintaining sql server databases designing implementing maintaining etl processes sql server ssis sql query tuning optimization saas iaas paas develop sustainable data driven solutions current gen data technologies meet needs organizationresponsible design development implementation optimal solutions integrate store process analyze huge data setsrecommend implement strategies bidirectional synchronization sourcing data repositories central normalized data repositorybuild data pipeline frameworks automate highvolume realtime data deliverybuild data apis data delivery services support critical operational analytical applications internal business operations customers partnerswork multiple projectstasks simultaneously meet project deadlines self others establish maintain positive relationships employeesall duties assigned,sql developing server objects e g store procedures tables triggers views functions least big data technologies coding environments design implementation distributed applications database architectures pipeline development conceptual reporting visualization tools ssrs powerbi tableau business intelligence manipulating multiple complex sources experienced science statistical models plus designing implementing maintaining databases etl processes ssis query tuning optimization saas iaas paas develop sustainable driven solutions current gen meet needs organizationresponsible optimal integrate process analyze huge setsrecommend implement strategies bidirectional synchronization sourcing repositories central normalized repositorybuild frameworks automate highvolume realtime deliverybuild apis delivery services support critical operational analytical internal operations customers partnerswork projectstasks simultaneously project deadlines self others establish maintain positive relationships employeesall duties assigned
142,"     Utilizing technical expertise; Creates system requirements, performs design and analysis, and coding and unit testing of complex to highly complex system functionality and/or defect correction across multiple platforms.  Identifies ideas to improve system performance and impact availability.  Resolves complex technical design issues.  Provides functional and/or technical guidance in evaluating applications systems or evaluating requests for proposals.  Coordinates changes and influences and prioritizes tasks with business or technical departments.  Analyzes and influences technical, system, and/or user requirements.  Identifies and creates solutions to improve system performance and availability.  Facilitates root cause analysis of system issues to minimize impact and future occurrences.  Creates system documentation/play book s  and serves as a lead technical reviewer and contributor in requirements, design and code reviews.  Typically serves as a resource to the business.  Develops accurate estimates on work packages and ensures the accuracy of estimates developed by less experienced internal and third-party team members.  Analyzes and designs specifications for less experienced internal and third-party team members to execute.  Acts as a technical resource throughout the development life cycle.  May also actively contribute to the technical and soft skills development of team members.  Assists team leads and management with delegation of technical work packages to cross functional and third-party team members for execution through the full development life cycle.  Keeps management appropriately informed of progress and issues.  Coordinates system application transition from development teams to maintenance and production teams, and/or constructs and implements necessary controls to assure system/application traceability. ",utilizing technical expertise creates performs design analysis coding unit testing complex highly complex functionality andor defect correction across multiple platforms identifies ideas improve performance impact availability resolves complex technical design issues provides functional andor technical guidance evaluating applications systems evaluating requests proposals coordinates changes influences prioritizes tasks business technical departments analyzes influences technical andor user identifies creates solutions improve performance availability facilitates root cause analysis issues minimize impact future occurrences creates documentationplay book serves lead technical reviewer contributor design code reviews typically serves resource business develops accurate estimates packages ensures accuracy estimates developed less experienced internal thirdparty team members analyzes designs specifications less experienced internal thirdparty team members execute acts technical resource throughout development life cycle may also actively contribute technical soft development team members assists team leads management delegation technical packages cross functional thirdparty team members execution full development life cycle keeps management appropriately informed progress issues coordinates application transition development teams maintenance production teams andor constructs implements necessary controls assure systemapplication traceability,utilizing technical expertise creates performs design analysis coding unit testing complex highly functionality andor defect correction across multiple platforms identifies ideas improve performance impact availability resolves issues provides functional guidance evaluating applications systems requests proposals coordinates changes influences prioritizes tasks business departments analyzes user solutions facilitates root cause minimize future occurrences documentationplay book serves lead reviewer contributor code reviews typically resource develops accurate estimates packages ensures accuracy developed less experienced internal thirdparty team members designs specifications execute acts throughout development life cycle may also actively contribute soft assists leads management delegation cross execution full keeps appropriately informed progress application transition teams maintenance production constructs implements necessary controls assure systemapplication traceability
143,"     Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure eg  Azure Good working experience on Cloud, Delta Lake, ETL processing.  Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc.  Working knowledge on Python and PySpark Programming.  Working with a wide range of data sources like  DB2, SAP HANA etc  and intermediate expertise in SQL and PL/SQL optional  Ability to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners.  Work in a highly agile environment Excellent communication and teamwork skills.  Knowledge on Data Governance & Security Principles ",developing deploying operating scale distributed systems commercial scale cloudbased big data infrastructure eg azure good cloud delta lake etl processing big data technologies like hdfs hadoop hive pig sqoop kafka spark python pyspark programming wide range data sources like db sap hana intermediate expertise sql plsql optional global team playing key role communicating problem context remote teams stake holders product owners highly agile communication teamwork data governance security principles,developing deploying operating scale distributed systems commercial cloudbased big data infrastructure eg azure good cloud delta lake etl processing technologies like hdfs hadoop hive pig sqoop kafka spark python pyspark programming wide range sources db sap hana intermediate expertise sql plsql optional global team playing key role communicating problem context remote teams stake holders product owners highly agile communication teamwork governance security principles
144,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
145," Commercial experience leading on client-facing projects, including working in close-knit teams 5+ years of experience and interest in Big Data technologies  Hadoop / Spark / NoSQL DBs  5+ years of experience working on projects within the cloud ideally AWS or Azure 5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent Experience with open source tools like Apache Airflow and Griffin Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma Experience building automated data quality and testing into data pipelines Experience with AI, NLP, Machine Learning, etc.  is a plus Strong development background with experience in at least two scripting, object oriented or functional programming language, etc.  SQL, Python, Java, Scala, C , R Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.  A deep personal motivation to always produce outstanding work for your clients and colleagues Excel in team collaboration and working with others from diverse skill-sets and backgrounds    ",commercial leading clientfacing projects closeknit teams interest big data technologies hadoop spark nosql dbs projects within cloud ideally aws azure streaming architectures patterns like kafka kinesis flink confluent open source tools like apache airflow griffin devops dataops patterns tools like jenkins kubernetes docker terraform data warehousing cloud products like snowflake azure dw redshift building operational etl data pipelines across number sources constructing relational dimensional data models one etlelt tools like talend matillion fivetran alooma building automated data testing data pipelines ai nlp machine plus development background least two scripting object oriented functional programming language sql python java scala c r lively projects consulting setting often different multiple projects time interpersonal interacting clients clear timely professional manner deep personal motivation always produce outstanding clients colleagues excel team collaboration others diverse skillsets backgrounds,commercial leading clientfacing projects closeknit teams interest big data technologies hadoop spark nosql dbs within cloud ideally aws azure streaming architectures patterns like kafka kinesis flink confluent open source tools apache airflow griffin devops dataops jenkins kubernetes docker terraform warehousing products snowflake dw redshift building operational etl pipelines across number sources constructing relational dimensional models one etlelt talend matillion fivetran alooma automated testing ai nlp machine plus development background least two scripting object oriented functional programming language sql python java scala c r lively consulting setting often different multiple time interpersonal interacting clients clear timely professional manner deep personal motivation always produce outstanding colleagues excel team collaboration others diverse skillsets backgrounds
146,"  Strong Database Development skills across multiple platforms Superior Interpersonal Skills  Ability to interface with a wide range of personalities and levels within Cvent and client organizations; Professional communication style Data Collection and Analysis  Proactive listening; resourceful in collecting sufficient data; Analysis of data to develop and implement best solution Initiative  Self-starter with strong sense of ownership; Tenacity in problem solving with positive outcomes; Motivated to increase capacity and responsibility Detailed Oriented  Detailed administrative skills for tracking and reporting  Design and implement database structures for OLAP and OLTP systems Design and implement ETL and ELT process to consolidate data across multiple systems Design and implement APIs, services, data transfers to internal and external systems Identify and resolve performance and security issues relating to data access and maintenance Define and enforce data design, security and performance standards Understand and contribute to a corporate data model and overall data governance Communicate with application, back-office and external customer teams regarding data requirements, standards, performance and access Define and follow best practices for a full software development lifecycle involving data and database code.  Define and perform unit testing of database code Contribute to the analysis and remediation of system behavior using tools like New Relic to understand application and process bottlenecks Perform code reviews and audits of application teamâs database code to ensure compliance with established best practices.  Contribute to new technology evaluations and recommended usage Provide on call support for database related issues affecting system or process availability.     Bachelors degree in Computer Science, CIS or related field 4+ yearsâ experience with multiple databases  SQL Server, Oracle, Postgres, NoSQL  Experience with ETL, ELT, Replication  SSIS, Informatica, GoldenGate  Experience with Data Marts, Data Warehouses, Data Lakes Experience with one or more reporting tools  Birst, Cognos, Business Objects  Experience with Amazon, RedShift, Cloud ",database development across multiple platforms superior interpersonal interface wide range personalities levels within cvent client organizations professional communication style data collection analysis proactive listening resourceful collecting sufficient data analysis data develop implement best solution initiative selfstarter sense ownership tenacity problem solving positive outcomes motivated increase capacity responsibility detailed oriented detailed administrative tracking reporting design implement database structures olap oltp systems design implement etl elt process consolidate data across multiple systems design implement apis services data transfers internal external systems identify resolve performance security issues relating data access maintenance define enforce data design security performance standards understand contribute corporate data model overall data governance communicate application backoffice external customer teams regarding data standards performance access define follow best practices full software development lifecycle involving data database code define perform unit testing database code contribute analysis remediation behavior tools like relic understand application process bottlenecks perform code reviews audits application teams database code compliance established best practices contribute technology evaluations recommended usage call support database issues affecting process availability bachelors degree computer science cis multiple databases sql server oracle postgres nosql etl elt replication ssis informatica goldengate data marts data warehouses data lakes one reporting tools birst cognos business objects amazon redshift cloud,database development across multiple platforms superior interpersonal interface wide range personalities levels within cvent client organizations professional communication style data collection analysis proactive listening resourceful collecting sufficient develop implement best solution initiative selfstarter sense ownership tenacity problem solving positive outcomes motivated increase capacity responsibility detailed oriented administrative tracking reporting design structures olap oltp systems etl elt process consolidate apis services transfers internal external identify resolve performance security issues relating access maintenance define enforce standards understand contribute corporate model overall governance communicate application backoffice customer teams regarding follow practices full software lifecycle involving code perform unit testing remediation behavior tools like relic bottlenecks reviews audits compliance established technology evaluations recommended usage call support affecting availability bachelors degree computer science cis databases sql server oracle postgres nosql replication ssis informatica goldengate marts warehouses lakes one birst cognos business objects amazon redshift cloud
147,"    Bachelor in Computer Science, Data Science, Informatics, Engineering, or a related field.   ",bachelor computer science data science informatics engineering,bachelor computer science data informatics engineering
148," You can consume and utilize new languages, design patterns, APIs and toolsets.  You can work in a fast-paced and collaborative environment.  Effective communication skills and a willingness to learn are a must.  Experience with Test-Driven Development and writing unit and integration tests.  Experience using a Behavior-Driven Development suite like Cucumber.  Competent writing software with JavaScript ecosystems like React.  Comfortable working in a cloud environment like AWS.  Must have experience basic Linux/Unix CLI and using Git and GitHub for source code control.  Knowledge of Continuous Integration/Continuous Delivery systems like Jenkins.  Knowledge of Docker and Kubernetes is a plus.  Exhibits enthusiasm and well-rounded knowledge of backend systems and software architecture.  Applies best practices including design patterns and linting to all software development.  Approaches engineering requests from a user's vantage point to form architectural and technical requirements.  Stays well-informed of emerging technologies and software trends.  Capable of debugging problems related to HTTP, XHR, JSON, CORS, SSL, S3, etc.  Able to investigate performance and memory issues.  Able to reduce complex requirements and user interaction flows into long-term API designs.  Good understanding of architectural messaging patterns and pitfalls using Kafka, Rabbit MQ, etc.  Endeavors to establish positive relationships, both inter-departmentally, and cross-functionally.   Proficient with interviewing and gathering business requirements, definition and design of data source and data flow, data quality analysis, and working with the data architect on the development of logical data models.  Proficient using Infosphere/DataStage or equivalent ETL software.  Proficient with relational databases and using SQL to query, create tables, views, indexes, joins Proficient using Unix and applicable scripting/scheduling tools Experience with the SDLC, ITSM and privacy and security concepts.     Bachelor's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology required upon hire Master's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology preferred  3 Years Working as a Data Integration Engineer or Data Integration Production Support Engineer with a Bachelor's degree is required.  Experience working in a healthcare or related field Teradata Database Experience Experience upgrading IBM Infosphere Tools",consume utilize languages design patterns apis toolsets fastpaced collaborative effective communication willingness learn must testdriven development writing unit integration tests behaviordriven development suite like cucumber competent writing software javascript ecosystems like react comfortable cloud like aws must basic linuxunix cli git github source code control continuous integrationcontinuous delivery systems like jenkins docker kubernetes plus exhibits enthusiasm wellrounded backend systems software architecture applies best practices design patterns linting software development approaches engineering requests users vantage point form architectural technical stays wellinformed emerging technologies software trends capable debugging problems http xhr json cors ssl able investigate performance memory issues able reduce complex user interaction flows longterm api designs good understanding architectural messaging patterns pitfalls kafka rabbit mq endeavors establish positive relationships interdepartmentally crossfunctionally proficient interviewing gathering business definition design data source data flow data analysis data architect development logical data models proficient infospheredatastage etl software proficient relational databases sql query create tables views indexes joins proficient unix applicable scriptingscheduling tools sdlc itsm privacy security concepts bachelors degree business administration information science computer science computer engineering information technology upon hire masters degree business administration information science computer science computer engineering information technology data integration engineer data integration production support engineer bachelors degree healthcare teradata database upgrading ibm infosphere tools,consume utilize languages design patterns apis toolsets fastpaced collaborative effective communication willingness learn must testdriven development writing unit integration tests behaviordriven suite like cucumber competent software javascript ecosystems react comfortable cloud aws basic linuxunix cli git github source code control continuous integrationcontinuous delivery systems jenkins docker kubernetes plus exhibits enthusiasm wellrounded backend architecture applies best practices linting approaches engineering requests users vantage point form architectural technical stays wellinformed emerging technologies trends capable debugging problems http xhr json cors ssl able investigate performance memory issues reduce complex user interaction flows longterm api designs good understanding messaging pitfalls kafka rabbit mq endeavors establish positive relationships interdepartmentally crossfunctionally proficient interviewing gathering business definition data flow analysis architect logical models infospheredatastage etl relational databases sql query create tables views indexes joins unix applicable scriptingscheduling tools sdlc itsm privacy security concepts bachelors degree administration information science computer technology upon hire masters engineer production support healthcare teradata database upgrading ibm infosphere
149,"  Lead a technical team to rapidly architect, design, prototype, and implement and optimize architectures to tackle the Big Data and Data Science problems. Design, maintain and oversee the operational process to develop modular code base to solve ârealâ world problems. Conduct regular peer code reviews to ensure code quality and compliance following best practices in the industry. Work in cross-disciplinary teams to understand client needs and ingest rich data sources. Research, experiment, and utilize leading Big Data methodologies  Hadoop, Spark, Kafka, Netezza, Snowflake, and AWS  with cloud/on premise/hybrid hosting solutions. Oversee a technical team to provide proficient documentation and operating guidance for users of all levels. Lead a technical team to architect, implement, and test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings  AWS, Azure, client technology stacks, and on-prem clusters Translate advanced business analytics problems into technical approaches that yield actionable recommendations across multiple, diverse domains; Communicate results and educate others through design and build of insightful visualizations, reports, and presentations. Develop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management and client relationship developmentHelp drive the process for pursuing innovations, target solutions, and extendable platforms for Merkleâs products. Participate in developing and presenting thought leadership, and assist in ensuring that Merkleâs âdata sourceâ technology stack incorporates and is optimized for using specific technologies. Promote the Merkle brand in the broader âdata sourceâ community.   ",lead technical team rapidly architect design prototype implement optimize architectures tackle big data data science problems design maintain oversee operational process develop modular code base solve real world problems conduct regular peer code reviews code compliance following best practices industry crossdisciplinary teams understand client needs ingest rich data sources research experiment utilize leading big data methodologies hadoop spark kafka netezza snowflake aws cloudon premisehybrid hosting solutions oversee technical team proficient documentation operating guidance users levels lead technical team architect implement test data processing pipelines data miningdata science algorithms variety hosted settings aws azure client technology stacks onprem clusters translate advanced business analytics problems technical approaches yield actionable recommendations across multiple diverse domains communicate results educate others design build insightful visualizations reports presentations develop business requirement capture translation hypothesisdriven consulting stream project management client relationship developmenthelp drive process pursuing innovations target solutions extendable platforms merkles products participate developing presenting thought leadership assist ensuring merkles data source technology stack incorporates optimized specific technologies promote merkle brand broader data source community,lead technical team rapidly architect design prototype implement optimize architectures tackle big data science problems maintain oversee operational process develop modular code base solve real world conduct regular peer reviews compliance following best practices industry crossdisciplinary teams understand client needs ingest rich sources research experiment utilize leading methodologies hadoop spark kafka netezza snowflake aws cloudon premisehybrid hosting solutions proficient documentation operating guidance users levels test processing pipelines miningdata algorithms variety hosted settings azure technology stacks onprem clusters translate advanced business analytics approaches yield actionable recommendations across multiple diverse domains communicate results educate others build insightful visualizations reports presentations requirement capture translation hypothesisdriven consulting stream project management relationship developmenthelp drive pursuing innovations target extendable platforms merkles products participate developing presenting thought leadership assist ensuring source stack incorporates optimized specific technologies promote merkle brand broader community
150,"3+ years of experience in SQL and developing SQL server objects e. g. , store procedures, tables, triggers, views and functions.  At least 2 years of experience with Big Data technologies.  At least 2 years of coding experience in data environments.  3+ years design & implementation experience with distributed applications.  2+ years of experience in database architectures and data pipeline development.  Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.  Experience in manipulating multiple, complex and large data sources.  Experienced in Data Science, statistical models as a plus.  Experience in Designing, implementing and maintaining SQL Server databases.  Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.  Experience in SQL query tuning and optimization.  Experience working in SaaS, IaaS, and PaaS.     3+ years of experience in SQL and developing SQL server objects e. g. , store procedures, tables, triggers, views and functions.  At least 2 years of experience with Big Data technologies.  At least 2 years of coding experience in data environments.  3+ years design & implementation experience with distributed applications.  2+ years of experience in database architectures and data pipeline development.  Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.  Experience in manipulating multiple, complex and large data sources.  Experienced in Data Science, statistical models as a plus.  Experience in Designing, implementing and maintaining SQL Server databases.  Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.  Experience in SQL query tuning and optimization.  Experience working in SaaS, IaaS, and PaaS.     ",sql developing sql server objects e g store procedures tables triggers views functions least big data technologies least coding data environments design implementation distributed applications database architectures data pipeline development conceptual reporting visualization tools ssrs powerbi tableau business intelligence tools manipulating multiple complex data sources experienced data science statistical models plus designing implementing maintaining sql server databases designing implementing maintaining etl processes sql server ssis sql query tuning optimization saas iaas paas sql developing sql server objects e g store procedures tables triggers views functions least big data technologies least coding data environments design implementation distributed applications database architectures data pipeline development conceptual reporting visualization tools ssrs powerbi tableau business intelligence tools manipulating multiple complex data sources experienced data science statistical models plus designing implementing maintaining sql server databases designing implementing maintaining etl processes sql server ssis sql query tuning optimization saas iaas paas,sql developing server objects e g store procedures tables triggers views functions least big data technologies coding environments design implementation distributed applications database architectures pipeline development conceptual reporting visualization tools ssrs powerbi tableau business intelligence manipulating multiple complex sources experienced science statistical models plus designing implementing maintaining databases etl processes ssis query tuning optimization saas iaas paas
151,"At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ",least consulting client service delivery azure devops azure platform proven build manage foster teamoriented,least consulting client service delivery azure devops platform proven build manage foster teamoriented
152,"BS Degree or equivalent work experience Formal training and /or certifications in programming languages a plus Typically have 4-8 years of directly related experience.  Experience with data used in CoreLogic products and processes  public record, tax, census, appraisal, etc.  .  Fluent in multiple program languages at an expert level Strong experience with various computer platforms and application environments Expertise with developing multiple tiers of multi-tiered software applications Expertise designing programs and data systems Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team Project management skills Experience with ETL methodologies Strong understanding of data structures and design Experience with data flow, data enrichment, transformations Ability to optimize database queries and performance Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation Experience in developing data service processes and system infrastructure to be used Enterprise Wide Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.   BS Degree or equivalent work experience Formal training and /or certifications in programming languages a plus Typically have 4-8 years of directly related experience.  Experience with data used in CoreLogic products and processes  public record, tax, census, appraisal, etc.  .  Fluent in multiple program languages at an expert level Strong experience with various computer platforms and application environments Expertise with developing multiple tiers of multi-tiered software applications Expertise designing programs and data systems Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team Project management skills Experience with ETL methodologies Strong understanding of data structures and design Experience with data flow, data enrichment, transformations Ability to optimize database queries and performance Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation Experience in developing data service processes and system infrastructure to be used Enterprise Wide Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.    BS Degree or equivalent work experience Formal training and /or certifications in programming languages a plus Typically have 4-8 years of directly related experience.  Experience with data used in CoreLogic products and processes  public record, tax, census, appraisal, etc.  .  Fluent in multiple program languages at an expert level Strong experience with various computer platforms and application environments Expertise with developing multiple tiers of multi-tiered software applications Expertise designing programs and data systems Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team Project management skills Experience with ETL methodologies Strong understanding of data structures and design Experience with data flow, data enrichment, transformations Ability to optimize database queries and performance Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation Experience in developing data service processes and system infrastructure to be used Enterprise Wide Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.   ",bs degree formal training certifications programming languages plus typically directly data used corelogic products processes public record tax census appraisal fluent multiple program languages expert level various computer platforms application environments expertise developing multiple tiers multitiered software applications expertise designing programs data systems constantly updating personal technical business mentoring others increase team project management etl methodologies understanding data structures design data flow data enrichment transformations optimize database queries performance demonstrates expertise variety concepts practices procedures database design implementation developing data service processes infrastructure used enterprise wide communication order participate business meetings clear written documentation variety complex technical issues wide audience bs degree formal training certifications programming languages plus typically directly data used corelogic products processes public record tax census appraisal fluent multiple program languages expert level various computer platforms application environments expertise developing multiple tiers multitiered software applications expertise designing programs data systems constantly updating personal technical business mentoring others increase team project management etl methodologies understanding data structures design data flow data enrichment transformations optimize database queries performance demonstrates expertise variety concepts practices procedures database design implementation developing data service processes infrastructure used enterprise wide communication order participate business meetings clear written documentation variety complex technical issues wide audience bs degree formal training certifications programming languages plus typically directly data used corelogic products processes public record tax census appraisal fluent multiple program languages expert level various computer platforms application environments expertise developing multiple tiers multitiered software applications expertise designing programs data systems constantly updating personal technical business mentoring others increase team project management etl methodologies understanding data structures design data flow data enrichment transformations optimize database queries performance demonstrates expertise variety concepts practices procedures database design implementation developing data service processes infrastructure used enterprise wide communication order participate business meetings clear written documentation variety complex technical issues wide audience,bs degree formal training certifications programming languages plus typically directly data used corelogic products processes public record tax census appraisal fluent multiple program expert level various computer platforms application environments expertise developing tiers multitiered software applications designing programs systems constantly updating personal technical business mentoring others increase team project management etl methodologies understanding structures design flow enrichment transformations optimize database queries performance demonstrates variety concepts practices procedures implementation service infrastructure enterprise wide communication order participate meetings clear written documentation complex issues audience
153,"Bachelorâs degree in Computer Science or any other relevant field.  Strong Database/SQL skills 5+ years hands on ETL skills â Informatica, Pentaho Unix/Perl scripting Strong knowledge of Data modeling/Data Warehousing concepts Experience of a scheduling tool - CA/Autosys Experience working within an Agile environment Demonstrated analytical and problem solving skills.   Bachelorâs degree in Computer Science or any other relevant field.  Strong Database/SQL skills 5+ years hands on ETL skills â Informatica, Pentaho Unix/Perl scripting Strong knowledge of Data modeling/Data Warehousing concepts Experience of a scheduling tool - CA/Autosys Experience working within an Agile environment Demonstrated analytical and problem solving skills.     ",bachelors degree computer science relevant databasesql hands etl informatica pentaho unixperl scripting data modelingdata warehousing concepts scheduling tool caautosys within agile demonstrated analytical problem solving bachelors degree computer science relevant databasesql hands etl informatica pentaho unixperl scripting data modelingdata warehousing concepts scheduling tool caautosys within agile demonstrated analytical problem solving,bachelors degree computer science relevant databasesql hands etl informatica pentaho unixperl scripting data modelingdata warehousing concepts scheduling tool caautosys within agile demonstrated analytical problem solving
154," Undergraduate degree in Computer Science, Mathematics, Engineering  or related field  or equivalent experience preferred 5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function Ability to work with broad parameters in complex situations Experience in developing, managing, and manipulating large, complex datasets   Responsible for design, prototyping and delivery of software solutions within the big data eco-system Leading projects and/or serving as analytics SME to provide new or enhanced data to the business Improving data governance and quality increasing the reliability of our data Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise   ",undergraduate degree computer science mathematics engineering data integration etl andor business intelligenceanalytics function broad parameters complex situations developing managing manipulating complex datasets responsible design prototyping delivery software solutions within big data ecosystem leading projects andor serving analytics sme enhanced data business improving data governance increasing reliability data influencing creation single trusted source key claims business data shared across enterprise,undergraduate degree computer science mathematics engineering data integration etl andor business intelligenceanalytics function broad parameters complex situations developing managing manipulating datasets responsible design prototyping delivery software solutions within big ecosystem leading projects serving analytics sme enhanced improving governance increasing reliability influencing creation single trusted source key claims shared across enterprise
155," Skilled in Python and preferably in one or more programming languages like C++, Java, Go, etc Experience with Docker and Kubernetes Experience working with SQL and NoSQL based database solutions Public cloud technology experience in production  Azure, AWS, or Equivalent  3+ years of collective experience in data engineering and data analysis 2+ years of experience architecting, building and administering large-scale distributed applications    Skilled in Python and preferably in one or more programming languages like C++, Java, Go, etc Experience with Docker and Kubernetes Experience working with SQL and NoSQL based database solutions Public cloud technology experience in production  Azure, AWS, or Equivalent  3+ years of collective experience in data engineering and data analysis 2+ years of experience architecting, building and administering large-scale distributed applications   ",skilled python preferably one programming languages like c java go docker kubernetes sql nosql based database solutions public cloud technology production azure aws collective data engineering data analysis architecting building administering largescale distributed applications skilled python preferably one programming languages like c java go docker kubernetes sql nosql based database solutions public cloud technology production azure aws collective data engineering data analysis architecting building administering largescale distributed applications,skilled python preferably one programming languages like c java go docker kubernetes sql nosql based database solutions public cloud technology production azure aws collective data engineering analysis architecting building administering largescale distributed applications
156,"Bachelorâs degree in Computer Science or any other relevant field.  Strong Database/SQL skills 5+ years hands on ETL skills â Informatica, Pentaho Unix/Perl scripting Strong knowledge of Data modeling/Data Warehousing concepts Experience of a scheduling tool - CA/Autosys Experience working within an Agile environment Demonstrated analytical and problem solving skills.   Bachelorâs degree in Computer Science or any other relevant field.  Strong Database/SQL skills 5+ years hands on ETL skills â Informatica, Pentaho Unix/Perl scripting Strong knowledge of Data modeling/Data Warehousing concepts Experience of a scheduling tool - CA/Autosys Experience working within an Agile environment Demonstrated analytical and problem solving skills.     ",bachelors degree computer science relevant databasesql hands etl informatica pentaho unixperl scripting data modelingdata warehousing concepts scheduling tool caautosys within agile demonstrated analytical problem solving bachelors degree computer science relevant databasesql hands etl informatica pentaho unixperl scripting data modelingdata warehousing concepts scheduling tool caautosys within agile demonstrated analytical problem solving,bachelors degree computer science relevant databasesql hands etl informatica pentaho unixperl scripting data modelingdata warehousing concepts scheduling tool caautosys within agile demonstrated analytical problem solving
157," Strong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI.  Experience working with large datasets  1B+ Records  Knowledge of Big Data or Cloud technologies Experience with version control  e. g.  TFS, SVN or Git  and build tools.  Tableau/BI Tools  Develop logical data models and processes to transform, cleanse, and normalize raw data into high-quality datasets aligned with our analytical requirements.  Develop and maintain comprehensive controls to ensure data quality and completeness.  Manage data movement through our infrastructure.  Streamline existing data workflows to create a flexible, reliable, and faster process.  Develop real-time data transformations and validations.  Identify and onboard new data sources.  Collaborate with data vendors and internal stakeholders to define requirements and build interfaces.  Troubleshoot and resolve issues with data feeds.  Work with our team of researchers to identify and analyze investment opportunities in real estate and fixed income securities markets.    3-5 years of experience in data analysis and/or management in an enterprise environment within finance, operations or analytics.  Passion for data organization, quality, and reliability.  MS SQL Server, Oracle, Postgres, Hive, Presto, etc.  preferred.  Experience developing complex efficient queries, designing and building logical data models, and working with large datasets on a relational database system Experience with at least one language  e. g.  Python, C , Scala, Java .  Strong problem-solving skills.  Must be an intellectually curious self-starter and motivated to continually learn.  Proactive, hardworking team player with excellent communication skills ",statistics handson sas r matlab machine ai datasets b records big data cloud technologies version control e g tfs svn git build tools tableaubi tools develop logical data models processes transform cleanse normalize raw data highquality datasets aligned analytical develop maintain comprehensive controls data completeness manage data movement infrastructure streamline existing data workflows create flexible reliable faster process develop realtime data transformations validations identify onboard data sources collaborate data vendors internal stakeholders define build interfaces troubleshoot resolve issues data feeds team researchers identify analyze investment opportunities real estate fixed income securities markets data analysis andor management enterprise within finance operations analytics passion data organization reliability ms sql server oracle postgres hive presto developing complex efficient queries designing building logical data models datasets relational database least one language e g python c scala java problemsolving must intellectually curious selfstarter motivated continually learn proactive hardworking team player communication,statistics handson sas r matlab machine ai datasets b records big data cloud technologies version control e g tfs svn git build tools tableaubi develop logical models processes transform cleanse normalize raw highquality aligned analytical maintain comprehensive controls completeness manage movement infrastructure streamline existing workflows create flexible reliable faster process realtime transformations validations identify onboard sources collaborate vendors internal stakeholders define interfaces troubleshoot resolve issues feeds team researchers analyze investment opportunities real estate fixed income securities markets analysis andor management enterprise within finance operations analytics passion organization reliability ms sql server oracle postgres hive presto developing complex efficient queries designing building relational database least one language python c scala java problemsolving must intellectually curious selfstarter motivated continually learn proactive hardworking player communication
158," Experience leading, managing and hiring a team of talented engineers Expertise in at least one of the following engineering domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Expertise in at least one of the following data domains    Predictive analytics  e. g. , recommendation systems, predictive maintenance  Natural language processing  e. g. , conversational chatbots  Document understanding Image classification Marketing analytics IoT systems Experience writing software in one or more languages such as Python or Java/Scala Experience in technical consulting or customer-facing role Excellent critical thinking, problem-solving and analytical skills     ",leading managing hiring team talented engineers expertise least one following engineering domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale expertise least one following data domains predictive analytics e g recommendation systems predictive maintenance natural language processing e g conversational chatbots document understanding image classification marketing analytics iot systems writing software one languages python javascala technical consulting customerfacing role critical thinking problemsolving analytical,leading managing hiring team talented engineers expertise least one following engineering domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale domains predictive analytics e g recommendation systems maintenance natural language conversational chatbots document understanding image classification marketing iot writing languages python javascala consulting customerfacing role critical thinking problemsolving analytical
159,"Bachelorâs degree in Engineering, Mathematics, Computer Science or related field Clinical and healthcare relevance; a Clinical degree isnât required, but a deep understanding of the healthcare market is.   Solid knowledge of SQL queries and relational databases Good understanding of math and statistics Deep experience with Excel and/or other data manipulation tools Familiarity with data visualization tools  e. g.  Looker, Tableau  Programming experience with R/Python for data analysis preferred  Assist in the design, preparation and distribution of reports and dashboards for end-users, management, and key stakeholders.  Use statistical methods to ensure metrics are well defined and match to the customerâs goals and desired outcomes documented during the discovery process.  Support the standardization of reporting for end-users.  Serve as internal expert user of the reporting system for input to the ongoing improvement and development of reporting tools.  Communicate reporting enhancements to internal and external users through presentations and documentation.  Seek and analyze trends and patterns across communities, populations, and contacts to assist in product management and development.   Bachelorâs degree in Engineering, Mathematics, Computer Science or related field Clinical and healthcare relevance; a Clinical degree isnât required, but a deep understanding of the healthcare market is.   Bachelorâs degree in Engineering, Mathematics, Computer Science or related field Clinical and healthcare relevance; a Clinical degree isnât required, but a deep understanding of the healthcare market is.  ",bachelors degree engineering mathematics computer science clinical healthcare relevance clinical degree isnt deep understanding healthcare market solid sql queries relational databases good understanding math statistics deep excel andor data manipulation tools familiarity data visualization tools e g looker tableau programming rpython data analysis assist design preparation distribution reports dashboards endusers management key stakeholders use statistical methods metrics well defined match customers goals desired outcomes documented discovery process support standardization reporting endusers serve internal expert user reporting input ongoing improvement development reporting tools communicate reporting enhancements internal external users presentations documentation seek analyze trends patterns across communities populations contacts assist product management development bachelors degree engineering mathematics computer science clinical healthcare relevance clinical degree isnt deep understanding healthcare market bachelors degree engineering mathematics computer science clinical healthcare relevance clinical degree isnt deep understanding healthcare market,bachelors degree engineering mathematics computer science clinical healthcare relevance isnt deep understanding market solid sql queries relational databases good math statistics excel andor data manipulation tools familiarity visualization e g looker tableau programming rpython analysis assist design preparation distribution reports dashboards endusers management key stakeholders use statistical methods metrics well defined match customers goals desired outcomes documented discovery process support standardization reporting serve internal expert user input ongoing improvement development communicate enhancements external users presentations documentation seek analyze trends patterns across communities populations contacts product
160,"   Lead efforts to design, build, scale, and maintain multiple data pipelines Architect highly scalable data solutions Be a technical thought leader within the org Work closely with business owners and external stakeholders to provide actionable data Ensure data accuracy and reliability    Experience building large scale streaming and batch data pipelines Experience using Big Data technologies  Spark, EMR, hadoop, data lakes, etc.   Mastery of multiple databases  e. g.  MongoDB, MySQL, etc.   Understanding of data security best practices ",lead efforts design build scale maintain multiple data pipelines architect highly scalable data solutions technical thought leader within org closely business owners external stakeholders actionable data data accuracy reliability building scale streaming batch data pipelines big data technologies spark emr hadoop data lakes mastery multiple databases e g mongodb mysql understanding data security best practices,lead efforts design build scale maintain multiple data pipelines architect highly scalable solutions technical thought leader within org closely business owners external stakeholders actionable accuracy reliability building streaming batch big technologies spark emr hadoop lakes mastery databases e g mongodb mysql understanding security best practices
161," Mastery in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or customer-facing role     ",mastery least one following domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale writing software one languages python java scala go building productiongrade data solutions relational nosql systems monitoringalerting capacity planning performance tuning technical consulting customerfacing role,mastery least one following domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale writing languages python java scala go relational nosql systems monitoringalerting capacity planning performance tuning consulting customerfacing role
162,"Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server Play a lead role in migrating data from legacy systems to cloud platforms Create custom data sets for use by business analysts and data scientists Rapidly prototype new data sets for exploratory analysis Use SQL skills to manage data Monitor database performance and tuning to improve query performance Design and automate data pipelines to integrate different data sources using SSIS Collaborate with IT partners to move data prototypes into production Develop real-time data integrations in MS SQL Server Establish techniques to monitor data quality and implement remediation procedures Working directly with non-technical users to identify complex needs/requirements and translate into technical solutions Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler Create data training programs Train analyst community on data sources and best practices Lead other staff on SSIS, performance tuning, and database administration   Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server Play a lead role in migrating data from legacy systems to cloud platforms Create custom data sets for use by business analysts and data scientists Rapidly prototype new data sets for exploratory analysis Use SQL skills to manage data Monitor database performance and tuning to improve query performance Design and automate data pipelines to integrate different data sources using SSIS Collaborate with IT partners to move data prototypes into production Develop real-time data integrations in MS SQL Server Establish techniques to monitor data quality and implement remediation procedures Working directly with non-technical users to identify complex needs/requirements and translate into technical solutions Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler Create data training programs Train analyst community on data sources and best practices Lead other staff on SSIS, performance tuning, and database administration   ",develop strategies data models automated etl processes stored procedures views ms sql server play lead role migrating data legacy systems cloud platforms create custom data sets use business analysts data scientists rapidly prototype data sets exploratory analysis use sql manage data monitor database performance tuning improve query performance design automate data pipelines integrate different data sources ssis collaborate partners move data prototypes production develop realtime data integrations ms sql server establish techniques monitor data implement remediation procedures directly nontechnical users identify complex needsrequirements translate technical solutions demonstrate competency tsql advanced functions performance tuning job scheduler create data training programs train analyst community data sources best practices lead staff ssis performance tuning database administration develop strategies data models automated etl processes stored procedures views ms sql server play lead role migrating data legacy systems cloud platforms create custom data sets use business analysts data scientists rapidly prototype data sets exploratory analysis use sql manage data monitor database performance tuning improve query performance design automate data pipelines integrate different data sources ssis collaborate partners move data prototypes production develop realtime data integrations ms sql server establish techniques monitor data implement remediation procedures directly nontechnical users identify complex needsrequirements translate technical solutions demonstrate competency tsql advanced functions performance tuning job scheduler create data training programs train analyst community data sources best practices lead staff ssis performance tuning database administration,develop strategies data models automated etl processes stored procedures views ms sql server play lead role migrating legacy systems cloud platforms create custom sets use business analysts scientists rapidly prototype exploratory analysis manage monitor database performance tuning improve query design automate pipelines integrate different sources ssis collaborate partners move prototypes production realtime integrations establish techniques implement remediation directly nontechnical users identify complex needsrequirements translate technical solutions demonstrate competency tsql advanced functions job scheduler training programs train analyst community best practices staff administration
163,"   Build and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies  Hadoop, Spark, Cloud, NoSQL etc.   in a agile manner.  Strong understanding of business and product data needs.  Close partnership with internal stakeholders and partners from Engineering, product, and business Finance, Sales, Customer Experience, Marketing etc.  .  Active role in hiring and growing the team in Austin with data Engineers, analysts, and data scientists.     Bachelor's or Master's Degree in Computer Science or Engineering or related experience required.  3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions.  Solid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka.  Experience in all aspects of data systems both Big data and traditional  including data schema design, ETL, aggregation strategy, and performance optimization.  Capable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality.  Experience in hiring data Engineers preferred.  Experience in Internet security industry preferred.  ",build support scalable reliable data solutions enable selfservice reporting advanced analytics cloudflare modern data lake edw technologies hadoop spark cloud nosql agile manner understanding business product data needs close partnership internal stakeholders partners engineering product business finance sales customer marketing active role hiring growing team austin data engineers analysts data scientists bachelors masters degree computer science engineering development big data space petabytes data building scale data solutions solid understanding google cloud platform hadoop python spark hive kafka aspects data systems big data traditional data schema design etl aggregation strategy performance optimization capable closely business product teams data solutions aligned business initiatives hiring data engineers internet security industry,build support scalable reliable data solutions enable selfservice reporting advanced analytics cloudflare modern lake edw technologies hadoop spark cloud nosql agile manner understanding business product needs close partnership internal stakeholders partners engineering finance sales customer marketing active role hiring growing team austin engineers analysts scientists bachelors masters degree computer science development big space petabytes building scale solid google platform python hive kafka aspects systems traditional schema design etl aggregation strategy performance optimization capable closely teams aligned initiatives internet security industry
164,"6+ years of professional experience as a data engineer or in a similar role 4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL Hands-on experience writing SQL, Perform SQL optimization and tuning Strong work experience with Python scripting or equivalent Working knowledge with AWS RDS, Data Lakes and data analytics Must be able to work in a diverse team environment Must possess problem-solving skills and ability to multitask Outstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude Bachelorâs degree in Computer Science or Engineering, or equivalent experience   Overall responsibility for day-to-day data pipeline operations and manage database acquisition and data delivery methods effectively Serve as a lead data engineer to manage the development of web services for data posting and delivery Provide technical leadership and expertise on data integration and data delivery Perform data quality procedures to ensure data consistency and data integrity Ensure proper documentation of data posting and related API objects Work directly with product and engineering team to understand data needs and provide end-to-end data solutions that address customer requirements Perform other duties as assigned   6+ years of professional experience as a data engineer or in a similar role 4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL Hands-on experience writing SQL, Perform SQL optimization and tuning Strong work experience with Python scripting or equivalent Working knowledge with AWS RDS, Data Lakes and data analytics Must be able to work in a diverse team environment Must possess problem-solving skills and ability to multitask Outstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude Bachelorâs degree in Computer Science or Engineering, or equivalent experience ",professional data engineer similar role database development relational databases oraclems sqlpostgresql handson writing sql perform sql optimization tuning python scripting aws rds data lakes data analytics must able diverse team must possess problemsolving multitask outstanding analytical problemsolving abilities drive results attention detail collaborative attitude bachelors degree computer science engineering overall responsibility daytoday data pipeline operations manage database acquisition data delivery methods effectively serve lead data engineer manage development web services data posting delivery technical leadership expertise data integration data delivery perform data procedures data consistency data integrity proper documentation data posting api objects directly product engineering team understand data needs endtoend data solutions address customer perform duties assigned professional data engineer similar role database development relational databases oraclems sqlpostgresql handson writing sql perform sql optimization tuning python scripting aws rds data lakes data analytics must able diverse team must possess problemsolving multitask outstanding analytical problemsolving abilities drive results attention detail collaborative attitude bachelors degree computer science engineering,professional data engineer similar role database development relational databases oraclems sqlpostgresql handson writing sql perform optimization tuning python scripting aws rds lakes analytics must able diverse team possess problemsolving multitask outstanding analytical abilities drive results attention detail collaborative attitude bachelors degree computer science engineering overall responsibility daytoday pipeline operations manage acquisition delivery methods effectively serve lead web services posting technical leadership expertise integration procedures consistency integrity proper documentation api objects directly product understand needs endtoend solutions address customer duties assigned
165," Minimum of 2-3 yearsâ experience in production ETL pipelines, utilizing big data engineering techniques that enable statistical solutions to solve business problems Post graduate degree in Computer Science/ Engineering, Information Science or a related discipline with strong technical experiences highly desired Previous exposure to financial services, credit cards or merchant analytics is a plus, but not required Extensive experience with SQL and big data technologies  Hadoop, Python , Java, Spark, Hive etc.   tools for large scale data processing, data transformation and machine learning pipelines Experience with data visualization and business intelligence tools like Tableau, Microstrategy, or other programs highly desired Experience with SAS as a statistical package is preferred Familiarity or experience with data mining and statistical modeling  e. g. , regression modeling, clustering techniques, decision trees, etc.   is preferred    ",minimum production etl pipelines utilizing big data engineering techniques enable statistical solutions solve business problems post graduate degree computer science engineering information science discipline technical experiences highly desired previous exposure financial services credit cards merchant analytics plus extensive sql big data technologies hadoop python java spark hive tools scale data processing data transformation machine pipelines data visualization business intelligence tools like tableau microstrategy programs highly desired sas statistical package familiarity data mining statistical modeling e g regression modeling clustering techniques decision trees,minimum production etl pipelines utilizing big data engineering techniques enable statistical solutions solve business problems post graduate degree computer science information discipline technical experiences highly desired previous exposure financial services credit cards merchant analytics plus extensive sql technologies hadoop python java spark hive tools scale processing transformation machine visualization intelligence like tableau microstrategy programs sas package familiarity mining modeling e g regression clustering decision trees
166,     B. S.  in Computer Science/Engineering and 5 years of professional software development experience or equivalent.  3+ years experience with Python using Django or Flask Experience with Python Celery or other task/job management frameworks 5+ years of experience in a software development environment Experience with AWS services Experience with data modeling techniques 3+ years experience with PostgreSQL or other SQL server ,b computer scienceengineering professional software development python django flask python celery taskjob management frameworks software development aws services data modeling techniques postgresql sql server,b computer scienceengineering professional software development python django flask celery taskjob management frameworks aws services data modeling techniques postgresql sql server
167,"Bachelor's Degree in Computer Science, Engineering, Math or related technical field  8 years of additional experience can be substituted for education  5+ years' relevant experience Experience in Data Platform Languages such as SSIS and TSQL Experience in Data Platform Tools such as SSIS, SSDT, SSMS and/or Visual Studio Experience working in an environment using Agile methodology Experience in Data Engineering concepts such as ETL, ELT or performance tuning Hands on experience writing SQL scripts Strong Communication, Presentation and Facilitation Skills.  Must be able to explain data quality issues and impacts to a non-technical audience  Develop, implement and maintain a scalable data management architecture to support the storage and querying of large datasets Create and maintain data pipelines to automate the processing of large data sets Help design and maintain efficient data collection workflows with other groups within the company Manage and perform data analysis to identify data quality issues Propose new technologies that could improve the way data is handled Manage data security and provide efficient access to engineering teams Communicate technical data and approaches to both technical and non-technical audiences Perform Database maintenance Building and analyzing dashboards and reports Evaluating and defining metrics and perform exploratory analysis Monitoring key product metrics and understanding root causes of changes in metrics Empower and assist operation and product teams through building key data sets and data-based recommendations Automating analyses and authoring pipelines via SQL/python based ETL framework  ",bachelors degree computer science engineering math technical additional substituted education relevant data platform languages ssis tsql data platform tools ssis ssdt ssms andor visual studio agile methodology data engineering concepts etl elt performance tuning hands writing sql scripts communication presentation facilitation must able explain data issues impacts nontechnical audience develop implement maintain scalable data management architecture support storage querying datasets create maintain data pipelines automate processing data sets help design maintain efficient data collection workflows groups within company manage perform data analysis identify data issues propose technologies could improve way data handled manage data security efficient access engineering teams communicate technical data approaches technical nontechnical audiences perform database maintenance building analyzing dashboards reports evaluating defining metrics perform exploratory analysis monitoring key product metrics understanding root causes changes metrics empower assist operation product teams building key data sets databased recommendations automating analyses authoring pipelines via sqlpython based etl framework,bachelors degree computer science engineering math technical additional substituted education relevant data platform languages ssis tsql tools ssdt ssms andor visual studio agile methodology concepts etl elt performance tuning hands writing sql scripts communication presentation facilitation must able explain issues impacts nontechnical audience develop implement maintain scalable management architecture support storage querying datasets create pipelines automate processing sets help design efficient collection workflows groups within company manage perform analysis identify propose technologies could improve way handled security access teams communicate approaches audiences database maintenance building analyzing dashboards reports evaluating defining metrics exploratory monitoring key product understanding root causes changes empower assist operation databased recommendations automating analyses authoring via sqlpython based framework
168,    7+ Years Data Engineering5+ Years RDBMS Management2+ Years of NoSQLAWS Ecosystem knowledgeSolid Data Modeling/Design Experience,data engineering rdbms management nosqlaws ecosystem knowledgesolid data modelingdesign,data engineering rdbms management nosqlaws ecosystem knowledgesolid modelingdesign
169,"Bachelorâs degree in a quantitative field  e. g. , engineering, statistics, mathematics, information technology, etc.   is preferred.  Master's degree is desired.  Must have at least 3 years of experience, preferably with a federal government customer.  Experience with big data tools  Hadoop, Spark, Kafka Experience with relational SQL and NoSQL databases  Postgres, Cassandra, MongoDB Experience with data governance tools  Collibra, Immuta Experience with AWS cloud services  EC2, EMR, RDS, Redshift Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala Must possess strong written and verbal communication skills.  Secret or Top Secret clearance is preferred.   Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.   ",bachelors degree quantitative e g engineering statistics mathematics information technology masters degree desired must least preferably federal government customer big data tools hadoop spark kafka relational sql nosql databases postgres cassandra mongodb data governance tools collibra immuta aws cloud services ec emr rds redshift objectorientedobject function scripting languages python java c scala must possess written verbal communication secret top secret clearance develop data structures systems support generation business insightsknowledge overall etl processesmaintain data infrastructure develop scripts regular processesdefine design develop data flow diagrams data dictionaries logical physical modelsdefine data document data elements capture maintain metadetaidentify clean incomplete incorrect inaccurate irrelevant dataidentify opportunities use data improve business performancecommunicate present data developing reports tableau business intelligence toolsadhere compliance audit data storage architecture cybersecurity,bachelors degree quantitative e g engineering statistics mathematics information technology masters desired must least preferably federal government customer big data tools hadoop spark kafka relational sql nosql databases postgres cassandra mongodb governance collibra immuta aws cloud services ec emr rds redshift objectorientedobject function scripting languages python java c scala possess written verbal communication secret top clearance develop structures systems support generation business insightsknowledge overall etl processesmaintain infrastructure scripts regular processesdefine design flow diagrams dictionaries logical physical modelsdefine document elements capture maintain metadetaidentify clean incomplete incorrect inaccurate irrelevant dataidentify opportunities use improve performancecommunicate present developing reports tableau intelligence toolsadhere compliance audit storage architecture cybersecurity
170," Designing, Architecting, and Developing solutions leveraging big data technology  Open Source, AWS, or Microsoft  to ingest, process and analyze large, disparate data sets to exceed business requirements Unifying, enriching, and analyzing customer data to derive insights and opportunities Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches Seeking out information to learn about emerging methodologies and technologies Clarifying problems by driving to understand the true issue Looking for opportunities for improving methods and outcomes Applying data driven approach  KPIs  in tying technology solutions to specific business outcomes Collaborating, influencing and building consensus through constructive relationships and effective listening Solving problems by incorporating data into decision making    ",designing architecting developing solutions leveraging big data technology open source aws microsoft ingest process analyze disparate data sets exceed business unifying enriching analyzing customer data derive insights opportunities leveraging inhouse data platforms needed recommending building data platformssolutions exceed business clearly communicating findings recommendations opportunities improve data systems solutions demonstrating deep understanding big data technology concepts tools features functions benefits different approaches seeking information learn emerging methodologies technologies clarifying problems driving understand true issue looking opportunities improving methods outcomes applying data driven approach kpis tying technology solutions specific business outcomes collaborating influencing building consensus constructive relationships effective listening solving problems incorporating data decision making,designing architecting developing solutions leveraging big data technology open source aws microsoft ingest process analyze disparate sets exceed business unifying enriching analyzing customer derive insights opportunities inhouse platforms needed recommending building platformssolutions clearly communicating findings recommendations improve systems demonstrating deep understanding concepts tools features functions benefits different approaches seeking information learn emerging methodologies technologies clarifying problems driving understand true issue looking improving methods outcomes applying driven approach kpis tying specific collaborating influencing consensus constructive relationships effective listening solving incorporating decision making
171,"Degree in computer science or related field 5+ years current programming experience operating as individual contributor/hands on developer with programming projects as primary part of job 3+ Python experience  years can include advanced degree python projects  3+ years building and maintaining data pipelines and data assets 2+ years working with distributed data processing frameworks such as Spark, Hive, and MapReduce Demonstrated knowledge of data management best practices Strong prioritization skills; ability to manage ad-hoc requests in parallel with ongoing projects Attention to detail, intellectual curiosity, collaborative attitude and strong communication skills Willingness to pick up new platforms and technologies and strong curiosity about new technologies    ",degree computer science current programming operating individual contributorhands developer programming projects primary part job python include advanced degree python projects building maintaining data pipelines data assets distributed data processing frameworks spark hive mapreduce demonstrated data management best practices prioritization manage adhoc requests parallel ongoing projects attention detail intellectual curiosity collaborative attitude communication willingness pick platforms technologies curiosity technologies,degree computer science current programming operating individual contributorhands developer projects primary part job python include advanced building maintaining data pipelines assets distributed processing frameworks spark hive mapreduce demonstrated management best practices prioritization manage adhoc requests parallel ongoing attention detail intellectual curiosity collaborative attitude communication willingness pick platforms technologies
172," Bachelorâs degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.  6 or more years of experience as a data engineer on enterprise-level data solutions.  Experience in SQL and scripting for automation with Python, Perl or Ruby.  Experience working with relational and unstructured databases and enterprise data warehouses, including MySQL, PostgreSQL, MongoDB, SQL Server or Oracle.     Participate in data architecture discussions to understand target data structures, required data transformations and inform architectural approach based on best practices for data processing.  Lead detailed exploration of new internal and external source data to advise strategic initiatives led by the Product, Artificial Intelligence and Business Intelligence teams.  Influence technical and business strategy by making insightful contributions to team priorities and overall data processing approach.  Work in close collaboration with data-minded colleagues focused on back-end  microservice  development, business intelligence reporting, machine learning and artificial intelligence models.  Participate in the hiring and mentoring of other data engineers.    ",bachelors degree computer science information management data science analytics data engineer enterpriselevel data solutions sql scripting automation python perl ruby relational unstructured databases enterprise data warehouses mysql postgresql mongodb sql server oracle participate data architecture discussions understand target data structures data transformations inform architectural approach based best practices data processing lead detailed exploration internal external source data advise strategic initiatives led product artificial intelligence business intelligence teams influence technical business strategy making insightful contributions team priorities overall data processing approach close collaboration dataminded colleagues focused backend microservice development business intelligence reporting machine artificial intelligence models participate hiring mentoring data engineers,bachelors degree computer science information management data analytics engineer enterpriselevel solutions sql scripting automation python perl ruby relational unstructured databases enterprise warehouses mysql postgresql mongodb server oracle participate architecture discussions understand target structures transformations inform architectural approach based best practices processing lead detailed exploration internal external source advise strategic initiatives led product artificial intelligence business teams influence technical strategy making insightful contributions team priorities overall close collaboration dataminded colleagues focused backend microservice development reporting machine models hiring mentoring engineers
173,"   Collaborate directly with external customers to understand their student success goals, specify and design data solutions and commission products into production.  Develop ETL transformations to map client data systems into our canonical data model.  Collaborate with teams across Civitas to drive innovation and best practices in our data and data science platform.  Up to 20% Travel to visit external clients for technical discovery and/or UAT/QA of data mappings.  Design, Implement and Maintain Database Models.  Build and operationalize data science models on AWS.    ",collaborate directly external customers understand student success goals specify design data solutions commission products production develop etl transformations map client data systems canonical data model collaborate teams across civitas drive innovation best practices data data science platform travel visit external clients technical discovery andor uatqa data mappings design implement maintain database models build operationalize data science models aws,collaborate directly external customers understand student success goals specify design data solutions commission products production develop etl transformations map client systems canonical model teams across civitas drive innovation best practices science platform travel visit clients technical discovery andor uatqa mappings implement maintain database models build operationalize aws
174," Graduation from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information systems or information science or a related field.  High School diploma or equivalent and additional directly related experience may substitute for the required education on a year-for-year basis.   Emerging data and analytics technologies  i. e.  Hadoop, Spark, MongoDB, Azure Data Lake, etc.   Cloud platforms and development patterns  i. e.  AWS, Azure, MapReduce, etc.   Machine-learning, statistical analysis, artificial intelligence, predictive analytics.  Relational and non-relational data structures, theories, principles, and practices.  Metadata management and associated processes.  Web services  REST, SOAP, XML, WSDL, JSON .  Data encryption and secure transmission practices  SSL, SSH, SFTP, Certificates, PKI, OAUTH2 .   Works closely with internal customers regarding their specific data needs to develop the requirements for data subject areas needed in the Data Warehouse.  Captures the inventory of data sources and dashboards to prepare and manage integrated data.  Acts as the IT knowledge leader on data warehousing and data analytics to the business.  Ensures data warehouse implementations meet business expectations and coordinates customer acceptance testing and training.  Ensures the customer can exploit the data warehouse solutions and helps identify additional possible uses of information; anticipates future needs and opportunities.  Assists in the identification and integration of potential new data sources.  Designs and develops ETL pipelines that extract data from various sources and load into the data warehouse or other systems.  Ensures that controls to verify the accuracy and consistency of data are implemented and monitored.  Provides ongoing operational support of the enterprise data warehouse, continued development and enhancement of the data warehouse, automation of daily data extracts and external system feeds, and development and enhancement of current dashboards.   Graduation from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information systems or information science or a related field.  High School diploma or equivalent and additional directly related experience may substitute for the required education on a year-for-year basis.   normal cognitive abilities including the ability to learn, recall, and apply certain practices and policies; marginal or corrected visual and auditory requirements; constant use of personal computers, copiers, printers, and telephones; the ability to move about the office to access file cabinets and office machinery; frequent sitting and/or remaining in a stationary position; and the ability to work under deadlines, as a team member, and in direct contact with others. ",graduation accredited fouryear college university major coursework computer information systems computer science data management information systems information science school diploma additional directly may substitute education yearforyear basis emerging data analytics technologies e hadoop spark mongodb azure data lake cloud platforms development patterns e aws azure mapreduce machinelearning statistical analysis artificial intelligence predictive analytics relational nonrelational data structures theories principles practices metadata management associated processes web services rest soap xml wsdl json data encryption secure transmission practices ssl ssh sftp certificates pki oauth works closely internal customers regarding specific data needs develop data subject areas needed data warehouse captures inventory data sources dashboards prepare manage integrated data acts leader data warehousing data analytics business ensures data warehouse implementations meet business expectations coordinates customer acceptance testing training ensures customer exploit data warehouse solutions helps identify additional possible uses information anticipates future needs opportunities assists identification integration potential data sources designs develops etl pipelines extract data various sources load data warehouse systems ensures controls verify accuracy consistency data implemented monitored provides ongoing operational support enterprise data warehouse continued development enhancement data warehouse automation daily data extracts external feeds development enhancement current dashboards graduation accredited fouryear college university major coursework computer information systems computer science data management information systems information science school diploma additional directly may substitute education yearforyear basis normal cognitive abilities learn recall apply certain practices policies marginal corrected visual auditory constant use personal computers copiers printers telephones move office access file cabinets office machinery frequent sitting andor remaining stationary position deadlines team member direct contact others,graduation accredited fouryear college university major coursework computer information systems science data management school diploma additional directly may substitute education yearforyear basis emerging analytics technologies e hadoop spark mongodb azure lake cloud platforms development patterns aws mapreduce machinelearning statistical analysis artificial intelligence predictive relational nonrelational structures theories principles practices metadata associated processes web services rest soap xml wsdl json encryption secure transmission ssl ssh sftp certificates pki oauth works closely internal customers regarding specific needs develop subject areas needed warehouse captures inventory sources dashboards prepare manage integrated acts leader warehousing business ensures implementations meet expectations coordinates customer acceptance testing training exploit solutions helps identify possible uses anticipates future opportunities assists identification integration potential designs develops etl pipelines extract various load controls verify accuracy consistency implemented monitored provides ongoing operational support enterprise continued enhancement automation daily extracts external feeds current normal cognitive abilities learn recall apply certain policies marginal corrected visual auditory constant use personal computers copiers printers telephones move office access file cabinets machinery frequent sitting andor remaining stationary position deadlines team member direct contact others
175,"At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ",least consulting client service delivery azure devops azure platform proven build manage foster teamoriented,least consulting client service delivery azure devops platform proven build manage foster teamoriented
176,"   Design, build, scale, and maintain multiple data pipelines Work closely with business owners and external stakeholders to provide actionable data Ensure data accuracy and reliability    Experience building large scale streaming and batch data pipelines Experience using Big Data technologies  Spark, EMR, hadoop, data lakes, etc.   Mastery of multiple databases  e. g.  MongoDB, MySQL, etc.   Understanding of data security best practices ",design build scale maintain multiple data pipelines closely business owners external stakeholders actionable data data accuracy reliability building scale streaming batch data pipelines big data technologies spark emr hadoop data lakes mastery multiple databases e g mongodb mysql understanding data security best practices,design build scale maintain multiple data pipelines closely business owners external stakeholders actionable accuracy reliability building streaming batch big technologies spark emr hadoop lakes mastery databases e g mongodb mysql understanding security best practices
177,"   2+ years of experience with building end-to-end scalable production-grade data pipelines Knowledge of data warehousing Understanding of modern data architecture, data modeling, and data management principles Experience with modern data pipeline technologies  Spark, Flink, Airflow, Beam, etc.   Good foundation in data structures and algorithms Understanding with Relational and NoSQL databases to help teams best organize their data for analysis Experienced in OOP design and development, preferably in Python Knowledge in writing, understanding and tuning PL/SQL and/or T-SQL BS/BA degree in Computer Science, Engineering or related fields or equivalent experience  2+ years of experience with building end-to-end scalable production-grade data pipelines Knowledge of data warehousing Understanding of modern data architecture, data modeling, and data management principles Experience with modern data pipeline technologies  Spark, Flink, Airflow, Beam, etc.   Good foundation in data structures and algorithms Understanding with Relational and NoSQL databases to help teams best organize their data for analysis Experienced in OOP design and development, preferably in Python Knowledge in writing, understanding and tuning PL/SQL and/or T-SQL BS/BA degree in Computer Science, Engineering or related fields or equivalent experience ",building endtoend scalable productiongrade data pipelines data warehousing understanding modern data architecture data modeling data management principles modern data pipeline technologies spark flink airflow beam good foundation data structures algorithms understanding relational nosql databases help teams best organize data analysis experienced oop design development preferably python writing understanding tuning plsql andor tsql bsba degree computer science engineering fields building endtoend scalable productiongrade data pipelines data warehousing understanding modern data architecture data modeling data management principles modern data pipeline technologies spark flink airflow beam good foundation data structures algorithms understanding relational nosql databases help teams best organize data analysis experienced oop design development preferably python writing understanding tuning plsql andor tsql bsba degree computer science engineering fields,building endtoend scalable productiongrade data pipelines warehousing understanding modern architecture modeling management principles pipeline technologies spark flink airflow beam good foundation structures algorithms relational nosql databases help teams best organize analysis experienced oop design development preferably python writing tuning plsql andor tsql bsba degree computer science engineering fields
178," At least one  1  year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models At least two  2  years of experience in Scala, Python, Apache Spark and SQL Experience working directly with relational database structures and flat files Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.  Good verbal and written communication skills, with both technical and non-technical stakeholders   Build pipelines to ingest and maintain complex data sets into Cerebri AIâs proprietary data stores for use in machine learning modeling Develop and maintain data ontologies for key market segments Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data Write quality documentation on the discovery process and software projects Work equally well in a team environment and on your own.  Communicate complex ideas clearly with both team members and clients Travel up to 25%  ",least one year designing building data processing solutions etl pipelines varied data formats ideally company leverages machine models least two scala python apache spark sql directly relational database structures flat files write efficient database queries functions views include complex joins identification development custom indices professional software engineering practices best practices full software development life cycle coding standards code reviews source control management build processes testing continuous integration development operations good verbal written communication technical nontechnical stakeholders build pipelines ingest maintain complex data sets cerebri ais proprietary data stores use machine modeling develop maintain data ontologies key market segments collaborate data scientists perform exploratory data analysis map data fields proprietary data stores find signals client data collaborate clients develop pipeline infrastructure ask appropriate questions gain deep understanding client data write documentation discovery process software projects equally well team communicate complex ideas clearly team members clients travel,least one year designing building data processing solutions etl pipelines varied formats ideally company leverages machine models two scala python apache spark sql directly relational database structures flat files write efficient queries functions views include complex joins identification development custom indices professional software engineering practices best full life cycle coding standards code reviews source control management build processes testing continuous integration operations good verbal written communication technical nontechnical stakeholders ingest maintain sets cerebri ais proprietary stores use modeling develop ontologies key market segments collaborate scientists perform exploratory analysis map fields find signals client clients pipeline infrastructure ask appropriate questions gain deep understanding documentation discovery process projects equally well team communicate ideas clearly members travel
179,"Use advanced SQL skills to manage data Monitor database performance and tuning to improve query performance Design and automate data pipelines and integrate different data sources using SSIS Apply advanced skills to develop real-time data integrations in MS SQL Server Establish techniques to monitor data quality and implement remediation procedures Partner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler Provide advanced technical expertise to staff and end-users   Use advanced SQL skills to manage data Monitor database performance and tuning to improve query performance Design and automate data pipelines and integrate different data sources using SSIS Apply advanced skills to develop real-time data integrations in MS SQL Server Establish techniques to monitor data quality and implement remediation procedures Partner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler Provide advanced technical expertise to staff and end-users   ",use advanced sql manage data monitor database performance tuning improve query performance design automate data pipelines integrate different data sources ssis apply advanced develop realtime data integrations ms sql server establish techniques monitor data implement remediation procedures partner nontechnical users identify needsrequirements translating technical solutions demonstrate competency tsql advanced functions performance tuning job scheduler advanced technical expertise staff endusers use advanced sql manage data monitor database performance tuning improve query performance design automate data pipelines integrate different data sources ssis apply advanced develop realtime data integrations ms sql server establish techniques monitor data implement remediation procedures partner nontechnical users identify needsrequirements translating technical solutions demonstrate competency tsql advanced functions performance tuning job scheduler advanced technical expertise staff endusers,use advanced sql manage data monitor database performance tuning improve query design automate pipelines integrate different sources ssis apply develop realtime integrations ms server establish techniques implement remediation procedures partner nontechnical users identify needsrequirements translating technical solutions demonstrate competency tsql functions job scheduler expertise staff endusers
180,"   5+ years of experience with building end-to-end scalable production-grade data pipelines In-depth knowledge of data warehousing and master data management Expertise with modern data architecture, data modeling, and data management principles Hands-on experience with modern data pipeline technologies  Spark, Flink, Airflow, Beam, etc.   Solid foundation in data structures, algorithms, and software design Expertise with Relational and NoSQL databases to help teams best organize their data for analysis Skilled in OOP design and development, preferably in Python Advanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL BS/BA degree in Computer Science, Engineering or related fields or equivalent experience 5+ years of experience with building end-to-end scalable production-grade data pipelines In-depth knowledge of data warehousing and master data management Expertise with modern data architecture, data modeling, and data management principles Hands-on experience with modern data pipeline technologies  Spark, Flink, Airflow, Beam, etc.   Solid foundation in data structures, algorithms, and software design Expertise with Relational and NoSQL databases to help teams best organize their data for analysis Skilled in OOP design and development, preferably in Python Advanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL BS/BA degree in Computer Science, Engineering or related fields or equivalent experience",building endtoend scalable productiongrade data pipelines indepth data warehousing master data management expertise modern data architecture data modeling data management principles handson modern data pipeline technologies spark flink airflow beam solid foundation data structures algorithms software design expertise relational nosql databases help teams best organize data analysis skilled oop design development preferably python advanced writing understanding tuning plsql andor tsql bsba degree computer science engineering fields building endtoend scalable productiongrade data pipelines indepth data warehousing master data management expertise modern data architecture data modeling data management principles handson modern data pipeline technologies spark flink airflow beam solid foundation data structures algorithms software design expertise relational nosql databases help teams best organize data analysis skilled oop design development preferably python advanced writing understanding tuning plsql andor tsql bsba degree computer science engineering fields,building endtoend scalable productiongrade data pipelines indepth warehousing master management expertise modern architecture modeling principles handson pipeline technologies spark flink airflow beam solid foundation structures algorithms software design relational nosql databases help teams best organize analysis skilled oop development preferably python advanced writing understanding tuning plsql andor tsql bsba degree computer science engineering fields
181," Bachelorâs degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.  3 or more years of experience as a data engineer on enterprise-level data solutions, specifically as a Data Engineer or ETL Developer.  2 or more years of experience working with relational and unstructured databases and enterprise data warehouses, such as work with MySQL, PostgreSQL, MongoDB, SQL Server, or Oracle.  Experience with Spark, Presto, Hive and/or other map/reduce ""big data"" systems and services.  Experience in SQL and Python for scripting automation.     Design, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources Participate in data architecture discussions to understand target data structures, required data transformations and deliver data pipelines/ETL loading processes that meet requirements.  Perform detailed exploration of new internal and external source data to perform source-to-target mapping to inform the development of new data pipelines/flows.  Work in close collaboration with your data-minded colleagues focused on back-end  microservice  development, business intelligence reporting, machine learning and artificial intelligence models.  Investigate the root cause of data-related issues and implement viable, sustainable solutions to correct issues.  Perform database administration activities such as refreshes, updates, migrations, etc.  in support of data pipeline maintenance.    ",bachelors degree computer science information management data science analytics data engineer enterpriselevel data solutions specifically data engineer etl developer relational unstructured databases enterprise data warehouses mysql postgresql mongodb sql server oracle spark presto hive andor mapreduce big data systems services sql python scripting automation design develop implement data infrastructure pipelines collect connect centralize curate data various internal external data sources participate data architecture discussions understand target data structures data transformations deliver data pipelinesetl loading processes meet perform detailed exploration internal external source data perform sourcetotarget mapping inform development data pipelinesflows close collaboration dataminded colleagues focused backend microservice development business intelligence reporting machine artificial intelligence models investigate root cause datarelated issues implement viable sustainable solutions correct issues perform database administration activities refreshes updates migrations support data pipeline maintenance,bachelors degree computer science information management data analytics engineer enterpriselevel solutions specifically etl developer relational unstructured databases enterprise warehouses mysql postgresql mongodb sql server oracle spark presto hive andor mapreduce big systems services python scripting automation design develop implement infrastructure pipelines collect connect centralize curate various internal external sources participate architecture discussions understand target structures transformations deliver pipelinesetl loading processes meet perform detailed exploration source sourcetotarget mapping inform development pipelinesflows close collaboration dataminded colleagues focused backend microservice business intelligence reporting machine artificial models investigate root cause datarelated issues viable sustainable correct database administration activities refreshes updates migrations support pipeline maintenance
182," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ",expertise least one following domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale writing software one languages python java scala go building productiongrade data solutions relational nosql systems monitoringalerting capacity planning performance tuning technical consulting customerfacing role,expertise least one following domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale writing languages python java scala go relational nosql systems monitoringalerting capacity planning performance tuning consulting customerfacing role
183,"Bachelorâs or Masterâs in Computer Science, Computer Engineering, Electrical Engineering, Computer Information Systems, MIS, or relevant technology degree from a top-tier school.  Minimum of 5+ years SQL Server Development experience preferred.  Demonstrate knowledge and ability of database development skills including physical structure, overall architecture, and database analysis.  Solid software development skills in an object-oriented programming language  C , C++, Java, etc.  .  Programming skills with statistical software including Python, R, SAS or Matlab a plus.  Proven database design and implementation experience with the ability to provide end-to-end database solutions and resolve complex database issues.  Experience in database query optimization, performance tuning, and monitoring.  Expert knowledge of best practices in database design.  Strong time management skills with the ability to participate in multiple projects/work streams simultaneously.  Detail-oriented, organized, highly motivated and able to work independently and in a team environment.  Excellent verbal and written communications skills.  Self-starter who is capable of managing multiple projects and meeting deadlines.  Experience with research in securities and financial markets preferred.  Knowledge of finance/asset pricing is preferred.    Design, compile and manage efficiently large financial databases from a variety of financial data vendors.  Develop sophisticated code in object-oriented programming languages such as C , C++ or Python for historical portfolio simulations and tools for investment and performance analysis.  Conduct data analysis for projects related to research on equities and fixed income markets, retirement research, and other investment research.    ",bachelors masters computer science computer engineering electrical engineering computer information systems mis relevant technology degree toptier school minimum sql server development demonstrate database development physical structure overall architecture database analysis solid software development objectoriented programming language c c java programming statistical software python r sas matlab plus proven database design implementation endtoend database solutions resolve complex database issues database query optimization performance tuning monitoring expert best practices database design time management participate multiple projectswork streams simultaneously detailoriented organized highly motivated able independently team verbal written communications selfstarter capable managing multiple projects meeting deadlines research securities financial markets financeasset pricing design compile manage efficiently financial databases variety financial data vendors develop sophisticated code objectoriented programming languages c c python historical portfolio simulations tools investment performance analysis conduct data analysis projects research equities fixed income markets retirement research investment research,bachelors masters computer science engineering electrical information systems mis relevant technology degree toptier school minimum sql server development demonstrate database physical structure overall architecture analysis solid software objectoriented programming language c java statistical python r sas matlab plus proven design implementation endtoend solutions resolve complex issues query optimization performance tuning monitoring expert best practices time management participate multiple projectswork streams simultaneously detailoriented organized highly motivated able independently team verbal written communications selfstarter capable managing projects meeting deadlines research securities financial markets financeasset pricing compile manage efficiently databases variety data vendors develop sophisticated code languages historical portfolio simulations tools investment conduct equities fixed income retirement
184," Designing, Architecting, and Developing solutions leveraging big data technology  Open Source, AWS, or Microsoft  to ingest, process and analyze large, disparate data sets to exceed business requirements Unifying, enriching, and analyzing customer data to derive insights and opportunities Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches Seeking out information to learn about emerging methodologies and technologies Clarifying problems by driving to understand the true issue Looking for opportunities for improving methods and outcomes Applying data driven approach  KPIs  in tying technology solutions to specific business outcomes Collaborating, influencing and building consensus through constructive relationships and effective listening Solving problems by incorporating data into decision making    ",designing architecting developing solutions leveraging big data technology open source aws microsoft ingest process analyze disparate data sets exceed business unifying enriching analyzing customer data derive insights opportunities leveraging inhouse data platforms needed recommending building data platformssolutions exceed business clearly communicating findings recommendations opportunities improve data systems solutions demonstrating deep understanding big data technology concepts tools features functions benefits different approaches seeking information learn emerging methodologies technologies clarifying problems driving understand true issue looking opportunities improving methods outcomes applying data driven approach kpis tying technology solutions specific business outcomes collaborating influencing building consensus constructive relationships effective listening solving problems incorporating data decision making,designing architecting developing solutions leveraging big data technology open source aws microsoft ingest process analyze disparate sets exceed business unifying enriching analyzing customer derive insights opportunities inhouse platforms needed recommending building platformssolutions clearly communicating findings recommendations improve systems demonstrating deep understanding concepts tools features functions benefits different approaches seeking information learn emerging methodologies technologies clarifying problems driving understand true issue looking improving methods outcomes applying driven approach kpis tying specific collaborating influencing consensus constructive relationships effective listening solving incorporating decision making
185,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
186,"   Ingestion of data from multiple, unstructured sources using multiple analytics tools Implementing ETL process Monitoring performance and advising any necessary infrastructure changes Defining data retention policies   Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey. ",ingestion data multiple unstructured sources multiple analytics tools implementing etl process monitoring performance advising necessary infrastructure changes defining data retention policies voted significantly services firms clients business impact execution predictability organizational commitment recent trianz wide client satisfaction survey,ingestion data multiple unstructured sources analytics tools implementing etl process monitoring performance advising necessary infrastructure changes defining retention policies voted significantly services firms clients business impact execution predictability organizational commitment recent trianz wide client satisfaction survey
187,"  Collaborate with peers on requirements, designs, code reviews, and testing Produce designs and rough estimates, and implement features based on product requirements Deliver efficient, maintainable, robust Java/Scala based microservices Produce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features Productize and operationalize machine learning algorithms Actively engage in technology discovery that can be applied to the product   7-10 years of professional software development experience 2+ years of data engineering or related experience Strong Java and/or Scala experience Experience with Agile development practices and continuous delivery Proficient understanding of distributed computing principles.  microservice architectures and patterns Experience with integration of data from multiple data sources Experience writing unit and integration tests Great communication skills BS in Computer Science or a related experience ",collaborate peers designs code reviews testing produce designs rough estimates implement features based product deliver efficient maintainable robust javascala based microservices produce unit endtoend tests improve code maximize code coverage existing features productize operationalize machine algorithms actively engage technology discovery applied product professional software development data engineering java andor scala agile development practices continuous delivery proficient understanding distributed computing principles microservice architectures patterns integration data multiple data sources writing unit integration tests great communication bs computer science,collaborate peers designs code reviews testing produce rough estimates implement features based product deliver efficient maintainable robust javascala microservices unit endtoend tests improve maximize coverage existing productize operationalize machine algorithms actively engage technology discovery applied professional software development data engineering java andor scala agile practices continuous delivery proficient understanding distributed computing principles microservice architectures patterns integration multiple sources writing great communication bs computer science
188,  Experience with RDBMS applications  SQL Server preferred  Good communication skills and experience working with cross-functional teams Exposure to the concepts of data warehouse design SQL programming familiarity in large RDBMS systems  T-SQL preferred   Troubleshoot and resolve issues as they arise related to all BI Tools Manage iteration and release cycles and deployments Assist in data modeling and design sessions Proactively maintain documentation and training materials  ,rdbms applications sql server good communication crossfunctional teams exposure concepts data warehouse design sql programming familiarity rdbms systems tsql troubleshoot resolve issues arise bi tools manage iteration release cycles deployments assist data modeling design sessions proactively maintain documentation training materials,rdbms applications sql server good communication crossfunctional teams exposure concepts data warehouse design programming familiarity systems tsql troubleshoot resolve issues arise bi tools manage iteration release cycles deployments assist modeling sessions proactively maintain documentation training materials
189,"Five or more years of professional experience as data engineer.  Bachelorâs degree in Computer Science or equivalent experience.  Demonstrated experience in data warehousing and ETL development.  Experience building complex data pipelines using large, disparate data sources.  Demonstrated expert knowledge in SQL.  Demonstrated experience working with relational databases such as Oracle, Postgres and other modern database technologies.  Proficiency in modern programming languages such as Python, R, Java.  Thorough understanding of data movement and transformation tools, such as Informatica, Datastage or equivalent.  Demonstrated experience in selecting tools, methods, techniques, and evaluation criteria for designing optimal data engineering solutions.  Demonstrated experience in leading complex technical projects, including assigning tasks and selecting team members.  Ability to make technical presentations to teams, focus groups, management, and governance committees.  Excellent customer service, communication and collaboration skills.    Design, develop, and automate scalable data engineering solutions by leveraging cloud infrastructure.  Extend or migrate existing data pipelines to new cloud environment.  Lead technical projects involving design and development of data pipelines for complex datasets.  Document project plans, outline tasks and milestones, provide estimation of effort.  Work closely with business partners to devise and manage data pipelines, load frequency, data delivery mechanisms, and performance tuning.  Identify and implement best practices for data engineering and software development to ensure quality delivery of enterprise solutions.  Help enable team alignment by participating in code reviews, change management and team meetings.  Develop and maintain detailed technical documentation of data engineering solutions.  Collaborate with key stakeholders, both internal and external, including enterprise data architect, data modelers, and subject matter experts  SMEs .    ",five professional data engineer bachelors degree computer science demonstrated data warehousing etl development building complex data pipelines disparate data sources demonstrated expert sql demonstrated relational databases oracle postgres modern database technologies proficiency modern programming languages python r java thorough understanding data movement transformation tools informatica datastage demonstrated selecting tools methods techniques evaluation criteria designing optimal data engineering solutions demonstrated leading complex technical projects assigning tasks selecting team members make technical presentations teams focus groups management governance committees customer service communication collaboration design develop automate scalable data engineering solutions leveraging cloud infrastructure extend migrate existing data pipelines cloud lead technical projects involving design development data pipelines complex datasets document project plans outline tasks milestones estimation effort closely business partners devise manage data pipelines load frequency data delivery mechanisms performance tuning identify implement best practices data engineering software development delivery enterprise solutions help enable team alignment participating code reviews change management team meetings develop maintain detailed technical documentation data engineering solutions collaborate key stakeholders internal external enterprise data architect data modelers subject matter experts smes,five professional data engineer bachelors degree computer science demonstrated warehousing etl development building complex pipelines disparate sources expert sql relational databases oracle postgres modern database technologies proficiency programming languages python r java thorough understanding movement transformation tools informatica datastage selecting methods techniques evaluation criteria designing optimal engineering solutions leading technical projects assigning tasks team members make presentations teams focus groups management governance committees customer service communication collaboration design develop automate scalable leveraging cloud infrastructure extend migrate existing lead involving datasets document project plans outline milestones estimation effort closely business partners devise manage load frequency delivery mechanisms performance tuning identify implement best practices software enterprise help enable alignment participating code reviews change meetings maintain detailed documentation collaborate key stakeholders internal external architect modelers subject matter experts smes
190,"   Act as a trusted technical advisor to customers and solve complex Big Data challenges.  Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.  Travel up to 30% of the time.  Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.   ",act trusted technical advisor customers solve complex big data challenges create deliver best practices recommendations tutorials blog articles sample code technical presentations adapting different levels key business technical stakeholders travel time communicate effectively via video conferencing meetings technical reviews onsite delivery activities,act trusted technical advisor customers solve complex big data challenges create deliver best practices recommendations tutorials blog articles sample code presentations adapting different levels key business stakeholders travel time communicate effectively via video conferencing meetings reviews onsite delivery activities
191,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
192,   You will design and create multi-tenant systems capable of loading and transforming a large volume of structured and semi-structured fast moving data Build robust and scalable data infrastructure  both batch processing and real-time  to support needs from internal and external users Build Data Pipelines Run ETL into Hadoop/Elastic Search  ,design create multitenant systems capable loading transforming volume structured semistructured fast moving data build robust scalable data infrastructure batch processing realtime support needs internal external users build data pipelines run etl hadoopelastic search,design create multitenant systems capable loading transforming volume structured semistructured fast moving data build robust scalable infrastructure batch processing realtime support needs internal external users pipelines run etl hadoopelastic search
193," Strong analytical and critical thinking skills Autonomous personality.  Weâll help guide you, but we wonât micromanage you.  We expect integrity and results.  Your work and deliverables will speak for themselves Strong written and verbal communication skills Enjoy challenging and thought-provoking work and have a strong desire to learn and progress Ability to manage multiple tasks and requests Must demonstrate a positive, team-focused attitude Ability to react positively under pressure to meet tight deadlines You listen to the input of your team members and take diverse perspectives into account to approach challenges from multiple angles Structured, disciplined approach to work, with attention to detail Flexible â able to meet changing requirements and priorities Maintenance of up-to-date knowledge in the appropriate technical areas Able to work in a global, multicultural environment Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Responsible to collect, process, and compute business metrics from activity & persisted data using Python/Spark Process, cleanse, and verify the integrity of data used for analysis; optimize data for consumption Build scalable OLAP backend storage for data in PB scale Develop data set processes for data discovery, modeling, mining, and archival  Bachelorâs or Master's Degree of Statistics, Computer Science or other similar advanced degrees from a top tier educational institution preferred ",analytical critical thinking autonomous personality well help guide wont micromanage expect integrity results deliverables speak written verbal communication enjoy challenging thoughtprovoking desire learn progress manage multiple tasks requests must demonstrate positive teamfocused attitude react positively pressure meet tight deadlines listen input team members take diverse perspectives account approach challenges multiple angles structured disciplined approach attention detail flexible able meet changing priorities maintenance uptodate appropriate technical areas able global multicultural build robust data pipelines public cloud aws kinesis kafka lambda technologies responsible collect process compute business metrics activity persisted data pythonspark process cleanse verify integrity data used analysis optimize data consumption build scalable olap backend storage data pb scale develop data set processes data discovery modeling mining archival bachelors masters degree statistics computer science similar advanced degrees top tier educational institution,analytical critical thinking autonomous personality well help guide wont micromanage expect integrity results deliverables speak written verbal communication enjoy challenging thoughtprovoking desire learn progress manage multiple tasks requests must demonstrate positive teamfocused attitude react positively pressure meet tight deadlines listen input team members take diverse perspectives account approach challenges angles structured disciplined attention detail flexible able changing priorities maintenance uptodate appropriate technical areas global multicultural build robust data pipelines public cloud aws kinesis kafka lambda technologies responsible collect process compute business metrics activity persisted pythonspark cleanse verify used analysis optimize consumption scalable olap backend storage pb scale develop set processes discovery modeling mining archival bachelors masters degree statistics computer science similar advanced degrees top tier educational institution
194," Strong analytical and critical thinking skills Autonomous personality.  Weâll help guide you, but we wonât micromanage you.  We expect integrity and results.  Your work and deliverables will speak for themselves Strong written and verbal communication skills Enjoy challenging and thought-provoking work and have a strong desire to learn and progress Ability to manage multiple tasks and requests Must demonstrate a positive, team-focused attitude Ability to react positively under pressure to meet tight deadlines You listen to the input of your team members and take diverse perspectives into account to approach challenges from multiple angles Structured, disciplined approach to work, with attention to detail Flexible â able to meet changing requirements and priorities Maintenance of up-to-date knowledge in the appropriate technical areas Able to work in a global, multicultural environment Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Responsible to collect, process, and compute business metrics from activity & persisted data using Python/Spark Process, cleanse, and verify the integrity of data used for analysis; optimize data for consumption Build scalable OLAP backend storage for data in PB scale Develop data set processes for data discovery, modeling, mining, and archival  Bachelorâs or masterâs degree of Statistics, Computer Science or other similar advanced degrees from a top tier educational institution preferred ",analytical critical thinking autonomous personality well help guide wont micromanage expect integrity results deliverables speak written verbal communication enjoy challenging thoughtprovoking desire learn progress manage multiple tasks requests must demonstrate positive teamfocused attitude react positively pressure meet tight deadlines listen input team members take diverse perspectives account approach challenges multiple angles structured disciplined approach attention detail flexible able meet changing priorities maintenance uptodate appropriate technical areas able global multicultural build robust data pipelines public cloud aws kinesis kafka lambda technologies responsible collect process compute business metrics activity persisted data pythonspark process cleanse verify integrity data used analysis optimize data consumption build scalable olap backend storage data pb scale develop data set processes data discovery modeling mining archival bachelors masters degree statistics computer science similar advanced degrees top tier educational institution,analytical critical thinking autonomous personality well help guide wont micromanage expect integrity results deliverables speak written verbal communication enjoy challenging thoughtprovoking desire learn progress manage multiple tasks requests must demonstrate positive teamfocused attitude react positively pressure meet tight deadlines listen input team members take diverse perspectives account approach challenges angles structured disciplined attention detail flexible able changing priorities maintenance uptodate appropriate technical areas global multicultural build robust data pipelines public cloud aws kinesis kafka lambda technologies responsible collect process compute business metrics activity persisted pythonspark cleanse verify used analysis optimize consumption scalable olap backend storage pb scale develop set processes discovery modeling mining archival bachelors masters degree statistics computer science similar advanced degrees top tier educational institution
195,"M. S.  in Computer Science, Informatics, Mathematics, Electronic/Electrical Engineering or other relevant field with an emphasis on data analytics. Experience with big data technologies such as Hadoop, Apache Spark, NoSQL databases. Strong computer science grounding, with knowledge of data structures, algorithms and computer architectures. Proficiency developing in one or more languages such as C++, Python or Java. Self-starting, requiring minimal supervision with strong problem-solving skills. Excellent communication and teamwork skills.     ",computer science informatics mathematics electronicelectrical engineering relevant emphasis data analytics big data technologies hadoop apache spark nosql databases computer science grounding data structures algorithms computer architectures proficiency developing one languages c python java selfstarting requiring minimal supervision problemsolving communication teamwork,computer science informatics mathematics electronicelectrical engineering relevant emphasis data analytics big technologies hadoop apache spark nosql databases grounding structures algorithms architectures proficiency developing one languages c python java selfstarting requiring minimal supervision problemsolving communication teamwork
196,3+ years of related work experience in Data Engineering or Data Warehousing  Work as part of a team to develop Cloud Data and Analytics solutions  ,data engineering data warehousing part team develop cloud data analytics solutions,data engineering warehousing part team develop cloud analytics solutions
197," A bachelor's degree or related field with at least 3 â 8 years of experience with Big Data or Hadoop tools such as Spark, Hive, Kafka and MapReduce, Spark, R, Python Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasks Experience in AI/ML on big data platforms preferred  Slid applied statistics skills, such as identifying distributions, statistical testing, regression, etc.  Prficiency querying both structured and unstructured data Experience with Pythn and Deep Learning packages  Keras, Tensorflow, mxnet  and NLP packages  nltk, spacy  is a plus Experience deplying models in production environments Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space Hands-on experience with Apache Spark and its components  Streaming, SQL, MLLib  is a strong advantage Excellent written and oral communication skills; must be able to effectively articulate technical concepts to non-technical audiences Software development experience using  Database prgramming using any flavor of SQL Expertise in relatinal and dimensional modelling   Understand problems from a client's point of view, build and execute solid analytics work plans, gather and organize large and complex data sets, perform relevant analyses  data exploration and statistical modeling , manage priorities and deadlines, foster teamwork in interactions, develop client relationships with client counterparts, and communicate hypotheses and findings in a structured way Partner with business teams in identifying business requirements and developing advanced analytical solutions to complex problems by utilizing statistical models and machine learning techniques and algorithms  ",bachelors degree least big data hadoop tools spark hive kafka mapreduce spark r python within linux computing use command line tools shellpython scripting automating common tasks aiml big data platforms slid applied statistics identifying distributions statistical testing regression prficiency querying structured unstructured data pythn deep packages keras tensorflow mxnet nlp packages nltk spacy plus deplying models production environments integration application development datawarehouse projects across technologies used enterprise space handson apache spark components streaming sql mllib advantage written oral communication must able effectively articulate technical concepts nontechnical audiences software development database prgramming flavor sql expertise relatinal dimensional modelling understand problems clients point view build execute solid analytics plans gather organize complex data sets perform relevant analyses data exploration statistical modeling manage priorities deadlines foster teamwork interactions develop client relationships client counterparts communicate hypotheses findings structured way partner business teams identifying business developing advanced analytical solutions complex problems utilizing statistical models machine techniques algorithms,bachelors degree least big data hadoop tools spark hive kafka mapreduce r python within linux computing use command line shellpython scripting automating common tasks aiml platforms slid applied statistics identifying distributions statistical testing regression prficiency querying structured unstructured pythn deep packages keras tensorflow mxnet nlp nltk spacy plus deplying models production environments integration application development datawarehouse projects across technologies used enterprise space handson apache components streaming sql mllib advantage written oral communication must able effectively articulate technical concepts nontechnical audiences software database prgramming flavor expertise relatinal dimensional modelling understand problems clients point view build execute solid analytics plans gather organize complex sets perform relevant analyses exploration modeling manage priorities deadlines foster teamwork interactions develop client relationships counterparts communicate hypotheses findings way partner business teams developing advanced analytical solutions utilizing machine techniques algorithms
198,"Qualifications  7 â 10  3 years min relevant experience in the role  years experience, Bachelorâs Degree. Must have experience in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Lifecycle and Data Management. Should be proficient in Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.     ",qualifications min relevant role bachelors degree must software engineering techniques software engineering architecture software engineering lifecycle data management proficient business analysis business software engineering leadership architecture technical solution design,qualifications min relevant role bachelors degree must software engineering techniques architecture lifecycle data management proficient business analysis leadership technical solution design
199," Previous management experience and successfully leading a team of direct reports Proven track record of effectively leading and managing employees, managing performance, setting goals for team/individuals and provide mentoring.  Strong Tableau, Adobe Analytics other BI solution Expert Oracle and Vertica skillsets Experience using JIRA and Agile Project Management software Experience using GitHub, Bit Bucket, or other code repository solution Strong written, verbal communication and presentation skills Ability to explain complex technical issues in a way that non-technical people may understand Able to work in a global, multicultural environment Self-motivated.  Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player Enjoy challenging and thought provoking work and have a strong desire to learn and progress Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Implement architectures to handle web-scale data and its organization Execute strategies that inform data design and architecture partnering with enterprise standard Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Assist in creating documents that ensure consistency in development across the online organization.  Implements and improves core software infrastructure Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem.  Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools  such as control charts and scorecards , readiness and adoption of data w/in the organization Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding Serve as strong advocate to improve analytical capability across the organization Communicate with various business areas and to gather and prioritize their business requirements Support various reporting and BI solutions  Tableau, Power BI, Salesforce Einstein  Manage application and data integration platforms  Informatica, Talend  Manage the full life cycle of development/reporting/integration projects  planning, design, develop, testing and rollout Manage solution providers, define sourcing approach and manage the providers Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement Gain adoption of architecture processes, standards and procedures  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience ",previous management successfully leading team direct reports proven track record effectively leading managing employees managing performance setting goals teamindividuals mentoring tableau adobe analytics bi solution expert oracle vertica skillsets jira agile project management software github bit bucket code repository solution written verbal communication presentation explain complex technical issues way nontechnical people may understand able global multicultural selfmotivated capable little supervision react positively pressure meet tight deadlines able independently team player enjoy challenging thought provoking desire learn progress development teams project leadersstakeholders technical solutions enable business capabilities supports data science community enabling data availability environments advanced analytical capabilities maintains broad understanding implementation integration interconnectivity issues emerging technologies define data strategies assist decisionmaking process selection software architecture solutions implement architectures handle webscale data organization execute strategies inform data design architecture partnering enterprise standard build robust data pipelines public cloud aws kinesis kafka lambda technologies assist creating documents consistency development across online organization implements improves core software infrastructure deep expertise sql language python hadoop ecosystem andor spark ecosystem writing complex programs implementing architectures enabling automation environments develop maintain business reporting ensuring reliability performance delivery performance management tools control charts scorecards readiness adoption data win organization consolidate standardize control changes capacity management data metric definitions ownership accountability taxonomy alignment understanding serve advocate improve analytical capability across organization communicate various business areas gather prioritize business support various reporting bi solutions tableau power bi salesforce einstein manage application data integration platforms informatica talend manage full life cycle developmentreportingintegration projects planning design develop testing rollout manage solution providers define sourcing approach manage providers create manage data applications technology architecture documentation design artifacts across teams deliver meaningful reference architectures outline architecture principles best practices technology advancement gain adoption architecture processes standards procedures bs computer science applied mathematics physics statistics area study data sciences data mining relevant,previous management successfully leading team direct reports proven track record effectively managing employees performance setting goals teamindividuals mentoring tableau adobe analytics bi solution expert oracle vertica skillsets jira agile project software github bit bucket code repository written verbal communication presentation explain complex technical issues way nontechnical people may understand able global multicultural selfmotivated capable little supervision react positively pressure meet tight deadlines independently player enjoy challenging thought provoking desire learn progress development teams leadersstakeholders solutions enable business capabilities supports data science community enabling availability environments advanced analytical maintains broad understanding implementation integration interconnectivity emerging technologies define strategies assist decisionmaking process selection architecture implement architectures handle webscale organization execute inform design partnering enterprise standard build robust pipelines public cloud aws kinesis kafka lambda creating documents consistency across online implements improves core infrastructure deep expertise sql language python hadoop ecosystem andor spark writing programs implementing automation develop maintain reporting ensuring reliability delivery tools control charts scorecards readiness adoption win consolidate standardize changes capacity metric definitions ownership accountability taxonomy alignment serve advocate improve capability communicate various areas gather prioritize support power salesforce einstein manage application platforms informatica talend full life cycle developmentreportingintegration projects planning testing rollout providers sourcing approach create applications technology documentation artifacts deliver meaningful reference outline principles best practices advancement gain processes standards procedures bs computer applied mathematics physics statistics area study sciences mining relevant
200," Exceptional interpersonal skills, including teamwork, facilitation, and negotiation.  Communicates IT requirements and guidelines to vendor partners.  Applies multiple technical solutions to business problems.  Quickly comprehends the functions and capabilities of new technology.  Excellent written and oral communications skills.  Design and work with Agile teams to implement highly scalable Enterprise approaches for metadata, taxonomy, tagging, folder structures providing the lightweight controls to ensure sustainable data quality and efficiencies Defines non-functional requirements including data cleansing and validation Mentors and coaches other members of the agile and\or Run team Interfaces with the Product Owner and Technology partners at the Program level to define and estimate features for agile teams Works within the SAFe Agile framework and employs ITIL best practices Client-facing, senior strategic support Content assessment.  Providing content revision/creation recommendations that help the business and its customers achieve their goals  based on brand documentation, competitive assessments, audience segmentation, SEO data, and site metrics  Generating and/or overseeing an inventory of relevant client content assets Creating and/or reviewing site structure and nomenclature for the most intuitive presentation of content, usually partnering with UX Creating taxonomy and tagging strategies Ensuring content is structured properly for any relevant backend systems including CMS, DAM, eCommerce and/or PIM Contributing to technical system evaluations  such as CMS, DAM, and/or PIM  from a content perspective in terms of requirements Establishing and/or maintaining editorial standards and accuracy/quality of content Overseeing content migration and creating or reviewing associated documentation Contributing to definition and management of, as well as periodic updates to, content governance and workflow  including content creation, content entry, publication, and decommission  as well as localization strategies Responsible for management of, as well as periodic updates to, personalization and content tagging strategies for the site Assists in production support and maintenance of applications as needed  Bachelor's degree in English, Library Science, Journalism, Technical Writing, Marketing or equivalent military experience preferred ",exceptional interpersonal teamwork facilitation negotiation communicates guidelines vendor partners applies multiple technical solutions business problems quickly comprehends functions capabilities technology written oral communications design agile teams implement highly scalable enterprise approaches metadata taxonomy tagging folder structures providing lightweight controls sustainable data efficiencies defines nonfunctional data cleansing validation mentors coaches members agile andor run team interfaces product owner technology partners program level define estimate features agile teams works within safe agile framework employs itil best practices clientfacing senior strategic support content assessment providing content revisioncreation recommendations help business customers achieve goals based brand documentation competitive assessments audience segmentation seo data site metrics generating andor overseeing inventory relevant client content assets creating andor reviewing site structure nomenclature intuitive presentation content usually partnering ux creating taxonomy tagging strategies ensuring content structured properly relevant backend systems cms dam ecommerce andor pim contributing technical evaluations cms dam andor pim content perspective terms establishing andor maintaining editorial standards accuracyquality content overseeing content migration creating reviewing associated documentation contributing definition management well periodic updates content governance workflow content creation content entry publication decommission well localization strategies responsible management well periodic updates personalization content tagging strategies site assists production support maintenance applications needed bachelors degree english library science journalism technical writing marketing military,exceptional interpersonal teamwork facilitation negotiation communicates guidelines vendor partners applies multiple technical solutions business problems quickly comprehends functions capabilities technology written oral communications design agile teams implement highly scalable enterprise approaches metadata taxonomy tagging folder structures providing lightweight controls sustainable data efficiencies defines nonfunctional cleansing validation mentors coaches members andor run team interfaces product owner program level define estimate features works within safe framework employs itil best practices clientfacing senior strategic support content assessment revisioncreation recommendations help customers achieve goals based brand documentation competitive assessments audience segmentation seo site metrics generating overseeing inventory relevant client assets creating reviewing structure nomenclature intuitive presentation usually partnering ux strategies ensuring structured properly backend systems cms dam ecommerce pim contributing evaluations perspective terms establishing maintaining editorial standards accuracyquality migration associated definition management well periodic updates governance workflow creation entry publication decommission localization responsible personalization assists production maintenance applications needed bachelors degree english library science journalism writing marketing military
201,"Qualifications  3-7 years  2 years min relevant experience in the role  experience; Bachelorâs degreeShould be proficient in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Life cycle and Data Management. Should have progressing skills on Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.     ",qualifications min relevant role bachelors degreeshould proficient software engineering techniques software engineering architecture software engineering life cycle data management progressing business analysis business software engineering leadership architecture technical solution design,qualifications min relevant role bachelors degreeshould proficient software engineering techniques architecture life cycle data management progressing business analysis leadership technical solution design
202,Monthly employee social functionsOn premises gymDaily afternoon yoga stretchingFriendly Monthly employee social functionsOn premises gymDaily afternoon yoga stretchingFriendly   ,monthly employee social functionson premises gymdaily afternoon yoga stretchingfriendly monthly employee social functionson premises gymdaily afternoon yoga stretchingfriendly,monthly employee social functionson premises gymdaily afternoon yoga stretchingfriendly
203,"Qualification  3-7 years  2 years min relevant experience in the role  experience, Bachelorâs DegreeCertification  Should have or seeking SE Level 1Should have progressing knowledge in Business Analysis, Business Knowledge, Software Engineering, Testing, Data Management, Architecture Knowledge and Technical Solution Design    ",qualification min relevant role bachelors degreecertification seeking se level progressing business analysis business software engineering testing data management architecture technical solution design,qualification min relevant role bachelors degreecertification seeking se level progressing business analysis software engineering testing data management architecture technical solution design
204,"3+ years of professional solution delivery experience and a Bachelor of Computer Science, MIS or equivalent degree; without a degree, three additional years of relevant professional experience  6+ years in total  2+ years designing and building modern data pipelines and data streams 2+ years designing Azure data storage solutions  SQL Database, SQL Data Warehouse, Cosmos DB, Data Lake Storage  2+ years developing data ingestion, data processing and data optimization  Databricks, Data Factory, Informatica, PolyBase  Understating of Master Data Management  MDM  and data quality tools and processes Understanding of Lambda and Kappa architecture patterns Understanding of DevOps including CI/CD Experience developing software architectures and key software components High-level understanding of common authentication patterns and flow including single sign-on and OAuth Experience with Agile/Scrum methodology Ability to apply technology and consulting to solve a client business problem Sufficient depth and breadth of technical knowledge to be individually responsible for the design and scope of deliverables within a field of expertise Able to communicate and present complex issues with preciseness, assurance, and confidence A disciplined approach to software development and problem solving Passion for technology and a high technical aptitude Insightful and always looking for breakthrough ideas Self-sufficient, high integrity, more than just competent Demonstrates the use of consulting skills including  questioning, listening, ideas development, permission and rapport, and influencing Ability to conduct/lead oral status/technical interchange meetings with clients Owns and produces customer documentation.  Ability to translate technical details into concise and easy to understand written form Ability to write relevant components of a proposal document  e. g.  participate in RFIs and RFPs including answering specific technology related questions and coming up with initial high-level technical design and architecture including any necessary Visio diagrams and PowerPoint slides  Ability to translate verbal requirements from face to face client meetings into requirements documents, statements of work, and proposals  Technical and thought-leadership of Azure data design and implementation engagements Contribute to customer elaboration and discovery sessions to inform solution design Lead data architecture design, solution structures and component design Craft high-quality Visio documents to communicate thoroughness of vision for system architecture topology, component design, networking and security/authentication of proposed solutions Ensure that technical software development process is followed on the project, be familiar with industry best practices for software development and agile practices Understand a broad spectrum of technology in order to provide part or all of a detailed technical design which meets customer requirements Act as lead or technical architect on customer projects, closely aligned with the Microsoft Azure Practice to deliver a wide-range of PaaS-focused solutions Provide technical leadership for on-premises and external integration points Communicate across the clientâs community â consistently viewed as adding value Contribute knowledge, tools, and positivity to the greater Perficient culture and community Serve as a technical leader and mentor  ",professional solution delivery bachelor computer science mis degree without degree three additional relevant professional total designing building modern data pipelines data streams designing azure data storage solutions sql database sql data warehouse cosmos db data lake storage developing data ingestion data processing data optimization databricks data factory informatica polybase understating master data management mdm data tools processes understanding lambda kappa architecture patterns understanding devops cicd developing software architectures key software components highlevel understanding common authentication patterns flow single signon oauth agilescrum methodology apply technology consulting solve client business problem sufficient depth breadth technical individually responsible design scope deliverables within expertise able communicate present complex issues preciseness assurance confidence disciplined approach software development problem solving passion technology technical aptitude insightful always looking breakthrough ideas selfsufficient integrity competent demonstrates use consulting questioning listening ideas development permission rapport influencing conductlead oral statustechnical interchange meetings clients owns produces customer documentation translate technical details concise easy understand written form write relevant components proposal document e g participate rfis rfps answering specific technology questions coming initial highlevel technical design architecture necessary visio diagrams powerpoint slides translate verbal face face client meetings documents statements proposals technical thoughtleadership azure data design implementation engagements contribute customer elaboration discovery sessions inform solution design lead data architecture design solution structures component design craft highquality visio documents communicate thoroughness vision architecture topology component design networking securityauthentication proposed solutions technical software development process followed project familiar industry best practices software development agile practices understand broad spectrum technology order part detailed technical design meets customer act lead technical architect customer projects closely aligned microsoft azure practice deliver widerange paasfocused solutions technical leadership onpremises external integration points communicate across clients community consistently viewed adding value contribute tools positivity greater perficient culture community serve technical leader mentor,professional solution delivery bachelor computer science mis degree without three additional relevant total designing building modern data pipelines streams azure storage solutions sql database warehouse cosmos db lake developing ingestion processing optimization databricks factory informatica polybase understating master management mdm tools processes understanding lambda kappa architecture patterns devops cicd software architectures key components highlevel common authentication flow single signon oauth agilescrum methodology apply technology consulting solve client business problem sufficient depth breadth technical individually responsible design scope deliverables within expertise able communicate present complex issues preciseness assurance confidence disciplined approach development solving passion aptitude insightful always looking breakthrough ideas selfsufficient integrity competent demonstrates use questioning listening permission rapport influencing conductlead oral statustechnical interchange meetings clients owns produces customer documentation translate details concise easy understand written form write proposal document e g participate rfis rfps answering specific questions coming initial necessary visio diagrams powerpoint slides verbal face documents statements proposals thoughtleadership implementation engagements contribute elaboration discovery sessions inform lead structures component craft highquality thoroughness vision topology networking securityauthentication proposed process followed project familiar industry best practices agile broad spectrum order part detailed meets act architect projects closely aligned microsoft practice deliver widerange paasfocused leadership onpremises external integration points across community consistently viewed adding value positivity greater perficient culture serve leader mentor
205," 5 or more years of experience required in related field  developing and implementing analytical solutions in Finance, Marketing, Sales or Operations .  3 or more years of experience required if candidate possesses a related advanced degree.  Requires strong skills in SQL writing and query optimization.  Requires experience building data workflows, manipulation of large data sets, or developing data pipelines in analytical tools such as Informatica, Alteryx, Tableau Prep, SQL, SSIS, etc.  Requires strong skills and experience with reporting and data visualization in analytical tools such as Tableau, Prep, SQL, etc.  Requires effective proficiency in teamwork, communication, presentation, and time management to work effectively with teams throughout organization, including strong verbal and written communication.  Experience manipulating large datasets and the ability to extrapolate conclusions from the data.  Demonstrated problem solving and analytical thinking skills.  Excellent interpersonal, leadership, presentation, and collaborative skills to work effectively with teams throughout organization.  BS/BA degree in related discipline    ",developing implementing analytical solutions finance marketing sales operations candidate possesses advanced degree requires sql writing query optimization requires building data workflows manipulation data sets developing data pipelines analytical tools informatica alteryx tableau prep sql ssis requires reporting data visualization analytical tools tableau prep sql requires effective proficiency teamwork communication presentation time management effectively teams throughout organization verbal written communication manipulating datasets extrapolate conclusions data demonstrated problem solving analytical thinking interpersonal leadership presentation collaborative effectively teams throughout organization bsba degree discipline,developing implementing analytical solutions finance marketing sales operations candidate possesses advanced degree requires sql writing query optimization building data workflows manipulation sets pipelines tools informatica alteryx tableau prep ssis reporting visualization effective proficiency teamwork communication presentation time management effectively teams throughout organization verbal written manipulating datasets extrapolate conclusions demonstrated problem solving thinking interpersonal leadership collaborative bsba discipline
206,"  Technology certifications such as Amazon Certified Solutions Architect Proficiency in programming in Spark, R and/or ML packages Exposure to applications developed to support manufacturing quality Experience with MS Dynamics CRM, MicroStrategy BI tools, SAP ERP Consulting experience  Drive design and development of internal data pipeline architecture to service BI and Analytics capabilities Work with key stakeholders including Product, Sales, Quality Engineering and Marketing to assist with data-related technical needs and support their data infrastructure needs.  Ensure operational resilience, stability, and scalability of our enterprise data platform by conduct continuous hardening activates that increase uptime, data quality and reduce cost Ensure tight data platform security during data transport and while at rest Build hybrid cloud/prem data platform to enable self-service Business Intelligence and Analytics functions Conduct data profiling and analysis of complex data sets to discover how to meet functional / non-functional requirements Institute data quality monitoring and alerting platform operations Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics Drive solutions for meta data and data lineage management Drive innovation by recommending and driving adoption of new technologies that provide competitive data advantages for enterprise Contributes to agile team alignment and is committed to constant improvement efforts by participating in team ceremonies sprint planning, stand-ups, backlog grooming and retrospectives Ensure technical delivery of detailed feature/story level solutions that satisfies the IT roadmapâs acceptance criteria Maintains professional and technical knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; participating in professional societies  ",technology certifications amazon certified solutions architect proficiency programming spark r andor ml packages exposure applications developed support manufacturing ms dynamics crm microstrategy bi tools sap erp consulting drive design development internal data pipeline architecture service bi analytics capabilities key stakeholders product sales engineering marketing assist datarelated technical needs support data infrastructure needs operational resilience stability scalability enterprise data platform conduct continuous hardening activates increase uptime data reduce cost tight data platform security data transport rest build hybrid cloudprem data platform enable selfservice business intelligence analytics functions conduct data profiling analysis complex data sets discover meet functional nonfunctional institute data monitoring alerting platform operations build analytics tools utilize data pipeline actionable insights customer acquisition operational efficiency key business performance metrics drive solutions meta data data lineage management drive innovation recommending driving adoption technologies competitive data advantages enterprise contributes agile team alignment committed constant improvement efforts participating team ceremonies sprint planning standups backlog grooming retrospectives technical delivery detailed featurestory level solutions satisfies roadmaps acceptance criteria maintains professional technical attending educational workshops reviewing professional publications establishing personal networks participating professional societies,technology certifications amazon certified solutions architect proficiency programming spark r andor ml packages exposure applications developed support manufacturing ms dynamics crm microstrategy bi tools sap erp consulting drive design development internal data pipeline architecture service analytics capabilities key stakeholders product sales engineering marketing assist datarelated technical needs infrastructure operational resilience stability scalability enterprise platform conduct continuous hardening activates increase uptime reduce cost tight security transport rest build hybrid cloudprem enable selfservice business intelligence functions profiling analysis complex sets discover meet functional nonfunctional institute monitoring alerting operations utilize actionable insights customer acquisition efficiency performance metrics meta lineage management innovation recommending driving adoption technologies competitive advantages contributes agile team alignment committed constant improvement efforts participating ceremonies sprint planning standups backlog grooming retrospectives delivery detailed featurestory level satisfies roadmaps acceptance criteria maintains professional attending educational workshops reviewing publications establishing personal networks societies
207," Previous management experience and successfully leading a team of direct reports Proven track record of effectively leading and managing employees, managing performance, setting goals for team/individuals and provide mentoring.  Strong Tableau, Adobe Analytics other BI solution Expert Oracle and Vertica skillsets Experience using JIRA and Agile Project Management software Experience using GitHub, Bit Bucket, or other code repository solution Strong written, verbal communication and presentation skills Ability to explain complex technical issues in a way that non-technical people may understand Able to work in a global, multicultural environment Self-motivated.  Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player Enjoy challenging and thought provoking work and have a strong desire to learn and progress Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Implement architectures to handle web-scale data and its organization Execute strategies that inform data design and architecture partnering with enterprise standard Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Assist in creating documents that ensure consistency in development across the online organization.  Implements and improves core software infrastructure Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem.  Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools  such as control charts and scorecards , readiness and adoption of data w/in the organization Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding Serve as strong advocate to improve analytical capability across the organization Communicate with various business areas and to gather and prioritize their business requirements Support various reporting and BI solutions  Tableau, Power BI, Salesforce Einstein  Manage application and data integration platforms  Informatica, Talend  Manage the full life cycle of development/reporting/integration projects  planning, design, develop, testing and rollout Manage solution providers, define sourcing approach and manage the providers Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement Gain adoption of architecture processes, standards and procedures  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience ",previous management successfully leading team direct reports proven track record effectively leading managing employees managing performance setting goals teamindividuals mentoring tableau adobe analytics bi solution expert oracle vertica skillsets jira agile project management software github bit bucket code repository solution written verbal communication presentation explain complex technical issues way nontechnical people may understand able global multicultural selfmotivated capable little supervision react positively pressure meet tight deadlines able independently team player enjoy challenging thought provoking desire learn progress development teams project leadersstakeholders technical solutions enable business capabilities supports data science community enabling data availability environments advanced analytical capabilities maintains broad understanding implementation integration interconnectivity issues emerging technologies define data strategies assist decisionmaking process selection software architecture solutions implement architectures handle webscale data organization execute strategies inform data design architecture partnering enterprise standard build robust data pipelines public cloud aws kinesis kafka lambda technologies assist creating documents consistency development across online organization implements improves core software infrastructure deep expertise sql language python hadoop ecosystem andor spark ecosystem writing complex programs implementing architectures enabling automation environments develop maintain business reporting ensuring reliability performance delivery performance management tools control charts scorecards readiness adoption data win organization consolidate standardize control changes capacity management data metric definitions ownership accountability taxonomy alignment understanding serve advocate improve analytical capability across organization communicate various business areas gather prioritize business support various reporting bi solutions tableau power bi salesforce einstein manage application data integration platforms informatica talend manage full life cycle developmentreportingintegration projects planning design develop testing rollout manage solution providers define sourcing approach manage providers create manage data applications technology architecture documentation design artifacts across teams deliver meaningful reference architectures outline architecture principles best practices technology advancement gain adoption architecture processes standards procedures bs computer science applied mathematics physics statistics area study data sciences data mining relevant,previous management successfully leading team direct reports proven track record effectively managing employees performance setting goals teamindividuals mentoring tableau adobe analytics bi solution expert oracle vertica skillsets jira agile project software github bit bucket code repository written verbal communication presentation explain complex technical issues way nontechnical people may understand able global multicultural selfmotivated capable little supervision react positively pressure meet tight deadlines independently player enjoy challenging thought provoking desire learn progress development teams leadersstakeholders solutions enable business capabilities supports data science community enabling availability environments advanced analytical maintains broad understanding implementation integration interconnectivity emerging technologies define strategies assist decisionmaking process selection architecture implement architectures handle webscale organization execute inform design partnering enterprise standard build robust pipelines public cloud aws kinesis kafka lambda creating documents consistency across online implements improves core infrastructure deep expertise sql language python hadoop ecosystem andor spark writing programs implementing automation develop maintain reporting ensuring reliability delivery tools control charts scorecards readiness adoption win consolidate standardize changes capacity metric definitions ownership accountability taxonomy alignment serve advocate improve capability communicate various areas gather prioritize support power salesforce einstein manage application platforms informatica talend full life cycle developmentreportingintegration projects planning testing rollout providers sourcing approach create applications technology documentation artifacts deliver meaningful reference outline principles best practices advancement gain processes standards procedures bs computer applied mathematics physics statistics area study sciences mining relevant
208,"  You are passionate aboutâ¯making a difference in the world and the impactâ¯technology canâ¯have in helping people get better faster You drive results effectively through cross functional teams You are flexible and adaptiveâ¯andâ¯can adjust quicklyâ¯in the face of changing situations and challenges You are disciplined and able to manage multiple responsibilities simultaneously You are a strong team player whoâ¯is happy to share knowledge and mentor others to grow their skills You work effectively with minimal supervision and can be relied on to deliver on commitments effectively   Create data ingestion pipelines and manage and maintain master data specification documents which reflect necessary data elements.  Cleanse, analyze, and maintain incoming client data.  Develop features relevant to healthcare use cases/vectors.  Generate data visualizations and presentations, including the design of interactive and intuitive dashboards.  Participate in process adherence and continuance through automation of workflow schedules and cyclic support to ensure stable production environment.    Bachelor's degreeâ¯in Information Technology/Computer Engineering 3+ years of experience in development and deployment of Big Data technologies with a focus on Data Engineering  SQL, AWS/Azure Cloud platform tools, Data handling, Shell Scripting, Airflow, Python, etc.   Expertise in cloud platforms  AWS, Azure  Experience in data interchange standards like EDI and HL7 is a plus Familiarity in Agile delivery framework Awareness of Artificial Intelligence; data science concepts a plus Experience working in healthcare setting preferred  ",passionate aboutmaking difference world impacttechnology canhave helping people get better faster drive results effectively cross functional teams flexible adaptiveandcan adjust quicklyin face changing situations challenges disciplined able manage multiple responsibilities simultaneously team player whois happy share mentor others grow effectively minimal supervision relied deliver commitments effectively create data ingestion pipelines manage maintain master data specification documents reflect necessary data elements cleanse analyze maintain incoming client data develop features relevant healthcare use casesvectors generate data visualizations presentations design interactive intuitive dashboards participate process adherence continuance automation workflow schedules cyclic support stable production bachelors degreein information technologycomputer engineering development deployment big data technologies focus data engineering sql awsazure cloud platform tools data handling shell scripting airflow python expertise cloud platforms aws azure data interchange standards like edi hl plus familiarity agile delivery framework awareness artificial intelligence data science concepts plus healthcare setting,passionate aboutmaking difference world impacttechnology canhave helping people get better faster drive results effectively cross functional teams flexible adaptiveandcan adjust quicklyin face changing situations challenges disciplined able manage multiple responsibilities simultaneously team player whois happy share mentor others grow minimal supervision relied deliver commitments create data ingestion pipelines maintain master specification documents reflect necessary elements cleanse analyze incoming client develop features relevant healthcare use casesvectors generate visualizations presentations design interactive intuitive dashboards participate process adherence continuance automation workflow schedules cyclic support stable production bachelors degreein information technologycomputer engineering development deployment big technologies focus sql awsazure cloud platform tools handling shell scripting airflow python expertise platforms aws azure interchange standards like edi hl plus familiarity agile delivery framework awareness artificial intelligence science concepts setting
209,"  You are passionate aboutâ¯making a difference in the world and the impactâ¯technology canâ¯have in helping people get better faster You drive results effectively through cross functional teams You are flexible and adaptiveâ¯andâ¯can adjust quicklyâ¯in the face of changing situations and challenges You are disciplined and able to manage multiple responsibilities simultaneously You are a strong team player whoâ¯is happy to share knowledge and mentor others to grow their skills You work effectively with minimal supervision and can be relied on to deliver on commitments effectively   Design and create data ingestion pipelines and manage and maintain master data specification documents which reflect necessary data elements.  Cleanse, analyze, and maintain incoming client data.  Design and develop features relevant to healthcare use cases/vectors.  Lead in generating data visualizations and presentations, including the design of interactive and intuitive dashboards.  Lead process adherence and continuance through automation of workflow schedules and cyclic support to ensure stable production environment.    Bachelor's degreeâ¯in Information Technology/Computer Engineering 5+ years of experience in development and deployment of Big Data technologies with a focus on Data Engineering  SQL, AWS/Azure Cloud platform tools, Data handling, Shell Scripting, Airflow, Python, etc.   Expertise in cloud platforms  AWS, Azure  Experience in data interchange standards like EDI and HL7 is a plus Familiarity in Agile delivery framework Awareness of Artificial Intelligence; data science concepts a plus Experience working in healthcare setting preferred  ",passionate aboutmaking difference world impacttechnology canhave helping people get better faster drive results effectively cross functional teams flexible adaptiveandcan adjust quicklyin face changing situations challenges disciplined able manage multiple responsibilities simultaneously team player whois happy share mentor others grow effectively minimal supervision relied deliver commitments effectively design create data ingestion pipelines manage maintain master data specification documents reflect necessary data elements cleanse analyze maintain incoming client data design develop features relevant healthcare use casesvectors lead generating data visualizations presentations design interactive intuitive dashboards lead process adherence continuance automation workflow schedules cyclic support stable production bachelors degreein information technologycomputer engineering development deployment big data technologies focus data engineering sql awsazure cloud platform tools data handling shell scripting airflow python expertise cloud platforms aws azure data interchange standards like edi hl plus familiarity agile delivery framework awareness artificial intelligence data science concepts plus healthcare setting,passionate aboutmaking difference world impacttechnology canhave helping people get better faster drive results effectively cross functional teams flexible adaptiveandcan adjust quicklyin face changing situations challenges disciplined able manage multiple responsibilities simultaneously team player whois happy share mentor others grow minimal supervision relied deliver commitments design create data ingestion pipelines maintain master specification documents reflect necessary elements cleanse analyze incoming client develop features relevant healthcare use casesvectors lead generating visualizations presentations interactive intuitive dashboards process adherence continuance automation workflow schedules cyclic support stable production bachelors degreein information technologycomputer engineering development deployment big technologies focus sql awsazure cloud platform tools handling shell scripting airflow python expertise platforms aws azure interchange standards like edi hl plus familiarity agile delivery framework awareness artificial intelligence science concepts setting
210," Degree in Computer Science, Engineering, Mathematics, Statistics or related quantitative field 5+ yearsâ experience in RDBMS systems, data warehousing, advanced SQL Server Analytical development, and sophisticated data analysis Expertise with Azure Cloud Technologies  Data Factory, PowerShell, Data Lake and Data Lake Analytics  Extensive experience with Data Modeling and ETL tools, Business Intelligence platforms, API Integration, and Object-Oriented Programming  OOP  Ability to thrive in a cross-functional environment utilizing modern technologies  Python, Git, Jenkins, Octopus Deploy, Tensorflow, Domo, ArcGIS, E/R Studio, RedGate DLM Automation and other tools  Experience with messaging/event processing tooling and frameworks such as Azure Event Hub, Kafka, Kinesis Working knowledge of Azure HDInsight + Spark, Azure Databricks, Azure Stream Analytics    ",degree computer science engineering mathematics statistics quantitative rdbms systems data warehousing advanced sql server analytical development sophisticated data analysis expertise azure cloud technologies data factory powershell data lake data lake analytics extensive data modeling etl tools business intelligence platforms api integration objectoriented programming oop thrive crossfunctional utilizing modern technologies python git jenkins octopus deploy tensorflow domo arcgis er studio redgate dlm automation tools messagingevent processing tooling frameworks azure event hub kafka kinesis azure hdinsight spark azure databricks azure stream analytics,degree computer science engineering mathematics statistics quantitative rdbms systems data warehousing advanced sql server analytical development sophisticated analysis expertise azure cloud technologies factory powershell lake analytics extensive modeling etl tools business intelligence platforms api integration objectoriented programming oop thrive crossfunctional utilizing modern python git jenkins octopus deploy tensorflow domo arcgis er studio redgate dlm automation messagingevent processing tooling frameworks event hub kafka kinesis hdinsight spark databricks stream
211,"Qualifications  3-7 years  2 years min relevant experience in the role  experience; Bachelorâs degreeShould be proficient in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Lifecycle and Data Management. Should have progressing skills on Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.     ",qualifications min relevant role bachelors degreeshould proficient software engineering techniques software engineering architecture software engineering lifecycle data management progressing business analysis business software engineering leadership architecture technical solution design,qualifications min relevant role bachelors degreeshould proficient software engineering techniques architecture lifecycle data management progressing business analysis leadership technical solution design
212," BS or MS degree in Computer Science or a related technical experience 3+ years of experience in custom or structured ETL design, implementation, maintenance and support 3+ years of experience with one or more programming language  Python, Scala, Java, R, etc.  3+ years of experience with SQL, data definition, and data manipulation 3+ years of experience designing, implementing and maintaining SSIS packages 3+ years of experience managing ETL with data warehouses  AWS Redshift, Google BigQuery, etc  Strong experience working with both structured and unstructured data Experience building and optimizing data pipelines and big data sets Fluency running automated jobs to manipulate and store data from APIs via Google Compute Engine, Google App Engine, Google Cloud Storage, EC2, AWS Lambda, S3, etc.  Experience with Spark Experience with container applications like Docker Expertise creating and managing dashboards in data visualization platforms such as Tableau, Microsoft PowerBI, Google Data Studio, SSRS, etc.  Excellent written and oral communication skills including facilitation, project management, and working with others in team communicate data-driven insights   Proactively drive and facilitate conversations about data needs with stakeholders across the company Build and maintain data pipelines and ETL process Design, develop, maintain and enhance data collection procedures and analytic systems Understand, unify and integrate data from internal and third-party data sources using industry best practices for scalability, quality, simplicity, and maintainability   BS or MS degree in Computer Science or a related technical experience 3+ years of experience in custom or structured ETL design, implementation, maintenance and support 3+ years of experience with one or more programming language  Python, Scala, Java, R, etc.  3+ years of experience with SQL, data definition, and data manipulation 3+ years of experience designing, implementing and maintaining SSIS packages 3+ years of experience managing ETL with data warehouses  AWS Redshift, Google BigQuery, etc  Strong experience working with both structured and unstructured data Experience building and optimizing data pipelines and big data sets Fluency running automated jobs to manipulate and store data from APIs via Google Compute Engine, Google App Engine, Google Cloud Storage, EC2, AWS Lambda, S3, etc.  Experience with Spark Experience with container applications like Docker Expertise creating and managing dashboards in data visualization platforms such as Tableau, Microsoft PowerBI, Google Data Studio, SSRS, etc.  Excellent written and oral communication skills including facilitation, project management, and working with others in team communicate data-driven insights",bs ms degree computer science technical custom structured etl design implementation maintenance support one programming language python scala java r sql data definition data manipulation designing implementing maintaining ssis packages managing etl data warehouses aws redshift google bigquery structured unstructured data building optimizing data pipelines big data sets fluency running automated jobs manipulate store data apis via google compute engine google app engine google cloud storage ec aws lambda spark container applications like docker expertise creating managing dashboards data visualization platforms tableau microsoft powerbi google data studio ssrs written oral communication facilitation project management others team communicate datadriven insights proactively drive facilitate conversations data needs stakeholders across company build maintain data pipelines etl process design develop maintain enhance data collection procedures analytic systems understand unify integrate data internal thirdparty data sources industry best practices scalability simplicity maintainability bs ms degree computer science technical custom structured etl design implementation maintenance support one programming language python scala java r sql data definition data manipulation designing implementing maintaining ssis packages managing etl data warehouses aws redshift google bigquery structured unstructured data building optimizing data pipelines big data sets fluency running automated jobs manipulate store data apis via google compute engine google app engine google cloud storage ec aws lambda spark container applications like docker expertise creating managing dashboards data visualization platforms tableau microsoft powerbi google data studio ssrs written oral communication facilitation project management others team communicate datadriven insights,bs ms degree computer science technical custom structured etl design implementation maintenance support one programming language python scala java r sql data definition manipulation designing implementing maintaining ssis packages managing warehouses aws redshift google bigquery unstructured building optimizing pipelines big sets fluency running automated jobs manipulate store apis via compute engine app cloud storage ec lambda spark container applications like docker expertise creating dashboards visualization platforms tableau microsoft powerbi studio ssrs written oral communication facilitation project management others team communicate datadriven insights proactively drive facilitate conversations needs stakeholders across company build maintain process develop enhance collection procedures analytic systems understand unify integrate internal thirdparty sources industry best practices scalability simplicity maintainability
213," Strong Tableau, Adobe Analytics other BI solution Expert Oracle and Vertica skillsets Experience using JIRA and Agile Project Management software Experience using GitHub, Bit Bucket, or other code repository solution Strong written, verbal communication and presentation skills Ability to explain complex technical issues in a way that non-technical people may understand Able to work in a global, multicultural environment Self-motivated.  Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player Enjoy challenging and thought provoking work and have a strong desire to learn and progress Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities Serve as a key driver on various new technology initiatives and programs on behalf of IT organization, including big data, machine learning and AI Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Assist in creating documents that ensure consistency in development across the online organization.  Implements and improves core software infrastructure Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem.  Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools  such as control charts and scorecards , readiness and adoption of data w/in the organization Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding Serve as strong advocate to improve analytical capability across the organization Support various reporting and BI solutions  Tableau, Power BI, Salesforce Einstein  Manage application and data integration platforms  Informatica, Talend  Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement Gain adoption of architecture processes, standards and procedures  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience ",tableau adobe analytics bi solution expert oracle vertica skillsets jira agile project management software github bit bucket code repository solution written verbal communication presentation explain complex technical issues way nontechnical people may understand able global multicultural selfmotivated capable little supervision react positively pressure meet tight deadlines able independently team player enjoy challenging thought provoking desire learn progress development teams project leadersstakeholders technical solutions enable business capabilities serve key driver various technology initiatives programs behalf organization big data machine ai supports data science community enabling data availability environments advanced analytical capabilities maintains broad understanding implementation integration interconnectivity issues emerging technologies define data strategies assist decisionmaking process selection software architecture solutions build robust data pipelines public cloud aws kinesis kafka lambda technologies assist creating documents consistency development across online organization implements improves core software infrastructure deep expertise sql language python hadoop ecosystem andor spark ecosystem writing complex programs implementing architectures enabling automation environments develop maintain business reporting ensuring reliability performance delivery performance management tools control charts scorecards readiness adoption data win organization consolidate standardize control changes capacity management data metric definitions ownership accountability taxonomy alignment understanding serve advocate improve analytical capability across organization support various reporting bi solutions tableau power bi salesforce einstein manage application data integration platforms informatica talend create manage data applications technology architecture documentation design artifacts across teams deliver meaningful reference architectures outline architecture principles best practices technology advancement gain adoption architecture processes standards procedures bs computer science applied mathematics physics statistics area study data sciences data mining relevant,tableau adobe analytics bi solution expert oracle vertica skillsets jira agile project management software github bit bucket code repository written verbal communication presentation explain complex technical issues way nontechnical people may understand able global multicultural selfmotivated capable little supervision react positively pressure meet tight deadlines independently team player enjoy challenging thought provoking desire learn progress development teams leadersstakeholders solutions enable business capabilities serve key driver various technology initiatives programs behalf organization big data machine ai supports science community enabling availability environments advanced analytical maintains broad understanding implementation integration interconnectivity emerging technologies define strategies assist decisionmaking process selection architecture build robust pipelines public cloud aws kinesis kafka lambda creating documents consistency across online implements improves core infrastructure deep expertise sql language python hadoop ecosystem andor spark writing implementing architectures automation develop maintain reporting ensuring reliability performance delivery tools control charts scorecards readiness adoption win consolidate standardize changes capacity metric definitions ownership accountability taxonomy alignment advocate improve capability support power salesforce einstein manage application platforms informatica talend create applications documentation design artifacts deliver meaningful reference outline principles best practices advancement gain processes standards procedures bs computer applied mathematics physics statistics area study sciences mining relevant
214,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform.  Multi-cloud experience a plus.    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",minimum previous consulting client service delivery google gcp devops gcp platform multicloud plus proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,minimum previous consulting client service delivery google gcp devops platform multicloud plus proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
215," Bachelors/Master/PHD in computer science, engineering, information technology, or related degree and/or equivalent work experience 4+ years of Java development experience in large scale enterprise development for Undergraduates 2+ years of Java development experience in large scale enterprise development for Masters or PHD Knowledge of multiple threading development and performance tuning Knowledge of implementing efficient logic using collections and data structures Knowledge of one or more Big Data Technologies like Elastic Search, Spark, Kafka, Map-Reduce, HDFS and Hive Knowledge in Design Patterns, OOP/OOD, Software Architecture Knowledge of how to assess the performance of data solutions, how to diagnose performance problems, and tools used to monitor and tune performance.  Excellent communication skills with both Technical and Business audience    ",bachelorsmasterphd computer science engineering information technology degree andor java development scale enterprise development undergraduates java development scale enterprise development masters phd multiple threading development performance tuning implementing efficient logic collections data structures one big data technologies like elastic search spark kafka mapreduce hdfs hive design patterns oopood software architecture assess performance data solutions diagnose performance problems tools used monitor tune performance communication technical business audience,bachelorsmasterphd computer science engineering information technology degree andor java development scale enterprise undergraduates masters phd multiple threading performance tuning implementing efficient logic collections data structures one big technologies like elastic search spark kafka mapreduce hdfs hive design patterns oopood software architecture assess solutions diagnose problems tools used monitor tune communication technical business audience
216,"  Development and implementation of Master Data Web services/APIs as part of the Crawford Master Data Services Framework Responsible for developing solutions in the Informatica MDM platform  also Salesforce and/or Mulesoft  by converting business requirements into quality technical solutions Assist with the development and enforcement architecture standards Serve as an expert developer, proficient in configuring, staging, loading, matching and merging processes and overseeing and promoting quality development standards Capture and rationalize key business requirements data definitions for attributes in conceptual and logical models Performing data extraction from legacy systems to map/load data into Informatica MDM performing validation routines and reviewing that all data was appropriately loaded Engage with business partners to understand functional requirements to design best possible global solutions Analyze business and data requirements and help define the best solution keeping our long term goal in mind of reducing data redundancy Provide business and technical input in project activities and decision making processes Performs analysis of cross-functional and complex business requirements.  Follow project management methodologies and ensure the timely delivery of project deliverables Understand, influence and provide feedback on technical solution in support of end-to-end data, processes and assets Flexibility and ability to work in a global and multi-cultural environment.  Team members are in multiple geographies, resulting in time constraints due to time zones differences  Bachelor's/Masterâs degree or equivalent in Information Technology, Computer Science, Engineering or related field.  1 â 3 years prior experience Certified Data Management Professional  CDMP  preferred Some experienced in extracting and loading large data volumes to-and-from platforms such a Microsoft Azure and Amazon Web Services  AWS  Hands on experience working on UI development and configuration Must have a minimum of exposure to tools such as and not limited to Java, C,C++, C ,VB, Sql Server/Oracle.  Experience with analysis and business intelligence tools  Tableau, Cognos, etc.   Basic understanding of databases  RDBMSs  and database scripting.  Have some performance tuning and SQL query skills Some experience with 3rd party package software  Oracle, Informatica, SAP  Some knowledge and experience in software development life cycle  SDLC , software development methodologies and standards",development implementation master data web servicesapis part crawford master data services framework responsible developing solutions informatica mdm platform also salesforce andor mulesoft converting business technical solutions assist development enforcement architecture standards serve expert developer proficient configuring staging loading matching merging processes overseeing promoting development standards capture rationalize key business data definitions attributes conceptual logical models performing data extraction legacy systems mapload data informatica mdm performing validation routines reviewing data appropriately loaded engage business partners understand functional design best possible global solutions analyze business data help define best solution keeping long term goal mind reducing data redundancy business technical input project activities decision making processes performs analysis crossfunctional complex business follow project management methodologies timely delivery project deliverables understand influence feedback technical solution support endtoend data processes assets flexibility global multicultural team members multiple geographies resulting time constraints due time zones differences bachelorsmasters degree information technology computer science engineering prior certified data management professional cdmp experienced extracting loading data volumes toandfrom platforms microsoft azure amazon web services aws hands ui development configuration must minimum exposure tools limited java cc c vb sql serveroracle analysis business intelligence tools tableau cognos basic understanding databases rdbmss database scripting performance tuning sql query rd party package software oracle informatica sap software development life cycle sdlc software development methodologies standards,development implementation master data web servicesapis part crawford services framework responsible developing solutions informatica mdm platform also salesforce andor mulesoft converting business technical assist enforcement architecture standards serve expert developer proficient configuring staging loading matching merging processes overseeing promoting capture rationalize key definitions attributes conceptual logical models performing extraction legacy systems mapload validation routines reviewing appropriately loaded engage partners understand functional design best possible global analyze help define solution keeping long term goal mind reducing redundancy input project activities decision making performs analysis crossfunctional complex follow management methodologies timely delivery deliverables influence feedback support endtoend assets flexibility multicultural team members multiple geographies resulting time constraints due zones differences bachelorsmasters degree information technology computer science engineering prior certified professional cdmp experienced extracting volumes toandfrom platforms microsoft azure amazon aws hands ui configuration must minimum exposure tools limited java cc c vb sql serveroracle intelligence tableau cognos basic understanding databases rdbmss database scripting performance tuning query rd party package software oracle sap life cycle sdlc
217,"  Technology certifications such as Amazon Certified Solutions Architect Proficiency in programming in Spark, R and/or ML packages Exposure to applications developed to support manufacturing quality Experience with MS Dynamics CRM, MicroStrategy BI tools, SAP ERP Consulting experience    ",technology certifications amazon certified solutions architect proficiency programming spark r andor ml packages exposure applications developed support manufacturing ms dynamics crm microstrategy bi tools sap erp consulting,technology certifications amazon certified solutions architect proficiency programming spark r andor ml packages exposure applications developed support manufacturing ms dynamics crm microstrategy bi tools sap erp consulting
218," Previous management experience and successfully leading a team of direct reports Proven track record of effectively leading and managing employees, managing performance, setting goals for team/individuals and provide mentoring.  Strong Tableau, Adobe Analytics other BI solution Expert Oracle and Vertica skillsets Experience using JIRA and Agile Project Management software Experience using GitHub, Bit Bucket, or other code repository solution Strong written, verbal communication and presentation skills Ability to explain complex technical issues in a way that non-technical people may understand Able to work in a global, multicultural environment Self-motivated.  Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player Enjoy challenging and thought provoking work and have a strong desire to learn and progress Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Implement architectures to handle web-scale data and its organization Execute strategies that inform data design and architecture partnering with enterprise standard Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Assist in creating documents that ensure consistency in development across the online organization.  Implements and improves core software infrastructure Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem.  Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools  such as control charts and scorecards , readiness and adoption of data w/in the organization Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding Serve as strong advocate to improve analytical capability across the organization Communicate with various business areas and to gather and prioritize their business requirements Support various reporting and BI solutions  Tableau, Power BI, Salesforce Einstein  Manage application and data integration platforms  Informatica, Talend  Manage the full life cycle of development/reporting/integration projects  planning, design, develop, testing and rollout Manage solution providers, define sourcing approach and manage the providers Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement Gain adoption of architecture processes, standards and procedures  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience ",previous management successfully leading team direct reports proven track record effectively leading managing employees managing performance setting goals teamindividuals mentoring tableau adobe analytics bi solution expert oracle vertica skillsets jira agile project management software github bit bucket code repository solution written verbal communication presentation explain complex technical issues way nontechnical people may understand able global multicultural selfmotivated capable little supervision react positively pressure meet tight deadlines able independently team player enjoy challenging thought provoking desire learn progress development teams project leadersstakeholders technical solutions enable business capabilities supports data science community enabling data availability environments advanced analytical capabilities maintains broad understanding implementation integration interconnectivity issues emerging technologies define data strategies assist decisionmaking process selection software architecture solutions implement architectures handle webscale data organization execute strategies inform data design architecture partnering enterprise standard build robust data pipelines public cloud aws kinesis kafka lambda technologies assist creating documents consistency development across online organization implements improves core software infrastructure deep expertise sql language python hadoop ecosystem andor spark ecosystem writing complex programs implementing architectures enabling automation environments develop maintain business reporting ensuring reliability performance delivery performance management tools control charts scorecards readiness adoption data win organization consolidate standardize control changes capacity management data metric definitions ownership accountability taxonomy alignment understanding serve advocate improve analytical capability across organization communicate various business areas gather prioritize business support various reporting bi solutions tableau power bi salesforce einstein manage application data integration platforms informatica talend manage full life cycle developmentreportingintegration projects planning design develop testing rollout manage solution providers define sourcing approach manage providers create manage data applications technology architecture documentation design artifacts across teams deliver meaningful reference architectures outline architecture principles best practices technology advancement gain adoption architecture processes standards procedures bs computer science applied mathematics physics statistics area study data sciences data mining relevant,previous management successfully leading team direct reports proven track record effectively managing employees performance setting goals teamindividuals mentoring tableau adobe analytics bi solution expert oracle vertica skillsets jira agile project software github bit bucket code repository written verbal communication presentation explain complex technical issues way nontechnical people may understand able global multicultural selfmotivated capable little supervision react positively pressure meet tight deadlines independently player enjoy challenging thought provoking desire learn progress development teams leadersstakeholders solutions enable business capabilities supports data science community enabling availability environments advanced analytical maintains broad understanding implementation integration interconnectivity emerging technologies define strategies assist decisionmaking process selection architecture implement architectures handle webscale organization execute inform design partnering enterprise standard build robust pipelines public cloud aws kinesis kafka lambda creating documents consistency across online implements improves core infrastructure deep expertise sql language python hadoop ecosystem andor spark writing programs implementing automation develop maintain reporting ensuring reliability delivery tools control charts scorecards readiness adoption win consolidate standardize changes capacity metric definitions ownership accountability taxonomy alignment serve advocate improve capability communicate various areas gather prioritize support power salesforce einstein manage application platforms informatica talend full life cycle developmentreportingintegration projects planning testing rollout providers sourcing approach create applications technology documentation artifacts deliver meaningful reference outline principles best practices advancement gain processes standards procedures bs computer applied mathematics physics statistics area study sciences mining relevant
219,"Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.  Experience with Data Analytics.  Experience with Web Services and APIs.  Experience in the development of batch and real-time data integration and data consolidation processes.  Experience with machine learning, AI, and data lakes.  Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.  Strong analytical skills with ability for problem-solving.  Understands the importance of data provenance and the ability to demonstrate it to clients.  Detail oriented, organized, self-motivated.     Develop strategy for new multi-platform data integration and analytics.  Develop strategy for new multi-platform-sourced data lake.  Contribute to API strategy to facilitate application connectivity and analytics.  Contribute to the maintenance and evolution of best practices.  Contribute to process documentation.  Perform multiple proofs of concept  POCs .  Contribute to implementation plan for decided-upon solution s .     ",javascriptjava python jitterbit developer languages data analytics web services apis development batch realtime data integration data consolidation processes machine ai data lakes proficiency tsqlplsql querywriting stored procedure development views analytical problemsolving understands importance data provenance demonstrate clients detail oriented organized selfmotivated develop strategy multiplatform data integration analytics develop strategy multiplatformsourced data lake contribute api strategy facilitate application connectivity analytics contribute maintenance evolution best practices contribute process documentation perform multiple proofs concept pocs contribute implementation plan decidedupon solution,javascriptjava python jitterbit developer languages data analytics web services apis development batch realtime integration consolidation processes machine ai lakes proficiency tsqlplsql querywriting stored procedure views analytical problemsolving understands importance provenance demonstrate clients detail oriented organized selfmotivated develop strategy multiplatform multiplatformsourced lake contribute api facilitate application connectivity maintenance evolution best practices process documentation perform multiple proofs concept pocs implementation plan decidedupon solution
220,"At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ",least consulting client service delivery azure devops azure platform proven build manage foster teamoriented,least consulting client service delivery azure devops platform proven build manage foster teamoriented
221,"     5 + years of experience designing data models 10 + years of experience implementing data models and data architecture Experience with debugging, performance profiling and optimization of data at each phase of the data pipeline Demonstrated analytical capabilities paired with a strong initiative to find ways to improve the analysis and communication of complex data Specific experience with Microsoft SQL Server Reporting Services  SSRS  and SQL Server toolset.  Candidates with extensive experience in other BI/reporting/analytics toolsets may be considered if they have a commitment to learning SSRS and a demonstrated ability to ramp up to proficiency rather quickly  within 2 months  in a primarily self-directed way Proficiency in either Tableau, SSRS or Power BI is required Capable of designing complex and performant SQL queries ",designing data models implementing data models data architecture debugging performance profiling optimization data phase data pipeline demonstrated analytical capabilities paired initiative find ways improve analysis communication complex data specific microsoft sql server reporting services ssrs sql server toolset candidates extensive bireportinganalytics toolsets may considered commitment ssrs demonstrated ramp proficiency rather quickly within months primarily selfdirected way proficiency either tableau ssrs power bi capable designing complex performant sql queries,designing data models implementing architecture debugging performance profiling optimization phase pipeline demonstrated analytical capabilities paired initiative find ways improve analysis communication complex specific microsoft sql server reporting services ssrs toolset candidates extensive bireportinganalytics toolsets may considered commitment ramp proficiency rather quickly within months primarily selfdirected way either tableau power bi capable performant queries
222," Familiarity with building and optimizing data pipelines, architectures, and datasets Familiarity with data virtualization, transformation, and various data structures Familiarity with high performance computing Developer technologies  Software management tools including those for testing, continuous integration, version control, debugging, profiling, and compiler optimization Environment Management  Data management and file systems System metrics and monitoring Job scheduler, resource allocation and job/workflow management   BS degree in engineering or Bachelor's degree in a science related field from an accredited college or university and a minimum of four years relevant experience; or MS degree in engineering or Master's degree in a science related field from an accredited college or university and a minimum of two years relevant experience Experience with relational databases, query authoring, programming, and hardware/software troubleshooting.  ",familiarity building optimizing data pipelines architectures datasets familiarity data virtualization transformation various data structures familiarity performance computing developer technologies software management tools testing continuous integration version control debugging profiling compiler optimization management data management file systems metrics monitoring job scheduler resource allocation jobworkflow management bs degree engineering bachelors degree science accredited college university minimum four relevant ms degree engineering masters degree science accredited college university minimum two relevant relational databases query authoring programming hardwaresoftware troubleshooting,familiarity building optimizing data pipelines architectures datasets virtualization transformation various structures performance computing developer technologies software management tools testing continuous integration version control debugging profiling compiler optimization file systems metrics monitoring job scheduler resource allocation jobworkflow bs degree engineering bachelors science accredited college university minimum four relevant ms masters two relational databases query authoring programming hardwaresoftware troubleshooting
223,     Experience with  Oracle technologies including Oracle database and ODI Cloud-based data warehouse technologies such as Snowflake and open source ETL technologies such as Snaplogic.  ,oracle technologies oracle database odi cloudbased data warehouse technologies snowflake open source etl technologies snaplogic,oracle technologies database odi cloudbased data warehouse snowflake open source etl snaplogic
224," Bachelorâs degree in CS, IT/IS, or a field related to a computational science and a minimum two years experience working as a data engineer Experience managing data ETL processes and making data available through service applications and databases.  Experience working with query authoring, relational databases, and a familiarity with a variety of databases  Cassandra or Elasticsearch preferred .  Ability to write efficient SQL queries Experience  3+ years  with programming languages  Python, Java, R, and/or Scala preferred  Familiarity with a variety of data processing technologies  e. g.  Spark, Kafka, Hadoop  Excellent communication skills, including a knack for clear documentation Experience with or knowledge of REST APIs and making data available through microservices.  Experience using version control  Git, Mercurial, SVN, etc.   for collaborative code development.  Experience with containerization and related technologies  e. g.  Docker, Kubernetes  Familiarity with core provider services from AWS, Azure or GCP, preferably having supported deployments on one or more of these platforms Experience working with MongoDB, PostgreSQL, and Redis Working knowledge of bash scripting and/or JavaScript Experience with automation and configuration management Expert level building pipelines using Apache Beam or Spark    Work with data scientists to build ML pipelines using heterogeneous sources and provide engineering services for data science applications Design and implement data warehouses, real-time ETL, and batch processing of data to support modeling and reporting needs Work with team to develop data expertise and resolve upstream issues relating to data quality Define best practices and design for the management of data With data scientists, build and maintain internal data processing and visualization tools Create tools to serve data such as APIs and packages Ensure automation through CI/CD across platforms both in cloud and on-premises Ability to research and assess open source technologies and components to recommend and integrate into the design and implementation   ",bachelors degree cs itis computational science minimum two data engineer managing data etl processes making data available service applications databases query authoring relational databases familiarity variety databases cassandra elasticsearch write efficient sql queries programming languages python java r andor scala familiarity variety data processing technologies e g spark kafka hadoop communication knack clear documentation rest apis making data available microservices version control git mercurial svn collaborative code development containerization technologies e g docker kubernetes familiarity core provider services aws azure gcp preferably supported deployments one platforms mongodb postgresql redis bash scripting andor javascript automation configuration management expert level building pipelines apache beam spark data scientists build ml pipelines heterogeneous sources engineering services data science applications design implement data warehouses realtime etl batch processing data support modeling reporting needs team develop data expertise resolve upstream issues relating data define best practices design management data data scientists build maintain internal data processing visualization tools create tools serve data apis packages automation cicd across platforms cloud onpremises research assess open source technologies components recommend integrate design implementation,bachelors degree cs itis computational science minimum two data engineer managing etl processes making available service applications databases query authoring relational familiarity variety cassandra elasticsearch write efficient sql queries programming languages python java r andor scala processing technologies e g spark kafka hadoop communication knack clear documentation rest apis microservices version control git mercurial svn collaborative code development containerization docker kubernetes core provider services aws azure gcp preferably supported deployments one platforms mongodb postgresql redis bash scripting javascript automation configuration management expert level building pipelines apache beam scientists build ml heterogeneous sources engineering design implement warehouses realtime batch support modeling reporting needs team develop expertise resolve upstream issues relating define best practices maintain internal visualization tools create serve packages cicd across cloud onpremises research assess open source components recommend integrate implementation
225,"Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, etc.  Experience with relational database design methodologies and authoring complex SQL queries Experience with NoSQL database technologies  MongoDB, Cassandra, etc .  Experience with Agile Development and Agile Deployment tools and versioning using Git or similar tools.  Experience with Hadoop and other Big Data technologies such as Spark, PySpark and Kafka.  Experience with data pipeline and workflow management tools  Azkaban, Luigi, Airflow, etc.  Experience with message queuing, stream processing, and highly scalable âbig dataâ data stores.  Experience building data pipelines utilizing Google Cloud platform.  Experience with git or other code repository tools Experience with Concourse or other CI/CD tools and methodologies.  3-5 years experience Google Cloud Platform  GCS, BQ, etc , Apache Kafka, Python  Build the infrastructure to support coding, testing, processing, and maintaining data resources in support of the Data Science, analytics and reporting organizations using SQL, SQOOP, Python, Google Big Query, Kafka and other Big Data technologies.  Collaborate with Data Scientists in the development of predictive models using machine learning, natural language and statistical analysis methods.  Identify, design and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc .  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.  Develop, refine and oversee data management standards, including establishing and enforcing governance procedures and ensuring data integrity across multiple functions.  Responsible for owning data quality metrics and meeting defined data accuracy goals according to industry best practices.   ",objectorientedobject function scripting languages python java c scala relational database design methodologies authoring complex sql queries nosql database technologies mongodb cassandra agile development agile deployment tools versioning git similar tools hadoop big data technologies spark pyspark kafka data pipeline workflow management tools azkaban luigi airflow message queuing stream processing highly scalable big data data stores building data pipelines utilizing google cloud platform git code repository tools concourse cicd tools methodologies google cloud platform gcs bq apache kafka python build infrastructure support coding testing processing maintaining data resources support data science analytics reporting organizations sql sqoop python google big query kafka big data technologies collaborate data scientists development predictive models machine natural language statistical analysis methods identify design implement internal process improvements automating manual processes optimizing data delivery redesigning infrastructure greater scalability build analytics tools utilize data pipeline actionable insights customer acquisition operational efficiency key business performance metrics stakeholders assist datarelated technical issues support data infrastructure needs develop refine oversee data management standards establishing enforcing governance procedures ensuring data integrity across multiple functions responsible owning data metrics meeting defined data accuracy goals according industry best practices,objectorientedobject function scripting languages python java c scala relational database design methodologies authoring complex sql queries nosql technologies mongodb cassandra agile development deployment tools versioning git similar hadoop big data spark pyspark kafka pipeline workflow management azkaban luigi airflow message queuing stream processing highly scalable stores building pipelines utilizing google cloud platform code repository concourse cicd gcs bq apache build infrastructure support coding testing maintaining resources science analytics reporting organizations sqoop query collaborate scientists predictive models machine natural language statistical analysis methods identify implement internal process improvements automating manual processes optimizing delivery redesigning greater scalability utilize actionable insights customer acquisition operational efficiency key business performance metrics stakeholders assist datarelated technical issues needs develop refine oversee standards establishing enforcing governance procedures ensuring integrity across multiple functions responsible owning meeting defined accuracy goals according industry best practices
226," B. S.  in Computer Science, Engineering, Mathematics is required  Any more is a bonus  3-5 years relevant experience is required.  Demonstrated understanding and use of software engineering concepts, practices, and procedures.  Proficient development skills  Python preferred  Linux development experience Technical communication skills Ability to participate in a multi-disciplinary team    ",b computer science engineering mathematics bonus relevant demonstrated understanding use software engineering concepts practices procedures proficient development python linux development technical communication participate multidisciplinary team,b computer science engineering mathematics bonus relevant demonstrated understanding use software concepts practices procedures proficient development python linux technical communication participate multidisciplinary team
227,"  Strong Microsoft SQL Skills including but not limited to views, stored procedures and function creation and optimization Ensure performance and availability of databases ETL knowledge and capability Data Modeling / Database Design Perform baseline Microsoft SQL database administrative tasks Job creations and failure monitoring Evaluate datasets for quality and accuracy SQL Training to others as needed  ",microsoft sql limited views stored procedures function creation optimization performance availability databases etl capability data modeling database design perform baseline microsoft sql database administrative tasks job creations failure monitoring evaluate datasets accuracy sql training others needed,microsoft sql limited views stored procedures function creation optimization performance availability databases etl capability data modeling database design perform baseline administrative tasks job creations failure monitoring evaluate datasets accuracy training others needed
228," Master's or bachelor's degree in Computer Science At least 10 years of hands-on software development experience in Python, Golang, Java, C++ or Scala Strong Object Oriented Programming skills Deep knowledge in data structures, algorithms, and software design Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources Highly proficient with relational and non-relational data storages Strong verbal and written communication skills   Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs Adheres to the best practices of software engineering  testing, integration, clean design and concern separation  and helps improve those practices over time Able to define new architectures and improve existing ones Can be the central focus for code reviews, architecture discussion and bug fixing Demonstrates code and product ownership in production Support the business teams and product managers in data extracts and data analysis Performs as a true agile team leader and exhibits competencies in all layers of the application stack Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration  ",masters bachelors degree computer science least handson software development python golang java c scala object oriented programming deep data structures algorithms software design volume performance applications dealing amounts structured unstructured data multiple sources highly proficient relational nonrelational data storages verbal written communication team tech product managers executing product backlog taking part creation grooming understanding stakeholders needs adheres best practices software engineering testing integration clean design concern separation helps improve practices time able define architectures improve existing ones central focus code reviews architecture discussion bug fixing demonstrates code product ownership production support business teams product managers data extracts data analysis performs true agile team leader exhibits competencies layers application stack demonstrate proficiency developing software user interface business logic data modeling systems component integration,masters bachelors degree computer science least handson software development python golang java c scala object oriented programming deep data structures algorithms design volume performance applications dealing amounts structured unstructured multiple sources highly proficient relational nonrelational storages verbal written communication team tech product managers executing backlog taking part creation grooming understanding stakeholders needs adheres best practices engineering testing integration clean concern separation helps improve time able define architectures existing ones central focus code reviews architecture discussion bug fixing demonstrates ownership production support business teams extracts analysis performs true agile leader exhibits competencies layers application stack demonstrate proficiency developing user interface logic modeling systems component
229," BS/MS in Computer Science or a related technical field Seeking candidates with 1+ years of experience in  Architecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services Designing data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure  shell scripts, job schedulers, message queues, etc.   Designing efficient data structures and database schemas Working with distributed systems architecture Incorporating data processing and workflow management tools into pipeline design  AWS EMR, Airflow, Kafka, etc.   Using profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements Developing for continuous integration and automated deployments Utilizing a variety of data stores, including data warehouses  ideally Redshift , RDBMSes  ideally MySQL , in-memory caches  ideally Aerospike and Redis , and searchable document DBs  ideally Elasticseach  Wrangling large-scale data sets   Ship high-quality, well-tested, secure, and maintainable code Design, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions Manage automated unit and integration test suites Work collaboratively and communicate effectively with a small, motivated team of engineers and product managers Experiment with and recommend new technologies that simplify or improve PromoteIQ's stack Participate in an on-call rotation and work occasional off-hours  ",bsms computer science technical seeking candidates architecting building maintaining endtoend highthroughput data systems supporting services designing data systems secure testable modular particularly python well support infrastructure shell scripts job schedulers message queues designing efficient data structures database schemas distributed systems architecture incorporating data processing workflow management tools pipeline design aws emr airflow kafka profiling tools debugging logs performance metrics data sources make code applicationlevel improvements developing continuous integration automated deployments utilizing variety data stores data warehouses ideally redshift rdbmses ideally mysql inmemory caches ideally aerospike redis searchable document dbs ideally elasticseach wrangling largescale data sets ship highquality welltested secure maintainable code design develop maintain data pipelines backend services realtime decisioning reporting optimization data collection functions manage automated unit integration test suites collaboratively communicate effectively small motivated team engineers product managers experiment recommend technologies simplify improve promoteiqs stack participate oncall rotation occasional offhours,bsms computer science technical seeking candidates architecting building maintaining endtoend highthroughput data systems supporting services designing secure testable modular particularly python well support infrastructure shell scripts job schedulers message queues efficient structures database schemas distributed architecture incorporating processing workflow management tools pipeline design aws emr airflow kafka profiling debugging logs performance metrics sources make code applicationlevel improvements developing continuous integration automated deployments utilizing variety stores warehouses ideally redshift rdbmses mysql inmemory caches aerospike redis searchable document dbs elasticseach wrangling largescale sets ship highquality welltested maintainable develop maintain pipelines backend realtime decisioning reporting optimization collection functions manage unit test suites collaboratively communicate effectively small motivated team engineers product managers experiment recommend technologies simplify improve promoteiqs stack participate oncall rotation occasional offhours
230," Experience with data dictionary concepts, data mapping, and data architecture.  Knowledge and experience with a wide range of tools and IT technologies.  Proven ability to research and learn tools, hardware, and languages quickly.  Understanding of data analysis strategies and concepts  e. g.  business intelligence, time series analysis, project management .  Excellent interpersonal, organizational, and communication skills required.  Must be a self-starter with the ability to work independently and manage multiple long-term projects.  Strong attention to detail and high concern for data accuracy.  Ability to interact with all levels of staff, with a high regard for confidentiality and diplomacy.  Ability to work efficiently to meet deadlines.  Dependable team player who works collaboratively and cooperatively with staff in a team-oriented environment.  Ability to multi-task in a fast-paced environment, prioritize among competing needs and respond quickly to requests for information.  Ability to follow directions and apply proper policies, procedures and guidelines.  Resourcefulness, initiative, and good judgment essential.     Develop expertise in the DANY data model.  Develop data dictionaries and best practices for data definitions and use.  Coordinate with network administrative staff to develop, implement, and maintain a secure and agile sandbox environment.  Identify tools and procedures to effectively use data.  Research and promote options for quick development.  Document standards for security and maintainability.  Supervise developers hired for special projects.  Interact with Enterprise projects development team to coordinate data collection, standards, and use.  Perform related tasks as assigned.    Bachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required.  Master's degree preferred.    Bachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required.  Master's degree preferred.  ",data dictionary concepts data mapping data architecture wide range tools technologies proven research learn tools hardware languages quickly understanding data analysis strategies concepts e g business intelligence time series analysis project management interpersonal organizational communication must selfstarter independently manage multiple longterm projects attention detail concern data accuracy interact levels staff regard confidentiality diplomacy efficiently meet deadlines dependable team player works collaboratively cooperatively staff teamoriented multitask fastpaced prioritize among competing needs respond quickly requests information follow directions apply proper policies procedures guidelines resourcefulness initiative good judgment essential develop expertise dany data model develop data dictionaries best practices data definitions use coordinate network administrative staff develop implement maintain secure agile sandbox identify tools procedures effectively use data research promote options quick development document standards security maintainability supervise developers hired special projects interact enterprise projects development team coordinate data collection standards use perform tasks assigned bachelors degree information technology computer scienceengineering machine masters degree bachelors degree information technology computer scienceengineering machine masters degree,data dictionary concepts mapping architecture wide range tools technologies proven research learn hardware languages quickly understanding analysis strategies e g business intelligence time series project management interpersonal organizational communication must selfstarter independently manage multiple longterm projects attention detail concern accuracy interact levels staff regard confidentiality diplomacy efficiently meet deadlines dependable team player works collaboratively cooperatively teamoriented multitask fastpaced prioritize among competing needs respond requests information follow directions apply proper policies procedures guidelines resourcefulness initiative good judgment essential develop expertise dany model dictionaries best practices definitions use coordinate network administrative implement maintain secure agile sandbox identify effectively promote options quick development document standards security maintainability supervise developers hired special enterprise collection perform tasks assigned bachelors degree technology computer scienceengineering machine masters
231," Bachelorâs degree in Computer Science or related field 3+ years of software development experience, as a developer Fluency in Scala and/or Java programming languages Strong OO & FP design patterns, data structure, and algorithm design skills Extensive experience developing Apache Spark applications 2+ years of experience with both relational database design  SQL , non-relational  NoSQL  databases, big data, real-time technologies Familiar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka Experience with software containerization, such as Docker Experience developing and/or consuming web interfaces  REST API  and associated skills  HTTP, web services  Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment   Serve as a senior data engineer for AdSmart products.  Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and leadership Develop and code the software components that are core to Audience Studio, under the leadership of the VP/Chief Architecture Support product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.  Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership    Interested candidate must submit a resume/CV through www. nbcunicareers. com to be considered Must be willing to work in New York, NY",bachelors degree computer science software development developer fluency scala andor java programming languages oo fp design patterns data structure algorithm design extensive developing apache spark applications relational database design sql nonrelational nosql databases big data realtime technologies familiar various cloud data sources architectures awss hdfs kafka software containerization docker developing andor consuming web interfaces rest api associated http web services selfdirected multitask sharp analytical abilities communication capable effectively dynamic serve senior data engineer adsmart products participate execute month product roadmap input delivery team stakeholders leadership develop code software components core audience studio leadership vpchief architecture support product overall roadmap updates senior leadership technically correct analyze report results adjust overall engineering strategy accordingly engineering leadership interested candidate must submit resumecv www nbcunicareers com considered must willing york ny,bachelors degree computer science software development developer fluency scala andor java programming languages oo fp design patterns data structure algorithm extensive developing apache spark applications relational database sql nonrelational nosql databases big realtime technologies familiar various cloud sources architectures awss hdfs kafka containerization docker consuming web interfaces rest api associated http services selfdirected multitask sharp analytical abilities communication capable effectively dynamic serve senior engineer adsmart products participate execute month product roadmap input delivery team stakeholders leadership develop code components core audience studio vpchief architecture support overall updates technically correct analyze report results adjust engineering strategy accordingly interested candidate must submit resumecv www nbcunicareers com considered willing york ny
232," Master's or bachelor's degree in Computer Science At least 5 years of hands-on software development experience in Python, Golang, Java, C++ or Scala Strong Object Oriented Programming skills Deep knowledge in data structures, algorithms, and software design Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources Highly proficient with relational and non-relational data storages Strong verbal and written communication skills  Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs Adheres to the best practices of software engineering  testing, integration, clean design and concern separation  and helps improve those practices over time Collaborates with code reviews, architecture discussion and bug fixing Demonstrates code and product ownership in production Support the business teams and product managers in data extracts and data analysis Performs as a true agile team member and exhibits competencies in all layers of the application stack.  Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration  ",masters bachelors degree computer science least handson software development python golang java c scala object oriented programming deep data structures algorithms software design volume performance applications dealing amounts structured unstructured data multiple sources highly proficient relational nonrelational data storages verbal written communication team tech product managers executing product backlog taking part creation grooming understanding stakeholders needs adheres best practices software engineering testing integration clean design concern separation helps improve practices time collaborates code reviews architecture discussion bug fixing demonstrates code product ownership production support business teams product managers data extracts data analysis performs true agile team member exhibits competencies layers application stack demonstrate proficiency developing software user interface business logic data modeling systems component integration,masters bachelors degree computer science least handson software development python golang java c scala object oriented programming deep data structures algorithms design volume performance applications dealing amounts structured unstructured multiple sources highly proficient relational nonrelational storages verbal written communication team tech product managers executing backlog taking part creation grooming understanding stakeholders needs adheres best practices engineering testing integration clean concern separation helps improve time collaborates code reviews architecture discussion bug fixing demonstrates ownership production support business teams extracts analysis performs true agile member exhibits competencies layers application stack demonstrate proficiency developing user interface logic modeling systems component
233," Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs.  when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e. g.  consumer segmentation, revenue forecasting, image classification, etc.  ; Understanding of machine learning algorithms  e. g.  k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.   and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.   Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs.  when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e. g.  consumer segmentation, revenue forecasting, image classification, etc.  ; Understanding of machine learning algorithms  e. g.  k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.   and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.     Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs.  when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e. g.  consumer segmentation, revenue forecasting, image classification, etc.  ; Understanding of machine learning algorithms  e. g.  k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.   and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus. ",exploring analytical technologies evaluating technical commercial viability quickly week sprint cycles develop proofofconcepts prototype models demoed explained data scientists internal stakeholders clients testing rejecting hypotheses around data processing machine model building demonstrating experiment fail quickly recognize need assistance vs conclude technology suitable task building machine pipelines ingest clean data make predictions staying abreast ai research leading labs reading papers experimenting code developing innovative solutions perspectives ai published academic journalsarxiv shared clients demonstrating continuously learn technologies quickly evaluate technical commercial viability applying machine techniques addressing variety problems e g consumer segmentation revenue forecasting image classification understanding machine algorithms e g knearest neighbors random forests ensemble methods deep neural networks appropriate use technique building machine models systems interpreting output communicating results moving models development production plus conducting research lab publishing plus exploring analytical technologies evaluating technical commercial viability quickly week sprint cycles develop proofofconcepts prototype models demoed explained data scientists internal stakeholders clients testing rejecting hypotheses around data processing machine model building demonstrating experiment fail quickly recognize need assistance vs conclude technology suitable task building machine pipelines ingest clean data make predictions staying abreast ai research leading labs reading papers experimenting code developing innovative solutions perspectives ai published academic journalsarxiv shared clients demonstrating continuously learn technologies quickly evaluate technical commercial viability applying machine techniques addressing variety problems e g consumer segmentation revenue forecasting image classification understanding machine algorithms e g knearest neighbors random forests ensemble methods deep neural networks appropriate use technique building machine models systems interpreting output communicating results moving models development production plus conducting research lab publishing plus exploring analytical technologies evaluating technical commercial viability quickly week sprint cycles develop proofofconcepts prototype models demoed explained data scientists internal stakeholders clients testing rejecting hypotheses around data processing machine model building demonstrating experiment fail quickly recognize need assistance vs conclude technology suitable task building machine pipelines ingest clean data make predictions staying abreast ai research leading labs reading papers experimenting code developing innovative solutions perspectives ai published academic journalsarxiv shared clients demonstrating continuously learn technologies quickly evaluate technical commercial viability applying machine techniques addressing variety problems e g consumer segmentation revenue forecasting image classification understanding machine algorithms e g knearest neighbors random forests ensemble methods deep neural networks appropriate use technique building machine models systems interpreting output communicating results moving models development production plus conducting research lab publishing plus,exploring analytical technologies evaluating technical commercial viability quickly week sprint cycles develop proofofconcepts prototype models demoed explained data scientists internal stakeholders clients testing rejecting hypotheses around processing machine model building demonstrating experiment fail recognize need assistance vs conclude technology suitable task pipelines ingest clean make predictions staying abreast ai research leading labs reading papers experimenting code developing innovative solutions perspectives published academic journalsarxiv shared continuously learn evaluate applying techniques addressing variety problems e g consumer segmentation revenue forecasting image classification understanding algorithms knearest neighbors random forests ensemble methods deep neural networks appropriate use technique systems interpreting output communicating results moving development production plus conducting lab publishing
234," Commercial experience leading on client-facing projects, including working in close-knit teams 5+ years of experience and interest in Big Data technologies  Hadoop / Spark / NoSQL DBs  5+ years of experience working on projects within the cloud ideally AWS or Azure 5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent Experience with open source tools like Apache Airflow and Griffin Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma Experience building automated data quality and testing into data pipelines Experience with AI, NLP, Machine Learning, etc.  is a plus Strong development background with experience in at least two scripting, object oriented or functional programming language, etc.  SQL, Python, Java, Scala, C , R Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.  A deep personal motivation to always produce outstanding work for your clients and colleagues Excel in team collaboration and working with others from diverse skill-sets and backgrounds    ",commercial leading clientfacing projects closeknit teams interest big data technologies hadoop spark nosql dbs projects within cloud ideally aws azure streaming architectures patterns like kafka kinesis flink confluent open source tools like apache airflow griffin devops dataops patterns tools like jenkins kubernetes docker terraform data warehousing cloud products like snowflake azure dw redshift building operational etl data pipelines across number sources constructing relational dimensional data models one etlelt tools like talend matillion fivetran alooma building automated data testing data pipelines ai nlp machine plus development background least two scripting object oriented functional programming language sql python java scala c r lively projects consulting setting often different multiple projects time interpersonal interacting clients clear timely professional manner deep personal motivation always produce outstanding clients colleagues excel team collaboration others diverse skillsets backgrounds,commercial leading clientfacing projects closeknit teams interest big data technologies hadoop spark nosql dbs within cloud ideally aws azure streaming architectures patterns like kafka kinesis flink confluent open source tools apache airflow griffin devops dataops jenkins kubernetes docker terraform warehousing products snowflake dw redshift building operational etl pipelines across number sources constructing relational dimensional models one etlelt talend matillion fivetran alooma automated testing ai nlp machine plus development background least two scripting object oriented functional programming language sql python java scala c r lively consulting setting often different multiple time interpersonal interacting clients clear timely professional manner deep personal motivation always produce outstanding colleagues excel team collaboration others diverse skillsets backgrounds
235,"THE PERSON Hands on research and development Find innovative solutions to difficult and unstructured problems that will support and expand our core business Use analytical rigor to analyze large amounts of data, help extract actionable insights using data analysis, feature engineering, optimization tools and machine learning techniques Develop and maintain scalable and reliable data pipelines for AI/ML processes Participate in all aspects of Teach For Americaâs agile software development cycle Other duties  research, presentations, communicating with other teams   THE MUST HAVES Prior Experience  Intermediate or higher experience working with machine learning tools in Python  TensorFlow, Keras, fast. ai, etc  Experience working with data solutions  data ingestion, preprocessing, analysis, predictive analytics  Experience with Reporting and Advanced Analytics Solutions Intermediate or higher experience working with Java  Spring, REST, JMS  and SQL Comfortable working in a Unix environment Experience with version control, containerization technologies Experience working with Continuous Integration / Automation architectures Experience interfacing and working with engineering teams throughout the product development lifecycle  leading projects and/or other developers  Experience partnering with other teams to test and roll out cognitive & predictive analytics solutions Optional but desired  Experience developing applications for and deploying applications in cloud environments Experience with big data and associated technologies  Hadoop, Databricks, Azure Machine Learning etc.   Experience with statistical modeling in R Skills  Be able to communicate in a clear and effective manner with both technical and non-technical audiences Be interested in staying up-to-date with recent advances in machine learning and predictive analytics Be detail oriented, able to work under pressure and effectively manage competing priorities Education  At least a four-year degree in Computer Science, BSEE, MIS or a related field, or equivalent experience Work Demands  This position is located on site in our New York National Office Limited travel may be required  THE TEAM Our team loves to collaborate.  We partner with every other team in the organization to create world-class technology solutions that staff and corps members use to more effectively and efficiently get all kids access to educational opportunity.  Our team works very hard, but we also have a lot of fun.  We enjoy game nights, quarterly trivia outings, and themed potlucks where we get together to eat and explore each other's cultures and favorite recipes.   THE PERKS By joining staff, you join a network of individuals committed to pursuing equity for all students and developing themselves as professionals in the process.  We as an organization value the longevity of our employees and offer a comprehensive and competitive benefits plan.  The salary for this position is also competitive and depends on your prior work experience.  Please be advised, you will have an opportunity to discuss salary in more detail after you begin the application process.   WE ARE DEEPLY COMMITTED TO DIVERSITY, EQUITY & INCLUSIVENESS Teach For America encourages individuals of all ethnic, racial, and socioeconomic backgrounds to apply for this position.  We are committed to maximizing the diversity of our organization, as we want to engage all those who can contribute to this effort.   Teach For America is committed to providing equal employment opportunities to all qualified individuals and does not discriminate on the basis of race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, parental status, genetic information or characteristics  or those of a family member  or any other basis prohibited by applicable law.   This job description reflects Teach For America's assignment of essential functions and qualifications of the role.  Nothing in this herein restricts management's right to assign, reassign or eliminate duties and responsibilities to this role at any time.   NEXT STEPS Interested in this position? Apply now! Scroll down to the bottom of the page to find the link to the online application.  If you still have questions regarding the role, feel free to contact our recruitment team at staffing@teachforamerica. org or visit www. teachforamerica. org/about-us/careers.     ",person hands research development find innovative solutions difficult unstructured problems support expand core business use analytical rigor analyze amounts data help extract actionable insights data analysis feature engineering optimization tools machine techniques develop maintain scalable reliable data pipelines aiml processes participate aspects teach americas agile software development cycle duties research presentations communicating teams must haves prior intermediate higher machine tools python tensorflow keras fast ai data solutions data ingestion preprocessing analysis predictive analytics reporting advanced analytics solutions intermediate higher java spring rest jms sql comfortable unix version control containerization technologies continuous integration automation architectures interfacing engineering teams throughout product development lifecycle leading projects andor developers partnering teams test roll cognitive predictive analytics solutions optional desired developing applications deploying applications cloud environments big data associated technologies hadoop databricks azure machine statistical modeling r able communicate clear effective manner technical nontechnical audiences interested staying uptodate recent advances machine predictive analytics detail oriented able pressure effectively manage competing priorities education least fouryear degree computer science bsee mis demands position located site york national office limited travel may team team loves collaborate partner every team organization create worldclass technology solutions staff corps members use effectively efficiently get kids access educational opportunity team works hard also lot fun enjoy game nights quarterly trivia outings themed potlucks get together eat explore others cultures favorite recipes perks joining staff join network individuals committed pursuing equity students developing professionals process organization value longevity employees offer comprehensive competitive benefits plan salary position also competitive depends prior please advised opportunity discuss salary detail begin application process deeply committed diversity equity inclusiveness teach america encourages individuals ethnic racial socioeconomic backgrounds apply position committed maximizing diversity organization want engage contribute effort teach america committed providing equal employment opportunities qualified individuals discriminate basis race color ethnicity religion sex gender gender identity expression sexual orientation national origin disability age marital status veteran status pregnancy parental status genetic information characteristics family member basis prohibited applicable law job description reflects teach americas assignment essential functions qualifications role nothing herein restricts managements right assign reassign eliminate duties responsibilities role time next steps interested position apply scroll bottom page find link online application still questions regarding role feel free contact recruitment team staffingteachforamerica org visit www teachforamerica orgaboutuscareers,person hands research development find innovative solutions difficult unstructured problems support expand core business use analytical rigor analyze amounts data help extract actionable insights analysis feature engineering optimization tools machine techniques develop maintain scalable reliable pipelines aiml processes participate aspects teach americas agile software cycle duties presentations communicating teams must haves prior intermediate higher python tensorflow keras fast ai ingestion preprocessing predictive analytics reporting advanced java spring rest jms sql comfortable unix version control containerization technologies continuous integration automation architectures interfacing throughout product lifecycle leading projects andor developers partnering test roll cognitive optional desired developing applications deploying cloud environments big associated hadoop databricks azure statistical modeling r able communicate clear effective manner technical nontechnical audiences interested staying uptodate recent advances detail oriented pressure effectively manage competing priorities education least fouryear degree computer science bsee mis demands position located site york national office limited travel may team loves collaborate partner every organization create worldclass technology staff corps members efficiently get kids access educational opportunity works hard also lot fun enjoy game nights quarterly trivia outings themed potlucks together eat explore others cultures favorite recipes perks joining join network individuals committed pursuing equity students professionals process value longevity employees offer comprehensive competitive benefits plan salary depends please advised discuss begin application deeply diversity inclusiveness america encourages ethnic racial socioeconomic backgrounds apply maximizing want engage contribute effort providing equal employment opportunities qualified discriminate basis race color ethnicity religion sex gender identity expression sexual orientation origin disability age marital status veteran pregnancy parental genetic information characteristics family member prohibited applicable law job description reflects assignment essential functions qualifications role nothing herein restricts managements right assign reassign eliminate responsibilities time next steps scroll bottom page link online still questions regarding feel free contact recruitment staffingteachforamerica org visit www teachforamerica orgaboutuscareers
236,"   Manage competing priorities across the company Maintain and automate reporting infrastructure Manage the design and architecture of our Data Warehouse Create Scripts to automate and manage ETL processes and Dependencies Advise on the design of our application DB, machine learning components, and our data infrastructure.  Cleaning and restructuring datasets Managing and optimizing reporting systems   At least two years experience as a Data Engineer, or related Software or DevOps experience Fluent in Python and SQL Experience with Data Warehouses and Schema Design Strong communication skills and experience communicating across business units Experience working with Data Scientists and Data Analysts Ability to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions Machine Learning experience a plus Analytical and BI skills a plus Postgres and RedShift experience is a plus",manage competing priorities across company maintain automate reporting infrastructure manage design architecture data warehouse create scripts automate manage etl processes dependencies advise design application db machine components data infrastructure cleaning restructuring datasets managing optimizing reporting systems least two data engineer software devops fluent python sql data warehouses schema design communication communicating across business units data scientists data analysts create fast solutions problems introduced changing iteration towards optimal solutions machine plus analytical bi plus postgres redshift plus,manage competing priorities across company maintain automate reporting infrastructure design architecture data warehouse create scripts etl processes dependencies advise application db machine components cleaning restructuring datasets managing optimizing systems least two engineer software devops fluent python sql warehouses schema communication communicating business units scientists analysts fast solutions problems introduced changing iteration towards optimal plus analytical bi postgres redshift
237," 5+ years of experience consulting in Data Engineering or Data Warehousing Hands-on experience with Google Cloud Platform Experience leading data warehousing, data ingestion, and data profiling activities Advanced SQL & Python skills Hands-on experience with Google cloud platform technologies  Google Cloud Platform Pub/Sub, Cloud Functions, DataFlow, DataProc  Hadoop, Spark, Hive , Cloud Machine Learning, Cloud Data Store and BigTable, BigQuery, DataLab, and DataStudio Migrating Data Pipelines to Google Cloud Platform  GCP     Build and Deploy Data Pipelines on Google Cloud to enable AI & ML capabilities.  Drive the development of cloud-based and hybrid data warehouses & business intelligence platforms Build Data Pipelines to ingest structured and Unstructured Data.  Gain hands-on experience with new data platforms and programming languages   ",consulting data engineering data warehousing handson google cloud platform leading data warehousing data ingestion data profiling activities advanced sql python handson google cloud platform technologies google cloud platform pubsub cloud functions dataflow dataproc hadoop spark hive cloud machine cloud data store bigtable bigquery datalab datastudio migrating data pipelines google cloud platform gcp build deploy data pipelines google cloud enable ai ml capabilities drive development cloudbased hybrid data warehouses business intelligence platforms build data pipelines ingest structured unstructured data gain handson data platforms programming languages,consulting data engineering warehousing handson google cloud platform leading ingestion profiling activities advanced sql python technologies pubsub functions dataflow dataproc hadoop spark hive machine store bigtable bigquery datalab datastudio migrating pipelines gcp build deploy enable ai ml capabilities drive development cloudbased hybrid warehouses business intelligence platforms ingest structured unstructured gain programming languages
238, A minimum of 5 years of hands-on technical experience with    A minimum of 5 years of hands-on technical experience with    A minimum of 5 years of hands-on technical experience with ,minimum handson technical minimum handson technical minimum handson technical,minimum handson technical
239,"BA/BS degree and 2+ years of experience in software engineering OR MS degree and 1+ years of experience  Degree in Computer Science or related field preferred Proficiency with large-scale distributed data processing systems like Hadoop, MR, Hive, Presto, and distributed data stores like Vertica and DruidExperience building large-scale multi-TB data processing systemsExperience supporting production systems    ",babs degree software engineering ms degree degree computer science proficiency largescale distributed data processing systems like hadoop mr hive presto distributed data stores like vertica druidexperience building largescale multitb data processing systemsexperience supporting production systems,babs degree software engineering ms computer science proficiency largescale distributed data processing systems like hadoop mr hive presto stores vertica druidexperience building multitb systemsexperience supporting production
240," Self-starter who can work in a highly demanding environment and maintains a positive attitude 4+ years of data engineering experience Experienced in Big Data development using AWS EMR, SQOOP, Hadoop, Spark, and HDFS Expert with one general purpose programming language, including but not limited to  Java, Scala, Python Awareness of new and emerging Big Data technologies and trends Advanced degree in computer science, engineering or a related field     ",selfstarter highly demanding maintains positive attitude data engineering experienced big data development aws emr sqoop hadoop spark hdfs expert one general purpose programming language limited java scala python awareness emerging big data technologies trends advanced degree computer science engineering,selfstarter highly demanding maintains positive attitude data engineering experienced big development aws emr sqoop hadoop spark hdfs expert one general purpose programming language limited java scala python awareness emerging technologies trends advanced degree computer science
241,"Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment. Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake. Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges. Mitigate Risks in our data infrastructure by developing the best in class tools and processes. Implement controls, policies, processes and best practices in the Data Engineering space. Evangelize an extremely high standard of code quality, system reliability, and performance. Help us improve our database deployment and change management process. Provide reliable and efficient Data services as part of the global data team. Work closely with the team on development best practices and standards. Be a mentor.     ",play pivotal design hands implementation role improving data infrastructure projectoriented influence cross functional architecture sprint planninggather process raw data scale internal external data sources expose mechanisms scale parallel processingdesign implement manage near realtime ingestion pipeline data warehouse hadoop data lake process unstructured data form suitable analysis empower stateoftheart analysis analysts scientists apissolve complex sql big data performance challenges mitigate risks data infrastructure developing best class tools processes implement controls policies processes best practices data engineering space evangelize extremely standard code reliability performance help us improve database deployment change management process reliable efficient data services part global data team closely team development best practices standards mentor,play pivotal design hands implementation role improving data infrastructure projectoriented influence cross functional architecture sprint planninggather process raw scale internal external sources expose mechanisms parallel processingdesign implement manage near realtime ingestion pipeline warehouse hadoop lake unstructured form suitable analysis empower stateoftheart analysts scientists apissolve complex sql big performance challenges mitigate risks developing best class tools processes controls policies practices engineering space evangelize extremely standard code reliability help us improve database deployment change management reliable efficient services part global team closely development standards mentor
242,"    Minimum of a degree in a quantitative subject, ideally containing computer science, software engineering, database, or data analysis modules 3 to 5 yearsâ experience working as a data engineer or software engineer with a strong focus on data, ideally in financial services  insurance, banking, instech, fintech  Strong and proven capability in dimensional data modelling  to design, implement and use dimensional database / datamart structures to ensure data is organised, aggregated and indexed effectively and ready for downstream reporting, analysis and statistical modelling Strong and proven capability in data acquisition  to design data capture systems, collect or stream data from a variety of systems, to creatively transform, manipulate, and methodically describe it Strong and proven capability with database administration  to maintain transactional and reference data, with migration alongside product development to ensure logical consistency Strong and proven capability with data validation and verification  to understand the pitfalls in different data sources and to perform necessary provenance and lineage checks, and implement various data testing A passion for quality software coding, data and analytics and some knowledge of statistics Experience working in a fast-paced environment and desire for good documentation, exceptional delivery, effective organisation, and high-quality communication with team and customers Strong and proven technical skills in MS SQL Server, SSIS, SSRS, Python, Git, Bash / Powershell Desirable technical skills in MS Azure  inc DataFactory , Apache Airflow, PowerBI, Linux  Redhat / CentOS  ",minimum degree quantitative subject ideally containing computer science software engineering database data analysis modules data engineer software engineer focus data ideally financial services insurance banking instech fintech proven capability dimensional data modelling design implement use dimensional database datamart structures data organised aggregated indexed effectively ready downstream reporting analysis statistical modelling proven capability data acquisition design data capture systems collect stream data variety systems creatively transform manipulate methodically describe proven capability database administration maintain transactional reference data migration alongside product development logical consistency proven capability data validation verification understand pitfalls different data sources perform necessary provenance lineage checks implement various data testing passion software coding data analytics statistics fastpaced desire good documentation exceptional delivery effective organisation highquality communication team customers proven technical ms sql server ssis ssrs python git bash powershell desirable technical ms azure inc datafactory apache airflow powerbi linux redhat centos,minimum degree quantitative subject ideally containing computer science software engineering database data analysis modules engineer focus financial services insurance banking instech fintech proven capability dimensional modelling design implement use datamart structures organised aggregated indexed effectively ready downstream reporting statistical acquisition capture systems collect stream variety creatively transform manipulate methodically describe administration maintain transactional reference migration alongside product development logical consistency validation verification understand pitfalls different sources perform necessary provenance lineage checks various testing passion coding analytics statistics fastpaced desire good documentation exceptional delivery effective organisation highquality communication team customers technical ms sql server ssis ssrs python git bash powershell desirable azure inc datafactory apache airflow powerbi linux redhat centos
243," Bachelorâs Degree  At least 8 years of experience in application development  At least 3 years of experience in big data technologies  Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper      ",bachelors degree least application development least big data technologies cassandra accumulo hbase spark hadoop hdfs avro mongodb zookeeper,bachelors degree least application development big data technologies cassandra accumulo hbase spark hadoop hdfs avro mongodb zookeeper
244," Proven software development ability, building complex and robust systems - at least 4 years Proven analytical/research ability - data science, cyber-security, intelligence, academic, etc.  Data engineering experience  data pipelines, ML architecture  - advantage Web scraping experience - advantage Python/Golang - advantage ML libraries and tools  pandas, scikit-learn, jupyter notebook  - advantage Team player, independent, able to work in an unstructured and fast-paced environment.    Maintain and improve our in-house git-inspired data pipeline orchestration system Write applications that fetch data by various means, including PDF parsing, HTML scraping, and more.  Perform Real Estate analysis based on the data repository we are continuously growing Extract insights from our data, and generate predictions using data science techniques Develop the algorithms which generate the datasets that are used by the whole R&D department  ",proven software development building complex robust systems least proven analyticalresearch data science cybersecurity intelligence academic data engineering data pipelines ml architecture advantage web scraping advantage pythongolang advantage ml libraries tools pandas scikitlearn jupyter notebook advantage team player independent able unstructured fastpaced maintain improve inhouse gitinspired data pipeline orchestration write applications fetch data various means pdf parsing html scraping perform real estate analysis based data repository continuously growing extract insights data generate predictions data science techniques develop algorithms generate datasets used whole rd department,proven software development building complex robust systems least analyticalresearch data science cybersecurity intelligence academic engineering pipelines ml architecture advantage web scraping pythongolang libraries tools pandas scikitlearn jupyter notebook team player independent able unstructured fastpaced maintain improve inhouse gitinspired pipeline orchestration write applications fetch various means pdf parsing html perform real estate analysis based repository continuously growing extract insights generate predictions techniques develop algorithms datasets used whole rd department
245,"  Strong skills in cloud, data pipelining, data modeling and productionizing AI/ML models.  Ability to multitask and work in a fast-paced, collaborative team environment Ability to travel in accordance with client and other job requirements Excellent written and oral communication skills; writing, publishing and conference-level presentation skills a plus  Understand business and technical requirements Assess the merits of different technology solutions  e. g. , cloud native vs.  cloud agnostic  to make recommendations Study and transform data science prototypes into production-ready systems with large volume data requirements Select appropriate datasets and data representation methods Design and implement solutions for data aggregation, improve data foundational procedures, integrate new data management technologies and software into the existing system and build data collection pipelines Conduct data discovery activities, performing root cause analysis, and make recommendations for the remediation of data quality issues Run  already existing  machine learning tests and experiments, retraining models when necessary Develop back-end components to improve responsiveness and overall performance Expose endpoints to provision model outputs in application frameworks  e. g. , Spring Boot for Java, Django or Flask for python.  Perform root cause analysis on data processes and pipelines to answer specific business questions, solve issues, and identify opportunities for improvement.  Make AI/ML insights available to the business and IT through data provisioning and channel integration Keep abreast of developments in the field; review solutions to ensure they are consistent with best practices in AI/ML architecture and engineering.  Be a team player using an agile delivery methodology while consistently delivering quality client services  ",cloud data pipelining data modeling productionizing aiml models multitask fastpaced collaborative team travel accordance client job written oral communication writing publishing conferencelevel presentation plus understand business technical assess merits different technology solutions e g cloud native vs cloud agnostic make recommendations study transform data science prototypes productionready systems volume data select appropriate datasets data representation methods design implement solutions data aggregation improve data foundational procedures integrate data management technologies software existing build data collection pipelines conduct data discovery activities performing root cause analysis make recommendations remediation data issues run already existing machine tests experiments retraining models necessary develop backend components improve responsiveness overall performance expose endpoints provision model outputs application frameworks e g spring boot java django flask python perform root cause analysis data processes pipelines answer specific business questions solve issues identify opportunities improvement make aiml insights available business data provisioning channel integration keep abreast developments review solutions consistent best practices aiml architecture engineering team player agile delivery methodology consistently delivering client services,cloud data pipelining modeling productionizing aiml models multitask fastpaced collaborative team travel accordance client job written oral communication writing publishing conferencelevel presentation plus understand business technical assess merits different technology solutions e g native vs agnostic make recommendations study transform science prototypes productionready systems volume select appropriate datasets representation methods design implement aggregation improve foundational procedures integrate management technologies software existing build collection pipelines conduct discovery activities performing root cause analysis remediation issues run already machine tests experiments retraining necessary develop backend components responsiveness overall performance expose endpoints provision model outputs application frameworks spring boot java django flask python perform processes answer specific questions solve identify opportunities improvement insights available provisioning channel integration keep abreast developments review consistent best practices architecture engineering player agile delivery methodology consistently delivering services
246,"Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs.  when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e. g.  consumer segmentation, revenue forecasting, image classification, etc.  ; Understanding of machine learning algorithms  e. g.  k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.   and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.   Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs.  when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e. g.  consumer segmentation, revenue forecasting, image classification, etc.  ; Understanding of machine learning algorithms  e. g.  k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.   and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.     Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs.  when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e. g.  consumer segmentation, revenue forecasting, image classification, etc.  ; Understanding of machine learning algorithms  e. g.  k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.   and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.  ",exploring analytical technologies evaluating technical commercial viability quickly week sprint cycles develop proofofconcepts prototype models demoed explained data scientists internal stakeholders clients testing rejecting hypotheses around data processing machine model building demonstrating experiment fail quickly recognize need assistance vs conclude technology suitable task building machine pipelines ingest clean data make predictions staying abreast ai research leading labs reading papers experimenting code developing innovative solutions perspectives ai published academic journalsarxiv shared clients demonstrating continuously learn technologies quickly evaluate technical commercial viability applying machine techniques addressing variety problems e g consumer segmentation revenue forecasting image classification understanding machine algorithms e g knearest neighbors random forests ensemble methods deep neural networks appropriate use technique building machine models systems interpreting output communicating results moving models development production plus conducting research lab publishing plus exploring analytical technologies evaluating technical commercial viability quickly week sprint cycles develop proofofconcepts prototype models demoed explained data scientists internal stakeholders clients testing rejecting hypotheses around data processing machine model building demonstrating experiment fail quickly recognize need assistance vs conclude technology suitable task building machine pipelines ingest clean data make predictions staying abreast ai research leading labs reading papers experimenting code developing innovative solutions perspectives ai published academic journalsarxiv shared clients demonstrating continuously learn technologies quickly evaluate technical commercial viability applying machine techniques addressing variety problems e g consumer segmentation revenue forecasting image classification understanding machine algorithms e g knearest neighbors random forests ensemble methods deep neural networks appropriate use technique building machine models systems interpreting output communicating results moving models development production plus conducting research lab publishing plus exploring analytical technologies evaluating technical commercial viability quickly week sprint cycles develop proofofconcepts prototype models demoed explained data scientists internal stakeholders clients testing rejecting hypotheses around data processing machine model building demonstrating experiment fail quickly recognize need assistance vs conclude technology suitable task building machine pipelines ingest clean data make predictions staying abreast ai research leading labs reading papers experimenting code developing innovative solutions perspectives ai published academic journalsarxiv shared clients demonstrating continuously learn technologies quickly evaluate technical commercial viability applying machine techniques addressing variety problems e g consumer segmentation revenue forecasting image classification understanding machine algorithms e g knearest neighbors random forests ensemble methods deep neural networks appropriate use technique building machine models systems interpreting output communicating results moving models development production plus conducting research lab publishing plus,exploring analytical technologies evaluating technical commercial viability quickly week sprint cycles develop proofofconcepts prototype models demoed explained data scientists internal stakeholders clients testing rejecting hypotheses around processing machine model building demonstrating experiment fail recognize need assistance vs conclude technology suitable task pipelines ingest clean make predictions staying abreast ai research leading labs reading papers experimenting code developing innovative solutions perspectives published academic journalsarxiv shared continuously learn evaluate applying techniques addressing variety problems e g consumer segmentation revenue forecasting image classification understanding algorithms knearest neighbors random forests ensemble methods deep neural networks appropriate use technique systems interpreting output communicating results moving development production plus conducting lab publishing
247,"   Create and maintain data pipeline architectures for providing a real time and batch processing platform for all models to run on Create and maintain APIs for our machine learning models Coordinating the movement of data between data sources in cloud environments  streaming and batch  Assemble large, complex data sets that meet functional / non-functional business requirements   4+ years' experience developing, maintaining, and testing APIs & infrastructure for data generation.  Experience with big data processing  Flink, Spark, Kafka, etc.  Experience with different databases, such as Redis, Elasticsearch, Postgres or Cassandra.  Strong understanding of one of  Python, Java, or Scala Experience with CI/CD infrastructure and a strong supporter of unit / integration testing",create maintain data pipeline architectures providing real time batch processing platform models run create maintain apis machine models coordinating movement data data sources cloud environments streaming batch assemble complex data sets meet functional nonfunctional business developing maintaining testing apis infrastructure data generation big data processing flink spark kafka different databases redis elasticsearch postgres cassandra understanding one python java scala cicd infrastructure supporter unit integration testing,create maintain data pipeline architectures providing real time batch processing platform models run apis machine coordinating movement sources cloud environments streaming assemble complex sets meet functional nonfunctional business developing maintaining testing infrastructure generation big flink spark kafka different databases redis elasticsearch postgres cassandra understanding one python java scala cicd supporter unit integration
248,"  At least 1-2 years of professional experience programming in Python Exposure to ETL/ELT pipeline automation Exposure to basic database concepts    STEM Bachelor's required, graduate degree is a big plus  ",least professional programming python exposure etlelt pipeline automation exposure basic database concepts stem bachelors graduate degree big plus,least professional programming python exposure etlelt pipeline automation basic database concepts stem bachelors graduate degree big plus
249,"Undergraduate or graduate degree in a technical or scientific field, such as Computer Science, Engineering, Mathematics, or similar 2-5 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role Analytically-minded and detail-oriented  you actually like staring at data, looking for patterns and outliers, establishing data models, and rigorously answering questions Expertise in data engineering languages such as Python, Java, Scala, SQL.  Data modeling and data governance experience; you've designed and implemented a data mart, a data warehouse, or the back-end database of an application Experience building ETL and data pipelines, especially via code-oriented systems like Spark, Airflow, Luigi, or similar, and with varied data formats Cloud-oriented but comfortable with on-premises infrastructure Experience operating in a secure networking environment  e. g.  behind a corporate proxy  is a plus Creative problem-solving skills, especially in situations where ""nobody has tried this before"" Excellent technical documentation and writing skills  you know Markdown syntax cold, and have published API documentation or similar You're not satisfied with ""close enough"" solutions, and you design long-term solutions that are robust over time You have a bias towards automation  ""one-time scripts"" eat away at your soul a little bit each time you write one Proficiency in statistics and machine learning is a nice-to-have, and interest in learning these is even better! Familiarity with visualizing data with Tableau and similar tools Great customer service and technical troubleshooting skills   Full-stack design, development, and operation of core data stack including data lake, data warehouse, and data pipelines Build data flows for data acquisition, aggregation, and modeling, using both batch and streaming paradigms Consolidate/join datasets to create easily consumable, consistent, holistic information Design and implement machine learning models and prediction APIs, and ensure their operational performance over time Empower data scientists and data analysts to be as self-sufficient as possible by building core systems and developing reusable library code Support and optimize desktop and cloud environments for data scientists and data analysts Ensure efficiency, quality, resiliency of data science core systems Work with senior technical staff throughout Blackstone's portfolio companies to develop data flows   ",undergraduate graduate degree technical scientific computer science engineering mathematics similar professional data engineer software engineer data analyst data scientist role analyticallyminded detailoriented actually like staring data looking patterns outliers establishing data models rigorously answering questions expertise data engineering languages python java scala sql data modeling data governance youve designed implemented data mart data warehouse backend database application building etl data pipelines especially via codeoriented systems like spark airflow luigi similar varied data formats cloudoriented comfortable onpremises infrastructure operating secure networking e g behind corporate proxy plus creative problemsolving especially situations nobody tried technical documentation writing know markdown syntax cold published api documentation similar youre satisfied close enough solutions design longterm solutions robust time bias towards automation onetime scripts eat away soul little bit time write one proficiency statistics machine nicetohave interest even better familiarity visualizing data tableau similar tools great customer service technical troubleshooting fullstack design development operation core data stack data lake data warehouse data pipelines build data flows data acquisition aggregation modeling batch streaming paradigms consolidatejoin datasets create easily consumable consistent holistic information design implement machine models prediction apis operational performance time empower data scientists data analysts selfsufficient possible building core systems developing reusable library code support optimize desktop cloud environments data scientists data analysts efficiency resiliency data science core systems senior technical staff throughout blackstones portfolio companies develop data flows,undergraduate graduate degree technical scientific computer science engineering mathematics similar professional data engineer software analyst scientist role analyticallyminded detailoriented actually like staring looking patterns outliers establishing models rigorously answering questions expertise languages python java scala sql modeling governance youve designed implemented mart warehouse backend database application building etl pipelines especially via codeoriented systems spark airflow luigi varied formats cloudoriented comfortable onpremises infrastructure operating secure networking e g behind corporate proxy plus creative problemsolving situations nobody tried documentation writing know markdown syntax cold published api youre satisfied close enough solutions design longterm robust time bias towards automation onetime scripts eat away soul little bit write one proficiency statistics machine nicetohave interest even better familiarity visualizing tableau tools great customer service troubleshooting fullstack development operation core stack lake build flows acquisition aggregation batch streaming paradigms consolidatejoin datasets create easily consumable consistent holistic information implement prediction apis operational performance empower scientists analysts selfsufficient possible developing reusable library code support optimize desktop cloud environments efficiency resiliency senior staff throughout blackstones portfolio companies develop
250,"   Identifying, ingesting, and enriching a wide range of structured and unstructured big data into datasets for analysis; Operating and extending the data infrastructure platform to deliver production-grade data curation and analysis services; Thinking and acting as data integrity managers - amplifying data quality and completeness with a process-driven approach and measurement dashboards; Owning end-to-end data workflows and developing deep domain expertise on the underlying actors and behaviors manifested through data; Communicating data-driven analysis and insights in the form of âgolden triangleâ investment insights that supports our clients investment process.    Masterâs degree or PHD in Mathematics, Finance, Computer Science, Engineering or related fields.  5+ years of experience working with large structured and unstructured datasets.  Expertise in Python and data analysis tools and languages  PyData, R, Julia, Matlab, Tableau .  Expertise in SQL and relational databases.  Deep intellectual curiosity and passion for data.  Excellent problem solving, communication, and analytical skills.  Eagerness to work in an evolving and fast-paced environment.  While financial industry experience is a plus, we are open-minded in our search for critical thinkers who are passionate about technology and data.  Proven track record of working with and managing junior researchers and developers",identifying ingesting enriching wide range structured unstructured big data datasets analysis operating extending data infrastructure platform deliver productiongrade data curation analysis services thinking acting data integrity managers amplifying data completeness processdriven approach measurement dashboards owning endtoend data workflows developing deep domain expertise underlying actors behaviors manifested data communicating datadriven analysis insights form golden triangle investment insights supports clients investment process masters degree phd mathematics finance computer science engineering fields structured unstructured datasets expertise python data analysis tools languages pydata r julia matlab tableau expertise sql relational databases deep intellectual curiosity passion data problem solving communication analytical eagerness evolving fastpaced financial industry plus openminded search critical thinkers passionate technology data proven track record managing junior researchers developers,identifying ingesting enriching wide range structured unstructured big data datasets analysis operating extending infrastructure platform deliver productiongrade curation services thinking acting integrity managers amplifying completeness processdriven approach measurement dashboards owning endtoend workflows developing deep domain expertise underlying actors behaviors manifested communicating datadriven insights form golden triangle investment supports clients process masters degree phd mathematics finance computer science engineering fields python tools languages pydata r julia matlab tableau sql relational databases intellectual curiosity passion problem solving communication analytical eagerness evolving fastpaced financial industry plus openminded search critical thinkers passionate technology proven track record managing junior researchers developers
251,"10+ years of experience working in Data Engineering or Data WarehousingHands-on experience with leading commercial Cloud platforms, including AWS, Azure, or GoogleExperience leading data warehousing, data ingestion, and data profiling activitiesAdvanced SQL & Python skillsStrong aptitude for learning new technologies and analytics techniquesHighly self-motivated and able to work independently as well as in a team environmentUnderstanding of agile project approaches and methodologiesExperience working with Business Stakeholders to elicit business requirementsExperience building and migrating complex ETL pipelinesFamiliarity with or strong desire to learn quantitative analysis techniques  e. g. , predictive modeling, machine learning, segmentation, optimization, clustering, regression Bachelor's degree in Business Analytics, Computer Science or a closely related field required  Lead a team to develop Cloud enabled Data and Analytics solutionsDrive the development of cloud-based and hybrid data warehouses & business intelligence platformsBuild Data Pipelines to ingest structured and Unstructured Data. Gain hands-on experience with new data platforms and programming languages  ",data engineering data warehousinghandson leading commercial cloud platforms aws azure googleexperience leading data warehousing data ingestion data profiling activitiesadvanced sql python skillsstrong aptitude technologies analytics techniqueshighly selfmotivated able independently well team environmentunderstanding agile project approaches methodologiesexperience business stakeholders elicit business requirementsexperience building migrating complex etl pipelinesfamiliarity desire learn quantitative analysis techniques e g predictive modeling machine segmentation optimization clustering regression bachelors degree business analytics computer science closely lead team develop cloud enabled data analytics solutionsdrive development cloudbased hybrid data warehouses business intelligence platformsbuild data pipelines ingest structured unstructured data gain handson data platforms programming languages,data engineering warehousinghandson leading commercial cloud platforms aws azure googleexperience warehousing ingestion profiling activitiesadvanced sql python skillsstrong aptitude technologies analytics techniqueshighly selfmotivated able independently well team environmentunderstanding agile project approaches methodologiesexperience business stakeholders elicit requirementsexperience building migrating complex etl pipelinesfamiliarity desire learn quantitative analysis techniques e g predictive modeling machine segmentation optimization clustering regression bachelors degree computer science closely lead develop enabled solutionsdrive development cloudbased hybrid warehouses intelligence platformsbuild pipelines ingest structured unstructured gain handson programming languages
252," Bachelor's degree in Computer Science, Information Systems, or related field 5+ years of experience in data engineering or data infrastructure role 3+ years of experience with Spark, Hadoop, Airflow, Kafka, etc Proficient in data modeling and system design skills Advanced experience of SQL and Python  data analysis libraries like pandas, numpy, scikit-learn, etc  Proficient experience with cloud such as Azure, GCP or AWS  Azure is target platform  Proficient experience in building and maintaining data processing pipelines Good understanding of SQL/NoSQL databases such as Cosmos DB, Postgres, MongoDB, MySQL etc.  Experience with version control git , CI/CD  Microsoft Azure Dev Ops, Jenkins  Experience with supporting Data Science teams on feature engineering, model training and deployment tasks Comfortable in Windows/Linux environment Strong business communication skills Strong drive to constantly learn and keep up to speed with the new technologies Ability to understand complex business priorities and translate them into clearly defined technical/data specifications for implementation Ability to deal with ambiguity and work with rapidly changing business data    ",bachelors degree computer science information systems data engineering data infrastructure role spark hadoop airflow kafka proficient data modeling design advanced sql python data analysis libraries like pandas numpy scikitlearn proficient cloud azure gcp aws azure target platform proficient building maintaining data processing pipelines good understanding sqlnosql databases cosmos db postgres mongodb mysql version control git cicd microsoft azure dev ops jenkins supporting data science teams feature engineering model training deployment tasks comfortable windowslinux business communication drive constantly learn keep speed technologies understand complex business priorities translate clearly defined technicaldata specifications implementation deal ambiguity rapidly changing business data,bachelors degree computer science information systems data engineering infrastructure role spark hadoop airflow kafka proficient modeling design advanced sql python analysis libraries like pandas numpy scikitlearn cloud azure gcp aws target platform building maintaining processing pipelines good understanding sqlnosql databases cosmos db postgres mongodb mysql version control git cicd microsoft dev ops jenkins supporting teams feature model training deployment tasks comfortable windowslinux business communication drive constantly learn keep speed technologies understand complex priorities translate clearly defined technicaldata specifications implementation deal ambiguity rapidly changing
253,"In pursuit of a Masterâs or PhD in Computer Science, Computer Engineering, Software Engineering, or other relevant quantitative fieldsWorking knowledge of at least two programming languages.  Examples  R, Python, Julia, Java, or ScalaWorking knowledge of cloud computing technologies such as AWS, Azure, or GCPFamiliarity with Spark, Hive and/or HadoopCumulative GPA of 3. 0 or aboveMust be 18 years of age or olderMust be authorized to work in the United States without visa sponsorship by NBCUniversalNeeds to be able to work on-site in New York, NYInternships at NBCUniversal are paid and do not require course creditDeadline  Interested applicants are encouraged to apply by November 1st, 2019Spring Program Dates  January â May 2020    In pursuit of a Masterâs or PhD in Computer Science, Computer Engineering, Software Engineering, or other relevant quantitative fieldsWorking knowledge of at least two programming languages.  Examples  R, Python, Julia, Java, or ScalaWorking knowledge of cloud computing technologies such as AWS, Azure, or GCPFamiliarity with Spark, Hive and/or HadoopCumulative GPA of 3. 0 or aboveMust be 18 years of age or olderMust be authorized to work in the United States without visa sponsorship by NBCUniversalNeeds to be able to work on-site in New York, NYInternships at NBCUniversal are paid and do not require course creditDeadline  Interested applicants are encouraged to apply by November 1st, 2019Spring Program Dates  January â May 2020",pursuit masters phd computer science computer engineering software engineering relevant quantitative fieldsworking least two programming languages examples r python julia java scalaworking cloud computing technologies aws azure gcpfamiliarity spark hive andor hadoopcumulative gpa abovemust age oldermust authorized united states without visa sponsorship nbcuniversalneeds able onsite york nyinternships nbcuniversal paid require course creditdeadline interested applicants encouraged apply november st spring program dates january may pursuit masters phd computer science computer engineering software engineering relevant quantitative fieldsworking least two programming languages examples r python julia java scalaworking cloud computing technologies aws azure gcpfamiliarity spark hive andor hadoopcumulative gpa abovemust age oldermust authorized united states without visa sponsorship nbcuniversalneeds able onsite york nyinternships nbcuniversal paid require course creditdeadline interested applicants encouraged apply november st spring program dates january may,pursuit masters phd computer science engineering software relevant quantitative fieldsworking least two programming languages examples r python julia java scalaworking cloud computing technologies aws azure gcpfamiliarity spark hive andor hadoopcumulative gpa abovemust age oldermust authorized united states without visa sponsorship nbcuniversalneeds able onsite york nyinternships nbcuniversal paid require course creditdeadline interested applicants encouraged apply november st spring program dates january may
254," 6-10 years of relevant experience in Apps Development or systems analysis role Extensive experience system analysis and in programming of software applications Experience in managing and implementing successful projects Subject Matter Expert  SME  in at least one area of Applications Development Ability to adjust priorities quickly as circumstances dictate Demonstrated leadership and project management skills Consistently demonstrates clear and concise written and verbal communication   Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.   Bachelorâs degree/University degree or equivalent experience Masterâs degree preferred ",relevant apps development systems analysis role extensive analysis programming software applications managing implementing successful projects subject matter expert sme least one area applications development adjust priorities quickly circumstances dictate demonstrated leadership project management consistently demonstrates clear concise written verbal communication partner multiple management teams appropriate integration functions meet goals well identify define necessary enhancements deploy products process improvements resolve variety impact problemsprojects indepth evaluation complex business processes processes industry standards expertise area advanced applications programming application design adheres overall architecture blueprint utilize advanced flow develop standards coding testing debugging implementation develop comprehensive areas business architecture infrastructure integrate accomplish business goals indepth analysis interpretive thinking define issues develop innovative solutions serve advisor coach midlevel developers analysts allocating necessary appropriately assess risk business decisions made demonstrating particular consideration firms reputation safeguarding citigroup clients assets driving compliance applicable laws rules regulations adhering policy applying sound ethical judgment regarding personal behavior conduct business practices escalating managing reporting control issues transparency bachelors degreeuniversity degree masters degree,relevant apps development systems analysis role extensive programming software applications managing implementing successful projects subject matter expert sme least one area adjust priorities quickly circumstances dictate demonstrated leadership project management consistently demonstrates clear concise written verbal communication partner multiple teams appropriate integration functions meet goals well identify define necessary enhancements deploy products process improvements resolve variety impact problemsprojects indepth evaluation complex business processes industry standards expertise advanced application design adheres overall architecture blueprint utilize flow develop coding testing debugging implementation comprehensive areas infrastructure integrate accomplish interpretive thinking issues innovative solutions serve advisor coach midlevel developers analysts allocating appropriately assess risk decisions made demonstrating particular consideration firms reputation safeguarding citigroup clients assets driving compliance applicable laws rules regulations adhering policy applying sound ethical judgment regarding personal behavior conduct practices escalating reporting control transparency bachelors degreeuniversity degree masters
255," Knowledge of emerging data integration technologies Spark Streaming, Kafka Streams, Kafka Connect, etc Data Science and Modeling pipeline experience Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools Strong expertise/background with Linux Familiarity with MS Sql Server, SSIS, SSAS    Determine optimal solutions for integrating data from a variety of sources into a common data warehouse Implement, maintain and monitor batch & stream data pipelines with best practice quality controls Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations Collaborate with data infrastructure team to deploy necessary infra capabilities Day to day operational support of data infrastructure, and services    5+ Years of related work experience Solid command of Python, Java, and/or Scala Experience in stream processing technology  Kafka, Spark, Storm, Samza, Flink, etc  In-depth knowledge of SQL, data modeling and data warehousing concepts Distributed and low latency  streaming  application architecture Familiarity with API design CI/CD systems experience  Jenkins, Github, etc  Experience adhering to robust audit standards BS or MS degree in Computer Science or Engineering related experience ",emerging data integration technologies spark streaming kafka streams kafka connect data science modeling pipeline familiarity machine frameworks ho scikitlearn similar tools expertisebackground linux familiarity ms sql server ssis ssas determine optimal solutions integrating data variety sources common data warehouse implement maintain monitor batch stream data pipelines best practice controls evaluate relevant mature technologies needs gaps opportunities arise closely collaboratively agile analysts engineers product teams analyze issues find insights covering business operations collaborate data infrastructure team deploy necessary infra capabilities day day operational support data infrastructure services solid command python java andor scala stream processing technology kafka spark storm samza flink indepth sql data modeling data warehousing concepts distributed low latency streaming application architecture familiarity api design cicd systems jenkins github adhering robust audit standards bs ms degree computer science engineering,emerging data integration technologies spark streaming kafka streams connect science modeling pipeline familiarity machine frameworks ho scikitlearn similar tools expertisebackground linux ms sql server ssis ssas determine optimal solutions integrating variety sources common warehouse implement maintain monitor batch stream pipelines best practice controls evaluate relevant mature needs gaps opportunities arise closely collaboratively agile analysts engineers product teams analyze issues find insights covering business operations collaborate infrastructure team deploy necessary infra capabilities day operational support services solid command python java andor scala processing technology storm samza flink indepth warehousing concepts distributed low latency application architecture api design cicd systems jenkins github adhering robust audit standards bs degree computer engineering
256," Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs.  5+ years professional experience working with data extract/manipulation logic.  5+ years professional experience with object-oriented programming, functional programming, and data design.  7+ years experience with Development, Engineering, R&D or Information Technology.  3+ years working with a public cloud big data ecosystem  certification in AWS a plus .  3+ years working with MPP databases, distributed databases, and/or Hadoop.   Passion for data engineering, able to excite and lead by example and mentoring others.  Hungry and eager to learn new systems and technologies.  Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.  Ability to deliver exceptional results through iterative improvement rather than initial perfection.  Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including  business users, technical staff, senior level colleagues, vendors, and partners.  An extensive track record that demonstrates effectiveness in driving business results through data and analytics.  The ability to develop and articulate a compelling vision and generate necessary consensus.  A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.  A proven ability to influence decision making across large organizations.  A proven ability to hire, develop, and effectively lead deeply technical resources.  Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.  Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.  Create an environment where people from diverse cultures and backgrounds work together effectively.    Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably.  Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions.  Gather and process all types of data including raw, structured, semi-structured, and unstructured data.  Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics.  Build and maintain dimensional data warehouses in support of business intelligence tools.  Develop data catalogs and data validations to ensure clarity and correctness of key business metrics.  Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result.  Derive an overall strategy of data management, within an established information architecture  including both structured and unstructured data , that supports the development and secure operation of existing and new information and digital services.  Plan effective data storage, security, sharing and publishing within the organization.  Ensure data quality and implement tools and frameworks for automating the identification of data quality issues.  Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.  Mentor and lead data engineers providing technical guidance and oversight.  Provide ongoing support, monitoring, and maintenance of deployed products.  Drive and maintain a culture of quality, innovation and experimentation.    Passion for data engineering, able to excite and lead by example and mentoring others.  Hungry and eager to learn new systems and technologies.  Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.  Ability to deliver exceptional results through iterative improvement rather than initial perfection.  Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including  business users, technical staff, senior level colleagues, vendors, and partners.  An extensive track record that demonstrates effectiveness in driving business results through data and analytics.  The ability to develop and articulate a compelling vision and generate necessary consensus.  A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.  A proven ability to influence decision making across large organizations.  A proven ability to hire, develop, and effectively lead deeply technical resources.  Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.  Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.  Create an environment where people from diverse cultures and backgrounds work together effectively.  ",advanced degree relevant study strongly desirable particularly computer science engineering level programs professional data extractmanipulation logic professional objectoriented programming functional programming data design development engineering rd information technology public cloud big data ecosystem certification aws plus mpp databases distributed databases andor hadoop passion data engineering able excite lead example mentoring others hungry eager learn systems technologies selfdirected enjoys challenge freedom deciding impactful thing next deliver exceptional results iterative improvement rather initial perfection communication presentation interact appropriately levels organization business users technical staff senior level colleagues vendors partners extensive track record demonstrates effectiveness driving business results data analytics develop articulate compelling vision generate necessary consensus successful history translating business objectives problems analytic problems analytic solutions actionable business solutions proven influence decision making across organizations proven hire develop effectively lead deeply technical resources demonstrate foster sense urgency commitment accountability making sound decisions achieving goals articulate inspire engage commitment plan action aligned organizational mission goals create people diverse cultures backgrounds together effectively build deploy streaming batch data pipelines capable processing storing petabytes data quickly reliably collaborate product teams data analysts data scientists design build dataforward solutions gather process types data raw structured semistructured unstructured data integrate variety data providers ranging marketing web analytics consumer devices iot telematics build maintain dimensional data warehouses support business intelligence tools develop data catalogs data validations clarity correctness key business metrics design code test correct document programs scripts agreed standards tools achieve wellengineered result derive overall strategy data management within established information architecture structured unstructured data supports development secure operation existing information digital services plan effective data storage security sharing publishing within organization data implement tools frameworks automating identification data issues collaborate internal external data providers data validation providing feedback making customized changes data feeds data mappings mentor lead data engineers providing technical guidance oversight ongoing support monitoring maintenance deployed products drive maintain culture innovation experimentation passion data engineering able excite lead example mentoring others hungry eager learn systems technologies selfdirected enjoys challenge freedom deciding impactful thing next deliver exceptional results iterative improvement rather initial perfection communication presentation interact appropriately levels organization business users technical staff senior level colleagues vendors partners extensive track record demonstrates effectiveness driving business results data analytics develop articulate compelling vision generate necessary consensus successful history translating business objectives problems analytic problems analytic solutions actionable business solutions proven influence decision making across organizations proven hire develop effectively lead deeply technical resources demonstrate foster sense urgency commitment accountability making sound decisions achieving goals articulate inspire engage commitment plan action aligned organizational mission goals create people diverse cultures backgrounds together effectively,advanced degree relevant study strongly desirable particularly computer science engineering level programs professional data extractmanipulation logic objectoriented programming functional design development rd information technology public cloud big ecosystem certification aws plus mpp databases distributed andor hadoop passion able excite lead example mentoring others hungry eager learn systems technologies selfdirected enjoys challenge freedom deciding impactful thing next deliver exceptional results iterative improvement rather initial perfection communication presentation interact appropriately levels organization business users technical staff senior colleagues vendors partners extensive track record demonstrates effectiveness driving analytics develop articulate compelling vision generate necessary consensus successful history translating objectives problems analytic solutions actionable proven influence decision making across organizations hire effectively deeply resources demonstrate foster sense urgency commitment accountability sound decisions achieving goals inspire engage plan action aligned organizational mission create people diverse cultures backgrounds together build deploy streaming batch pipelines capable processing storing petabytes quickly reliably collaborate product teams analysts scientists dataforward gather process types raw structured semistructured unstructured integrate variety providers ranging marketing web consumer devices iot telematics maintain dimensional warehouses support intelligence tools catalogs validations clarity correctness key metrics code test correct document scripts agreed standards achieve wellengineered result derive overall strategy management within established architecture supports secure operation existing digital services effective storage security sharing publishing implement frameworks automating identification issues internal external validation providing feedback customized changes feeds mappings mentor engineers guidance oversight ongoing monitoring maintenance deployed products drive culture innovation experimentation
257,"     3+ years experience in software development 3+ years experience designing SQL tables, choosing indexes, tuning queries and understanding the intricacies required to optimize a table in different environments ",software development designing sql tables choosing indexes tuning queries understanding intricacies optimize table different environments,software development designing sql tables choosing indexes tuning queries understanding intricacies optimize table different environments
258,"  You will join a small team partnering with product owners and developers at Roivant and Vants to provide end-to-end data solutions for technology tools and products.  You will follow best coding practices to build and optimize tools for data ingestion and storage, including components of client's Data Lake/Warehouse platform.  You will automate and maintain data processing pipelines, implement modern ETL infrastructure, and continuously improve the efficiency of our platform.  You will serve as a subject matter expert on big data analytics projects that provide insights for business and technical stakeholders.    BA/BS degree with strong academic performance, preferably in a quantitative field 4+ years experience with Python, database development, Git, Linux and AWS  S3, EC2, SNS, Lambda, SQS  Experience with Spark, terraform, Docker, big data and/or healthcare data preferred Knowledge of Scrum and desire to work in an incredibly fast-moving, agile environment Team player with strong communication skills and the ability to work with minimal supervision Quick and scrappy learner who adapts well to a fast-moving environment and gets things done; experience in high-growth or startup environments a plus",join small team partnering product owners developers roivant vants endtoend data solutions technology tools products follow best coding practices build optimize tools data ingestion storage components clients data lakewarehouse platform automate maintain data processing pipelines implement modern etl infrastructure continuously improve efficiency platform serve subject matter expert big data analytics projects insights business technical stakeholders babs degree academic performance preferably quantitative python database development git linux aws ec sns lambda sqs spark terraform docker big data andor healthcare data scrum desire incredibly fastmoving agile team player communication minimal supervision quick scrappy learner adapts well fastmoving gets things done highgrowth startup environments plus,join small team partnering product owners developers roivant vants endtoend data solutions technology tools products follow best coding practices build optimize ingestion storage components clients lakewarehouse platform automate maintain processing pipelines implement modern etl infrastructure continuously improve efficiency serve subject matter expert big analytics projects insights business technical stakeholders babs degree academic performance preferably quantitative python database development git linux aws ec sns lambda sqs spark terraform docker andor healthcare scrum desire incredibly fastmoving agile player communication minimal supervision quick scrappy learner adapts well gets things done highgrowth startup environments plus
259," Bachelorâs Degree in related field and 3 years of experience in JavaScript, SQL and/or Python U. S.  Citizenship Ability to work in fast paced prototyping environment  Average project length is 6-12 weeks, multiple projects assigned simultaneously  JavaScript GitOps  Github, Gitlab  Pandas Python Angular React Tensorflow / Keras / Pytorch / Fastai Flask / Django Jupyter Notebooks / Anaconda Spark Databricks Docker Kubernetes SQL Docker Compose Docker Swarm Vue Jenkins Gatsby   ",bachelors degree javascript sql andor python u citizenship fast paced prototyping average project length weeks multiple projects assigned simultaneously javascript gitops github gitlab pandas python angular react tensorflow keras pytorch fastai flask django jupyter notebooks anaconda spark databricks docker kubernetes sql docker compose docker swarm vue jenkins gatsby,bachelors degree javascript sql andor python u citizenship fast paced prototyping average project length weeks multiple projects assigned simultaneously gitops github gitlab pandas angular react tensorflow keras pytorch fastai flask django jupyter notebooks anaconda spark databricks docker kubernetes compose swarm vue jenkins gatsby
260,"  Knowledge of emerging data integration technologies Spark Streaming, Kafka Streams, Kafka Connect, etc Exposure to Kubernetes and Helm Data Science and Modeling pipeline experience Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools Strong expertise/background with Linux Familiarity with MS Sql Server, SSIS, SSAS   Determine optimal solutions for integrating data from a variety of sources into a common data warehouse Implement, maintain and monitor batch & stream data pipelines with best practice quality controls Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations Collaborate with our other data teams, as they are customers Day to day operational support of data infrastructure, and services    Solid command Java or Scala, and Python Experience in stream processing technology  Kafka, Spark, Storm, Samza, Flink, etc  Knowledge of SQL, data modeling and data warehousing concepts Distributed and low latency  streaming  application architecture Familiarity with API design CI/CD systems experience  Jenkins, Github, etc  Experience adhering to robust audit standards BS or MS degree in Computer Science or Engineering related experience ",emerging data integration technologies spark streaming kafka streams kafka connect exposure kubernetes helm data science modeling pipeline familiarity machine frameworks ho scikitlearn similar tools expertisebackground linux familiarity ms sql server ssis ssas determine optimal solutions integrating data variety sources common data warehouse implement maintain monitor batch stream data pipelines best practice controls evaluate relevant mature technologies needs gaps opportunities arise closely collaboratively agile analysts engineers product teams analyze issues find insights covering business operations collaborate data teams customers day day operational support data infrastructure services solid command java scala python stream processing technology kafka spark storm samza flink sql data modeling data warehousing concepts distributed low latency streaming application architecture familiarity api design cicd systems jenkins github adhering robust audit standards bs ms degree computer science engineering,emerging data integration technologies spark streaming kafka streams connect exposure kubernetes helm science modeling pipeline familiarity machine frameworks ho scikitlearn similar tools expertisebackground linux ms sql server ssis ssas determine optimal solutions integrating variety sources common warehouse implement maintain monitor batch stream pipelines best practice controls evaluate relevant mature needs gaps opportunities arise closely collaboratively agile analysts engineers product teams analyze issues find insights covering business operations collaborate customers day operational support infrastructure services solid command java scala python processing technology storm samza flink warehousing concepts distributed low latency application architecture api design cicd systems jenkins github adhering robust audit standards bs degree computer engineering
261,"At least 5 years of software development experience deploying enterprise applications related to Data Management/Data Analytics & reporting.  2+ years being part of Agile teams â Scrum or Kanban 2+ years of programming experience in Python.  5+ years of Database experience â SQL, Teradata, Oracle 2+ years of experience with big data technologies - Hadoop, Hive, Spark  PySpark or Spark Scala  Experience with Git/SVN or similar configuration management tool Experience with ETL tools like Informatica is a plus Experience with Reporting tools like Tableau/Looker is a plus.  Excellent troubleshooting skills Strong communication skills Fluent in BDD and TDD development methodologies Work in an agile CI/CD environment  Jenkins experience a plus  Prior experience with Health care domains is a plus     ",least software development deploying enterprise applications data managementdata analytics reporting part agile teams scrum kanban programming python database sql teradata oracle big data technologies hadoop hive spark pyspark spark scala gitsvn similar configuration management tool etl tools like informatica plus reporting tools like tableaulooker plus troubleshooting communication fluent bdd tdd development methodologies agile cicd jenkins plus prior health care domains plus,least software development deploying enterprise applications data managementdata analytics reporting part agile teams scrum kanban programming python database sql teradata oracle big technologies hadoop hive spark pyspark scala gitsvn similar configuration management tool etl tools like informatica plus tableaulooker troubleshooting communication fluent bdd tdd methodologies cicd jenkins prior health care domains
262,"Experience of near Real Time & Batch Data Pipeline development in a similar Big Data Engineering roleProgramming skills in one or more of the following  Java, Scala, R, Python, SQL and experience in writing reusable/efficient code to automate analysis and data processesExperience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devicesExperience implementing scalable, distributed, and highly available systems using Google CloudHands on programming experience of the following  or similar  technologies  Apache Beam, Scio, Apache Spark, and Snowflake. Experience in progressive data application development, working in large scale/distributed SQL, NoSQL, and/or Hadoop environment. Build and maintain dimensional data warehouses in support of BI toolsDevelop data catalogs and data cleanliness to ensure clarity and correctness of key business metricsExperience building streaming data pipelines using Kafka, Spark or FlinkData modelling experience  operationalizing data science models/products  a plusBachelorsâ degree with a specialization in Computer Science, Engineering, Physics, other quantitative field or equivalent industry experience.     Experience of near Real Time & Batch Data Pipeline development in a similar Big Data Engineering roleProgramming skills in one or more of the following  Java, Scala, R, Python, SQL and experience in writing reusable/efficient code to automate analysis and data processesExperience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devicesExperience implementing scalable, distributed, and highly available systems using Google CloudHands on programming experience of the following  or similar  technologies  Apache Beam, Scio, Apache Spark, and Snowflake. Experience in progressive data application development, working in large scale/distributed SQL, NoSQL, and/or Hadoop environment. Build and maintain dimensional data warehouses in support of BI toolsDevelop data catalogs and data cleanliness to ensure clarity and correctness of key business metricsExperience building streaming data pipelines using Kafka, Spark or FlinkData modelling experience  operationalizing data science models/products  a plusBachelorsâ degree with a specialization in Computer Science, Engineering, Physics, other quantitative field or equivalent industry experience. ",near real time batch data pipeline development similar big data engineering roleprogramming one following java scala r python sql writing reusableefficient code automate analysis data processesexperience processing structured unstructured data form suitable analysis reporting integration variety data metric providers ranging advertising web analytics consumer devicesexperience implementing scalable distributed highly available systems google cloudhands programming following similar technologies apache beam scio apache spark snowflake progressive data application development scaledistributed sql nosql andor hadoop build maintain dimensional data warehouses support bi toolsdevelop data catalogs data cleanliness clarity correctness key business metricsexperience building streaming data pipelines kafka spark flinkdata modelling operationalizing data science modelsproducts plusbachelors degree specialization computer science engineering physics quantitative industry near real time batch data pipeline development similar big data engineering roleprogramming one following java scala r python sql writing reusableefficient code automate analysis data processesexperience processing structured unstructured data form suitable analysis reporting integration variety data metric providers ranging advertising web analytics consumer devicesexperience implementing scalable distributed highly available systems google cloudhands programming following similar technologies apache beam scio apache spark snowflake progressive data application development scaledistributed sql nosql andor hadoop build maintain dimensional data warehouses support bi toolsdevelop data catalogs data cleanliness clarity correctness key business metricsexperience building streaming data pipelines kafka spark flinkdata modelling operationalizing data science modelsproducts plusbachelors degree specialization computer science engineering physics quantitative industry,near real time batch data pipeline development similar big engineering roleprogramming one following java scala r python sql writing reusableefficient code automate analysis processesexperience processing structured unstructured form suitable reporting integration variety metric providers ranging advertising web analytics consumer devicesexperience implementing scalable distributed highly available systems google cloudhands programming technologies apache beam scio spark snowflake progressive application scaledistributed nosql andor hadoop build maintain dimensional warehouses support bi toolsdevelop catalogs cleanliness clarity correctness key business metricsexperience building streaming pipelines kafka flinkdata modelling operationalizing science modelsproducts plusbachelors degree specialization computer physics quantitative industry
263,"Experience using Snowflake and Google Cloud Platform preferred.  Experience with AWS ecosystem.  Some preferred services are Redshift, RDS, S3, and SWF.  Proven experience with ETL frameworks  Airflow, Luigi, or our own open sourced garcon  .  Expertise with at least one distributed data stores  Redshift, Cassandra, Snowflake .  Familiarity with noSQL technologies  mongoDB, DynamoDB .  Proficient in scripting language of choice.  Python is strongly preferred, PHP a plus.  Highly proficient in writing SQL for a relational datastore  MySQL, PostgreSQL .  Knowledge of technologies that can deal with Big Data is a Big Plus  Kafka, Spark, Hive, Hadoop/MapReduce .  Ability to write automated tests  unit, functional, and integration  to ensure code works as expected.  Desire to collaborate with other engineers through peer code reviews.  Deep understanding of data structures and schema design.  Detail-oriented, proactive problem solving skills.   Create and maintain systems to load and transform very large data sets from digital media retailers  iTunes, Spotify, YouTube, etc  as well as social media sources.  Work with a cross-functional team to create data-driven insights and reports for business stakeholders.  Work with other members of the team to create customer-facing analytics tools and visualizations.  Process millions of rows of data daily to provide analytics to our end users.  Take advantage of our continuous integration and deployment.  Participate in technical design and peer review for new projects.   ",snowflake google cloud platform aws ecosystem services redshift rds swf proven etl frameworks airflow luigi open sourced garcon expertise least one distributed data stores redshift cassandra snowflake familiarity nosql technologies mongodb dynamodb proficient scripting language choice python strongly php plus highly proficient writing sql relational datastore mysql postgresql technologies deal big data big plus kafka spark hive hadoopmapreduce write automated tests unit functional integration code works expected desire collaborate engineers peer code reviews deep understanding data structures schema design detailoriented proactive problem solving create maintain systems load transform data sets digital media retailers itunes spotify youtube well social media sources crossfunctional team create datadriven insights reports business stakeholders members team create customerfacing analytics tools visualizations process millions rows data daily analytics end users take advantage continuous integration deployment participate technical design peer review projects,snowflake google cloud platform aws ecosystem services redshift rds swf proven etl frameworks airflow luigi open sourced garcon expertise least one distributed data stores cassandra familiarity nosql technologies mongodb dynamodb proficient scripting language choice python strongly php plus highly writing sql relational datastore mysql postgresql deal big kafka spark hive hadoopmapreduce write automated tests unit functional integration code works expected desire collaborate engineers peer reviews deep understanding structures schema design detailoriented proactive problem solving create maintain systems load transform sets digital media retailers itunes spotify youtube well social sources crossfunctional team datadriven insights reports business stakeholders members customerfacing analytics tools visualizations process millions rows daily end users take advantage continuous deployment participate technical review projects
264,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform.  Multi-cloud experience a plus.    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",minimum previous consulting client service delivery google gcp devops gcp platform multicloud plus proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,minimum previous consulting client service delivery google gcp devops platform multicloud plus proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
265,"Excellent attention to detail and accuracy; strong critical self-review skills.  Demonstrates ability to deliver communication which is clear, concise, and relevant to audience through appropriate methods and tools.  Appreciates what constitutes good customer service and displays consistent commitment to delivering.  Exhibits a high degree of professionalism.  Organized methodical application of established data governance standards.  Proactive approach to role and problem solving; solution rather than problem focused.  Able to work collaboratively and communicate effectively with key stakeholders both within and outside of the MDM team to get the job done.  Comfortable pushing back with customers and stakeholders to protect the integrity of the data management process.  Inquisitive and thorough in approach.  Displays a passion for working in master data management.  Self-motivated, flexible, with the ability to deal with high levels of complexity, change and evolving processes, often at short notice.   Excellent attention to detail and accuracy; strong critical self-review skills.  Demonstrates ability to deliver communication which is clear, concise, and relevant to audience through appropriate methods and tools.  Appreciates what constitutes good customer service and displays consistent commitment to delivering.  Exhibits a high degree of professionalism.  Organized methodical application of established data governance standards.  Proactive approach to role and problem solving; solution rather than problem focused.  Able to work collaboratively and communicate effectively with key stakeholders both within and outside of the MDM team to get the job done.  Comfortable pushing back with customers and stakeholders to protect the integrity of the data management process.  Inquisitive and thorough in approach.  Displays a passion for working in master data management.  Self-motivated, flexible, with the ability to deal with high levels of complexity, change and evolving processes, often at short notice.    2+ years of business experience 2+ years of analytical experience Bachelorâs Degree in relevant field of study or within one year of completing Bachelorâs Degree and with 3+ years of relevant work experience.   ",attention detail accuracy critical selfreview demonstrates deliver communication clear concise relevant audience appropriate methods tools appreciates constitutes good customer service displays consistent commitment delivering exhibits degree professionalism organized methodical application established data governance standards proactive approach role problem solving solution rather problem focused able collaboratively communicate effectively key stakeholders within outside mdm team get job done comfortable pushing back customers stakeholders protect integrity data management process inquisitive thorough approach displays passion master data management selfmotivated flexible deal levels complexity change evolving processes often short notice attention detail accuracy critical selfreview demonstrates deliver communication clear concise relevant audience appropriate methods tools appreciates constitutes good customer service displays consistent commitment delivering exhibits degree professionalism organized methodical application established data governance standards proactive approach role problem solving solution rather problem focused able collaboratively communicate effectively key stakeholders within outside mdm team get job done comfortable pushing back customers stakeholders protect integrity data management process inquisitive thorough approach displays passion master data management selfmotivated flexible deal levels complexity change evolving processes often short notice business analytical bachelors degree relevant study within one year completing bachelors degree relevant,attention detail accuracy critical selfreview demonstrates deliver communication clear concise relevant audience appropriate methods tools appreciates constitutes good customer service displays consistent commitment delivering exhibits degree professionalism organized methodical application established data governance standards proactive approach role problem solving solution rather focused able collaboratively communicate effectively key stakeholders within outside mdm team get job done comfortable pushing back customers protect integrity management process inquisitive thorough passion master selfmotivated flexible deal levels complexity change evolving processes often short notice business analytical bachelors study one year completing
266," Deep knowledge of software engineering best practices Experience in mentoring and leading junior engineers Experience in serving as the technical lead for complex software development projects Experience with large scale distributed data technologies and tools Strong coding skills for analytics and data engineering  Scala, Java and Python  Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Googleâs Cloud Platform T-Shaped.  Your primary area is data engineering but you are comfortable working in a second area such as data presentation, backend engineering or front-end development Experience working in a large scale, global consumer product company, in an engineering or insights role Understands how to translate business requirements to technical architectures and designs Comfortable communicating with stakeholders  customers, product managers, C-level management    ",deep software engineering best practices mentoring leading junior engineers serving technical lead complex software development projects scale distributed data technologies tools coding analytics data engineering scala java python performing analysis datasets cloud basedenvironment preferably understanding googles cloud platform tshaped primary area data engineering comfortable second area data presentation backend engineering frontend development scale global consumer product company engineering insights role understands translate business technical architectures designs comfortable communicating stakeholders customers product managers clevel management,deep software engineering best practices mentoring leading junior engineers serving technical lead complex development projects scale distributed data technologies tools coding analytics scala java python performing analysis datasets cloud basedenvironment preferably understanding googles platform tshaped primary area comfortable second presentation backend frontend global consumer product company insights role understands translate business architectures designs communicating stakeholders customers managers clevel management
267,"   Provide recommendations on data team roles and responsibilities as the company continues to grow Help manage technical planning for all client implementations to ensure the data team is successfully meeting deadline and project deliverables Identifying areas of opportunities and providing machine learning solutions to enhance AllazoEngine Build client reporting from performance monitoring to ad hoc requests Provide planning and continuous development of the database architecture    2-4 years of experience in advanced SQL Experience in SSIS, Talend or any other ETL tools.  Experience in performance tuning and reports development.  Demonstrated success in high growth and early-stage environment Demonstrated GSD ""Get Stuff Done"" attitude and results Strong influencer and communicator across all levels of the organization Detail and metric-oriented Proficient in Microsoft Office and technology ",recommendations data team roles responsibilities company continues grow help manage technical planning client implementations data team successfully meeting deadline project deliverables identifying areas opportunities providing machine solutions enhance allazoengine build client reporting performance monitoring ad hoc requests planning continuous development database architecture advanced sql ssis talend etl tools performance tuning reports development demonstrated success growth earlystage demonstrated gsd get stuff done attitude results influencer communicator across levels organization detail metricoriented proficient microsoft office technology,recommendations data team roles responsibilities company continues grow help manage technical planning client implementations successfully meeting deadline project deliverables identifying areas opportunities providing machine solutions enhance allazoengine build reporting performance monitoring ad hoc requests continuous development database architecture advanced sql ssis talend etl tools tuning reports demonstrated success growth earlystage gsd get stuff done attitude results influencer communicator across levels organization detail metricoriented proficient microsoft office technology
268,"     Highly proficient in Python Proficient working with data and distributed systems Experience with Python scientific libraries such as SciPy, Scikit, Pandas, and NumPy Experience conducting methods using any of the following  machine learning, predictive modeling, statistical inference, experimental design, data mining, and optimization Experience in Linux/Unix environment and shell scripting Experience with ETL Deep understanding of data structures and schema design Familiarity with both SQL and NoSQL technologies Experience with AWS and Azure platforms a plus Knowledgeable about data modeling, data access, and data storage techniques Critical thinking  ability to track down complex data and engineering issues, evaluate different algorithmic approaches, and analyze data to solve problems Creativity  conceive of new data driven products, features, and technologies ",highly proficient python proficient data distributed systems python scientific libraries scipy scikit pandas numpy conducting methods following machine predictive modeling statistical inference experimental design data mining optimization linuxunix shell scripting etl deep understanding data structures schema design familiarity sql nosql technologies aws azure platforms plus knowledgeable data modeling data access data storage techniques critical thinking track complex data engineering issues evaluate different algorithmic approaches analyze data solve problems creativity conceive data driven products features technologies,highly proficient python data distributed systems scientific libraries scipy scikit pandas numpy conducting methods following machine predictive modeling statistical inference experimental design mining optimization linuxunix shell scripting etl deep understanding structures schema familiarity sql nosql technologies aws azure platforms plus knowledgeable access storage techniques critical thinking track complex engineering issues evaluate different algorithmic approaches analyze solve problems creativity conceive driven products features
269,"  3+ years of engineering experience in a fast-paced environment; 1+ years of experience in scalable data architecture, fault-tolerant ETL, and monitoring of data quality in the cloud Proficiency in SQL Proficiency in Python Proficiency with modern source control systems, especially Git Experience working with non-technical business stakeholders on technical projects    ",engineering fastpaced scalable data architecture faulttolerant etl monitoring data cloud proficiency sql proficiency python proficiency modern source control systems especially git nontechnical business stakeholders technical projects,engineering fastpaced scalable data architecture faulttolerant etl monitoring cloud proficiency sql python modern source control systems especially git nontechnical business stakeholders technical projects
270,"   Contribute to the overall design, scope, and roadmap of TrueChoice analytics capabilities Develop and deliver software solutions to meet the evolving requirements of our business users Work with business users and analysts to gather requirements and produce detailed functional design specifications Work with the internal IT team & business users to identify requirements, create prototypes, develop and test solutions Develop, validate, and maintain the core functionality and capabilities associated with Spotfire visualizations and dashboards Build and maintain documentation, procedures and guidelines around analytical solution development Help manage execution of all Spotfire version upgrades   Bachelorâs degree in a technical related field  e. g.  Computer Science, Engineering, Math  3+ years of experience in C  and . NET development Hands-on experience in database/SQL programming and Oracle Proven success in contributing to a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Ability to work effectively both independently and as part of an integrated team Excellent communication skills  written and verbal Strong team work and interpersonal skills",contribute overall design scope roadmap truechoice analytics capabilities develop deliver software solutions meet evolving business users business users analysts gather produce detailed functional design specifications internal team business users identify create prototypes develop test solutions develop validate maintain core functionality capabilities associated spotfire visualizations dashboards build maintain documentation procedures guidelines around analytical solution development help manage execution spotfire version upgrades bachelors degree technical e g computer science engineering math c net development handson databasesql programming oracle proven success contributing teamoriented proven creatively analytically problemsolving effectively independently part integrated team communication written verbal team interpersonal,contribute overall design scope roadmap truechoice analytics capabilities develop deliver software solutions meet evolving business users analysts gather produce detailed functional specifications internal team identify create prototypes test validate maintain core functionality associated spotfire visualizations dashboards build documentation procedures guidelines around analytical solution development help manage execution version upgrades bachelors degree technical e g computer science engineering math c net handson databasesql programming oracle proven success contributing teamoriented creatively analytically problemsolving effectively independently part integrated communication written verbal interpersonal
271,"  Architect, Design and Maintain Data Pipelines through the lifecycle of the product. Optimize and Monitor existing data pipelines using AWS infrastructure. WritePython/Scala applications for data processing and job scheduling. Understand and Manage massive data-stores. Integrate products from data projects into APIs built in Ruby/RailsExpose large data setsEnjoy being challenged and solve complex problems on a daily basisDesign efficient and robust ETL workflowsManage real time streaming application and data flowInvestigate, procure and ramp up to new technologiesAbility to work in teams and collaborate with others to clarify requirementsBuild analytics tools that utilize the data pipelines to provide meaningful insights into data  ",architect design maintain data pipelines lifecycle product optimize monitor existing data pipelines aws infrastructure writepythonscala applications data processing job scheduling understand manage massive datastores integrate products data projects apis built rubyrailsexpose data setsenjoy challenged solve complex problems daily basisdesign efficient robust etl workflowsmanage real time streaming application data flowinvestigate procure ramp technologiesability teams collaborate others clarify requirementsbuild analytics tools utilize data pipelines meaningful insights data,architect design maintain data pipelines lifecycle product optimize monitor existing aws infrastructure writepythonscala applications processing job scheduling understand manage massive datastores integrate products projects apis built rubyrailsexpose setsenjoy challenged solve complex problems daily basisdesign efficient robust etl workflowsmanage real time streaming application flowinvestigate procure ramp technologiesability teams collaborate others clarify requirementsbuild analytics tools utilize meaningful insights
272,"  Familiar with big data processing and concepts like MapReduce, spark RDDs etc.  Knowledge of cloud storage platforms on AWS, Azure etc.  Java, Python and Scala are required skill sets.  Good command over SQL, ETL Best practices and data warehousing concepts.  Familiarity with ELT.  Familiarity with various modes of file storage like Parquet, ORC, Avro etc.  Prior experience in working with Redshift/Snowflake a plus.   Interact with external / internal data providers to understand the nature of data.  Gather information on general data delivery schedule for all sources.  Build databases and schemas for data warehousing and analysis.  Develop ETLs to move data into the warehouse and analysis layers.  Develop logical checks in ETL process to check for duplicates, obvious data errors etc.  Judge whether an ETL needs to be a batch process or stream process and use appropriate libraries.  Schedule and maintain ETLs for different sources.  Structure / Re-Structure various schema objects and periodically check on query execution plans to maintain optimal performance.  Work with data scientists to ensure the data is formatted in a way that is optimal for their model.  Create and maintain schemas to store MMM results.  Serve as a go to person for ad hoc queries, schemas, views etc.   ",familiar big data processing concepts like mapreduce spark rdds cloud storage platforms aws azure java python scala skill sets good command sql etl best practices data warehousing concepts familiarity elt familiarity various modes file storage like parquet orc avro prior redshiftsnowflake plus interact external internal data providers understand nature data gather information general data delivery schedule sources build databases schemas data warehousing analysis develop etls move data warehouse analysis layers develop logical checks etl process check duplicates obvious data errors judge whether etl needs batch process stream process use appropriate libraries schedule maintain etls different sources structure restructure various schema objects periodically check query execution plans maintain optimal performance data scientists data formatted way optimal model create maintain schemas store mmm results serve go person ad hoc queries schemas views,familiar big data processing concepts like mapreduce spark rdds cloud storage platforms aws azure java python scala skill sets good command sql etl best practices warehousing familiarity elt various modes file parquet orc avro prior redshiftsnowflake plus interact external internal providers understand nature gather information general delivery schedule sources build databases schemas analysis develop etls move warehouse layers logical checks process check duplicates obvious errors judge whether needs batch stream use appropriate libraries maintain different structure restructure schema objects periodically query execution plans optimal performance scientists formatted way model create store mmm results serve go person ad hoc queries views
273,"     A BS/MS degree in Computer Science or relevant field or equivalent professional experience with solid computer science fundamentals in object-oriented design, data structure, algorithms and database systems  SQL and NoSQL  Experience building and optimizing âbig dataâ data pipelines, architectures and data sets; Strong analytic skills related to working with unstructured datasets Experience with big data tools  Spark, Kafka; Experience with AWS cloud services  EC2, EMR, RDS, Redshift; Experience with stream-processing systems like Spark-Streaming Proficient in coding languages such as Python, JavaScript, Scala and Shell scripts",bsms degree computer science relevant professional solid computer science fundamentals objectoriented design data structure algorithms database systems sql nosql building optimizing big data data pipelines architectures data sets analytic unstructured datasets big data tools spark kafka aws cloud services ec emr rds redshift streamprocessing systems like sparkstreaming proficient coding languages python javascript scala shell scripts,bsms degree computer science relevant professional solid fundamentals objectoriented design data structure algorithms database systems sql nosql building optimizing big pipelines architectures sets analytic unstructured datasets tools spark kafka aws cloud services ec emr rds redshift streamprocessing like sparkstreaming proficient coding languages python javascript scala shell scripts
274,"BA/BS Degree  advanced degrees and/or CFAs are great too  10+ years general business experience, especially in financial services, asset management, and/or management consulting or similar environments 6+ years of data engineering, data architecture, data science, and/or software engineering experience Excel proficiency SQL proficiency Python expertise Experience with machine learning algorithms and techniques Experience with modern data pipelines and/or data operating platforms  e. g.  Dataiku, Alteryx, Spark  Able to lead ad-hoc and structured product teams within an agile framework Excellent interpersonal skills necessary to accomplish goals through others, including employees, peers, and other function/business areas of the company  Lead data engineering and/or data science projects to support IM firms Use domain expertise to understand business requirements and design the right solution Build or implement data pipelines, databases, visualizations, and other data tools  hands-on  Contribute to team in a wide range of technical areas by instituting new practices and staying abreast of the latest technical developments Take initiative to lead/contribute to overall team efforts in software development, data engineering, data science, and technical consulting Work closely with other data engineers and data scientists to improve processes and enable faster insight-generation from complex datasets   ",babs degree advanced degrees andor cfas great general business especially financial services asset management andor management consulting similar environments data engineering data architecture data science andor software engineering excel proficiency sql proficiency python expertise machine algorithms techniques modern data pipelines andor data operating platforms e g dataiku alteryx spark able lead adhoc structured product teams within agile framework interpersonal necessary accomplish goals others employees peers functionbusiness areas company lead data engineering andor data science projects support im firms use domain expertise understand business design right solution build implement data pipelines databases visualizations data tools handson contribute team wide range technical areas instituting practices staying abreast latest technical developments take initiative leadcontribute overall team efforts software development data engineering data science technical consulting closely data engineers data scientists improve processes enable faster insightgeneration complex datasets,babs degree advanced degrees andor cfas great general business especially financial services asset management consulting similar environments data engineering architecture science software excel proficiency sql python expertise machine algorithms techniques modern pipelines operating platforms e g dataiku alteryx spark able lead adhoc structured product teams within agile framework interpersonal necessary accomplish goals others employees peers functionbusiness areas company projects support im firms use domain understand design right solution build implement databases visualizations tools handson contribute team wide range technical instituting practices staying abreast latest developments take initiative leadcontribute overall efforts development closely engineers scientists improve processes enable faster insightgeneration complex datasets
275," 3+ yearsâ experience with ETL tool SQL Server Integration Services  SSIS  or similar ETL Tool 3+ yearsâ experience with SQL Server, strong expertise in T-SQL required 3+ yearsâ experience with Web Services  RESTful APIs, . NET WCF and SOAP 1+ yearsâ experience with . NET C  or other OO language required Experience with Windows Server Operating Systems required Experience with SSAS and building cubes required Experience with statistics and machine learning tools preferred   Python, R, Matlab or Spark  Experience with public cloud preferred  AWS or Azure Experience with Big Data or NoSQL technology preferred  Hadoop, Spark, AWS EMR, Hive, etc.  Experience in healthcare a plus  3+ yearsâ experience with ETL tool SQL Server Integration Services  SSIS  or similar ETL Tool 3+ yearsâ experience with SQL Server, strong expertise in T-SQL required 3+ yearsâ experience with Web Services  RESTful APIs, . NET WCF and SOAP 1+ yearsâ experience with . NET C  or other OO language required Experience with Windows Server Operating Systems required Experience with SSAS and building cubes required Experience with statistics and machine learning tools preferred   Python, R, Matlab or Spark  Experience with public cloud preferred  AWS or Azure Experience with Big Data or NoSQL technology preferred  Hadoop, Spark, AWS EMR, Hive, etc.  Experience in healthcare a plus   MS/BS in Computer Science or related field ",etl tool sql server integration services ssis similar etl tool sql server expertise tsql web services restful apis net wcf soap net c oo language windows server operating systems ssas building cubes statistics machine tools python r matlab spark public cloud aws azure big data nosql technology hadoop spark aws emr hive healthcare plus etl tool sql server integration services ssis similar etl tool sql server expertise tsql web services restful apis net wcf soap net c oo language windows server operating systems ssas building cubes statistics machine tools python r matlab spark public cloud aws azure big data nosql technology hadoop spark aws emr hive healthcare plus msbs computer science,etl tool sql server integration services ssis similar expertise tsql web restful apis net wcf soap c oo language windows operating systems ssas building cubes statistics machine tools python r matlab spark public cloud aws azure big data nosql technology hadoop emr hive healthcare plus msbs computer science
276," 5+ years of experience in using SQL and databases in a business environment 5+ years of experience in custom ETL/ELT design, implementation, and maintenance 3+ years of experience with schema design and data modeling 3+ years of experience applying data architecture or engineering to solve real-world business problems 2+ years of experience with building integration and ingestion frameworks leveraging API based tools and platforms  i. e.  Dell Boomi  Manipulating/mining data from database tables  i. e.  SQL Server, Redshift, Oracle  SQL, ETL/ELT optimization, and analytics tools experience  i. e.  R, HiveQL  Prior implementation experience in building both batch and real-time/near real-time data ingestion frameworks using technologies like Kafka, AWS Kinesis etc.    Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions Conduct and support white-boarding sessions, workshops, design sessions, and project meetings as needed, playing a key role in client relations Design and build data extraction, transformation, and loading processes by writing custom data pipelines using either cloud native services  AWS/GCP/Azure  or using open source tools  like Airflow and Python  Design, implement, and support an Enterprise Data platform  Data Lake, Data Warehouse, etc.   that can provide ad-hoc access to large scale structured, semi-structured, and/or unstructured datasets Experience with both traditional  i. e.  SSIS, Informatica, Talend  and modern  i. e.  Dell Boomi  data integration iPaaS technologies Highly self-motivated to deliver both independently and with strong team collaboration Ability to creatively take on new challenges and work outside comfort zone Strong written and oral communications along with presentation and interpersonal skills Deliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data architecture and engineering space  ",sql databases business custom etlelt design implementation maintenance schema design data modeling applying data architecture engineering solve realworld business problems building integration ingestion frameworks leveraging api based tools platforms e dell boomi manipulatingmining data database tables e sql server redshift oracle sql etlelt optimization analytics tools e r hiveql prior implementation building batch realtimenear realtime data ingestion frameworks technologies like kafka aws kinesis collaborate engineers business customers understand data needs capture deliver complete bi solutions conduct support whiteboarding sessions workshops design sessions project meetings needed playing key role client relations design build data extraction transformation loading processes writing custom data pipelines either cloud native services awsgcpazure open source tools like airflow python design implement support enterprise data platform data lake data warehouse adhoc access scale structured semistructured andor unstructured datasets traditional e ssis informatica talend modern e dell boomi data integration ipaas technologies highly selfmotivated deliver independently team collaboration creatively take challenges outside comfort zone written oral communications along presentation interpersonal deliver transformative solutions clients aligned industry best practices thought leadership data architecture engineering space,sql databases business custom etlelt design implementation maintenance schema data modeling applying architecture engineering solve realworld problems building integration ingestion frameworks leveraging api based tools platforms e dell boomi manipulatingmining database tables server redshift oracle optimization analytics r hiveql prior batch realtimenear realtime technologies like kafka aws kinesis collaborate engineers customers understand needs capture deliver complete bi solutions conduct support whiteboarding sessions workshops project meetings needed playing key role client relations build extraction transformation loading processes writing pipelines either cloud native services awsgcpazure open source airflow python implement enterprise platform lake warehouse adhoc access scale structured semistructured andor unstructured datasets traditional ssis informatica talend modern ipaas highly selfmotivated independently team collaboration creatively take challenges outside comfort zone written oral communications along presentation interpersonal transformative clients aligned industry best practices thought leadership space
277,"  Python & Pandas Spark, MapReduce AWS SQL, Postgresql Past ETL experience Experience with data governance and ownership of data pipelines   ",python pandas spark mapreduce aws sql postgresql past etl data governance ownership data pipelines,python pandas spark mapreduce aws sql postgresql past etl data governance ownership pipelines
278,"10+ years of progressive data application development experience, working in large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ years of experience modeling and implementing ETL/ELT on columnar MPP database technologies such as Snowflake, BigQuery and/or RedshiftExperience with streaming architectures  Kafka, Kinesis, Pub/Sub, etc.  Experience working in hybrid cloud/on-premise environments as well as multi-cloudExperience implementing scalable, distributed, and highly available systems using cloud technologies, such as Microsoft Azure, Amazon Web Services, and/or Google CloudExperience building microservices topologies, including operational concerns such as resiliency, observability, discovery and routing, etc. Undergraduate degree in the field of computer science or engineering, or focus on statistical analysis or equivalent experience highly desired    10+ years of progressive data application development experience, working in large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ years of experience modeling and implementing ETL/ELT on columnar MPP database technologies such as Snowflake, BigQuery and/or RedshiftExperience with streaming architectures  Kafka, Kinesis, Pub/Sub, etc.  Experience working in hybrid cloud/on-premise environments as well as multi-cloudExperience implementing scalable, distributed, and highly available systems using cloud technologies, such as Microsoft Azure, Amazon Web Services, and/or Google CloudExperience building microservices topologies, including operational concerns such as resiliency, observability, discovery and routing, etc. Undergraduate degree in the field of computer science or engineering, or focus on statistical analysis or equivalent experience highly desired",progressive data application development scaledistributed sql nosql andor hadoop environments modeling implementing etlelt columnar mpp database technologies snowflake bigquery andor redshiftexperience streaming architectures kafka kinesis pubsub hybrid cloudonpremise environments well multicloudexperience implementing scalable distributed highly available systems cloud technologies microsoft azure amazon web services andor google cloudexperience building microservices topologies operational concerns resiliency observability discovery routing undergraduate degree computer science engineering focus statistical analysis highly desired progressive data application development scaledistributed sql nosql andor hadoop environments modeling implementing etlelt columnar mpp database technologies snowflake bigquery andor redshiftexperience streaming architectures kafka kinesis pubsub hybrid cloudonpremise environments well multicloudexperience implementing scalable distributed highly available systems cloud technologies microsoft azure amazon web services andor google cloudexperience building microservices topologies operational concerns resiliency observability discovery routing undergraduate degree computer science engineering focus statistical analysis highly desired,progressive data application development scaledistributed sql nosql andor hadoop environments modeling implementing etlelt columnar mpp database technologies snowflake bigquery redshiftexperience streaming architectures kafka kinesis pubsub hybrid cloudonpremise well multicloudexperience scalable distributed highly available systems cloud microsoft azure amazon web services google cloudexperience building microservices topologies operational concerns resiliency observability discovery routing undergraduate degree computer science engineering focus statistical analysis desired
279," 6-10 years of relevant experience in Apps Development or systems analysis role Extensive experience system analysis and in programming of software applications Experience in managing and implementing successful projects Subject Matter Expert  SME  in at least one area of Applications Development Ability to adjust priorities quickly as circumstances dictate Demonstrated leadership and project management skills Consistently demonstrates clear and concise written and verbal communication  Proven ability in working with the development team members and other partners, with minimal supervision Strong verbal and written communications skills, excellent interpersonal skills with ability to communicate well at all levels Team Player, self-starter and thorough who is willing to take on any assigned job/responsibilities Ability to learn new skills quickly with little supervision and ensuring the detail is of high priority Efficiently and effectively manages work, time, and resources Ability to work under high-pressure situations and effectively prioritize in a highly dynamic work environment that includes a global focus Strong problem solving and program execution skills while being process orientated Ability to understand the big picture â can step back and understand the context of problems before applying analytical skills to address the issues  Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.   Bachelorâs degree/University degree or equivalent experience Masterâs degree preferred ",relevant apps development systems analysis role extensive analysis programming software applications managing implementing successful projects subject matter expert sme least one area applications development adjust priorities quickly circumstances dictate demonstrated leadership project management consistently demonstrates clear concise written verbal communication proven development team members partners minimal supervision verbal written communications interpersonal communicate well levels team player selfstarter thorough willing take assigned jobresponsibilities learn quickly little supervision ensuring detail priority efficiently effectively manages time resources highpressure situations effectively prioritize highly dynamic includes global focus problem solving program execution process orientated understand big picture step back understand context problems applying analytical address issues partner multiple management teams appropriate integration functions meet goals well identify define necessary enhancements deploy products process improvements resolve variety impact problemsprojects indepth evaluation complex business processes processes industry standards expertise area advanced applications programming application design adheres overall architecture blueprint utilize advanced flow develop standards coding testing debugging implementation develop comprehensive areas business architecture infrastructure integrate accomplish business goals indepth analysis interpretive thinking define issues develop innovative solutions serve advisor coach midlevel developers analysts allocating necessary appropriately assess risk business decisions made demonstrating particular consideration firms reputation safeguarding citigroup clients assets driving compliance applicable laws rules regulations adhering policy applying sound ethical judgment regarding personal behavior conduct business practices escalating managing reporting control issues transparency bachelors degreeuniversity degree masters degree,relevant apps development systems analysis role extensive programming software applications managing implementing successful projects subject matter expert sme least one area adjust priorities quickly circumstances dictate demonstrated leadership project management consistently demonstrates clear concise written verbal communication proven team members partners minimal supervision communications interpersonal communicate well levels player selfstarter thorough willing take assigned jobresponsibilities learn little ensuring detail priority efficiently effectively manages time resources highpressure situations prioritize highly dynamic includes global focus problem solving program execution process orientated understand big picture step back context problems applying analytical address issues partner multiple teams appropriate integration functions meet goals identify define necessary enhancements deploy products improvements resolve variety impact problemsprojects indepth evaluation complex business processes industry standards expertise advanced application design adheres overall architecture blueprint utilize flow develop coding testing debugging implementation comprehensive areas infrastructure integrate accomplish interpretive thinking innovative solutions serve advisor coach midlevel developers analysts allocating appropriately assess risk decisions made demonstrating particular consideration firms reputation safeguarding citigroup clients assets driving compliance applicable laws rules regulations adhering policy sound ethical judgment regarding personal behavior conduct practices escalating reporting control transparency bachelors degreeuniversity degree masters
280," Bachelorâs degree in computer science, software/computer engineering, applied math, statistics or related field required 4+ years of related experience as a data engineer 4+ SQL-based technologies  e. g.  PostgreSQL and MySQL  2+ NoSQL technologies  e. g.  Cassandra and MongoDB  Experience with Hadoop-based technologies  e. g.  MapReduce, Hive and Pig  Good with scripting languages  Python, Perl, etc.   Data warehousing solutions Statistical analysis and modeling is a PLUS Machine learning and data mining is a PLUS  No sponsorship available at this time.    Build real-time data capture and transformation functionality across all products Work with other engineers to enhance data models and improve data query efficiency Create complex data queries to facilitate ad hoc and exploratory analytics Act as in-house data expert and make recommendations regarding standards quality and timeliness Build out technology stack for Business Intelligence and Data Warehouse Clean data  review for data inconsistencies and identify opportunities to improve data collection process Wrangle/Munge data  transform or map data from one raw data form into another format with the intent of making it more appropriate and valuable for analytics Develop, construct, test and maintain architectures such as databases and large-scale data processing systems Design, construct, install, test and maintain highly scalable data management systems Build high-performance algorithms, prototypes, predictive models and proof of concepts Research opportunities for data acquisition and new uses for existing data Employ a variety of languages and tools  e. g.  scripting languages  to marry systems together Recommend ways to improve data reliability, efficiency and quality Collaborate with analysts and team members on project goals and targeted, relevant data sets Build or recommend data visualization tools and business intelligence tools such as interactive dashboards and automated reports, to enable leaders to make swift, fact-based decisions  ",bachelors degree computer science softwarecomputer engineering applied math statistics data engineer sqlbased technologies e g postgresql mysql nosql technologies e g cassandra mongodb hadoopbased technologies e g mapreduce hive pig good scripting languages python perl data warehousing solutions statistical analysis modeling plus machine data mining plus sponsorship available time build realtime data capture transformation functionality across products engineers enhance data models improve data query efficiency create complex data queries facilitate ad hoc exploratory analytics act inhouse data expert make recommendations regarding standards timeliness build technology stack business intelligence data warehouse clean data review data inconsistencies identify opportunities improve data collection process wranglemunge data transform map data one raw data form another format intent making appropriate valuable analytics develop construct test maintain architectures databases largescale data processing systems design construct install test maintain highly scalable data management systems build highperformance algorithms prototypes predictive models proof concepts research opportunities data acquisition uses existing data employ variety languages tools e g scripting languages marry systems together recommend ways improve data reliability efficiency collaborate analysts team members project goals targeted relevant data sets build recommend data visualization tools business intelligence tools interactive dashboards automated reports enable leaders make swift factbased decisions,bachelors degree computer science softwarecomputer engineering applied math statistics data engineer sqlbased technologies e g postgresql mysql nosql cassandra mongodb hadoopbased mapreduce hive pig good scripting languages python perl warehousing solutions statistical analysis modeling plus machine mining sponsorship available time build realtime capture transformation functionality across products engineers enhance models improve query efficiency create complex queries facilitate ad hoc exploratory analytics act inhouse expert make recommendations regarding standards timeliness technology stack business intelligence warehouse clean review inconsistencies identify opportunities collection process wranglemunge transform map one raw form another format intent making appropriate valuable develop construct test maintain architectures databases largescale processing systems design install highly scalable management highperformance algorithms prototypes predictive proof concepts research acquisition uses existing employ variety tools marry together recommend ways reliability collaborate analysts team members project goals targeted relevant sets visualization interactive dashboards automated reports enable leaders swift factbased decisions
281,   Build efficient codes to extract data and documents from various sources Build OCR and NLP pipeline to retrieve data from unstructured data Build standard reports from extracted data per business requirement Conduct unit testing and document the finalized code set Assist production implementation in set up infrastructure and automated processes   ,build efficient codes extract data documents various sources build ocr nlp pipeline retrieve data unstructured data build standard reports extracted data per business requirement conduct unit testing document finalized code set assist production implementation set infrastructure automated processes,build efficient codes extract data documents various sources ocr nlp pipeline retrieve unstructured standard reports extracted per business requirement conduct unit testing document finalized code set assist production implementation infrastructure automated processes
282," PhD in Computer Science, Mathematics, or related technical field Familiarity with open source cloud and application platforms, AWS development experience Experience with big data technologies such as Spark, Hive, Presto and Impala.  Experience working with MPP databases such as Redshift, Snowflake, Vertica and Netezza Hands-on experience working in SOA and high throughput environments    Code in a variety of languages, primarily Python, Java and/or C++ Design and implement data pipelines, building scalable and optimized enterprise level data systems Work cross functionally with Product, Ops and Engineering counterparts Participation and collaboration from inception to deployment    MS in Computer Science, Math, related technical field or equivalent practical experience 4+ years of general software programming experience in Java, C/C++, Python and SQL Large systems software design and development experience, with knowledge of Unix/Linux Knowledge of database technology, schema design, and query optimization techniques Solid foundation in data structures, algorithms and software design with strong analytical and debugging skills ",phd computer science mathematics technical familiarity open source cloud application platforms aws development big data technologies spark hive presto impala mpp databases redshift snowflake vertica netezza handson soa throughput environments code variety languages primarily python java andor c design implement data pipelines building scalable optimized enterprise level data systems cross functionally product ops engineering counterparts participation collaboration inception deployment ms computer science math technical practical general software programming java cc python sql systems software design development unixlinux database technology schema design query optimization techniques solid foundation data structures algorithms software design analytical debugging,phd computer science mathematics technical familiarity open source cloud application platforms aws development big data technologies spark hive presto impala mpp databases redshift snowflake vertica netezza handson soa throughput environments code variety languages primarily python java andor c design implement pipelines building scalable optimized enterprise level systems cross functionally product ops engineering counterparts participation collaboration inception deployment ms math practical general software programming cc sql unixlinux database technology schema query optimization techniques solid foundation structures algorithms analytical debugging
283,"Strong programming experience in at least one compiled language  e. g.  C, C++, Java In-depth knowledge of relational and columnar SQL databases, including database designExperience with continuous delivery and deploymentProficient at working with large and complex code basesComfortable working in highly dynamic and rapid development environment  Agile development experience Technologies  Web/RESTful service development  HTML 5, JavaScript/AngularJS, JSONTechnologies  Linux and shell scripting, TDD  JUnit , build tools  Maven/Gradle/Ant , Scala, Spark, Tableau    ",programming least one compiled language e g c c java indepth relational columnar sql databases database designexperience continuous delivery deploymentproficient complex code basescomfortable highly dynamic rapid development agile development technologies webrestful service development html javascriptangularjs jsontechnologies linux shell scripting tdd junit build tools mavengradleant scala spark tableau,programming least one compiled language e g c java indepth relational columnar sql databases database designexperience continuous delivery deploymentproficient complex code basescomfortable highly dynamic rapid development agile technologies webrestful service html javascriptangularjs jsontechnologies linux shell scripting tdd junit build tools mavengradleant scala spark tableau
284," AWS Certified DevOps Engineer; AWS Certified Solutions Architect; AWS Certified Big Data Azure or Google Certified Professional Data Engineer Master's or Ph. D degree   Enable machine learning and data analysis Model data and metadata to support dashboards, ad-hoc, and pre-built reporting Adopt best practices in reporting and analysis  data integrity, analysis, validation, and documentation Ability to engage clients and lead relevant data discussions  Provide technical design leadership with the responsibility to ensure the efficient use of resources, the selection of appropriate technology, and use of appropriate design methodologies Work closely with business/product stakeholders in understanding requirements and translating them to engineering requirements Support and enhance data architecture, data instrumentation, define database schema, create ETL pipelining, generate reports/insights, a guide algorithm design Define and evangelize data warehouse fundamentals and best practices Work across the organization in optimizing data capture  parameters, metadata, etc.   Work in DevOps to bring up new data systems and supporting existing data services Evaluate SaaS solutions  BI, pipelining, etc.   and make build/buy recommendations   Bachelorâs degree in Computer Science, Engineering, Math, Physics, or equivalent work experience 5+ years of software development experience Proficient in SQL, NoSQL databases, and GNU Linux Experience building secure, concurrent, distributed server applications Experience in data science, analytics, or big data solutions [Hadoop, Spark, AWS, Python, etc. ] Experience with Scala, MongoDB, Cassandra, PostgreSQL, Docker, Kubernetes Ability to work collaboratively with a distributed team or remotely with clients",aws certified devops engineer aws certified solutions architect aws certified big data azure google certified professional data engineer masters ph degree enable machine data analysis model data metadata support dashboards adhoc prebuilt reporting adopt best practices reporting analysis data integrity analysis validation documentation engage clients lead relevant data discussions technical design leadership responsibility efficient use resources selection appropriate technology use appropriate design methodologies closely businessproduct stakeholders understanding translating engineering support enhance data architecture data instrumentation define database schema create etl pipelining generate reportsinsights guide algorithm design define evangelize data warehouse fundamentals best practices across organization optimizing data capture parameters metadata devops bring data systems supporting existing data services evaluate saas solutions bi pipelining make buildbuy recommendations bachelors degree computer science engineering math physics software development proficient sql nosql databases gnu linux building secure concurrent distributed server applications data science analytics big data solutions hadoop spark aws python scala mongodb cassandra postgresql docker kubernetes collaboratively distributed team remotely clients,aws certified devops engineer solutions architect big data azure google professional masters ph degree enable machine analysis model metadata support dashboards adhoc prebuilt reporting adopt best practices integrity validation documentation engage clients lead relevant discussions technical design leadership responsibility efficient use resources selection appropriate technology methodologies closely businessproduct stakeholders understanding translating engineering enhance architecture instrumentation define database schema create etl pipelining generate reportsinsights guide algorithm evangelize warehouse fundamentals across organization optimizing capture parameters bring systems supporting existing services evaluate saas bi make buildbuy recommendations bachelors computer science math physics software development proficient sql nosql databases gnu linux building secure concurrent distributed server applications analytics hadoop spark python scala mongodb cassandra postgresql docker kubernetes collaboratively team remotely
285," BS or MS in management information systems, computer science, or a related field.    Support the design, build and execution of post source system extraction and data lake ingestion and business transformation, CARR creation framework and development processes in production.   ",bs ms management information systems computer science support design build execution post source extraction data lake ingestion business transformation carr creation framework development processes production,bs ms management information systems computer science support design build execution post source extraction data lake ingestion business transformation carr creation framework development processes production
286,"  Implementation including loading from disparate data sets, preprocessing using Hive and Pig.  Manage the technical communication between the team and client Work with big data team to deliver cutting edge solutions   ",implementation loading disparate data sets preprocessing hive pig manage technical communication team client big data team deliver cutting edge solutions,implementation loading disparate data sets preprocessing hive pig manage technical communication team client big deliver cutting edge solutions
287," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ",expertise least one following domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale writing software one languages python java scala go building productiongrade data solutions relational nosql systems monitoringalerting capacity planning performance tuning technical consulting customerfacing role,expertise least one following domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale writing languages python java scala go relational nosql systems monitoringalerting capacity planning performance tuning consulting customerfacing role
288,"Bachelorâs Degree  MS preferred  in Computer Science, Statistics, Math or equivalent combination of education and experience 5+ years of sound experience with data modelling, data intake and data-curation procedures as well as outstanding SQL skills, incl.  advanced concepts such as distributed queries and spatial queries.  Strong hands on experience with Power BI dashboard design, development and WS modeling.  Knowledge of modern web-based procedures for data visualization Ability to think and work in a Group-wide network, strongly service orientated, team player and highly motivated.  Strong interpersonal skills.  Proficient oral and written communication skills.  Ability to schedule and prioritize workload demands, including multi-tasking.  Experience in results monitoring and rate/product development.  4+ years of hands-on HQL, SQL, Power BI, SAS  a plus .  Insurance/reinsurance/actuarial experience highly preferred.  Experience in the commercial insurance industry  especially property and casualty  is preferred but not necessary.  2+ years of Data architecture experience with regards to duplicative or redundant metadata, data structures or processes.  Proven experience in software development projects with Java Spring, Docker, Git, Maven/Gradle and Jenkins, proven agile project management and requirements engineering skills  SCRUM, Design Thinking .  Proven experience in Data Visualization tools   such as Power BI, SAS VA, etc.   Experience leading projects preferred.     ",bachelors degree ms computer science statistics math combination education sound data modelling data intake datacuration procedures well outstanding sql incl advanced concepts distributed queries spatial queries hands power bi dashboard design development ws modeling modern webbased procedures data visualization think groupwide network strongly service orientated team player highly motivated interpersonal proficient oral written communication schedule prioritize workload demands multitasking results monitoring rateproduct development handson hql sql power bi sas plus insurancereinsuranceactuarial highly commercial insurance industry especially property casualty necessary data architecture regards duplicative redundant metadata data structures processes proven software development projects java spring docker git mavengradle jenkins proven agile project management engineering scrum design thinking proven data visualization tools power bi sas va leading projects,bachelors degree ms computer science statistics math combination education sound data modelling intake datacuration procedures well outstanding sql incl advanced concepts distributed queries spatial hands power bi dashboard design development ws modeling modern webbased visualization think groupwide network strongly service orientated team player highly motivated interpersonal proficient oral written communication schedule prioritize workload demands multitasking results monitoring rateproduct handson hql sas plus insurancereinsuranceactuarial commercial insurance industry especially property casualty necessary architecture regards duplicative redundant metadata structures processes proven software projects java spring docker git mavengradle jenkins agile project management engineering scrum thinking tools va leading
289,"     8+ years of software development experience focused on web technologies including significant production work with Python 3+ years of experience designing, building and maintaining enterprise data pipelines and/or warehouses High level knowledge of machine learning algorithms and how ML models are built and deployed Demonstrable knowledge of big data databases such as columnar data stores  e. g.  Cassandra or BigTable  or Hadoop as well as SQL  MySQL, MSSQL or PostgreSQL  and ability to select the right tool for the job Experience with queued work management and message processing  e. g. , Kafka, RabbitMQ  Experience working closely with product and account support personnel to help prioritize the best solutions to the largest problems.  Reliable organization and communication skills and follow through on verbal and written commitments.  Persistent approach to problem-solving and ability to see solutions through to completion even in the face of complexities or unknowns.  A proactive mindset that drives you to pursue solutions rather than waiting for the answers to come to you.  Attention to detail in work and ability to identify ambiguities in specifications.  Exceptional written and verbal communication skills, especially when communicating trade-offs between technical decisions to non-technical colleagues.  Flexibility to work and maintain focus in an evolving environment.  Ability to let go of previous projects and move on to new ones or to dig deeper into existing projects and grow them depending on the business needs.  ",software development focused web technologies significant production python designing building maintaining enterprise data pipelines andor warehouses level machine algorithms ml models built deployed demonstrable big data databases columnar data stores e g cassandra bigtable hadoop well sql mysql mssql postgresql select right tool job queued management message processing e g kafka rabbitmq closely product account support personnel help prioritize best solutions largest problems reliable organization communication follow verbal written commitments persistent approach problemsolving see solutions completion even face complexities unknowns proactive mindset drives pursue solutions rather waiting answers come attention detail identify ambiguities specifications exceptional written verbal communication especially communicating tradeoffs technical decisions nontechnical colleagues flexibility maintain focus evolving let go previous projects move ones dig deeper existing projects grow depending business needs,software development focused web technologies significant production python designing building maintaining enterprise data pipelines andor warehouses level machine algorithms ml models built deployed demonstrable big databases columnar stores e g cassandra bigtable hadoop well sql mysql mssql postgresql select right tool job queued management message processing kafka rabbitmq closely product account support personnel help prioritize best solutions largest problems reliable organization communication follow verbal written commitments persistent approach problemsolving see completion even face complexities unknowns proactive mindset drives pursue rather waiting answers come attention detail identify ambiguities specifications exceptional especially communicating tradeoffs technical decisions nontechnical colleagues flexibility maintain focus evolving let go previous projects move ones dig deeper existing grow depending business needs
290," Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.  Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured  Postgres, MySQL  and unstructured data sources  Elastic, Kafka, and the Hadoop ecosystem  as implemented at Internet-scale.  Experience writing and optimizing streaming and batch analytics.  Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.  BS/BA in Computer Science, Engineering, or relevant field experience.     ",handson programming expertise multiple implementation languagesframeworks subset python java scala delivery background middleware backend implementations familiarity largescale big data streaming data technologies well exposure variety structured postgres mysql unstructured data sources elastic kafka hadoop ecosystem implemented internetscale writing optimizing streaming batch analytics agile frameworks secure software design testdriven development modern containerdelivered code deployment cloudbased devops bsba computer science engineering relevant,handson programming expertise multiple implementation languagesframeworks subset python java scala delivery background middleware backend implementations familiarity largescale big data streaming technologies well exposure variety structured postgres mysql unstructured sources elastic kafka hadoop ecosystem implemented internetscale writing optimizing batch analytics agile frameworks secure software design testdriven development modern containerdelivered code deployment cloudbased devops bsba computer science engineering relevant
291,"  Bachelor's degree  CS, EE or Math preferred  or equivalent work experience as well as interest in a fast paced, complex environment.  5+ years of experience Scala preferred in a commercial environment Expert in Spark, experience with the Hadoop ecosystem and similar frameworks Expert in SQL Familiarity with various tools such as AWS and Docker and an instinct for automation Strong understanding of Software Architecture principles and patterns.  Experience working with 3rd party software and libraries, including open source Experience with Postgres    ",bachelors degree cs ee math well interest fast paced complex scala commercial expert spark hadoop ecosystem similar frameworks expert sql familiarity various tools aws docker instinct automation understanding software architecture principles patterns rd party software libraries open source postgres,bachelors degree cs ee math well interest fast paced complex scala commercial expert spark hadoop ecosystem similar frameworks sql familiarity various tools aws docker instinct automation understanding software architecture principles patterns rd party libraries open source postgres
292,"  Eat, sleep, breathe data/databases  SQL  MySQL, etc.  , NoSQL/BigData Cassandra, etc.   Better delivery than Dominos  architectural expertise, CI/CD experience, PjM skills  More renown architecturally than Frank Lloyd Wright Knows clouds  AWS  better than the sky Plays well with others  Understand and align data architecture strategy to the business and technology strategy Partner with architecture and development teams to evolve software products to exceed the needs and expectations of the consumer Deliver technology products that yield immediate business value Collaborate as part of a high-performing team and strengthen the NML tech community as a whole Understand development costs and resourcing, compliance, security, and risk  ",eat sleep breathe datadatabases sql mysql nosqlbigdata cassandra better delivery dominos architectural expertise cicd pjm renown architecturally frank lloyd wright knows clouds aws better sky plays well others understand align data architecture strategy business technology strategy partner architecture development teams evolve software products exceed needs expectations consumer deliver technology products yield immediate business value collaborate part highperforming team strengthen nml tech community whole understand development costs resourcing compliance security risk,eat sleep breathe datadatabases sql mysql nosqlbigdata cassandra better delivery dominos architectural expertise cicd pjm renown architecturally frank lloyd wright knows clouds aws sky plays well others understand align data architecture strategy business technology partner development teams evolve software products exceed needs expectations consumer deliver yield immediate value collaborate part highperforming team strengthen nml tech community whole costs resourcing compliance security risk
293,"   Design and development of ETL and data pipeline solutions for complex business problems to load Data Warehouse Data Stewardship - own or support the data definitions and lineage across our organization.  Create a data integration plan and build data integrations between systems.  Figure out the best way to share information and build the tech needed to execute.  Mentoring - help teach other team members about data architecture and also be a consultant for developers who need help with data.     At least 3 years of relevant experience.  Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments like RDS, MySQL, Python, Pyspark, Airtable, Talend.  Experience developing, deploying, and testing in AWS ",design development etl data pipeline solutions complex business problems load data warehouse data stewardship support data definitions lineage across organization create data integration plan build data integrations systems figure best way share information build tech needed execute mentoring help teach team members data architecture also consultant developers need help data least relevant data warehouses data warehouse technical architectures infrastructure components etlelt reportinganalytic tools environments like rds mysql python pyspark airtable talend developing deploying testing aws,design development etl data pipeline solutions complex business problems load warehouse stewardship support definitions lineage across organization create integration plan build integrations systems figure best way share information tech needed execute mentoring help teach team members architecture also consultant developers need least relevant warehouses technical architectures infrastructure components etlelt reportinganalytic tools environments like rds mysql python pyspark airtable talend developing deploying testing aws
294," Bachelor degree required in Computer Science, Technology, or similar field.  3 - 5 years of work experience.  Experience supporting and working with cross-functional teams in a dynamic environment.  Experience building and optimizing data pipelines, architectures and data sets.  Experience creating data profiling, cleansing and data management services Build processes supporting data transformation, data structures, metadata, dependency and workload management.  Working knowledge of message queuing, stream processing, and highly scalable big data stores.  Strong project management and organizational skills.  Interest in Machine Learning & Analytics Operations     Experience with relational SQL, NoSQL, and graph databases, including DynamoDB, Redis, Postgres, Neo4J and Cassandra.  Experience with AWS cloud services  EC2, EMR, RDS, Redshift, Athena Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, Gremlin etc.  and experience building MVPs.  Experience with big data tools  Spark, Kafka, Hadoop.  Experience/working with ETL patterns and tools.  Experience with deployment and containerization, e. g. , Docker and Kubernetes. ",bachelor degree computer science technology similar supporting crossfunctional teams dynamic building optimizing data pipelines architectures data sets creating data profiling cleansing data management services build processes supporting data transformation data structures metadata dependency workload management message queuing stream processing highly scalable big data stores project management organizational interest machine analytics operations relational sql nosql graph databases dynamodb redis postgres neoj cassandra aws cloud services ec emr rds redshift athena objectorientedobject function scripting languages python java c scala gremlin building mvps big data tools spark kafka hadoop experienceworking etl patterns tools deployment containerization e g docker kubernetes,bachelor degree computer science technology similar supporting crossfunctional teams dynamic building optimizing data pipelines architectures sets creating profiling cleansing management services build processes transformation structures metadata dependency workload message queuing stream processing highly scalable big stores project organizational interest machine analytics operations relational sql nosql graph databases dynamodb redis postgres neoj cassandra aws cloud ec emr rds redshift athena objectorientedobject function scripting languages python java c scala gremlin mvps tools spark kafka hadoop experienceworking etl patterns deployment containerization e g docker kubernetes
295," Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs.  Minimum 2 years professional experience working with data extract/manipulation logic.  Minimum 2 years professional experience with object-oriented programming, functional programming, and data design.  1-3 years working with a public cloud big data ecosystem  certification in AWS a plus .  1-3 years working with MPP databases, distributed databases, and/or Hadoop.    Passion for data engineering, able to excite and lead by example.  Hungry and eager to learn new systems and technologies.  Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.  Ability to deliver exceptional results through iterative improvement rather than initial perfection.  Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including  business users, technical staff, senior level colleagues, vendors, and partners.  An extensive track record that demonstrates effectiveness in driving business results through data and analytics.  The ability to develop and articulate a compelling vision and generate necessary consensus.  A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.  A proven ability to influence decision making across large organizations.  A proven ability to hire, develop, and effectively lead deeply technical resources.  Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.  Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.  Create an environment where people from diverse cultures and backgrounds work together effectively.    Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably.  Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions.  Gather and process all types of data including raw, structured, semi-structured, and unstructured data.  Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics.  Build and maintain dimensional data warehouses in support of business intelligence tools.  Develop data catalogs and data validations to ensure clarity and correctness of key business metrics.  Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result.  Derive an overall strategy of data management, within an established information architecture  including both structured and unstructured data , that supports the development and secure operation of existing and new information and digital services.  Plan effective data storage, security, sharing and publishing within the organization.  Ensure data quality and implement tools and frameworks for automating the identification of data quality issues.  Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.  Provide ongoing support, monitoring, and maintenance of deployed products.  Drive and maintain a culture of quality, innovation and experimentation.     Passion for data engineering, able to excite and lead by example.  Hungry and eager to learn new systems and technologies.  Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.  Ability to deliver exceptional results through iterative improvement rather than initial perfection.  Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including  business users, technical staff, senior level colleagues, vendors, and partners.  An extensive track record that demonstrates effectiveness in driving business results through data and analytics.  The ability to develop and articulate a compelling vision and generate necessary consensus.  A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.  A proven ability to influence decision making across large organizations.  A proven ability to hire, develop, and effectively lead deeply technical resources.  Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.  Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.  Create an environment where people from diverse cultures and backgrounds work together effectively.  ",advanced degree relevant study strongly desirable particularly computer science engineering level programs minimum professional data extractmanipulation logic minimum professional objectoriented programming functional programming data design public cloud big data ecosystem certification aws plus mpp databases distributed databases andor hadoop passion data engineering able excite lead example hungry eager learn systems technologies selfdirected enjoys challenge freedom deciding impactful thing next deliver exceptional results iterative improvement rather initial perfection communication presentation interact appropriately levels organization business users technical staff senior level colleagues vendors partners extensive track record demonstrates effectiveness driving business results data analytics develop articulate compelling vision generate necessary consensus successful history translating business objectives problems analytic problems analytic solutions actionable business solutions proven influence decision making across organizations proven hire develop effectively lead deeply technical resources demonstrate foster sense urgency commitment accountability making sound decisions achieving goals articulate inspire engage commitment plan action aligned organizational mission goals create people diverse cultures backgrounds together effectively build deploy streaming batch data pipelines capable processing storing petabytes data quickly reliably collaborate product teams data analysts data scientists design build dataforward solutions gather process types data raw structured semistructured unstructured data integrate variety data providers ranging marketing web analytics consumer devices iot telematics build maintain dimensional data warehouses support business intelligence tools develop data catalogs data validations clarity correctness key business metrics design code test correct document programs scripts agreed standards tools achieve wellengineered result derive overall strategy data management within established information architecture structured unstructured data supports development secure operation existing information digital services plan effective data storage security sharing publishing within organization data implement tools frameworks automating identification data issues collaborate internal external data providers data validation providing feedback making customized changes data feeds data mappings ongoing support monitoring maintenance deployed products drive maintain culture innovation experimentation passion data engineering able excite lead example hungry eager learn systems technologies selfdirected enjoys challenge freedom deciding impactful thing next deliver exceptional results iterative improvement rather initial perfection communication presentation interact appropriately levels organization business users technical staff senior level colleagues vendors partners extensive track record demonstrates effectiveness driving business results data analytics develop articulate compelling vision generate necessary consensus successful history translating business objectives problems analytic problems analytic solutions actionable business solutions proven influence decision making across organizations proven hire develop effectively lead deeply technical resources demonstrate foster sense urgency commitment accountability making sound decisions achieving goals articulate inspire engage commitment plan action aligned organizational mission goals create people diverse cultures backgrounds together effectively,advanced degree relevant study strongly desirable particularly computer science engineering level programs minimum professional data extractmanipulation logic objectoriented programming functional design public cloud big ecosystem certification aws plus mpp databases distributed andor hadoop passion able excite lead example hungry eager learn systems technologies selfdirected enjoys challenge freedom deciding impactful thing next deliver exceptional results iterative improvement rather initial perfection communication presentation interact appropriately levels organization business users technical staff senior colleagues vendors partners extensive track record demonstrates effectiveness driving analytics develop articulate compelling vision generate necessary consensus successful history translating objectives problems analytic solutions actionable proven influence decision making across organizations hire effectively deeply resources demonstrate foster sense urgency commitment accountability sound decisions achieving goals inspire engage plan action aligned organizational mission create people diverse cultures backgrounds together build deploy streaming batch pipelines capable processing storing petabytes quickly reliably collaborate product teams analysts scientists dataforward gather process types raw structured semistructured unstructured integrate variety providers ranging marketing web consumer devices iot telematics maintain dimensional warehouses support intelligence tools catalogs validations clarity correctness key metrics code test correct document scripts agreed standards achieve wellengineered result derive overall strategy management within established information architecture supports development secure operation existing digital services effective storage security sharing publishing implement frameworks automating identification issues internal external validation providing feedback customized changes feeds mappings ongoing monitoring maintenance deployed products drive culture innovation experimentation
296,"  You hold at least a bachelorâs degree in Computer Science or a related discipline You have at least 3 yearsâ experience working with large and complex data sets You have at least 6 years experience working with clinical data  e. g.  clinical trial data, lab data, EMR data, etc.   You are a proficient Java developer You are a proficient SQL developer You have experience writing ETL/ELT code You have experience with data profiling tools and concepts   ",hold least bachelors degree computer science discipline least complex data sets least clinical data e g clinical trial data lab data emr data proficient java developer proficient sql developer writing etlelt code data profiling tools concepts,hold least bachelors degree computer science discipline complex data sets clinical e g trial lab emr proficient java developer sql writing etlelt code profiling tools concepts
297,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
298,"   Collaborating with the Engineering team to design, build and improve Reonomyâs complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software  ",collaborating engineering team design build improve reonomys complex data layer creating data systems consistency data platform solving real challenges around creating systems import cleanse structure display huge volumes data playing major role future architecture rapidly expanding backend platform writing code participating actively code reviews consistently helping ship software,collaborating engineering team design build improve reonomys complex data layer creating systems consistency platform solving real challenges around import cleanse structure display huge volumes playing major role future architecture rapidly expanding backend writing code participating actively reviews consistently helping ship software
299," 4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.  2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.  1+ years of experience on distributed, high throughput and low latency architecture.  1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.  A successful track-record of manipulating, processing and extracting value from large disconnected datasets.     ",software development substantial part gained highthroughput decisionautomation big data technologies like spark kafka flink hadoop nosql datastores distributed throughput low latency architecture deploying managing data pipelines supporting datasciencedriven decisioning scale successful trackrecord manipulating processing extracting value disconnected datasets,software development substantial part gained highthroughput decisionautomation big data technologies like spark kafka flink hadoop nosql datastores distributed throughput low latency architecture deploying managing pipelines supporting datasciencedriven decisioning scale successful trackrecord manipulating processing extracting value disconnected datasets
300,"Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment. Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake. Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges. Mitigate Risks in our data infrastructure by developing the best in class tools and processes. Implement controls, policies, processes and best practices in the Data Engineering space. Evangelize an extremely high standard of code quality, system reliability, and performance. Help us improve our database deployment and change management process. Provide reliable and efficient Data services as part of the global data team. Work closely with the team on development best practices and standards. Be a mentor.     ",play pivotal design hands implementation role improving data infrastructure projectoriented influence cross functional architecture sprint planninggather process raw data scale internal external data sources expose mechanisms scale parallel processingdesign implement manage near realtime ingestion pipeline data warehouse hadoop data lake process unstructured data form suitable analysis empower stateoftheart analysis analysts scientists apissolve complex sql big data performance challenges mitigate risks data infrastructure developing best class tools processes implement controls policies processes best practices data engineering space evangelize extremely standard code reliability performance help us improve database deployment change management process reliable efficient data services part global data team closely team development best practices standards mentor,play pivotal design hands implementation role improving data infrastructure projectoriented influence cross functional architecture sprint planninggather process raw scale internal external sources expose mechanisms parallel processingdesign implement manage near realtime ingestion pipeline warehouse hadoop lake unstructured form suitable analysis empower stateoftheart analysts scientists apissolve complex sql big performance challenges mitigate risks developing best class tools processes controls policies practices engineering space evangelize extremely standard code reliability help us improve database deployment change management reliable efficient services part global team closely development standards mentor
301,"Three or more years of experience working with  nix-based, open source data processing tools Fluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets Two or more years of experience developing production ETL applications Four or more years of experience in software development Three or more years of experience with SQL Curious, informed and opinionated about data processing technologies Experience with structured and unstructured data storage and modeling Deep understanding of database and filesystem storage/access Experience with various data engineering architecture patterns Interest in Data Science and Data Analysis   Building python-based ETL jobs Light web application development of purpose-built internal tools Technical guidance in support of our project management team when defining the scope of data integration projects Contribute to designs of new components in modeling and data pipelines Remain current on emerging open source data processing projects and tools Must have experience with AWS services, Redshift data bases and HIPAA compliant architecture models.   BS or MS in Computer Science Experience writing production Python Implementation experience with Airflow, Python, Spark etc.  Experience with healthcare data formats  x12 EDI, HL7, etc  Experience implementing stream processing pipelines  Spark, etc  MapReduce/Hadoop ecosystem experience  Hive, HDFS/S3, Presto  Experience with source control technology like Git Experience with testing frameworks  unit and end-to-end  Familiar with the usage of Continuous Integration/Continuous Deployment frameworks in AWS like Jenkins, CircleCI, Code deploy and code commit Understanding of Docker implementation in AWS Understanding of AWS serverless services like Lambda/API gateway Good to have  AWS elastic beanstalk, AWS cloud formation  ",three nixbased open source data processing tools fluent python sql spark hadoop airflow andor similar technologiestoolsets two developing production etl applications four software development three sql curious informed opinionated data processing technologies structured unstructured data storage modeling deep understanding database filesystem storageaccess various data engineering architecture patterns interest data science data analysis building pythonbased etl jobs light web application development purposebuilt internal tools technical guidance support project management team defining scope data integration projects contribute designs components modeling data pipelines remain current emerging open source data processing projects tools must aws services redshift data bases hipaa compliant architecture models bs ms computer science writing production python implementation airflow python spark healthcare data formats x edi hl implementing stream processing pipelines spark mapreducehadoop ecosystem hive hdfss presto source control technology like git testing frameworks unit endtoend familiar usage continuous integrationcontinuous deployment frameworks aws like jenkins circleci code deploy code commit understanding docker implementation aws understanding aws serverless services like lambdaapi gateway good aws elastic beanstalk aws cloud formation,three nixbased open source data processing tools fluent python sql spark hadoop airflow andor similar technologiestoolsets two developing production etl applications four software development curious informed opinionated technologies structured unstructured storage modeling deep understanding database filesystem storageaccess various engineering architecture patterns interest science analysis building pythonbased jobs light web application purposebuilt internal technical guidance support project management team defining scope integration projects contribute designs components pipelines remain current emerging must aws services redshift bases hipaa compliant models bs ms computer writing implementation healthcare formats x edi hl implementing stream mapreducehadoop ecosystem hive hdfss presto control technology like git testing frameworks unit endtoend familiar usage continuous integrationcontinuous deployment jenkins circleci code deploy commit docker serverless lambdaapi gateway good elastic beanstalk cloud formation
302,"     Weâre looking for individuals who have proven big data experience, either from an implementation or a data science prospective.  The desire to learn and code in Scala Experience in working in an Agile environment Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.  A strong coding background in either Java, Python or Scala ",looking individuals proven big data either implementation data science prospective desire learn code scala agile expert least one big data technology spark hadoop elasticsearch coding background either java python scala,looking individuals proven big data either implementation science prospective desire learn code scala agile expert least one technology spark hadoop elasticsearch coding background java python
303,"3+ years of Software Engineering experience 2+ years experience work with real-time/streaming data Experience with a RDBMS  e. g.  MySQL, PostgreSQL  Experience with real time data streaming tools  e. g.  Apache Kafka, AWS Kinesis  Experience working with an OLAP or Time Series Databases  e. g.  Druid  Experience with a data processing solution.   e. g.  AWS Athena, Apache Spark      ",software engineering realtimestreaming data rdbms e g mysql postgresql real time data streaming tools e g apache kafka aws kinesis olap time series databases e g druid data processing solution e g aws athena apache spark,software engineering realtimestreaming data rdbms e g mysql postgresql real time streaming tools apache kafka aws kinesis olap series databases druid processing solution athena spark
304,"  Serve as a senior data engineer for audience studio data products. Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and SRAT leadershipDevelop and code the data management services that is core to Audience Studio, under the leadership of the VP ArchitectureSupport product with the overall roadmap and ensure updates to senior leadership are 100% technically correct. Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership  ",serve senior data engineer audience studio data products participate execute month product roadmap input delivery team stakeholders srat leadershipdevelop code data management services core audience studio leadership vp architecturesupport product overall roadmap updates senior leadership technically correct analyze report results adjust overall engineering strategy accordingly engineering leadership,serve senior data engineer audience studio products participate execute month product roadmap input delivery team stakeholders srat leadershipdevelop code management services core leadership vp architecturesupport overall updates technically correct analyze report results adjust engineering strategy accordingly
305," Bachelorâs Degree Required  Computer Science or Engineering discipline preferred.  3+ years technology experience working in Software Engineering capacity.  3+ years working in Python  other modern languages considered .  1+ years working within Analytic/Data Warehouse/Data Lake environment.  1+ years working with AWS public cloud  certification a plus .  Expertise in SQL  10 out of 10, SQL Ninja analytic capabilities.  Strong working knowledge of with Linux Shell.  Understanding of Data Warehouse principles, including Dimensional Modeling.  Creative, flexible, and quick to learn.  Experience with Redshift a plus.    Design, develop, deploy and manage a reliable and scalable data analysis pipeline, using technologies including Python, S3, and Redshift.  Participate in cross-functional initiatives to develop new capabilities, including hands-on development responsibilities.  Ability to integrate data from a variety of sources, assuring they adhere to data quality and accessibility standards.  Document processes and standard operating procedures.  Evaluate and conduct POCâs with new technologies.     ",bachelors degree computer science engineering discipline technology software engineering capacity python modern languages considered within analyticdata warehousedata lake aws public cloud certification plus expertise sql sql ninja analytic capabilities linux shell understanding data warehouse principles dimensional modeling creative flexible quick learn redshift plus design develop deploy manage reliable scalable data analysis pipeline technologies python redshift participate crossfunctional initiatives develop capabilities handson development responsibilities integrate data variety sources assuring adhere data accessibility standards document processes standard operating procedures evaluate conduct pocs technologies,bachelors degree computer science engineering discipline technology software capacity python modern languages considered within analyticdata warehousedata lake aws public cloud certification plus expertise sql ninja analytic capabilities linux shell understanding data warehouse principles dimensional modeling creative flexible quick learn redshift design develop deploy manage reliable scalable analysis pipeline technologies participate crossfunctional initiatives handson development responsibilities integrate variety sources assuring adhere accessibility standards document processes standard operating procedures evaluate conduct pocs
306,"At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ",least consulting client service delivery azure devops azure platform proven build manage foster teamoriented,least consulting client service delivery azure devops platform proven build manage foster teamoriented
307,"   Manage partner technical integration projects and ensure the prompt and proper resolution of technical challenges.  Develop and maintain third-party data validation methodologies, including building and maintaining automated and scalable technical infrastructure.  Guarantee the technical aspects of a partnerâs integration  both new and ongoing  by providing technical guidance and documentation.  Identify, drive, and optimize new third-party reporting opportunities by leveraging YouTube technologies.  Write and maintain lines of code  Python, C++, etc.   to support your own small to medium scale Extract, Transform, Load  ETL  pipelines.    ",manage partner technical integration projects prompt proper resolution technical challenges develop maintain thirdparty data validation methodologies building maintaining automated scalable technical infrastructure guarantee technical aspects partners integration ongoing providing technical guidance documentation identify drive optimize thirdparty reporting opportunities leveraging youtube technologies write maintain lines code python c support small medium scale extract transform load etl pipelines,manage partner technical integration projects prompt proper resolution challenges develop maintain thirdparty data validation methodologies building maintaining automated scalable infrastructure guarantee aspects partners ongoing providing guidance documentation identify drive optimize reporting opportunities leveraging youtube technologies write lines code python c support small medium scale extract transform load etl pipelines
308," Mastery in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or customer-facing role     ",mastery least one following domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale writing software one languages python java scala go building productiongrade data solutions relational nosql systems monitoringalerting capacity planning performance tuning technical consulting customerfacing role,mastery least one following domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale writing languages python java scala go relational nosql systems monitoringalerting capacity planning performance tuning consulting customerfacing role
309,"At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ",least consulting client service delivery azure devops azure platform proven build manage foster teamoriented,least consulting client service delivery azure devops platform proven build manage foster teamoriented
310,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
311,"  Enhance and further develop Big Data processing pipelines for data sources containing structured and unstructured data Data Warehousing with Data modeler experience Monitor and optimize key infrastructure components such as Databases, EC2 Clusters, and other aspects of the stack Help promote best practices for Big Data development Act as a bridge between the infrastructure and application engineering teams Provide infrastructure support with a focus on cloud based computing Build and support visualization and exploration capabilities around our Data Sets Work with the Data Extraction and Data Science engineers on normalization and analytical processes Work in an Agile manner with business users and data scientists to understand and discover the potential business value of new and existing Data Sets and help productize those discoveries Help design and implement disaster recovery efforts Analyze requirements and architecture specifications to create detailed design Research areas of interest to the team and help facilitate solutions Design Cloud Architecture, SaaS, PaaS, and SaaS   ",enhance develop big data processing pipelines data sources containing structured unstructured data data warehousing data modeler monitor optimize key infrastructure components databases ec clusters aspects stack help promote best practices big data development act bridge infrastructure application engineering teams infrastructure support focus cloud based computing build support visualization exploration capabilities around data sets data extraction data science engineers normalization analytical processes agile manner business users data scientists understand discover potential business value existing data sets help productize discoveries help design implement disaster recovery efforts analyze architecture specifications create detailed design research areas interest team help facilitate solutions design cloud architecture saas paas saas,enhance develop big data processing pipelines sources containing structured unstructured warehousing modeler monitor optimize key infrastructure components databases ec clusters aspects stack help promote best practices development act bridge application engineering teams support focus cloud based computing build visualization exploration capabilities around sets extraction science engineers normalization analytical processes agile manner business users scientists understand discover potential value existing productize discoveries design implement disaster recovery efforts analyze architecture specifications create detailed research areas interest team facilitate solutions saas paas
312,"    Legal authorization to work in the US on a full-time basis for anyone other than current employer Bachelor's degree in a relevant technical discipline.  5+ years of experience with demonstrable proficiency in one or more DB and data pipeline tooling  mySQL, PostgreSQL, OSI Pi historian, TimescaleDB, Streamsets, Apache Drill, Apache Parquet, Dremio â¦ You are passionate about building scalable, high performing, data pipelines, and analytic catalogs.  Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, etc Proficient with Containerized environments and workloads Excellent analytical, problem-solving, and troubleshooting skills.  Experience architecting, deploying, and supporting production applications.  You care deeply about performance, accessibility and API design.  Great communication skills.  Work locations may include Houston, TX or San Francisco, CA",legal authorization us fulltime basis anyone current employer bachelors degree relevant technical discipline demonstrable proficiency one db data pipeline tooling mysql postgresql osi pi historian timescaledb streamsets apache drill apache parquet dremio passionate building scalable performing data pipelines analytic catalogs objectorientedobject function scripting languages python java c scala proficient containerized environments workloads analytical problemsolving troubleshooting architecting deploying supporting production applications care deeply performance accessibility api design great communication locations may include houston tx san francisco ca,legal authorization us fulltime basis anyone current employer bachelors degree relevant technical discipline demonstrable proficiency one db data pipeline tooling mysql postgresql osi pi historian timescaledb streamsets apache drill parquet dremio passionate building scalable performing pipelines analytic catalogs objectorientedobject function scripting languages python java c scala proficient containerized environments workloads analytical problemsolving troubleshooting architecting deploying supporting production applications care deeply performance accessibility api design great communication locations may include houston tx san francisco ca
313,"  3+ years experience in data/software engineering or related field Fluency in Python and SQL, experience with Golang, C, C++, Java, or Scala is a plus Demonstrated experience with distributed computing  Kafka, Storm, Spark, Hadoop, etc.   Experience with NoSQL databases Proficiency in using and managing cloud infrastructure, preferably AWS Linux and Bash competence Experience integrating with Salesforce a plus  Establish and maintain best practices for our data infrastructure Develop next-gen data pipelining and ETL based on open source data pipeline tools and cloud-based ecosystems that can deal with varied data types from disparate sources Develop and tune data storage and processing systems at scale Build real-time data processing systems Build automated systems to continually monitor data quality and integrity Work closely with stakeholders across Vroom to ensure data is accurate, timely, and useful  ",datasoftware engineering fluency python sql golang c c java scala plus demonstrated distributed computing kafka storm spark hadoop nosql databases proficiency managing cloud infrastructure preferably aws linux bash competence integrating salesforce plus establish maintain best practices data infrastructure develop nextgen data pipelining etl based open source data pipeline tools cloudbased ecosystems deal varied data types disparate sources develop tune data storage processing systems scale build realtime data processing systems build automated systems continually monitor data integrity closely stakeholders across vroom data accurate timely useful,datasoftware engineering fluency python sql golang c java scala plus demonstrated distributed computing kafka storm spark hadoop nosql databases proficiency managing cloud infrastructure preferably aws linux bash competence integrating salesforce establish maintain best practices data develop nextgen pipelining etl based open source pipeline tools cloudbased ecosystems deal varied types disparate sources tune storage processing systems scale build realtime automated continually monitor integrity closely stakeholders across vroom accurate timely useful
314,"   Lead, design, develop, and deliver large-scale Azure data systems, data processing, and data transformation projects.  Execute technical feasibility assessments and project estimates for moving databases and data processing to Azure.  Design and advocate solutions using modern cloud technologies, design principles, integration points, and automation methods.  Mentor and share knowledge with customers as well as provide architecture reviews, discussions, and prototypes.  Participate in overall engagement from strategy, assessment, migration, and implementations.  Work with customers to deploy, manage, and audit best practices for cloud products.   5 to 10 years of professional experience in the information technology industry.  BS in Computer Science or equivalent education/professional experience is required.   Demonstrated experience designing, implementing, and supporting enterprise-grade technical solutions in the cloud for meeting complex business data requirements.  Experience with Databricks and using Spark for data processing.  Experience with Azure Data Factory â ADF.  Advanced experience with different query languages  i. e.  T-SQL, PostgreSQL, PL-SQL .  Experience designing and building data marts, warehouses, customer profile databases, etc.  Experience with data modeling, table design, and mapping business needs to data structures.  Experience with Azure Data Lake, Azure SQL Data Warehouse, and Cosmos DB are a plus.  Experience with Data Management Gateway, Azure Storage Options, Stream Analytics, and Event Hubs is a plus.  Experience with other cloud based big data architectures is a plus. ",lead design develop deliver largescale azure data systems data processing data transformation projects execute technical feasibility assessments project estimates moving databases data processing azure design advocate solutions modern cloud technologies design principles integration points automation methods mentor share customers well architecture reviews discussions prototypes participate overall engagement strategy assessment migration implementations customers deploy manage audit best practices cloud products professional information technology industry bs computer science educationprofessional demonstrated designing implementing supporting enterprisegrade technical solutions cloud meeting complex business data databricks spark data processing azure data factory adf advanced different query languages e tsql postgresql plsql designing building data marts warehouses customer profile databases data modeling table design mapping business needs data structures azure data lake azure sql data warehouse cosmos db plus data management gateway azure storage options stream analytics event hubs plus cloud based big data architectures plus,lead design develop deliver largescale azure data systems processing transformation projects execute technical feasibility assessments project estimates moving databases advocate solutions modern cloud technologies principles integration points automation methods mentor share customers well architecture reviews discussions prototypes participate overall engagement strategy assessment migration implementations deploy manage audit best practices products professional information technology industry bs computer science educationprofessional demonstrated designing implementing supporting enterprisegrade meeting complex business databricks spark factory adf advanced different query languages e tsql postgresql plsql building marts warehouses customer profile modeling table mapping needs structures lake sql warehouse cosmos db plus management gateway storage options stream analytics event hubs based big architectures
315,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform.  Multi-cloud experience a plus.    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",minimum previous consulting client service delivery google gcp devops gcp platform multicloud plus proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,minimum previous consulting client service delivery google gcp devops platform multicloud plus proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
316," Experience working on Hadoop platform components Knowledge of Big Data tools, such as zookeeper, Kafka Streaming.  Shell scripting experience Experience with integration of data from multiple data sources  NoSQL, Mongo, SQL  Experience working with Structured/Unstructured data.  Experience creating ETL pipelines Experience in Docker builds and Git file versioning Demonstrated ability to quickly learn new tools and paradigms to deploy cutting edge solutions.  Knowledge of programming in Python Knowledge of MapR Knowledge of Scala framework Experience with Spark, Storm or Flink Integrate Data from multiple data sources Create ETL Pipelines Work under the guidance of Lead to develop based on design/architecture.   ",hadoop platform components big data tools zookeeper kafka streaming shell scripting integration data multiple data sources nosql mongo sql structuredunstructured data creating etl pipelines docker builds git file versioning demonstrated quickly learn tools paradigms deploy cutting edge solutions programming python mapr scala framework spark storm flink integrate data multiple data sources create etl pipelines guidance lead develop based designarchitecture,hadoop platform components big data tools zookeeper kafka streaming shell scripting integration multiple sources nosql mongo sql structuredunstructured creating etl pipelines docker builds git file versioning demonstrated quickly learn paradigms deploy cutting edge solutions programming python mapr scala framework spark storm flink integrate create guidance lead develop based designarchitecture
317,"    Must have legal authorization to work in the US on a full-time basis for anyone other than current employer Bachelor's Degree preferably higher, in mathematics, statistics, computer science, or another relevant discipline Minimum five  5  years of relevant experience with a strong background in data and software engineering, with experience of writing code.  Strong experience with Python and relevant libraries.  The ability to work across structured, semi-structured, and unstructured data, extracting information and identifying irregularities and linkages across disparate data sets.  Meaningful experience in distributed processing.  Deep understanding of information security principles to ensure compliant handling and management of client data.  Experience in traditional data warehousing / ETL tools.  Experience and interest in cloud infrastructure and containerization.  Preferably some experience programming with Julia.  Experience or interest in building robust and practical data pipelines on top of cloud infrastructure will also be an advantage. ",must legal authorization us fulltime basis anyone current employer bachelors degree preferably higher mathematics statistics computer science another relevant discipline minimum five relevant background data software engineering writing code python relevant libraries across structured semistructured unstructured data extracting information identifying irregularities linkages across disparate data sets meaningful distributed processing deep understanding information security principles compliant handling management client data traditional data warehousing etl tools interest cloud infrastructure containerization preferably programming julia interest building robust practical data pipelines top cloud infrastructure also advantage,must legal authorization us fulltime basis anyone current employer bachelors degree preferably higher mathematics statistics computer science another relevant discipline minimum five background data software engineering writing code python libraries across structured semistructured unstructured extracting information identifying irregularities linkages disparate sets meaningful distributed processing deep understanding security principles compliant handling management client traditional warehousing etl tools interest cloud infrastructure containerization programming julia building robust practical pipelines top also advantage
318," Comfortable working with ambiguity  e. g.  imperfect data, loosely defined concepts, ideas, or goals  and translating these into more tangible outputs Strong analytical and critical thinking skills Self-motivated.  Capable of working with little or no supervision Strong written and verbal communication skills Enjoy challenging and thought-provoking work and have a strong desire to learn and progress Ability to manage multiple tasks and requests Must demonstrate a positive, team-focused attitude Ability to react positively under pressure to meet tight deadlines Good inter-personal skills combined with willingness to listen Structured, disciplined approach to work, with attention to detail Flexible â able to meet changing requirements and priorities Maintenance of up-to-date knowledge in the appropriate technical areas Able to work in a global, multicultural environment Work on new and innovative portfolio construction and analytics applications along with other experienced developers.  Identify, ingest, and enrich a diverse set of structured and unstructured big data into datasets for analysis.  Operate and extend the data research platform to deliver production-quality data on time for analysis.  Own end-to-end data workflows and develop deep domain expertise to ensure data quality and completeness Experiment with new technologies and acquire new skills to find creative solutions to the unique challenges we will encounter along the way  Masters in Statistics, Computer Science or other similar advanced degrees from a top tier educational institution preferred CFA, CPA, CIPM, CAIA, and/or FRM preferred, but not required.  ",comfortable ambiguity e g imperfect data loosely defined concepts ideas goals translating tangible outputs analytical critical thinking selfmotivated capable little supervision written verbal communication enjoy challenging thoughtprovoking desire learn progress manage multiple tasks requests must demonstrate positive teamfocused attitude react positively pressure meet tight deadlines good interpersonal combined willingness listen structured disciplined approach attention detail flexible able meet changing priorities maintenance uptodate appropriate technical areas able global multicultural innovative portfolio construction analytics applications along experienced developers identify ingest enrich diverse set structured unstructured big data datasets analysis operate extend data research platform deliver productionquality data time analysis endtoend data workflows develop deep domain expertise data completeness experiment technologies acquire find creative solutions unique challenges encounter along way masters statistics computer science similar advanced degrees top tier educational institution cfa cpa cipm caia andor frm,comfortable ambiguity e g imperfect data loosely defined concepts ideas goals translating tangible outputs analytical critical thinking selfmotivated capable little supervision written verbal communication enjoy challenging thoughtprovoking desire learn progress manage multiple tasks requests must demonstrate positive teamfocused attitude react positively pressure meet tight deadlines good interpersonal combined willingness listen structured disciplined approach attention detail flexible able changing priorities maintenance uptodate appropriate technical areas global multicultural innovative portfolio construction analytics applications along experienced developers identify ingest enrich diverse set unstructured big datasets analysis operate extend research platform deliver productionquality time endtoend workflows develop deep domain expertise completeness experiment technologies acquire find creative solutions unique challenges encounter way masters statistics computer science similar advanced degrees top tier educational institution cfa cpa cipm caia andor frm
319,"  Extensive experience using statistics, mathematics, algorithms and programming languages to solve big data challenges.  Fluent in structured and unstructured data, its management, and modern data transformation methodologies.  Ability to define and create complex models to pull valuable insights, predictions and innovation from data.  Effectively and creatively tell stories and create visualizations to describe and communicate data insights.  Strong analytical and problem-solving skills.  Excellent written and verbal communication skills; mastery in English and local language.  Ability to effectively communicate data insights and negotiate options at senior management levels.   Mines data using modern tools and programming languages.  Defines and implements models to uncover patterns and predictions creating business value and innovation.  Manages relationships with business partners to evaluate and foster data driven innovation, provide domain-specific expertise in cross-organization projects/initiatives.  Ties insights into effective visualizations communicating business value and innovation potential.  Maintains proficiency within the data science domain by keeping up with technology and trend shifts.  Contributes to industry data science domain initiatives.  Leads project team s  of data science professionals, assuring insights are communicated regularly and effectively, reviewing designs, models and accuracy and data compliance.  Collaborates and communicates with project team regarding project progress and issue resolution.  Communicates and drives data insights/innovation into the business.  Represents the data science team for all phases of larger and more-complex development projects.  Provides guidance, training and mentoring to less experienced staff members.  Strong Scala, Python programming experience is a must Databricks Spark experience  Bachelor's, Master's or PHD degree in Mathematics, Economics, Physics, Computer Science, or equivalent.  6-10 yearsâ professional experience.  ",extensive statistics mathematics algorithms programming languages solve big data challenges fluent structured unstructured data management modern data transformation methodologies define create complex models pull valuable insights predictions innovation data effectively creatively tell stories create visualizations describe communicate data insights analytical problemsolving written verbal communication mastery english local language effectively communicate data insights negotiate options senior management levels mines data modern tools programming languages defines implements models uncover patterns predictions creating business value innovation manages relationships business partners evaluate foster data driven innovation domainspecific expertise crossorganization projectsinitiatives ties insights effective visualizations communicating business value innovation potential maintains proficiency within data science domain keeping technology trend shifts contributes industry data science domain initiatives leads project team data science professionals assuring insights communicated regularly effectively reviewing designs models accuracy data compliance collaborates communicates project team regarding project progress issue resolution communicates drives data insightsinnovation business represents data science team phases larger morecomplex development projects provides guidance training mentoring less experienced staff members scala python programming must databricks spark bachelors masters phd degree mathematics economics physics computer science professional,extensive statistics mathematics algorithms programming languages solve big data challenges fluent structured unstructured management modern transformation methodologies define create complex models pull valuable insights predictions innovation effectively creatively tell stories visualizations describe communicate analytical problemsolving written verbal communication mastery english local language negotiate options senior levels mines tools defines implements uncover patterns creating business value manages relationships partners evaluate foster driven domainspecific expertise crossorganization projectsinitiatives ties effective communicating potential maintains proficiency within science domain keeping technology trend shifts contributes industry initiatives leads project team professionals assuring communicated regularly reviewing designs accuracy compliance collaborates communicates regarding progress issue resolution drives insightsinnovation represents phases larger morecomplex development projects provides guidance training mentoring less experienced staff members scala python must databricks spark bachelors masters phd degree economics physics computer professional
320,"  Thrives in a fast-paced, startup environment, is adaptable and versatile 3+ years of python Experience in python data libraries  pandas, luigi, dask, etc  Solid understanding of data structures and algorithms Building distributed data systems    ",thrives fastpaced startup adaptable versatile python python data libraries pandas luigi dask solid understanding data structures algorithms building distributed data systems,thrives fastpaced startup adaptable versatile python data libraries pandas luigi dask solid understanding structures algorithms building distributed systems
321," Over five years of experience as a DBA Working knowledge of Oracle Autonomous Data Warehouse environments in Oracleâs OCI  Oracle Cloud Infrastructure  Experience in tuning and optimizing DB performance Knowledge of DB security best practices, and back and recovery processes Experience in developing data architectures Years of experience in ETL  Extract, Load, and Transform  from various data sources Knowledge of Big Data and Time-series databases Data Cleansing expertise Working knowledge of data analysis / data science   Over five years of experience as a DBA Working knowledge of Oracle Autonomous Data Warehouse environments in Oracleâs OCI  Oracle Cloud Infrastructure  Experience in tuning and optimizing DB performance Knowledge of DB security best practices, and back and recovery processes Experience in developing data architectures Years of experience in ETL  Extract, Load, and Transform  from various data sources Knowledge of Big Data and Time-series databases Data Cleansing expertise Working knowledge of data analysis / data science  ",five dba oracle autonomous data warehouse environments oracles oci oracle cloud infrastructure tuning optimizing db performance db security best practices back recovery processes developing data architectures etl extract load transform various data sources big data timeseries databases data cleansing expertise data analysis data science five dba oracle autonomous data warehouse environments oracles oci oracle cloud infrastructure tuning optimizing db performance db security best practices back recovery processes developing data architectures etl extract load transform various data sources big data timeseries databases data cleansing expertise data analysis data science,five dba oracle autonomous data warehouse environments oracles oci cloud infrastructure tuning optimizing db performance security best practices back recovery processes developing architectures etl extract load transform various sources big timeseries databases cleansing expertise analysis science
322,"3+ years working with SQL and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases.  Experience building and optimizing data pipelines, architectures and data sets.  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Strong analytic skills related to working with unstructured datasets.  Build processes supporting data transformation, data structures, metadata, dependency and workload management.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Working knowledge of message queuing and stream processing Strong project management and organizational skills.  Experience supporting and working with cross-functional teams in a dynamic environment.  We are looking for a candidate with 3+ years of experience in a Data Engineer or Database Developer role Experience with big data tools  Azure SQL Server Analysis Services, Azure Data Warehouse Experience with relational SQL and NoSQL databases Experience with SQL Server Enterprise and Microsoft Master Data Services would be a plus Experience with data pipeline and workflow management tools  Azure Service Bus, Azure Data Pipelines, Dell Boomi or similar technologies Experience with Azure cloud services Experience with object-oriented/object function scripting languages  C , C++, JavaScript, etc.   Create and maintain master data solutions for critical business data Create and maintain optimal data pipeline architecture Assemble large, complex data sets that meet functional / non-functional business requirements.  Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure or similar cloud âbig dataâ technologies.  Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.  Work with data and analytics experts to strive for greater functionality in our data systems.    ",sql relational databases query authoring sql well familiarity variety databases building optimizing data pipelines architectures data sets performing root cause analysis internal external data processes answer specific business questions identify opportunities improvement analytic unstructured datasets build processes supporting data transformation data structures metadata dependency workload management successful history manipulating processing extracting value disconnected datasets message queuing stream processing project management organizational supporting crossfunctional teams dynamic looking candidate data engineer database developer role big data tools azure sql server analysis services azure data warehouse relational sql nosql databases sql server enterprise microsoft master data services would plus data pipeline workflow management tools azure service bus azure data pipelines dell boomi similar technologies azure cloud services objectorientedobject function scripting languages c c javascript create maintain master data solutions critical business data create maintain optimal data pipeline architecture assemble complex data sets meet functional nonfunctional business identify design implement internal process improvements automating manual processes optimizing data delivery redesigning infrastructure greater scalability build infrastructure optimal extraction transformation loading data wide variety data sources sql azure similar cloud big data technologies stakeholders executive product data design teams assist datarelated technical issues support data infrastructure needs data analytics experts strive greater functionality data systems,sql relational databases query authoring well familiarity variety building optimizing data pipelines architectures sets performing root cause analysis internal external processes answer specific business questions identify opportunities improvement analytic unstructured datasets build supporting transformation structures metadata dependency workload management successful history manipulating processing extracting value disconnected message queuing stream project organizational crossfunctional teams dynamic looking candidate engineer database developer role big tools azure server services warehouse nosql enterprise microsoft master would plus pipeline workflow service bus dell boomi similar technologies cloud objectorientedobject function scripting languages c javascript create maintain solutions critical optimal architecture assemble complex meet functional nonfunctional design implement process improvements automating manual delivery redesigning infrastructure greater scalability extraction loading wide sources stakeholders executive product assist datarelated technical issues support needs analytics experts strive functionality systems
323,"Education - Sys/Div/Mkt/Local Manager - Bachelor's Degree and minimum of 3 years leadership experience OR minimum of 5 years leadership experience in the discipline OR Master's Degree and no experience.  Bachelor's Degree in Information Systems, Business, Engineering or closely related field Experience - 5+ years of experience in data engineering and/or devops 1 years of production experience with the SQL Server ecosystem 2 years of production experience with cloud or distributed platforms and architectures such as Azure, AWS, Spark, and/or Docker Skills - Python or Scala; advanced SQL Knowledge of and experience with DevOps concepts, tools, and architectures Comfort with Linux and Windows server environments, web APIs Experience with Airflow or similar orchestration tools.  SSIS experience helpful.    Design, build, test, and maintain data pipelines integrating data from source systems into a cohesive data warehouse and other repositories for reporting and analysis.  Expand and maintain source control, build scripts, and other elements supporting increasingly continuous deployment.  Institute a testing framework for data pipelines and schemas and write tests.  Coordinate the automated transfer of data between local and remote systems.  Work with data science and analytics team to deploy machine learning models.  Implement tools to monitor data quality, availability, and pipeline performance.  Work with IT to plan and execute further improvements to Supply Chain's data platform as needed, potentially including cloud  e. g, Azure or AWS , containerization  e. g. , Docker , or other  e. g. , Spark  components.    ",education sysdivmktlocal manager bachelors degree minimum leadership minimum leadership discipline masters degree bachelors degree information systems business engineering closely data engineering andor devops production sql server ecosystem production cloud distributed platforms architectures azure aws spark andor docker python scala advanced sql devops concepts tools architectures comfort linux windows server environments web apis airflow similar orchestration tools ssis helpful design build test maintain data pipelines integrating data source systems cohesive data warehouse repositories reporting analysis expand maintain source control build scripts elements supporting increasingly continuous deployment institute testing framework data pipelines schemas write tests coordinate automated transfer data local remote systems data science analytics team deploy machine models implement tools monitor data availability pipeline performance plan execute improvements supply chains data platform needed potentially cloud e g azure aws containerization e g docker e g spark components,education sysdivmktlocal manager bachelors degree minimum leadership discipline masters information systems business engineering closely data andor devops production sql server ecosystem cloud distributed platforms architectures azure aws spark docker python scala advanced concepts tools comfort linux windows environments web apis airflow similar orchestration ssis helpful design build test maintain pipelines integrating source cohesive warehouse repositories reporting analysis expand control scripts elements supporting increasingly continuous deployment institute testing framework schemas write tests coordinate automated transfer local remote science analytics team deploy machine models implement monitor availability pipeline performance plan execute improvements supply chains platform needed potentially e g containerization components
324," 3+ years previous consulting experience preferred 3+ years architecting and implementing Microsoft Azure, Amazon Web Services, and/or Google Cloud Platform infrastructure and topologies Experience implementing core infrastructure, networking, and cloud-based services for business teams or consumers Experience implementing Lambda architecture-based data designs Deep product knowledge and understanding of Azure, AWS, and/or GCP Experience configuring and tuning virtual private clouds Practical experience sizing hardware and storage needs Strong analytical problem solving ability Good written and verbal communication + presentation skills Self-starter with the ability to work independently or as part of a project team Capability to execute performance analysis, troubleshooting, and remediation Knowledge of High Availability and Disaster Recovery principles, patterns, and usage Understanding of the cloud ecosystem and leading emerging technologies/interdependencies   Work as part of a team to design and develop cloud data solutions Gather technical requirements, assess client capabilities, and analyze findings to provide appropriate cloud solution recommendations and adoption strategy Define Cloud Data strategies, including designing multi-phased implementation roadmaps Lead analysis, architecture, design, and development of data warehouse and business intelligence solutions Be versed in Amazon Web Services, Google Cloud Platform, and/or Microsoft Azure cloud solutions, architecture, related technologies, and their interdependencies Research, analyze, recommend, and select technical approaches for solving development and integration problems Learn and adopt new tools/techniques to increase performance, automation, and scalability Assist business development teams with pre-sales activities and RFPs Understand how to translate business goals and drivers into a technical solution Provide technical direction and oversight to cloud implementation teams  ",previous consulting architecting implementing microsoft azure amazon web services andor google cloud platform infrastructure topologies implementing core infrastructure networking cloudbased services business teams consumers implementing lambda architecturebased data designs deep product understanding azure aws andor gcp configuring tuning virtual private clouds practical sizing hardware storage needs analytical problem solving good written verbal communication presentation selfstarter independently part project team capability execute performance analysis troubleshooting remediation availability disaster recovery principles patterns usage understanding cloud ecosystem leading emerging technologiesinterdependencies part team design develop cloud data solutions gather technical assess client capabilities analyze findings appropriate cloud solution recommendations adoption strategy define cloud data strategies designing multiphased implementation roadmaps lead analysis architecture design development data warehouse business intelligence solutions versed amazon web services google cloud platform andor microsoft azure cloud solutions architecture technologies interdependencies research analyze recommend select technical approaches solving development integration problems learn adopt toolstechniques increase performance automation scalability assist business development teams presales activities rfps understand translate business goals drivers technical solution technical direction oversight cloud implementation teams,previous consulting architecting implementing microsoft azure amazon web services andor google cloud platform infrastructure topologies core networking cloudbased business teams consumers lambda architecturebased data designs deep product understanding aws gcp configuring tuning virtual private clouds practical sizing hardware storage needs analytical problem solving good written verbal communication presentation selfstarter independently part project team capability execute performance analysis troubleshooting remediation availability disaster recovery principles patterns usage ecosystem leading emerging technologiesinterdependencies design develop solutions gather technical assess client capabilities analyze findings appropriate solution recommendations adoption strategy define strategies designing multiphased implementation roadmaps lead architecture development warehouse intelligence versed technologies interdependencies research recommend select approaches integration problems learn adopt toolstechniques increase automation scalability assist presales activities rfps understand translate goals drivers direction oversight
325,"Bachelorâs in computer science or related field is required  masters preferred You have a minimum of 7 years of experience in the design, development, and deployment of large-scale, distributed, and cloud-deployed software services. Expected to be an expert in SQL and RDBMS.  Good at modeling data for relational, analytical and big data workloadsYou have a minimum of 4 years of experience in Big Data software development technologies  e. g. , Hadoop, Hive, Spark, Kafka  and exposure to resource/cluster management technologies. Minimum of 3 year of experience with AWS  e. g. , EC2, S3, EMR, SNS, SQS, Aurora, Redshift . Experience with various software technologies/solutions and understand where to use them. 2 years of experience with Data Virtualization like Denodo  Ability to quickly identify an opportunity and recommend possible technical solutions. Utilize multiple development languages/tools such as Python, SPARK, Hive to build prototypes and evaluate results for effectiveness and feasibility. Operationalize data ingestion and data-analytic tools for enterprise use. Utilize tools available to you across AWS ServicesDevelop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, NIFI, Python, HBase and Hadoop. Custom Data pipeline development  Cloud and locally hosted Work heavily within AWS and Hadoop ecosystemsProvide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers. Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes. Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc  ",bachelors computer science masters minimum design development deployment largescale distributed clouddeployed software services expected expert sql rdbms good modeling data relational analytical big data workloadsyou minimum big data software development technologies e g hadoop hive spark kafka exposure resourcecluster management technologies minimum year aws e g ec emr sns sqs aurora redshift various software technologiessolutions understand use data virtualization like denodo quickly identify opportunity recommend possible technical solutions utilize multiple development languagestools python spark hive build prototypes evaluate results effectiveness feasibility operationalize data ingestion dataanalytic tools enterprise use utilize tools available across aws servicesdevelop realtime data ingestion streamanalytic solutions leveraging technologies kafka apache spark nifi python hbase hadoop custom data pipeline development cloud locally hosted heavily within aws hadoop ecosystemsprovide support deployed data applications analytical models trusted advisor data scientists data consumers identifying data problems guiding issue resolution partner data engineers source data providers subject matter expertise analysis preparation specifications plans development data processes proper data governance policies followed implementing validating data lineage checks classification,bachelors computer science masters minimum design development deployment largescale distributed clouddeployed software services expected expert sql rdbms good modeling data relational analytical big workloadsyou technologies e g hadoop hive spark kafka exposure resourcecluster management year aws ec emr sns sqs aurora redshift various technologiessolutions understand use virtualization like denodo quickly identify opportunity recommend possible technical solutions utilize multiple languagestools python build prototypes evaluate results effectiveness feasibility operationalize ingestion dataanalytic tools enterprise available across servicesdevelop realtime streamanalytic leveraging apache nifi hbase custom pipeline cloud locally hosted heavily within ecosystemsprovide support deployed applications models trusted advisor scientists consumers identifying problems guiding issue resolution partner engineers source providers subject matter expertise analysis preparation specifications plans processes proper governance policies followed implementing validating lineage checks classification
326,"  Experience working in a Microsoft SQL Server environment, especially with SSIS, stored procedures and query development.  Knowledge of data warehousing best practices, concepts and processes.  Strong analytical and problem-solving skills, with demonstrated change management experience.  Demonstrated ability to set and meet project timelines and deliverables.  Effective interpersonal and communications skills with the ability to interact with various levels of personnel.  Must be flexible, organized, self-directed, able to prioritize multiple tasks, and able to manage a full workload.  Experienced in Microsoft Office and Microsoft Operating Systems.  Must be able to work in CCMCN's main office and travel to all required meetings Fluency in written and spoken English.    ",microsoft sql server especially ssis stored procedures query development data warehousing best practices concepts processes analytical problemsolving demonstrated change management demonstrated set meet project timelines deliverables effective interpersonal communications interact various levels personnel must flexible organized selfdirected able prioritize multiple tasks able manage full workload experienced microsoft office microsoft operating systems must able ccmcns main office travel meetings fluency written spoken english,microsoft sql server especially ssis stored procedures query development data warehousing best practices concepts processes analytical problemsolving demonstrated change management set meet project timelines deliverables effective interpersonal communications interact various levels personnel must flexible organized selfdirected able prioritize multiple tasks manage full workload experienced office operating systems ccmcns main travel meetings fluency written spoken english
327," Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns.  Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem Youâre proficient with Linux operations and development, including basic commands and shell scripting You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices Must be fluent in Python, R, and Java Must have excellent experience command of SQL Must have good experience and knowledge with Data Modeling concepts.  You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills    ",must solid understanding data engineering integration warehousing concepts patterns must design build maintain batch streaming data solutions scale onpremises cloud environments specifically hadoop ecosystem youre proficient linux operations development basic commands shell scripting demonstrate devops methodologies continuous integrationcontinuous delivery practices must fluent python r java must command sql must good data modeling concepts passion data science machine desire develop analysis modeling,must solid understanding data engineering integration warehousing concepts patterns design build maintain batch streaming solutions scale onpremises cloud environments specifically hadoop ecosystem youre proficient linux operations development basic commands shell scripting demonstrate devops methodologies continuous integrationcontinuous delivery practices fluent python r java command sql good modeling passion science machine desire develop analysis
328,"  Linux command line experience.  Python experience.  Working knowledge and experience with AWS, S3, RDS/Postgres.  Experience with Php / Laravel framework.  Experience with JIRA.  Experience working in an Agile/SCRUM environment.     Experience with Relational Database Experience with query tools like SQL Experience using a report designer such as Power BI Ability to work in a team environment.  Strong communication skills. ",linux command line python aws rdspostgres php laravel framework jira agilescrum relational database query tools like sql report designer power bi team communication,linux command line python aws rdspostgres php laravel framework jira agilescrum relational database query tools like sql report designer power bi team communication
329,"Seeking of 3-5+ years of experience with data analysis, data warehousing, and delivering reporting Fluent in at least SQL syntax  Read functions  Experience with Tableau, Power BI, or Looker Proficiency with MS Excel Experience with statistical models and methodologies Experience with eCommerce analysis  CLV, AOV, Retention Rate, etc  a plus Experience with Python, Javascript, and R are a plus Experience with Financial Management, Supply Chain, Warehouse Management, and Manufacturing Experience working with an eCommerce platform Bachelorâs Degree in Information Technology or Mathematics or Statistics, or related field preferred Excellent planning and project management skills Self-starter with a high level of initiative and strong sense of ownership Strong communication and interpersonal skills Experience working in the CPG industry preferred Experience with both B2C and B2B business models preferred     ",seeking data analysis data warehousing delivering reporting fluent least sql syntax read functions tableau power bi looker proficiency ms excel statistical models methodologies ecommerce analysis clv aov retention rate plus python javascript r plus financial management supply chain warehouse management manufacturing ecommerce platform bachelors degree information technology mathematics statistics planning project management selfstarter level initiative sense ownership communication interpersonal cpg industry bc bb business models,seeking data analysis warehousing delivering reporting fluent least sql syntax read functions tableau power bi looker proficiency ms excel statistical models methodologies ecommerce clv aov retention rate plus python javascript r financial management supply chain warehouse manufacturing platform bachelors degree information technology mathematics statistics planning project selfstarter level initiative sense ownership communication interpersonal cpg industry bc bb business
330," Clearance  Active TS/SCI with current Poly Education  Bachelorâs degree or equivalent  Master's preferred  Experience  A minimum of five years of related work experience  Minimum 5 years experience with Linux/Unix environments Strong software scripting skills in Python and other scripting languages  Bash, Perl, etc.   Strong understanding of RDBMS databases  PostgreSQL, Oracle, MySql, etc.   Prior experience working with the Git version-control system Strong systems engineering background Must be certified to meet DoD 8570 level IAT-II qualifications.  Security+ certification or CISSP preferred.    ",clearance active tssci current poly education bachelors degree masters minimum five minimum linuxunix environments software scripting python scripting languages bash perl understanding rdbms databases postgresql oracle mysql prior git versioncontrol systems engineering background must certified meet dod level iatii qualifications security certification cissp,clearance active tssci current poly education bachelors degree masters minimum five linuxunix environments software scripting python languages bash perl understanding rdbms databases postgresql oracle mysql prior git versioncontrol systems engineering background must certified meet dod level iatii qualifications security certification cissp
331,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform.  Multi-cloud experience a plus.    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",minimum previous consulting client service delivery google gcp devops gcp platform multicloud plus proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,minimum previous consulting client service delivery google gcp devops platform multicloud plus proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
332,"Minimum of a Bachelorâs degree in Computer Science, MIS or related degree and five  5  years of relevant development or engineering experience or combination of education, training and experience. Expert/Advanced level experience with ETL Tools, ODI preferably. Expert Level experience with Oracle as a Database Platform. Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases. Experience or understanding of programming languages like Python, Java, R etc. Experience or understanding of Cloud Data Platforms a plus. Strong understanding of Data Warehousing concepts. Financial Services Industry knowledge is a plus. May occasionally work a non-standard shift including nights and/or weekends and/or have on-call responsibilities.   Builds scalable and reliable Data Integration solutions which are flexible, scalable and elastic. Develops low latency Data Integration solutions to provision data near real time for multiple consumers. Collaborates with Data Engineers, Data Architects and Service developers to build optimal and efficient ETL and Database code. Produces dynamic, data driven solutions to support the strategic business goals. Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute to meet business objectives. Help develop an enterprise scale Data WarehouseDesign and develop new systems and tools to enable stakeholders to consume and understand data fasterSupports ETL processing. Provides on-call support of Data Integration processes on a rotating basis and other on-call as required. Produces dynamic, data driven solutions to support the strategic business goals. Performs other duties and responsibilities as assigned.   ",minimum bachelors degree computer science mis degree five relevant development engineering combination education training expertadvanced level etl tools odi preferably expert level oracle database platform deep sql tuning tuning etl solutions physical optimization databases understanding programming languages like python java r understanding cloud data platforms plus understanding data warehousing concepts financial services industry plus may occasionally nonstandard shift nights andor weekends andor oncall responsibilities builds scalable reliable data integration solutions flexible scalable elastic develops low latency data integration solutions provision data near real time multiple consumers collaborates data engineers data architects service developers build optimal efficient etl database code produces dynamic data driven solutions support strategic business goals focus designing building launching efficient reliable data infrastructure scale compute meet business objectives help develop enterprise scale data warehousedesign develop systems tools enable stakeholders consume understand data fastersupports etl processing provides oncall support data integration processes rotating basis oncall produces dynamic data driven solutions support strategic business goals performs duties responsibilities assigned,minimum bachelors degree computer science mis five relevant development engineering combination education training expertadvanced level etl tools odi preferably expert oracle database platform deep sql tuning solutions physical optimization databases understanding programming languages like python java r cloud data platforms plus warehousing concepts financial services industry may occasionally nonstandard shift nights andor weekends oncall responsibilities builds scalable reliable integration flexible elastic develops low latency provision near real time multiple consumers collaborates engineers architects service developers build optimal efficient code produces dynamic driven support strategic business goals focus designing building launching infrastructure scale compute meet objectives help develop enterprise warehousedesign systems enable stakeholders consume understand fastersupports processing provides processes rotating basis performs duties assigned
333," Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns.  Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem Youâre proficient with Linux operations and development, including basic commands and shell scripting You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices Must be fluent in Python, R, and Java Must have excellent experience command of SQL Must have good experience and knowledge with Data Modeling concepts.  You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills    ",must solid understanding data engineering integration warehousing concepts patterns must design build maintain batch streaming data solutions scale onpremises cloud environments specifically hadoop ecosystem youre proficient linux operations development basic commands shell scripting demonstrate devops methodologies continuous integrationcontinuous delivery practices must fluent python r java must command sql must good data modeling concepts passion data science machine desire develop analysis modeling,must solid understanding data engineering integration warehousing concepts patterns design build maintain batch streaming solutions scale onpremises cloud environments specifically hadoop ecosystem youre proficient linux operations development basic commands shell scripting demonstrate devops methodologies continuous integrationcontinuous delivery practices fluent python r java command sql good modeling passion science machine desire develop analysis
334,"     Must have a Data warehouse/Big Data background 3+ years experience with Hadoop 3+ years experience with Java 3+ years SQL experience 2+ years experience in AWS services  EMR, Glue, S3, Lambda, ect.   The ability to build CI/CD pipelines with large data sets and cloud technologies ",must data warehousebig data background hadoop java sql aws services emr glue lambda ect build cicd pipelines data sets cloud technologies,must data warehousebig background hadoop java sql aws services emr glue lambda ect build cicd pipelines sets cloud technologies
335," Bachelorâs degree in a technical field  e. g.  Comp Science, Math, Engineering  or related experience 2+ years of collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization 2+ years of experience architecting, building and administering big data and real-time streaming analytics architectures in both on premises and cloud environments  AWS, Azure, Google  leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB 1+ years of experience architecting, building and administering large-scale distributed applications 1+ years of experience with Linux operations and development, including basic commands and shell scripting 2+ years of experience with execution of DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environment Software development experience in least two or more of following languages  Java, Python, Scala, Node. js Expertise in usage of SQL for data profiling, analysis and extraction   Working collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment  Architect, build and support the operation of our Cloud and On-Premises enterprise data infrastructure and tools Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications Assist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage  ",bachelors degree technical e g comp science math engineering collective data engineering data analysis data warehousing data integration business intelligence similarly sized organization architecting building administering big data realtime streaming analytics architectures premises cloud environments aws azure google leveraging technologies hadoop spark emr aurora dynamodb redshift neptune cosmos db architecting building administering largescale distributed applications linux operations development basic commands shell scripting execution devops methodologies continuous integrationcontinuous delivery within scale data delivery software development least two following languages java python scala node js expertise usage sql data profiling analysis extraction collaboratively engineers data scientists analytics teams business product owners agile architect build support operation cloud onpremises enterprise data infrastructure tools design robust reusable scalable data driven solutions data pipeline frameworks automate ingestion processing delivery structured unstructured batch realtime streaming data build data apis data delivery services support critical operational processes analytical models machine applications assist selection integration data tools frameworks applications expand platform capabilities understand implement best practices management enterprise data master data reference data metadata data lineage,bachelors degree technical e g comp science math engineering collective data analysis warehousing integration business intelligence similarly sized organization architecting building administering big realtime streaming analytics architectures premises cloud environments aws azure google leveraging technologies hadoop spark emr aurora dynamodb redshift neptune cosmos db largescale distributed applications linux operations development basic commands shell scripting execution devops methodologies continuous integrationcontinuous delivery within scale software least two following languages java python scala node js expertise usage sql profiling extraction collaboratively engineers scientists teams product owners agile architect build support operation onpremises enterprise infrastructure tools design robust reusable scalable driven solutions pipeline frameworks automate ingestion processing structured unstructured batch apis services critical operational processes analytical models machine assist selection expand platform capabilities understand implement best practices management master reference metadata lineage
336,"   You will be responsible for architecting, building and maintaining data pipelines, storage systems, analysis flows, and procedures to support IAâs product objectives and business goals.  You should be comfortable working in a fast-paced environment with quick changes and a high degree of uncertainty.  A high level of proficiency in tools and architectures for building modern data infrastructure is a must, as is a desire to learn quickly to adapt to changing technologies and business challenges.  As a member of the engineering team, you will help guide both the technical and usability aspects of the software we build and will be involved all stages of the cycle of iterative product development.  Youâll work with partners and stakeholders to analyze data sources and gather requirements, with designers to define user flows and look and feel, with other engineers to implement functionality, and with customers to provide support and gather feedback.  You will have a high degree of freedom to pick the tools and frameworks that you feel best address the problems at hand and will be responsible for managing and maintaining those technology investments over their entire lifetime.   ",responsible architecting building maintaining data pipelines storage systems analysis flows procedures support ias product objectives business goals comfortable fastpaced quick changes degree uncertainty level proficiency tools architectures building modern data infrastructure must desire learn quickly adapt changing technologies business challenges member engineering team help guide technical usability aspects software build involved stages cycle iterative product development youll partners stakeholders analyze data sources gather designers define user flows look feel engineers implement functionality customers support gather feedback degree freedom pick tools frameworks feel best address problems hand responsible managing maintaining technology investments entire lifetime,responsible architecting building maintaining data pipelines storage systems analysis flows procedures support ias product objectives business goals comfortable fastpaced quick changes degree uncertainty level proficiency tools architectures modern infrastructure must desire learn quickly adapt changing technologies challenges member engineering team help guide technical usability aspects software build involved stages cycle iterative development youll partners stakeholders analyze sources gather designers define user look feel engineers implement functionality customers feedback freedom pick frameworks best address problems hand managing technology investments entire lifetime
337," Bachelorâs degree in Computer Science, Computer Engineering or Information Technology 8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills 5 years of experience of ETL development in a big data environment 5 years working in an agile development environment.  Experience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS   Bachelorâs degree in Computer Science, Computer Engineering or Information Technology 8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills 5 years of experience of ETL development in a big data environment 5 years working in an agile development environment.  Experience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS    ",bachelors degree computer science computer engineering information technology data engineering building business intelligence applications exceptional sql plsql andor python etl development big data agile development developing aws ec redshift glue athena rds bachelors degree computer science computer engineering information technology data engineering building business intelligence applications exceptional sql plsql andor python etl development big data agile development developing aws ec redshift glue athena rds,bachelors degree computer science engineering information technology data building business intelligence applications exceptional sql plsql andor python etl development big agile developing aws ec redshift glue athena rds
338,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
339,"  Development of jobs & pipelines from multiple production data sources into Data Lake environments Engineers production ready solutions, inclusive of alerting and error handling Works with Cloud based tools  Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc.   to deliver best-in-class cloud based data solutions Works collaboratively with DBA team for operational execution and reliability of data solutions, both in Oracle and BigQuery Assists in maintaining data governance through documentation of data solutions, through ERDs, Confluence documentation, or external tools Engineer & model curated and keyed Data Warehouse solutions that meet business objectives that perform efficiently and effectively Works in Agile product management method, managing tasks & objectives  user stories  through JIRA and providing updates to SCRUM master Partners with Product Manager  PO  to understand business requirements across multiple functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer & Loyalty, Legal, & Data Science Support current Data Warehouse ETL jobs, respond to tickets and inquiries from business partners when data quality issues occur Other projects and duties as assigned   ",development jobs pipelines multiple production data sources data lake environments engineers production ready solutions inclusive alerting error handling works cloud based tools google gcp big query dataproc composer steamsets looker deliver bestinclass cloud based data solutions works collaboratively dba team operational execution reliability data solutions oracle bigquery assists maintaining data governance documentation data solutions erds confluence documentation external tools engineer model curated keyed data warehouse solutions meet business objectives perform efficiently effectively works agile product management method managing tasks objectives user stories jira providing updates scrum master partners product manager po understand business across multiple functional areas store operations merchandising supply chain finance digital customer loyalty legal data science support current data warehouse etl jobs respond tickets inquiries business partners data issues occur projects duties assigned,development jobs pipelines multiple production data sources lake environments engineers ready solutions inclusive alerting error handling works cloud based tools google gcp big query dataproc composer steamsets looker deliver bestinclass collaboratively dba team operational execution reliability oracle bigquery assists maintaining governance documentation erds confluence external engineer model curated keyed warehouse meet business objectives perform efficiently effectively agile product management method managing tasks user stories jira providing updates scrum master partners manager po understand across functional areas store operations merchandising supply chain finance digital customer loyalty legal science support current etl respond tickets inquiries issues occur projects duties assigned
340,"   Microsoft SQL Server PostgreSQL MongoDB Microsoft SSIS BIRST BI Valen InsureRight platform  2+ years Data Engineering experience with TSQL, python, map reduce or functional programming Bachelorâs degree in programming or related technical areas Developing and supporting an end user production system Reporting/data warehousing experience Insurance industry knowledge or experience with insurance data a plus ",microsoft sql server postgresql mongodb microsoft ssis birst bi valen insureright platform data engineering tsql python map reduce functional programming bachelors degree programming technical areas developing supporting end user production reportingdata warehousing insurance industry insurance data plus,microsoft sql server postgresql mongodb ssis birst bi valen insureright platform data engineering tsql python map reduce functional programming bachelors degree technical areas developing supporting end user production reportingdata warehousing insurance industry plus
341," Data modeling and corporate-level data management experience Previously worked with Windows Workflow Foundation, Windows Workflow Designer and Windows Presentation Foundation technologies Experience with Elasticsearch, Aurora, MySQL, Spark, Streamsets, Kafka, Python, and Scala is a plus    ",data modeling corporatelevel data management previously worked windows workflow foundation windows workflow designer windows presentation foundation technologies elasticsearch aurora mysql spark streamsets kafka python scala plus,data modeling corporatelevel management previously worked windows workflow foundation designer presentation technologies elasticsearch aurora mysql spark streamsets kafka python scala plus
342," Experience with databases SQL and NoSQL Deep knowledge of Node, JavaScript, Experience with scaling backend of large-scale applications Algorithms Positive attitude Curious, eager to learn, and colaborate    Required  Bachelorâs degree in Computer Science or equivalent work experience Preferred  Masterâs degree ",databases sql nosql deep node javascript scaling backend largescale applications algorithms positive attitude curious eager learn colaborate bachelors degree computer science masters degree,databases sql nosql deep node javascript scaling backend largescale applications algorithms positive attitude curious eager learn colaborate bachelors degree computer science masters
343," 4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.  2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.  1+ years of experience on distributed, high throughput and low latency architecture.  1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.  A successful track-record of manipulating, processing and extracting value from large disconnected datasets.     ",software development substantial part gained highthroughput decisionautomation big data technologies like spark kafka flink hadoop nosql datastores distributed throughput low latency architecture deploying managing data pipelines supporting datasciencedriven decisioning scale successful trackrecord manipulating processing extracting value disconnected datasets,software development substantial part gained highthroughput decisionautomation big data technologies like spark kafka flink hadoop nosql datastores distributed throughput low latency architecture deploying managing pipelines supporting datasciencedriven decisioning scale successful trackrecord manipulating processing extracting value disconnected datasets
344,"2+ years designing and developing data analytics solutions 2+ years with RDBMS such as SQL Server, Oracle, MySQL 2+ years data warehouse, dimensional modeling design and architecture A passion to learn and improve your skills to deliver the best possible solutions to customers Experience with cloud based data services offered by Azure, AWS and Google Experience with data visualization tools such as Power BI and Tableau Previous consulting experience preferred Degree in computer science, information technology, engineering or business Must be authorized to work in the US.  We are unable to sponsor H-1B visas at this time.    Hands-on development and serve as technical expert on projects Develop data solutions leveraging traditional and cloud product offerings from leading vendors Develop data models to meet client needs Develop data models to meet client needs, including transactional, third-normal form, dimensional, columnar, distributed and NoSQL Develop ETL/ELT processes and patterns to efficiently move data Create data visualizations, dashboards and reports as needed Develop and scope requirements Travel as needed  currently less than 5%  Maintain effective communication with team and customers  ",designing developing data analytics solutions rdbms sql server oracle mysql data warehouse dimensional modeling design architecture passion learn improve deliver best possible solutions customers cloud based data services offered azure aws google data visualization tools power bi tableau previous consulting degree computer science information technology engineering business must authorized us unable sponsor hb visas time handson development serve technical expert projects develop data solutions leveraging traditional cloud product offerings leading vendors develop data models meet client needs develop data models meet client needs transactional thirdnormal form dimensional columnar distributed nosql develop etlelt processes patterns efficiently move data create data visualizations dashboards reports needed develop scope travel needed currently less maintain effective communication team customers,designing developing data analytics solutions rdbms sql server oracle mysql warehouse dimensional modeling design architecture passion learn improve deliver best possible customers cloud based services offered azure aws google visualization tools power bi tableau previous consulting degree computer science information technology engineering business must authorized us unable sponsor hb visas time handson development serve technical expert projects develop leveraging traditional product offerings leading vendors models meet client needs transactional thirdnormal form columnar distributed nosql etlelt processes patterns efficiently move create visualizations dashboards reports needed scope travel currently less maintain effective communication team
345," Bachelorâs degree in computer science, math, engineering, or relevant technical field 4+ years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration, and data integration concepts and methodologies 3+ years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments 3+ years of experience with execution of DevOps methodologies and continuous integration/continuous delivery Object-oriented/object function scripting languages  Python, R, C/C++, Java, Scala, etc.  SQL, relational databases and NoSQL databases Data integration tools  e. g.  Talend, SnapLogic, Informatica  and data warehousing / data lake tools API based data acquisition and management MSSQL, PostgreSQL, MySQL, etc.  - MemSQL, CrateDB, etc.  Business intelligence tools such as Tableau, PowerBI, Zoomdata, Pentaho, etc.  Data modeling tools such as ERWin, Enterprise Architect, Visio, etc.  Data integration tools such as Boomi, Pentaho, Talend, Informatica, SnapLogic, etc.  Familiarity with cloud-based data engineering  AWS, GCP, or Azure  Familiarity with data science techniques and frameworks Creative thinker with strong analytical skills Ability to work in a team environment Strong technical communication skills Ability to prioritize work to meet tight deadlines Ability to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables    Architect, build, and support the operation of enterprise data and analytical infrastructure and tools Design robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications Assist in the selection and integration of data related tools, frameworks, and applications required to expand platform capabilities Understand and implement best practices in the management of enterprise data, including master data, reference data, metadata, data quality and lineage Develop and prepare strategies for Business Intelligence processes for the organization Manage and customize all ETL processes as per customer requirement and analyze all processes for same Perform assessment on all reporting requirements and contribute to the development of a long-term strategy for various reporting solutions Coordinate with data generator and ensure compliance to all enterprise data model according to data standards   ",bachelors degree computer science math engineering relevant technical collective application data engineering data analytics data warehousing business intelligence database administration data integration concepts methodologies architecting building administering big data realtime streaming analytics architectures onpremises cloud environments execution devops methodologies continuous integrationcontinuous delivery objectorientedobject function scripting languages python r cc java scala sql relational databases nosql databases data integration tools e g talend snaplogic informatica data warehousing data lake tools api based data acquisition management mssql postgresql mysql memsql cratedb business intelligence tools tableau powerbi zoomdata pentaho data modeling tools erwin enterprise architect visio data integration tools boomi pentaho talend informatica snaplogic familiarity cloudbased data engineering aws gcp azure familiarity data science techniques frameworks creative thinker analytical team technical communication prioritize meet tight deadlines learn keep pace latest technology advances quickly grasp technologies support contribute project deliverables architect build support operation enterprise data analytical infrastructure tools design robust reusable scalable data driven solutions data pipeline frameworks automate ingestion processing delivery structured unstructured batch realtime streaming data build data apis data delivery services support critical operational processes analytical models machine applications assist selection integration data tools frameworks applications expand platform capabilities understand implement best practices management enterprise data master data reference data metadata data lineage develop prepare strategies business intelligence processes organization manage customize etl processes per customer requirement analyze processes perform assessment reporting contribute development longterm strategy various reporting solutions coordinate data generator compliance enterprise data model according data standards,bachelors degree computer science math engineering relevant technical collective application data analytics warehousing business intelligence database administration integration concepts methodologies architecting building administering big realtime streaming architectures onpremises cloud environments execution devops continuous integrationcontinuous delivery objectorientedobject function scripting languages python r cc java scala sql relational databases nosql tools e g talend snaplogic informatica lake api based acquisition management mssql postgresql mysql memsql cratedb tableau powerbi zoomdata pentaho modeling erwin enterprise architect visio boomi familiarity cloudbased aws gcp azure techniques frameworks creative thinker analytical team communication prioritize meet tight deadlines learn keep pace latest technology advances quickly grasp technologies support contribute project deliverables build operation infrastructure design robust reusable scalable driven solutions pipeline automate ingestion processing structured unstructured batch apis services critical operational processes models machine applications assist selection expand platform capabilities understand implement best practices master reference metadata lineage develop prepare strategies organization manage customize etl per customer requirement analyze perform assessment reporting development longterm strategy various coordinate generator compliance model according standards
346,"United Statesâ citizen with current TS/SCI, SSBI and polyBachelorâs Degree with at least 4-8 years of applicable experience or Masters degree with 2-6 years of experience or 4 additional years in lieu of degree  Support of production data processing and data distribution systemsWork with data providers and customers to ensure data quality and availabilityGather requirements and work with data providers to enable distribution of new data sourcesSupport software deployments and integration of geographically diverse computing systemsProvide direct support to end usersConfigure and maintain data ingest workflows  ETL  across several production systemsInstall, configure, and update a wide array of COTS/GOTS and homegrown software applicationsSupport and troubleshoot diverse IT infrastructure hardware platforms and protocolsWork with software development and systems administration staff to monitor and troubleshoot production systemsGenerate and maintain systems documentation and diagramsTroubleshoot network issues and establish new connectivityMonitor and maintain a variety of databases  ",united states citizen current tssci ssbi polybachelors degree least applicable masters degree additional lieu degree support production data processing data distribution systemswork data providers customers data availabilitygather data providers enable distribution data sourcessupport software deployments integration geographically diverse computing systemsprovide direct support end usersconfigure maintain data ingest workflows etl across several production systemsinstall configure update wide array cotsgots homegrown software applicationssupport troubleshoot diverse infrastructure hardware platforms protocolswork software development systems administration staff monitor troubleshoot production systemsgenerate maintain systems documentation diagramstroubleshoot network issues establish connectivitymonitor maintain variety databases,united states citizen current tssci ssbi polybachelors degree least applicable masters additional lieu support production data processing distribution systemswork providers customers availabilitygather enable sourcessupport software deployments integration geographically diverse computing systemsprovide direct end usersconfigure maintain ingest workflows etl across several systemsinstall configure update wide array cotsgots homegrown applicationssupport troubleshoot infrastructure hardware platforms protocolswork development systems administration staff monitor systemsgenerate documentation diagramstroubleshoot network issues establish connectivitymonitor variety databases
347,"  Design, Develop and Maintain Data Storage and Data Analytics Products, Systems and Solutions for clients in the Media Industry Develop, Enhance & Configure data transformation and storage solutions in support of our product offerings Work closely with other internal engineering teams focused on front-end and back-end APIs development and configuration in direct support of various products Work closely with internal product and project management teams to design and prioritize features and their associated delivery timelines Diagnose and correct system and product faults, designing & implementing solutions to correct   ",design develop maintain data storage data analytics products systems solutions clients media industry develop enhance configure data transformation storage solutions support product offerings closely internal engineering teams focused frontend backend apis development configuration direct support various products closely internal product project management teams design prioritize features associated delivery timelines diagnose correct product faults designing implementing solutions correct,design develop maintain data storage analytics products systems solutions clients media industry enhance configure transformation support product offerings closely internal engineering teams focused frontend backend apis development configuration direct various project management prioritize features associated delivery timelines diagnose correct faults designing implementing
348," Degree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development Proficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc.  Experience with developing on an MPP database Redshift/Teradata/Snowflake Proficient handling large data sets using SQL and databases in a business and engineering environment Experience with operations in a Public Cloud Environment  AWS/Azure/GCP  Experience with ETL and Data Warehouse/Lake processes Excellent verbal and written communication skills Strong troubleshooting and problem-solving skills Thrive in a fast-paced, innovative environment     ",degree computer engineeringscience professional databasedata lake development proficient processing data relational databases like oraclesql servermysqletc developing mpp database redshiftteradatasnowflake proficient handling data sets sql databases business engineering operations public cloud awsazuregcp etl data warehouselake processes verbal written communication troubleshooting problemsolving thrive fastpaced innovative,degree computer engineeringscience professional databasedata lake development proficient processing data relational databases like oraclesql servermysqletc developing mpp database redshiftteradatasnowflake handling sets sql business engineering operations public cloud awsazuregcp etl warehouselake processes verbal written communication troubleshooting problemsolving thrive fastpaced innovative
349,"  Collect, store, and aggregate data to support the creation of great data products for the business and our customers Be creative and cooperative in designing and building data pipelines Use Cloud-based infrastructure and applications  we use AWS and maintain our own Kubernetes clusters  Continually learn and seek ways to improve our data flows Collaborate well in a team environment  we use agile  Validate data and maintain healthy infrastructure and data flows   ",collect store aggregate data support creation great data products business customers creative cooperative designing building data pipelines use cloudbased infrastructure applications use aws maintain kubernetes clusters continually learn seek ways improve data flows collaborate well team use agile validate data maintain healthy infrastructure data flows,collect store aggregate data support creation great products business customers creative cooperative designing building pipelines use cloudbased infrastructure applications aws maintain kubernetes clusters continually learn seek ways improve flows collaborate well team agile validate healthy
350,"     3-5 Years BI Experience developing BI dashboards, reporting, visualizations and data models.  Proven BI dashboard, report, and visualization design and delivery.  1-2 years Tableau Server experience.  Proven Tableau performance tuning reporting and dashboard solutions.  Tableau Server Experience  Configuration, Administration, Tuning & Performance, Data Connections, APIs Tableau Desktop & Tableau Prep Experience  Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience â including data extraction, transformation and load  ETL  from many sources; OLAP, OLTP, Datawarehouseâs, Hadoop/Cloudera, and Cloud platforms.  Experience with Tableau and Cognos Development.  Experience in data modeling and data prep  discovery, structuring, cleaning, enriching, validation, publishing .  Tableau Prep preferred.  Source to Target mapping.  Tableau Prep experience a major plus.  SQL Experience  joins, queries, select statements  â Oracle, SQL Server database back ends.  BI performance tuning across front end, ETL, Data back end optimization.  Solid Agile experience on BI projects â Gathering requirements, translating to use cases, BI stories, design/delivery of BI use cases â using SAFe, SCRUM, Agile, Kanban or similar method.  Exposure Python. ",bi developing bi dashboards reporting visualizations data models proven bi dashboard report visualization design delivery tableau server proven tableau performance tuning reporting dashboard solutions tableau server configuration administration tuning performance data connections apis tableau desktop tableau prep advanced data visualizations custom visualizations advanced data modeling data extraction transformation load etl many sources olap oltp datawarehouses hadoopcloudera cloud platforms tableau cognos development data modeling data prep discovery structuring cleaning enriching validation publishing tableau prep source target mapping tableau prep major plus sql joins queries select statements oracle sql server database back ends bi performance tuning across front end etl data back end optimization solid agile bi projects gathering translating use cases bi stories designdelivery bi use cases safe scrum agile kanban similar method exposure python,bi developing dashboards reporting visualizations data models proven dashboard report visualization design delivery tableau server performance tuning solutions configuration administration connections apis desktop prep advanced custom modeling extraction transformation load etl many sources olap oltp datawarehouses hadoopcloudera cloud platforms cognos development discovery structuring cleaning enriching validation publishing source target mapping major plus sql joins queries select statements oracle database back ends across front end optimization solid agile projects gathering translating use cases stories designdelivery safe scrum kanban similar method exposure python
351,"  SQL Python Cloud Data Warehouse Systems ETL Workflow Tools Batch Processing Spark CD/CI tools  3+ years experience with Data Warehouse Systems and working on an ETL system, either a commercial one like Matillion or Fivetran, an open-source one like Airflow, or a custom one you or your company built Experience with one major cloud analytics database  Snowflake, Redshift, Google Big Query , Snowflake preferred Strong familiarity with SQL Python development experience in production Familiarity with Spark A strong desire to show ownership of problems you identify and proven ability to empower others to get more done Familiarity with modern BI and exploration tools, Looker is preferred.  Familiarity with GitHub or other CD/CI tools Basic AWS experience  S3, EC2   1-2 years  Some familiarity with streaming approaches preferred CS Degree preferred Some experience preferred with Jenkins, Docker, Kubernetes  ",sql python cloud data warehouse systems etl workflow tools batch processing spark cdci tools data warehouse systems etl either commercial one like matillion fivetran opensource one like airflow custom one company built one major cloud analytics database snowflake redshift google big query snowflake familiarity sql python development production familiarity spark desire show ownership problems identify proven empower others get done familiarity modern bi exploration tools looker familiarity github cdci tools basic aws ec familiarity streaming approaches cs degree jenkins docker kubernetes,sql python cloud data warehouse systems etl workflow tools batch processing spark cdci either commercial one like matillion fivetran opensource airflow custom company built major analytics database snowflake redshift google big query familiarity development production desire show ownership problems identify proven empower others get done modern bi exploration looker github basic aws ec streaming approaches cs degree jenkins docker kubernetes
352," Deep experience with RDMS databases  SQL Server  and Data Warehouse  OLAP, Redshift  managing connection-pools, performance tuning and optimizations.    Deep experience with RDMS databases  SQL Server  and Data Warehouse  OLAP, Redshift  managing connection-pools, performance tuning and optimizations.    Deliver on large and complex solution data needs by leading requirements and technical specifications that drive resulting data designs.  Participate in strategic and innovative data design, requirements, walkthroughs, and reviews.  Design and document system integration/configuration as required.  Implement and advance data models and configurations in support of integrating data from source systems and environments to promote Continuous Integration/Continuous Deployment  CI/CD  and CQRS pattern caching.  Research, design, and implement next generation analytics and machine learning platforms.  Build tools, frameworks, APIs, and dashboards to support telemetry and advanced analytics focusing on ways to improve data security, accessibility, reliability, scalability, efficiency and quality.  Lead data governance and guide all data schema/configuration changes.   Deep experience with RDMS databases  SQL Server  and Data Warehouse  OLAP, Redshift  managing connection-pools, performance tuning and optimizations.   ",deep rdms databases sql server data warehouse olap redshift managing connectionpools performance tuning optimizations deep rdms databases sql server data warehouse olap redshift managing connectionpools performance tuning optimizations deliver complex solution data needs leading technical specifications drive resulting data designs participate strategic innovative data design walkthroughs reviews design document integrationconfiguration implement advance data models configurations support integrating data source systems environments promote continuous integrationcontinuous deployment cicd cqrs pattern caching research design implement next generation analytics machine platforms build tools frameworks apis dashboards support telemetry advanced analytics focusing ways improve data security accessibility reliability scalability efficiency lead data governance guide data schemaconfiguration changes deep rdms databases sql server data warehouse olap redshift managing connectionpools performance tuning optimizations,deep rdms databases sql server data warehouse olap redshift managing connectionpools performance tuning optimizations deliver complex solution needs leading technical specifications drive resulting designs participate strategic innovative design walkthroughs reviews document integrationconfiguration implement advance models configurations support integrating source systems environments promote continuous integrationcontinuous deployment cicd cqrs pattern caching research next generation analytics machine platforms build tools frameworks apis dashboards telemetry advanced focusing ways improve security accessibility reliability scalability efficiency lead governance guide schemaconfiguration changes
353," Degree in Computer Science, Computer Engineering, or Statistics or commensurate experience 6-8 years of software engineering, Big Data Engineering and/or team lead experience Proven track record of building and shipping large-scale engineering products Experience working with large, complex data sets from a variety of sources Ability to collaborate with a diverse set of engineers, data scientists and product managers Experience with functional and object-oriented programming, Scala a plus Experience with Databricks, Spark, AWS and EMR Effective communication skills  both written and verbal , comfortable presenting to a group Comfort in a fast-paced startup environment     ",degree computer science computer engineering statistics commensurate software engineering big data engineering andor team lead proven track record building shipping largescale engineering products complex data sets variety sources collaborate diverse set engineers data scientists product managers functional objectoriented programming scala plus databricks spark aws emr effective communication written verbal comfortable presenting group comfort fastpaced startup,degree computer science engineering statistics commensurate software big data andor team lead proven track record building shipping largescale products complex sets variety sources collaborate diverse set engineers scientists product managers functional objectoriented programming scala plus databricks spark aws emr effective communication written verbal comfortable presenting group comfort fastpaced startup
354,"   Design, implement, and support a platform providing secured access to large datasets.  Collaborate with customers from Finance, HR and other shared service functions understanding their requirements and delivering data solutions they need.  Understand the data sources, develop an ETL strategy with these sources, perform data modeling to meet customersâ data needs.  Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.  Recognize and adopt best practices in reporting and analysis  data integrity, data security, analysis, validation, and documentation.  Monitor, manage and administer the Enterprise Applications Data Warehouse and ensure optimal performance at scale.  Design, implement and manage the access to the datasets based on the Zillowâs data access policies.   ",design implement support platform providing secured access datasets collaborate customers finance hr shared service functions understanding delivering data solutions need understand data sources develop etl strategy sources perform data modeling meet customers data needs continually improve ongoing reporting analysis processes automating simplifying selfservice support datasets recognize adopt best practices reporting analysis data integrity data security analysis validation documentation monitor manage administer enterprise applications data warehouse optimal performance scale design implement manage access datasets based zillows data access policies,design implement support platform providing secured access datasets collaborate customers finance hr shared service functions understanding delivering data solutions need understand sources develop etl strategy perform modeling meet needs continually improve ongoing reporting analysis processes automating simplifying selfservice recognize adopt best practices integrity security validation documentation monitor manage administer enterprise applications warehouse optimal performance scale based zillows policies
355," 4+ years of relevant technical experience, including 2+ years with noSQL databases  MongoDB preferred  as well as experience with SQL Strong Python coding skills Experience developing and implementing ETL architectures with large, complex data sets Understanding of database architecture and data lakes Distributing computing  parallel processing, multi-threading  â Hadoop, MapReduce, Spark Hands-on experience with web crawling/web scraping required  6+ months  Experience developing APIs Experience with Node. js and familiarity with Machine Learning are pluses Strong quantitative data analysis skills Beyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech Curiosity about anomalies in the data and the ability to identify the business opportunities they represent.  Strong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels Azure experience is a plus      4+ years of relevant technical experience, including 2+ years with noSQL databases  MongoDB preferred  as well as experience with SQL Strong Python coding skills Experience developing and implementing ETL architectures with large, complex data sets Understanding of database architecture and data lakes Distributing computing  parallel processing, multi-threading  â Hadoop, MapReduce, Spark Hands-on experience with web crawling/web scraping required  6+ months  Experience developing APIs Experience with Node. js and familiarity with Machine Learning are pluses Strong quantitative data analysis skills Beyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech Curiosity about anomalies in the data and the ability to identify the business opportunities they represent.  Strong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels Azure experience is a plus ",relevant technical nosql databases mongodb well sql python coding developing implementing etl architectures complex data sets understanding database architecture data lakes distributing computing parallel processing multithreading hadoop mapreduce spark handson web crawlingweb scraping months developing apis node js familiarity machine pluses quantitative data analysis beyond technical business thinking interest consumer appsconsumer tech curiosity anomalies data identify business opportunities represent communication excitement around championing great ideas insights stakeholders levels azure plus relevant technical nosql databases mongodb well sql python coding developing implementing etl architectures complex data sets understanding database architecture data lakes distributing computing parallel processing multithreading hadoop mapreduce spark handson web crawlingweb scraping months developing apis node js familiarity machine pluses quantitative data analysis beyond technical business thinking interest consumer appsconsumer tech curiosity anomalies data identify business opportunities represent communication excitement around championing great ideas insights stakeholders levels azure plus,relevant technical nosql databases mongodb well sql python coding developing implementing etl architectures complex data sets understanding database architecture lakes distributing computing parallel processing multithreading hadoop mapreduce spark handson web crawlingweb scraping months apis node js familiarity machine pluses quantitative analysis beyond business thinking interest consumer appsconsumer tech curiosity anomalies identify opportunities represent communication excitement around championing great ideas insights stakeholders levels azure plus
356,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
357, 5+ years of experience in core JAVA and SQL    ,core java sql,core java sql
358,"   Translate requirements from various internal business teams to deliveroptimal data structures used to drive insightful, actionable reports and dashboards.  Plan and develop complex ETL processes extracting data from source systems and loading cleaned, transformed, and conformed data into our enterprise data warehouse Ensure data integrity by validating against existing reports and source data Work cross functionally with our Data Services and Data Science teams to architect and implement optimized data pipelines Create dimensional models that meet both immediate analytical and reporting needs and also conform to long-term data strategy Scope projects and provide estimates of work to be performed and timelines Develop ETL specifications, source to target mappings, and other documentation required for ETL development and maintenance  ",translate various internal business teams deliveroptimal data structures used drive insightful actionable reports dashboards plan develop complex etl processes extracting data source systems loading cleaned transformed conformed data enterprise data warehouse data integrity validating existing reports source data cross functionally data services data science teams architect implement optimized data pipelines create dimensional models meet immediate analytical reporting needs also conform longterm data strategy scope projects estimates performed timelines develop etl specifications source target mappings documentation etl development maintenance,translate various internal business teams deliveroptimal data structures used drive insightful actionable reports dashboards plan develop complex etl processes extracting source systems loading cleaned transformed conformed enterprise warehouse integrity validating existing cross functionally services science architect implement optimized pipelines create dimensional models meet immediate analytical reporting needs also conform longterm strategy scope projects estimates performed timelines specifications target mappings documentation development maintenance
359," 5-8 years of Python or Java/J2EE development experience 3+ years of demonstrated technical proficiency with Hadoop and big data projects 5-8 years of demonstrated experience and success in data modeling Fluent in writing shell scripts [bash, korn] Writing high-performance, reliable and maintainable code Ability to write MapReduce jobs Knowledge of database structures, theories, principles, and practices Understand how to develop code in an environment secured using a local KDC and OpenLDAP Familiarity with and implementation knowledge of loading data using Sqoop Knowledge and ability to implement workflow/schedulers within Oozie Experience working with AWS components [EC2, S3, SNS, SQS] Analytical and problem-solving skills, applied to Big Data domain Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark Aptitude in multi-threading and concurrency concepts M. S.  in Computer Science or Engineering   Translate complex functional and technical requirements into detailed design Hadoop technical development and implementation Loading from disparate data sets by leveraging various big data technology e. g.  Kafka Pre-processing using Hive, Impala, Spark, and Pig Design and implement data modeling Maintain security and data privacy in an environment secured using Kerberos and LDAP High-speed querying using in-memory technologies such as Spark Following and contributing best engineering practice for source control, release management, deployment etc Production support, job scheduling/monitoring, ETL data quality, data freshness reporting  ",python javajee development demonstrated technical proficiency hadoop big data projects demonstrated success data modeling fluent writing shell scripts bash korn writing highperformance reliable maintainable code write mapreduce jobs database structures theories principles practices understand develop code secured local kdc openldap familiarity implementation loading data sqoop implement workflowschedulers within oozie aws components ec sns sqs analytical problemsolving applied big data domain proven understanding hands hadoop hive pig impala spark aptitude multithreading concurrency concepts computer science engineering translate complex functional technical detailed design hadoop technical development implementation loading disparate data sets leveraging various big data technology e g kafka preprocessing hive impala spark pig design implement data modeling maintain security data privacy secured kerberos ldap highspeed querying inmemory technologies spark following contributing best engineering practice source control release management deployment production support job schedulingmonitoring etl data data freshness reporting,python javajee development demonstrated technical proficiency hadoop big data projects success modeling fluent writing shell scripts bash korn highperformance reliable maintainable code write mapreduce jobs database structures theories principles practices understand develop secured local kdc openldap familiarity implementation loading sqoop implement workflowschedulers within oozie aws components ec sns sqs analytical problemsolving applied domain proven understanding hands hive pig impala spark aptitude multithreading concurrency concepts computer science engineering translate complex functional detailed design disparate sets leveraging various technology e g kafka preprocessing maintain security privacy kerberos ldap highspeed querying inmemory technologies following contributing best practice source control release management deployment production support job schedulingmonitoring etl freshness reporting
360," Degree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development Proficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc.  Experience with developing on an MPP database Redshift/Teradata/Snowflake Proficient handling large data sets using SQL and databases in a business and engineering environment Experience with operations in a Public Cloud Environment  AWS/Azure/GCP  Experience with ETL and Data Warehouse/Lake processes Excellent verbal and written communication skills Strong troubleshooting and problem-solving skills Thrive in a fast-paced, innovative environment     ",degree computer engineeringscience professional databasedata lake development proficient processing data relational databases like oraclesql servermysqletc developing mpp database redshiftteradatasnowflake proficient handling data sets sql databases business engineering operations public cloud awsazuregcp etl data warehouselake processes verbal written communication troubleshooting problemsolving thrive fastpaced innovative,degree computer engineeringscience professional databasedata lake development proficient processing data relational databases like oraclesql servermysqletc developing mpp database redshiftteradatasnowflake handling sets sql business engineering operations public cloud awsazuregcp etl warehouselake processes verbal written communication troubleshooting problemsolving thrive fastpaced innovative
361," BS or advanced degree in Computer Science/Engineering, information systems or related technical field.  Prior experience supporting platforms built using open source technologies like Jupyter, Hadoop, Hive, Presto, Spark etc.  Big Data tech - Hadoop, Hive, Presto, Yarn, HDFS, Spark, Tez etc.  Significant experience with other ETL tech  Informatica, SSIS, etc  is very valuable, but expect to work in a ""Big Data"" environment.  Experience in Hadoop cluster admin is preferred.  Python for scripting and automation and basic SQL experience is required.  Prior experience in either AWS or Azure or Cloudera technologies, and any MPP/Cloud data warehouse solutions.  Solid understanding of JRE  Java Runtime Environment  and JVM langauges  Java, Scala  You have the desire and aptitude to learn how the pieces of big data platform work together Curious, Determined, Talented and Inspired to solve user issues.  Sharp communicator who can explain complex data problems in clear and concise language.  Love freedom and hate being micromanaged.  Given context, you're capable of self-direction.  And be comfortable outside of your comfort zone.      ",bs advanced degree computer scienceengineering information systems technical prior supporting platforms built open source technologies like jupyter hadoop hive presto spark big data tech hadoop hive presto yarn hdfs spark tez significant etl tech informatica ssis valuable expect big data hadoop cluster admin python scripting automation basic sql prior either aws azure cloudera technologies mppcloud data warehouse solutions solid understanding jre java runtime jvm langauges java scala desire aptitude learn pieces big data platform together curious determined talented inspired solve user issues sharp communicator explain complex data problems clear concise language love freedom hate micromanaged given context youre capable selfdirection comfortable outside comfort zone,bs advanced degree computer scienceengineering information systems technical prior supporting platforms built open source technologies like jupyter hadoop hive presto spark big data tech yarn hdfs tez significant etl informatica ssis valuable expect cluster admin python scripting automation basic sql either aws azure cloudera mppcloud warehouse solutions solid understanding jre java runtime jvm langauges scala desire aptitude learn pieces platform together curious determined talented inspired solve user issues sharp communicator explain complex problems clear concise language love freedom hate micromanaged given context youre capable selfdirection comfortable outside comfort zone
362," BS or advanced degree in Computer Science, Computer Science/ Engineering, or Electrical Engineering, technology, information systems or related technical field.  5 years of software industry experience or related 4+ years of experience in data integration applications, data processing applications, data warehouse, business intelligence, SQL 3 years of experience in Hadoop, Map reduce, Hive, HBase, Pig or other big data frameworks 3 years of experience either AWS or Azure or Cloudera technologies- Spark, Redshift, EMR, HDInsight or equivalent data platform technologies.  Experienced in architecting, building and maintaining large -scale data infrastructures Possesses full life cycle knowledge of big data technologies including map reduce, hive and HBase Experienced with python scripting and other platform agnostic language including Java.  Experienced in data structures, algorithm design and troubleshoot using Java/Python.     Architect and drive the build out of Coupang's next generation platform.  Build durable code, with the ability to scale with very large data volumes.  Engage in system architecture decisions.  Apply knowledge and implement big data technologies such as Hadoop, Hive, HBase, Spark and enhance AWS technologies to support Coupang. com's big data requirements in support of its online e-commerce system.  Foster, mentor, and enforce industry best practices in data architecture design, design patterns and coding standard.    ",bs advanced degree computer science computer science engineering electrical engineering technology information systems technical software industry data integration applications data processing applications data warehouse business intelligence sql hadoop map reduce hive hbase pig big data frameworks either aws azure cloudera technologies spark redshift emr hdinsight data platform technologies experienced architecting building maintaining scale data infrastructures possesses full life cycle big data technologies map reduce hive hbase experienced python scripting platform agnostic language java experienced data structures algorithm design troubleshoot javapython architect drive build coupangs next generation platform build durable code scale data volumes engage architecture decisions apply implement big data technologies hadoop hive hbase spark enhance aws technologies support coupang coms big data support online ecommerce foster mentor enforce industry best practices data architecture design design patterns coding standard,bs advanced degree computer science engineering electrical technology information systems technical software industry data integration applications processing warehouse business intelligence sql hadoop map reduce hive hbase pig big frameworks either aws azure cloudera technologies spark redshift emr hdinsight platform experienced architecting building maintaining scale infrastructures possesses full life cycle python scripting agnostic language java structures algorithm design troubleshoot javapython architect drive build coupangs next generation durable code volumes engage architecture decisions apply implement enhance support coupang coms online ecommerce foster mentor enforce best practices patterns coding standard
363,"   Curate very large-scale data from a multitude of sources into appropriate sets for research and development for the data science, threat analysts, and developers across the company Design, test, and implement storage solutions for various consumers of the data Design and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methods Leverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analytics Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines Collaborate on design, implementation, and deployment of applications with the rest of software engineering Support data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage data Build and maintain tools for automation, deployment, monitoring, and operations Create test plans, test cases, and run tests with automated tools  MS or BS in Computer Science or a related field, or equivalent work experience required  5+ years of experience with Python3, and 2+ years experience with Spark.  Scala experience is helpful 5+ years of experience in data engineering, data science, and related data-centric fields using large-scale data environments 3+ years of experience in using SQL and working with modern relational databases, including MySQL or PostgreSQL 3+ years of experience with developing ETL pipelines and data manipulation scripts Proficient in Object Oriented Design and S. O. L. I. D principles.  Strong emphasis on unit testing and code quality Proficient with AWS products  EMR S3, Lambda, VPC, EC2, API Gateway, etc ",curate largescale data multitude sources appropriate sets research development data science threat analysts developers across company design test implement storage solutions various consumers data design implement mechanisms monitor data sources time changes summarization monitoring statistical methods leverage computer science algorithms constructs probabilistic data structures distill data sources insight enable future analytics convert prototypes production data engineering solutions disciplined software engineering practices spark optimizations modern deployment pipelines collaborate design implementation deployment applications rest software engineering support data scientists threat analysts building debugging deploying spark applications best leverage data build maintain tools automation deployment monitoring operations create test plans test cases run tests automated tools ms bs computer science python spark scala helpful data engineering data science datacentric fields largescale data environments sql modern relational databases mysql postgresql developing etl pipelines data manipulation scripts proficient object oriented design l principles emphasis unit testing code proficient aws products emr lambda vpc ec api gateway,curate largescale data multitude sources appropriate sets research development science threat analysts developers across company design test implement storage solutions various consumers mechanisms monitor time changes summarization monitoring statistical methods leverage computer algorithms constructs probabilistic structures distill insight enable future analytics convert prototypes production engineering disciplined software practices spark optimizations modern deployment pipelines collaborate implementation applications rest support scientists building debugging deploying best build maintain tools automation operations create plans cases run tests automated ms bs python scala helpful datacentric fields environments sql relational databases mysql postgresql developing etl manipulation scripts proficient object oriented l principles emphasis unit testing code aws products emr lambda vpc ec api gateway
364,"5+ years of data engineering, ETL and/or data warehouse development Masterâs Degree in Computer Science  or similar area of study  Technical expertise and experience both SQL and NOSQL databases Advanced understanding of a wide array of data models including relational, dimensional, document-based, object oriented, object-relational, and graphical Advanced experience in database interrogation of SQL and NOSQL databases Experience implementing High Availability systems requirement Experience with web based APIs  e. g.  REST, SOAP  Experience with AWS Stack  RDS, Kinesis, Lambda, Redshift, SQS, etc  Proficiency in scripting languages  e. g.  Python, Bash  Strong analytic skill set and a high degree of proficiency in data mining Excellent written communication and presentation skills Must be a U. S.  citizen or national, U. S.  permanent resident  current Green Card holder , or lawfully admitted into the U. S.  as a refugee or granted asylum.    Collaborate with departments and technical product managers to collect, transform and aggregate information that leads to business insights Build and maintain tools, data pipelines, analytics, reports to highlight technical performance metrics and other key information identified by programs and functional leadership Work with application developers to collect data from custom applications Establish processes and tools for monitoring and improving performance and effectivity of new and existing data integrations and pipelines Perform quality assurance and code reviews to ensure both functional and non-functional requirements are being met   ",data engineering etl andor data warehouse development masters degree computer science similar area study technical expertise sql nosql databases advanced understanding wide array data models relational dimensional documentbased object oriented objectrelational graphical advanced database interrogation sql nosql databases implementing availability systems requirement web based apis e g rest soap aws stack rds kinesis lambda redshift sqs proficiency scripting languages e g python bash analytic skill set degree proficiency data mining written communication presentation must u citizen national u permanent resident current green card holder lawfully admitted u refugee granted asylum collaborate departments technical product managers collect transform aggregate information leads business insights build maintain tools data pipelines analytics reports highlight technical performance metrics key information identified programs functional leadership application developers collect data custom applications establish processes tools monitoring improving performance effectivity existing data integrations pipelines perform assurance code reviews functional nonfunctional met,data engineering etl andor warehouse development masters degree computer science similar area study technical expertise sql nosql databases advanced understanding wide array models relational dimensional documentbased object oriented objectrelational graphical database interrogation implementing availability systems requirement web based apis e g rest soap aws stack rds kinesis lambda redshift sqs proficiency scripting languages python bash analytic skill set mining written communication presentation must u citizen national permanent resident current green card holder lawfully admitted refugee granted asylum collaborate departments product managers collect transform aggregate information leads business insights build maintain tools pipelines analytics reports highlight performance metrics key identified programs functional leadership application developers custom applications establish processes monitoring improving effectivity existing integrations perform assurance code reviews nonfunctional met
365,"     Develop data processes for data modeling, mining, reporting and QA Ensure architecture will support the business requirements Employ a variety of languages and tools  e. g.  scripting languages  to integrate data from different systems.  Recommend ways to improve data reliability, efficiency and quality.  Employ sophisticated analytics programs, machine learning and statistical methods to prepare data for use in predictive and prescriptive modeling.  Explore and examine data to find hidden patterns.  Analyze potential data quality issues to determine the root cause and create effective solutions.  Optimize processes involving large data sets to improve performance.  Work with stakeholder to understand their business and make recommendations to help solve problems or improve processes.  Deliver high quality projects on time and budget in a fast-paced environment.  Preparing and presenting technical information to non-technical and highly technical audiences.  Working on multiple projects simultaneously.  Training and supporting others as needed.  ",develop data processes data modeling mining reporting qa architecture support business employ variety languages tools e g scripting languages integrate data different systems recommend ways improve data reliability efficiency employ sophisticated analytics programs machine statistical methods prepare data use predictive prescriptive modeling explore examine data find hidden patterns analyze potential data issues determine root cause create effective solutions optimize processes involving data sets improve performance stakeholder understand business make recommendations help solve problems improve processes deliver projects time budget fastpaced preparing presenting technical information nontechnical highly technical audiences multiple projects simultaneously training supporting others needed,develop data processes modeling mining reporting qa architecture support business employ variety languages tools e g scripting integrate different systems recommend ways improve reliability efficiency sophisticated analytics programs machine statistical methods prepare use predictive prescriptive explore examine find hidden patterns analyze potential issues determine root cause create effective solutions optimize involving sets performance stakeholder understand make recommendations help solve problems deliver projects time budget fastpaced preparing presenting technical information nontechnical highly audiences multiple simultaneously training supporting others needed
366,"Bachelorâs Degree in computer science, engineering, mathematics, MIS or similar field.  10 years in technology roles.  Must have experience with the following technologies  C  ASP. net T-SQL HTML/CSS JavaScript Nodejs Demonstrated analytical skills Demonstrated problem solving skills Promotes information sharing Ability to work within tight timeframes and meet strict deadlines.  Possesses strong technical Aptitude.      ",bachelors degree computer science engineering mathematics mis similar technology roles must following technologies c asp net tsql htmlcss javascript nodejs demonstrated analytical demonstrated problem solving promotes information sharing within tight timeframes meet strict deadlines possesses technical aptitude,bachelors degree computer science engineering mathematics mis similar technology roles must following technologies c asp net tsql htmlcss javascript nodejs demonstrated analytical problem solving promotes information sharing within tight timeframes meet strict deadlines possesses technical aptitude
367," 8+ years of BI development experience with 4+ of cloud analytics experience Cloud solution implementation experience with some of the following technologies  Jenkins Sqoop Hadoop including Hive and Pig Spark and SparkSQL AWS S3, Redshift, Dinamo DB Azure Data Lake Store, HDInsight, U-SQL Python, JSON, Java    ",bi development cloud analytics cloud solution implementation following technologies jenkins sqoop hadoop hive pig spark sparksql aws redshift dinamo db azure data lake store hdinsight usql python json java,bi development cloud analytics solution implementation following technologies jenkins sqoop hadoop hive pig spark sparksql aws redshift dinamo db azure data lake store hdinsight usql python json java
368,"3+ years of industry experience Experience with high scale, distributed, 24x7 systems and applications 5+ years of experience with SQL in both transactional and analytical applications Experience with Linux platforms Experience with AWS data technologies Experience with Ruby, Python, or a similar programming language Strong analytical and design skills Capacity for learning quickly in a fast paced environment and handling multiple tasks simultaneously Strong written and oral communication skills  3+ years of industry experience Experience with high scale, distributed, 24x7 systems and applications 5+ years of experience with SQL in both transactional and analytical applications Experience with Linux platforms Experience with AWS data technologies Experience with Ruby, Python, or a similar programming language Strong analytical and design skills Capacity for learning quickly in a fast paced environment and handling multiple tasks simultaneously Strong written and oral communication skills  Define the architecture, technologies, and processes used to create a high scale data processing pipeline Work with engineers to create the tools and infrastructure used to collect, transform, and store data to be used for analytics Identify and implement analytics tools used by internal stakeholders to engage with data Ensure proper security and access control to analytics data Work collaboratively with the team to deploy, support and operate your services and applications.   BS/MS in Computer Science or Engineering ",industry scale distributed x systems applications sql transactional analytical applications linux platforms aws data technologies ruby python similar programming language analytical design capacity quickly fast paced handling multiple tasks simultaneously written oral communication industry scale distributed x systems applications sql transactional analytical applications linux platforms aws data technologies ruby python similar programming language analytical design capacity quickly fast paced handling multiple tasks simultaneously written oral communication define architecture technologies processes used create scale data processing pipeline engineers create tools infrastructure used collect transform store data used analytics identify implement analytics tools used internal stakeholders engage data proper security access control analytics data collaboratively team deploy support operate services applications bsms computer science engineering,industry scale distributed x systems applications sql transactional analytical linux platforms aws data technologies ruby python similar programming language design capacity quickly fast paced handling multiple tasks simultaneously written oral communication define architecture processes used create processing pipeline engineers tools infrastructure collect transform store analytics identify implement internal stakeholders engage proper security access control collaboratively team deploy support operate services bsms computer science engineering
369,"   Design, architect, implement, and support key datasets that provide structured and timely access to actionable business information with the needs of the end customer always in view Retrieve and analyze data using SQL, Excel, and other data management systems.  Create ETLs/ELTs to take data from various operational systems and create a unified dimensional or star schema data model for analytics and reporting Develop a deep understanding of vast data sources  existing on the cloud  and know exactly how, when, and which data to use to solve particular business problems  ",design architect implement support key datasets structured timely access actionable business information needs end customer always view retrieve analyze data sql excel data management systems create etlselts take data various operational systems create unified dimensional star schema data model analytics reporting develop deep understanding vast data sources existing cloud know exactly data use solve particular business problems,design architect implement support key datasets structured timely access actionable business information needs end customer always view retrieve analyze data sql excel management systems create etlselts take various operational unified dimensional star schema model analytics reporting develop deep understanding vast sources existing cloud know exactly use solve particular problems
370,"   Build and operate large scale data infrastructure in production.  Design, implement and debug distributed data processing systems.  Thinking through long-term impacts of key design decisions and handling failure scenarios.  Building self-service platforms to power all other OfferUp teams.  Mentor other engineers and help them with their growth.  Drive engineering best practices, set standards and propose larger projects which may require cross-team collaboration.  Contribute at a senior level to the data warehouse design and data preparation by implementing a robust and extensible design Design and develop applications to process large amounts of critical information in batch and near real-time to power user-facing features.  Influence technical direction for the company, leveraging your prior experiences and helping evaluate emerging technologies and approaches.     5+ years of professional software development experience.  Strong ability in distributed systems for processing large scale data processing.  Ability to communicate technical information effectively to technical and non-technical audiences.  Proficiency in Java, Scala and Python.  Experience leveraging open source data infrastructure projects, such as Apache Spark, Airflow, Kafka, Flink, Samza, Avro, Parquet, Hadoop, Hive, HBase.  Experience building data pipelines and real-time data streams.  Experience building software in AWS or a similar cloud environment is highly desirable.  Experience with AWS services like EMR, Kinesis, Firehose, Lambda, Sagemaker, Athena, Elasticsearch is a big plus.  Computer Science or Engineering degree required, Masters degree preferred.  Must be eligible to work in the United States.  ",build operate scale data infrastructure production design implement debug distributed data processing systems thinking longterm impacts key design decisions handling failure scenarios building selfservice platforms power offerup teams mentor engineers help growth drive engineering best practices set standards propose larger projects may require crossteam collaboration contribute senior level data warehouse design data preparation implementing robust extensible design design develop applications process amounts critical information batch near realtime power userfacing features influence technical direction company leveraging prior experiences helping evaluate emerging technologies approaches professional software development distributed systems processing scale data processing communicate technical information effectively technical nontechnical audiences proficiency java scala python leveraging open source data infrastructure projects apache spark airflow kafka flink samza avro parquet hadoop hive hbase building data pipelines realtime data streams building software aws similar cloud highly desirable aws services like emr kinesis firehose lambda sagemaker athena elasticsearch big plus computer science engineering degree masters degree must eligible united states,build operate scale data infrastructure production design implement debug distributed processing systems thinking longterm impacts key decisions handling failure scenarios building selfservice platforms power offerup teams mentor engineers help growth drive engineering best practices set standards propose larger projects may require crossteam collaboration contribute senior level warehouse preparation implementing robust extensible develop applications process amounts critical information batch near realtime userfacing features influence technical direction company leveraging prior experiences helping evaluate emerging technologies approaches professional software development communicate effectively nontechnical audiences proficiency java scala python open source apache spark airflow kafka flink samza avro parquet hadoop hive hbase pipelines streams aws similar cloud highly desirable services like emr kinesis firehose lambda sagemaker athena elasticsearch big plus computer science degree masters must eligible united states
371," Enthusiasm for cars and/or gaming - Our priority is making amazing racing games.  The ideal candidate must have enthusiasm for our products and empathy for our customers.  Enthusaism for cloud data technology - Our pipelines are fully supported by Azure leveraging things like Data Explorer  Kusto , Data Warehouse, Data Factory, Data Lake, SQL and Power BI.  The ideal candidate has a passion for cloud technology and a minimum of 5 years' experience.  A drive to develop data insights - Collecting data is the easy part.  Helping business leaders and game designers ask the right questions and answering these questions with a relentless attention to details  accuracy  is where the fun begins.  The ideal candidate is a meticulous gatekeeper for data and code quality, passionate about generating insights from data, and a strong communicator and collaborator.  Enthusiasm for AI/ML or an interest to learn - We don't do science projects, but we have an aspiration to build AI/ML capabilities that generate customer value.    Evolving our big data pipelines to streamline data collection  measure things  and democratize the consumption of data  generate information .  Working with business leaders and game designers to answer the key questions that enable the team to drive franchise growth and create experiences that thrill customers.   ",enthusiasm cars andor gaming priority making amazing racing games ideal candidate must enthusiasm products empathy customers enthusaism cloud data technology pipelines fully supported azure leveraging things like data explorer kusto data warehouse data factory data lake sql power bi ideal candidate passion cloud technology minimum drive develop data insights collecting data easy part helping business leaders game designers ask right questions answering questions relentless attention details accuracy fun begins ideal candidate meticulous gatekeeper data code passionate generating insights data communicator collaborator enthusiasm aiml interest learn dont science projects aspiration build aiml capabilities generate customer value evolving big data pipelines streamline data collection measure things democratize consumption data generate information business leaders game designers answer key questions enable team drive franchise growth create experiences thrill customers,enthusiasm cars andor gaming priority making amazing racing games ideal candidate must products empathy customers enthusaism cloud data technology pipelines fully supported azure leveraging things like explorer kusto warehouse factory lake sql power bi passion minimum drive develop insights collecting easy part helping business leaders game designers ask right questions answering relentless attention details accuracy fun begins meticulous gatekeeper code passionate generating communicator collaborator aiml interest learn dont science projects aspiration build capabilities generate customer value evolving big streamline collection measure democratize consumption information answer key enable team franchise growth create experiences thrill
372,"Masterâs degree in Computer Science, Engineering, or equivalent work experience Two to four yearsâ experience working with datasets with hundreds of millions of rows using a variety of technologies Intermediate to expert level programming experience in Python and SQL in Windows and Mac/Linux environment Intermediate level experience working with distributed computing frameworks, especially Spark  Build and Maintain serverless data ingestion and refresh pipelines in terabyte scale using AWS cloud services â Amazon Glue, Amazon Redshift, Amazon S3, Amazon Athena, DynamoDB, and others Incorporate new data sources from external vendors using flat files, APIs, web-scraping, and databases.  Maintain and provide support for the existing data pipelines using Python, Glue, Spark, and SQL Work to develop and enhance the database architecture of the new analytic data environment that includes recommending optimal choices between relational, columnar, and document databases based on requirement Identify and deploy appropriate file formats for data ingestion into various storage and/or compute services via Glue for multiple use cases Develop real-time/near real-time data ingestion from web and web service logs from Splunk Maintain existing processes and develop new methods to match external data sources to Homesite data using exact and fuzzy methods Implement and use machine learning based data wrangling tools like Trifacta to cleanse and reshape 3rd party data to make suitable for use.  Develop and implement tests to ensure data quality across all integrated data sources.  Serve as internal subject matter expert and coach to train team members in the use of distributed computing frameworks for data analysis and modeling including AWS services and Apache projects   ",masters degree computer science engineering two four datasets hundreds millions rows variety technologies intermediate expert level programming python sql windows maclinux intermediate level distributed computing frameworks especially spark build maintain serverless data ingestion refresh pipelines terabyte scale aws cloud services amazon glue amazon redshift amazon amazon athena dynamodb others incorporate data sources external vendors flat files apis webscraping databases maintain support existing data pipelines python glue spark sql develop enhance database architecture analytic data includes recommending optimal choices relational columnar document databases based requirement identify deploy appropriate file formats data ingestion various storage andor compute services via glue multiple use cases develop realtimenear realtime data ingestion web web service logs splunk maintain existing processes develop methods match external data sources homesite data exact fuzzy methods implement use machine based data wrangling tools like trifacta cleanse reshape rd party data make suitable use develop implement tests data across integrated data sources serve internal subject matter expert coach train team members use distributed computing frameworks data analysis modeling aws services apache projects,masters degree computer science engineering two four datasets hundreds millions rows variety technologies intermediate expert level programming python sql windows maclinux distributed computing frameworks especially spark build maintain serverless data ingestion refresh pipelines terabyte scale aws cloud services amazon glue redshift athena dynamodb others incorporate sources external vendors flat files apis webscraping databases support existing develop enhance database architecture analytic includes recommending optimal choices relational columnar document based requirement identify deploy appropriate file formats various storage andor compute via multiple use cases realtimenear realtime web service logs splunk processes methods match homesite exact fuzzy implement machine wrangling tools like trifacta cleanse reshape rd party make suitable tests across integrated serve internal subject matter coach train team members analysis modeling apache projects
373,Bachelor's in Computer Science or related degree  Bachelor's in Computer Science or related degree    ,bachelors computer science degree bachelors computer science degree,bachelors computer science degree
374,"Minimum of a B. S.  degree in Electrical Engineering, Systems Engineering, Computer Science, Computer Engineering, Physics, Mathematics, or other major requiring engineering core courses Minimum of 5+ years of experience with aerospace/ control systems and software Excellent written communication and presentation skills Ability to collaborate across teams and balance priorities The ability to quickly absorb information in an unfamiliar domain A self-driven nature with the ability to seek out requirements and propose solutions with minimal direction Technical expertise in data visualization tools  e. g.  Tableau, PowerBI  Strong analytic skill set and a high degree of proficiency in data mining Must be a U. S.  citizen or national, U. S.  permanent resident  current Green Card holder , or lawfully admitted into the U. S.  as a refugee or granted asylum.    Work with various subject matter experts to build reports and run analysis for various program and tactical related questions Give voice to the data, talk with people, ensure they understand what it means and it meets their needs Act as a change agent, help engineering teams adopt new more efficient methods of operating within large related data sets Determine and articulate balance of priorities to enable incremental delivery of needed functionality Perform data mining for valued projects Capture user feedback Work as a flexible contributor in an agile team including building consensus on designs and participating in code reviews Participate in Strategic Development of technical performance / launch system health monitoring solutions   ",minimum b degree electrical engineering systems engineering computer science computer engineering physics mathematics major requiring engineering core courses minimum aerospace control systems software written communication presentation collaborate across teams balance priorities quickly absorb information unfamiliar domain selfdriven nature seek propose solutions minimal direction technical expertise data visualization tools e g tableau powerbi analytic skill set degree proficiency data mining must u citizen national u permanent resident current green card holder lawfully admitted u refugee granted asylum various subject matter experts build reports run analysis various program tactical questions give voice data talk people understand means meets needs act change agent help engineering teams adopt efficient methods operating within data sets determine articulate balance priorities enable incremental delivery needed functionality perform data mining valued projects capture user feedback flexible contributor agile team building consensus designs participating code reviews participate strategic development technical performance launch health monitoring solutions,minimum b degree electrical engineering systems computer science physics mathematics major requiring core courses aerospace control software written communication presentation collaborate across teams balance priorities quickly absorb information unfamiliar domain selfdriven nature seek propose solutions minimal direction technical expertise data visualization tools e g tableau powerbi analytic skill set proficiency mining must u citizen national permanent resident current green card holder lawfully admitted refugee granted asylum various subject matter experts build reports run analysis program tactical questions give voice talk people understand means meets needs act change agent help adopt efficient methods operating within sets determine articulate enable incremental delivery needed functionality perform valued projects capture user feedback flexible contributor agile team building consensus designs participating code reviews participate strategic development performance launch health monitoring
375,     Required minimum Bachelorâs degree in Computer Science,minimum bachelors degree computer science,minimum bachelors degree computer science
376," Bachelorâs Degree in computer science or relevant information technology field 3-4 years of DBA experience 3-4 years of SQL replication experience and managing complex ETL processes  Ability to communicate effectively with stakeholders to define requirements Proficiency in writing SQL procedures and functions for administration and application support Strong knowledge of operating systems, shell scripting, and python scripting Good interpersonal skills along with effective communication  both written and verbal  Demonstrated ability to solve complex systems and database environment issues Experience with cloud platforms  AWS and AZURE  Experience building data pipelines & ETL Experience with command line Experience with version control software  git  and best practices Demonstrated experience leading database architecture development Experience with big data technologies at scale Strong understanding of PostgreSQL database fundamentals Strong understanding of database security management Experience with ETL tools Experience with BI tools Willingness to take ownership over wide range of databases and processes  Collaborate with senior management, product management, data analytics, and web/app developers in the development of data products, pipelines, and infrastructure Develop reliable and near real time data pipelines that make data easily consumable by end users and other systems that we use at All Star Develop tools to monitor, debug, and analyze data pipeline health Design and implement data schemas and models that can scale Mentor technology team members to build the company's overall expertise Administer all database automation and take corrective action when required  ",bachelors degree computer science relevant information technology dba sql replication managing complex etl processes communicate effectively stakeholders define proficiency writing sql procedures functions administration application support operating systems shell scripting python scripting good interpersonal along effective communication written verbal demonstrated solve complex systems database issues cloud platforms aws azure building data pipelines etl command line version control software git best practices demonstrated leading database architecture development big data technologies scale understanding postgresql database fundamentals understanding database security management etl tools bi tools willingness take ownership wide range databases processes collaborate senior management product management data analytics webapp developers development data products pipelines infrastructure develop reliable near real time data pipelines make data easily consumable end users systems use star develop tools monitor debug analyze data pipeline health design implement data schemas models scale mentor technology team members build companys overall expertise administer database automation take corrective action,bachelors degree computer science relevant information technology dba sql replication managing complex etl processes communicate effectively stakeholders define proficiency writing procedures functions administration application support operating systems shell scripting python good interpersonal along effective communication written verbal demonstrated solve database issues cloud platforms aws azure building data pipelines command line version control software git best practices leading architecture development big technologies scale understanding postgresql fundamentals security management tools bi willingness take ownership wide range databases collaborate senior product analytics webapp developers products infrastructure develop reliable near real time make easily consumable end users use star monitor debug analyze pipeline health design implement schemas models mentor team members build companys overall expertise administer automation corrective action
377," 5-years in SQL, SQL Server, Oracle, JDBC 5-years in Hadoop, HDFS, MapReduce, YARN 5-years in Sqoop, Oozie, Parquet, Hive, Impala, Spark, HBase, HUE    ",sql sql server oracle jdbc hadoop hdfs mapreduce yarn sqoop oozie parquet hive impala spark hbase hue,sql server oracle jdbc hadoop hdfs mapreduce yarn sqoop oozie parquet hive impala spark hbase hue
378,"   Serve as a subject matter expert in Data Science, developing strong relationships with partners across Talking Rain as you conduct and support white-boarding sessions, workshops, design sessions, and project meetings.  Deliver solutions leveraging the emerging machine learning  ML  methods and technologies  Big Data and streaming analytics Exploratory data analysis  EDA  & cleansing Feature engineering Model selection, model evaluation, and cross-validation Hyperparameter tuning, containerization, and deployment at all scales Own productionalization and ongoing performance tuning for all models.  Ensure weâre ahead of the curve, staying abreast of the ever-shifting retail landscape and how we can best leverage syndicated data.  Develop custom data models and algorithms to generate predictive insights.  Ensure food safety, quality, and SQF practices are followed at all times, notifying immediate manager of any food safety and/or quality issues.  Complete other responsibilities as assigned.   ",serve subject matter expert data science developing relationships partners across talking rain conduct support whiteboarding sessions workshops design sessions project meetings deliver solutions leveraging emerging machine ml methods technologies big data streaming analytics exploratory data analysis eda cleansing feature engineering model selection model evaluation crossvalidation hyperparameter tuning containerization deployment scales productionalization ongoing performance tuning models ahead curve staying abreast evershifting retail landscape best leverage syndicated data develop custom data models algorithms generate predictive insights food safety sqf practices followed times notifying immediate manager food safety andor issues complete responsibilities assigned,serve subject matter expert data science developing relationships partners across talking rain conduct support whiteboarding sessions workshops design project meetings deliver solutions leveraging emerging machine ml methods technologies big streaming analytics exploratory analysis eda cleansing feature engineering model selection evaluation crossvalidation hyperparameter tuning containerization deployment scales productionalization ongoing performance models ahead curve staying abreast evershifting retail landscape best leverage syndicated develop custom algorithms generate predictive insights food safety sqf practices followed times notifying immediate manager andor issues complete responsibilities assigned
379," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ",expertise least one following domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale writing software one languages python java scala go building productiongrade data solutions relational nosql systems monitoringalerting capacity planning performance tuning technical consulting customerfacing role,expertise least one following domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale writing languages python java scala go relational nosql systems monitoringalerting capacity planning performance tuning consulting customerfacing role
380," Experience leading, managing and hiring a team of talented engineers Expertise in at least one of the following engineering domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Expertise in at least one of the following data domains    Predictive analytics  e. g. , recommendation systems, predictive maintenance  Natural language processing  e. g. , conversational chatbots  Document understanding Image classification Marketing analytics IoT systems Experience writing software in one or more languages such as Python or Java/Scala Experience in technical consulting or customer-facing role Excellent critical thinking, problem-solving and analytical skills     ",leading managing hiring team talented engineers expertise least one following engineering domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale expertise least one following data domains predictive analytics e g recommendation systems predictive maintenance natural language processing e g conversational chatbots document understanding image classification marketing analytics iot systems writing software one languages python javascala technical consulting customerfacing role critical thinking problemsolving analytical,leading managing hiring team talented engineers expertise least one following engineering domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale domains predictive analytics e g recommendation systems maintenance natural language conversational chatbots document understanding image classification marketing iot writing languages python javascala consulting customerfacing role critical thinking problemsolving analytical
381," Mastery in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or customer-facing role     ",mastery least one following domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale writing software one languages python java scala go building productiongrade data solutions relational nosql systems monitoringalerting capacity planning performance tuning technical consulting customerfacing role,mastery least one following domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale writing languages python java scala go relational nosql systems monitoringalerting capacity planning performance tuning consulting customerfacing role
382," Cloud solution implementation experience with Azure Data Lake and Spark preferred Minimum 8 years hands-on experience with SQL At least one year of experience in scripting languages such as Python Demonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform Big data processing techniques, preferred Can work independently in ambiguous environment    ",cloud solution implementation azure data lake spark minimum handson sql least one year scripting languages python demonstrated cloudbased computing aws azure google cloud platform big data processing techniques independently ambiguous,cloud solution implementation azure data lake spark minimum handson sql least one year scripting languages python demonstrated cloudbased computing aws google platform big processing techniques independently ambiguous
383," Bachelorâs degree in Computer Science, Engineering, or closely related field 8+ years of experience designing and developing software Strong scripting skills to perform data/file manipulation 5+years of experience with data aggregation platforms based on technologies such as SQL, Azure Data Lake, Hadoop, Cosmos, CosmosDB, HDInsights etc.  Solid understanding and proven skills in raw and processed stream design, relational database design and dimensional models Solid understanding of event processing including publish/subscribe mechanisms Demonstrated ability to create and ship high quality code by using engineering best practices.  Hands on experience in big data components.  Experience with data warehousing and datamart design and implementation Experience with data security and compliance  PII, PHI, GDPR etc.   Strong understanding and inner workings of metadata management, data lineage, and data governance Strong experience in structured, unstructured, semi-structured data techniques and processes   Be part of a small, agile team working with experienced engineers that behaves more like a start-up than an established team.  Leverage Microsoft BI Suites to provide actionable insights into customer acquisition, and other key business performance metrics Engineer a modern data pipeline to collect, organize, and process data Produce clean, reusable code that is unit tested, code reviewed, and adheres to code standards Lead and facilitate a close development partnership with our Multi-tenant Operations team Drive the design and creation of solutions that allow MMD to offer a high level of service â reliable, available, and scalable Ideate and propose new product innovations to meet the needs of our customers.   ",bachelors degree computer science engineering closely designing developing software scripting perform datafile manipulation data aggregation platforms based technologies sql azure data lake hadoop cosmos cosmosdb hdinsights solid understanding proven raw processed stream design relational database design dimensional models solid understanding event processing publishsubscribe mechanisms demonstrated create ship code engineering best practices hands big data components data warehousing datamart design implementation data security compliance pii phi gdpr understanding inner workings metadata management data lineage data governance structured unstructured semistructured data techniques processes part small agile team experienced engineers behaves like startup established team leverage microsoft bi suites actionable insights customer acquisition key business performance metrics engineer modern data pipeline collect organize process data produce clean reusable code unit tested code reviewed adheres code standards lead facilitate close development partnership multitenant operations team drive design creation solutions allow mmd offer level service reliable available scalable ideate propose product innovations meet needs customers,bachelors degree computer science engineering closely designing developing software scripting perform datafile manipulation data aggregation platforms based technologies sql azure lake hadoop cosmos cosmosdb hdinsights solid understanding proven raw processed stream design relational database dimensional models event processing publishsubscribe mechanisms demonstrated create ship code best practices hands big components warehousing datamart implementation security compliance pii phi gdpr inner workings metadata management lineage governance structured unstructured semistructured techniques processes part small agile team experienced engineers behaves like startup established leverage microsoft bi suites actionable insights customer acquisition key business performance metrics engineer modern pipeline collect organize process produce clean reusable unit tested reviewed adheres standards lead facilitate close development partnership multitenant operations drive creation solutions allow mmd offer level service reliable available scalable ideate propose product innovations meet needs customers
384,"Bachelor's degree in computer science or closely related field with strong software development and machine learning skills with 2 years' experience or a Master's degree with 0-2 years' experience, or a Ph. D with 0 yearsâ experience is required.      ",bachelors degree computer science closely software development machine masters degree ph,bachelors degree computer science closely software development machine masters ph
385," Bachelor CS degree and 7+ years of experience in software engineering.  Track record of developing and maintaining reliable, highly available, secure, high throughput web-scale data systems  e. g.  Social, AdTech, MarTech, Heathcare, FinTech, etc.  .  Experience with big data pipelines and processing  e. g.  MapReduce, Hadoop, Big Query, Hive, Tez, Spark, etc.  .  Experience with realtime streaming event logs  e. g.  Kafka, GCP Cloud Pub/Sub, SNS/SQS .  You influence your peers, advise senior leaders, coach and mentor junior team members.  You facilitate cross-team collaboration among engineers and contribute to the broader community of senior engineers.  Must pass a Criminal Justice Information Services  CJIS  background check and maintain confidential and highly sensitive information.     ",bachelor cs degree software engineering track record developing maintaining reliable highly available secure throughput webscale data systems e g social adtech martech heathcare fintech big data pipelines processing e g mapreduce hadoop big query hive tez spark realtime streaming event logs e g kafka gcp cloud pubsub snssqs influence peers advise senior leaders coach mentor junior team members facilitate crossteam collaboration among engineers contribute broader community senior engineers must pass criminal justice information services cjis background check maintain confidential highly sensitive information,bachelor cs degree software engineering track record developing maintaining reliable highly available secure throughput webscale data systems e g social adtech martech heathcare fintech big pipelines processing mapreduce hadoop query hive tez spark realtime streaming event logs kafka gcp cloud pubsub snssqs influence peers advise senior leaders coach mentor junior team members facilitate crossteam collaboration among engineers contribute broader community must pass criminal justice information services cjis background check maintain confidential sensitive
386," Bachelorâs degree required 3-5 years of experience in database technologies Minimum 3 years of experience in Data Warehousing Experience with AWS tools  S3/EC2/Athena/Redshift Spectrum/IAM  Experience with Python Familiarity with Tableau or other data visualization tools.  Experienced with Linux administration and scripting Deep experience with data modeling, data pipelines, SQL, AWS, and distributed compute platforms Experience building effective relationships with a broad range of partners Ability to communicate well with partners, both technical and non-technical   Collect, transform, analyze, and refine operational and customer data.  Build-out data structures designed to efficiently answer business questions.  Assist in evolving data structures from a MSSQL Server footprint into a data lake environment.  Develop, implement and tune current ETL processes.  Assist with ad hoc report generation and analysis for merchandise and customers.  Create pipelines from internal and external data sources to AWS using custom python scripts.  Assist in managing our Tableau ecosystem and provide technical support.   Bachelorâs degree required 3-5 years of experience in database technologies Minimum 3 years of experience in Data Warehousing Experience with AWS tools  S3/EC2/Athena/Redshift Spectrum/IAM  Experience with Python Familiarity with Tableau or other data visualization tools.  Experienced with Linux administration and scripting Deep experience with data modeling, data pipelines, SQL, AWS, and distributed compute platforms Experience building effective relationships with a broad range of partners Ability to communicate well with partners, both technical and non-technical  While performing the duties of this job, the associate is regularly required to talk or hear.  The associate is frequently required to sit; stand; walk; use hands to finger, handle or feel; as well as reach with hands and arms.  The associate must frequently lift and/or move up to 15 pounds and occasionally lift and/or move up to 35 pounds.  Specific vision abilities required by this job include close vision, distance vision, depth perception and ability to adjust focus.  Ability to work in open environment with fluctuating temperatures and standard lighting Ability to work on computer and mobile phone for multiple hours; with frequent interruptions Required to travel in elevator or stairwells to attend meetings and engage with associates on multiple floors throughout building Hotel, Airplane, and Car Travel Required",bachelors degree database technologies minimum data warehousing aws tools secathenaredshift spectrumiam python familiarity tableau data visualization tools experienced linux administration scripting deep data modeling data pipelines sql aws distributed compute platforms building effective relationships broad range partners communicate well partners technical nontechnical collect transform analyze refine operational customer data buildout data structures designed efficiently answer business questions assist evolving data structures mssql server footprint data lake develop implement tune current etl processes assist ad hoc report generation analysis merchandise customers create pipelines internal external data sources aws custom python scripts assist managing tableau ecosystem technical support bachelors degree database technologies minimum data warehousing aws tools secathenaredshift spectrumiam python familiarity tableau data visualization tools experienced linux administration scripting deep data modeling data pipelines sql aws distributed compute platforms building effective relationships broad range partners communicate well partners technical nontechnical performing duties job associate regularly talk hear associate frequently sit stand walk use hands finger handle feel well reach hands arms associate must frequently lift andor move pounds occasionally lift andor move pounds specific vision abilities job include close vision distance vision depth perception adjust focus open fluctuating temperatures standard lighting computer mobile phone multiple hours frequent interruptions travel elevator stairwells attend meetings engage associates multiple floors throughout building hotel airplane car travel,bachelors degree database technologies minimum data warehousing aws tools secathenaredshift spectrumiam python familiarity tableau visualization experienced linux administration scripting deep modeling pipelines sql distributed compute platforms building effective relationships broad range partners communicate well technical nontechnical collect transform analyze refine operational customer buildout structures designed efficiently answer business questions assist evolving mssql server footprint lake develop implement tune current etl processes ad hoc report generation analysis merchandise customers create internal external sources custom scripts managing ecosystem support performing duties job associate regularly talk hear frequently sit stand walk use hands finger handle feel reach arms must lift andor move pounds occasionally specific vision abilities include close distance depth perception adjust focus open fluctuating temperatures standard lighting computer mobile phone multiple hours frequent interruptions travel elevator stairwells attend meetings engage associates floors throughout hotel airplane car
387,"  Minimum of 7+ years of proven technical experience in data engineering, data warehousing, and analytics. Expertise in structuring, cleansing, and preparing large, complex datasets for analysis and modeling. Expertise in building robust and scalable data pipelines using structured and unstructured data as well as batch data and real time streaming data services. Expertise with SQL and other data querying techniques  Scala, Shell . Experience with Apache Spark and Presto. Expertise in data warehousing solutions and techniques. Experience with Amazon Web Services  Redshift, S3, EC2, EMR, etc.   is required. Experience with Adobe Analytics is a plus. Experience with statistical programming languages, such as Python or R is a plus. Demonstrated ability to work across product, engineering, and analytics teams to evaluate new ideas, discuss technical concepts, create scalable designs, and make tradeoffs to remove roadblocks. Strong written and verbal communication skills.  Can communicate the results of your work clearly to your audience. Masters in quantitative or technical field  statistics, mathematics, computer sciences, etc.   is a plus. Must have the legal right to work in the United States  Minimum of 7+ years of proven technical experience in data engineering, data warehousing, and analytics. Expertise in structuring, cleansing, and preparing large, complex datasets for analysis and modeling. Expertise in building robust and scalable data pipelines using structured and unstructured data as well as batch data and real time streaming data services. Expertise with SQL and other data querying techniques  Scala, Shell . Experience with Apache Spark and Presto. Expertise in data warehousing solutions and techniques. Experience with Amazon Web Services  Redshift, S3, EC2, EMR, etc.   is required. Experience with Adobe Analytics is a plus. Experience with statistical programming languages, such as Python or R is a plus. Demonstrated ability to work across product, engineering, and analytics teams to evaluate new ideas, discuss technical concepts, create scalable designs, and make tradeoffs to remove roadblocks. Strong written and verbal communication skills.  Can communicate the results of your work clearly to your audience. Masters in quantitative or technical field  statistics, mathematics, computer sciences, etc.   is a plus. Must have the legal right to work in the United States",minimum proven technical data engineering data warehousing analytics expertise structuring cleansing preparing complex datasets analysis modeling expertise building robust scalable data pipelines structured unstructured data well batch data real time streaming data services expertise sql data querying techniques scala shell apache spark presto expertise data warehousing solutions techniques amazon web services redshift ec emr adobe analytics plus statistical programming languages python r plus demonstrated across product engineering analytics teams evaluate ideas discuss technical concepts create scalable designs make tradeoffs remove roadblocks written verbal communication communicate results clearly audience masters quantitative technical statistics mathematics computer sciences plus must legal right united states minimum proven technical data engineering data warehousing analytics expertise structuring cleansing preparing complex datasets analysis modeling expertise building robust scalable data pipelines structured unstructured data well batch data real time streaming data services expertise sql data querying techniques scala shell apache spark presto expertise data warehousing solutions techniques amazon web services redshift ec emr adobe analytics plus statistical programming languages python r plus demonstrated across product engineering analytics teams evaluate ideas discuss technical concepts create scalable designs make tradeoffs remove roadblocks written verbal communication communicate results clearly audience masters quantitative technical statistics mathematics computer sciences plus must legal right united states,minimum proven technical data engineering warehousing analytics expertise structuring cleansing preparing complex datasets analysis modeling building robust scalable pipelines structured unstructured well batch real time streaming services sql querying techniques scala shell apache spark presto solutions amazon web redshift ec emr adobe plus statistical programming languages python r demonstrated across product teams evaluate ideas discuss concepts create designs make tradeoffs remove roadblocks written verbal communication communicate results clearly audience masters quantitative statistics mathematics computer sciences must legal right united states
388,"Bachelorâs Degree in computer science, engineering, mathematics, MIS or similar field.   10 years in technology roles.   Must have experience with the following technologies   C   ASP. net  T-SQL  HTML/CSS  JavaScript  Nodejs  Demonstrated analytical skills  Demonstrated problem solving skills  Promotes information sharing  Ability to work within tight timeframes and meet strict deadlines.   Possesses strong technical Aptitude.      ",bachelors degree computer science engineering mathematics mis similar technology roles must following technologies c asp net tsql htmlcss javascript nodejs demonstrated analytical demonstrated problem solving promotes information sharing within tight timeframes meet strict deadlines possesses technical aptitude,bachelors degree computer science engineering mathematics mis similar technology roles must following technologies c asp net tsql htmlcss javascript nodejs demonstrated analytical problem solving promotes information sharing within tight timeframes meet strict deadlines possesses technical aptitude
389,"     4+ years of experience in design, develop, and maintain ETL solutions that support building SAP Concurâs AWS Enterprise Data Warehouse environment  EDW  in the cloud.  Experience with AWS code  glue, EMR, python, pyspark and redshift  to support dimensions and data warehouse tables Knowledge of scripting languages  Python, Java, Shell, Unix, etc  Experience with troubleshooting and maintaining ETL jobs for production and non-production support needs Experience with performance tune ETL processing to meet business requirements and performance expectations Demonstrated strength in data modeling, ETL development, and data warehousing Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations Excellent verbal/written communication & data presentation skills, including experience communicating to both business and technical teams Ability to work effectively with both technical and non-technical staff Knowledge of data management fundamentals and data storage principles Knowledge of distributed systems as it pertains to data storage and computing Must be comfortable with ambiguity and fast change with an ability to adapt quickly and easily.  Ability to analyze complex problems and move them to resolution.  Be aware of, and comply with, all corporate policies. ",design develop maintain etl solutions support building sap concurs aws enterprise data warehouse edw cloud aws code glue emr python pyspark redshift support dimensions data warehouse tables scripting languages python java shell unix troubleshooting maintaining etl jobs production nonproduction support needs performance tune etl processing meet business performance expectations demonstrated strength data modeling etl development data warehousing proven success communicating users technical teams senior management collect describe data modeling decisions data engineering strategy software engineering best practices across development lifecycle agile methodologies coding standards code reviews source management build processes testing operations verbalwritten communication data presentation communicating business technical teams effectively technical nontechnical staff data management fundamentals data storage principles distributed systems pertains data storage computing must comfortable ambiguity fast change adapt quickly easily analyze complex problems move resolution aware comply corporate policies,design develop maintain etl solutions support building sap concurs aws enterprise data warehouse edw cloud code glue emr python pyspark redshift dimensions tables scripting languages java shell unix troubleshooting maintaining jobs production nonproduction needs performance tune processing meet business expectations demonstrated strength modeling development warehousing proven success communicating users technical teams senior management collect describe decisions engineering strategy software best practices across lifecycle agile methodologies coding standards reviews source build processes testing operations verbalwritten communication presentation effectively nontechnical staff fundamentals storage principles distributed systems pertains computing must comfortable ambiguity fast change adapt quickly easily analyze complex problems move resolution aware comply corporate policies
390, BS degree in CS or related engineering field Excellent communication and collaboration skills Passion for quality with strong customer empathy and focus Strong intellectual curiosity and passion about learning new technologies    ,bs degree cs engineering communication collaboration passion customer empathy focus intellectual curiosity passion technologies,bs degree cs engineering communication collaboration passion customer empathy focus intellectual curiosity technologies
391, Hadoop/MapR Certification    B. S.  in Computer Science  or similar  ,hadoopmapr certification b computer science similar,hadoopmapr certification b computer science similar
392,"   Build large-scale batch and real-time data pipelines with data processing frameworks by leveraging Google Cloud Platform, AWS, and Python data science tools.  Help drive optimization, testing and tooling to improve data quality.  Collaborate with other software engineers, ML experts and business stakeholders.  And the opportunity to learn and lead every single day.  Work in multi-functional and agile teams to continuously experiment, iterate and deliver on new product objectives to meet customers needs.   ",build largescale batch realtime data pipelines data processing frameworks leveraging google cloud platform aws python data science tools help drive optimization testing tooling improve data collaborate software engineers ml experts business stakeholders opportunity learn lead every single day multifunctional agile teams continuously experiment iterate deliver product objectives meet customers needs,build largescale batch realtime data pipelines processing frameworks leveraging google cloud platform aws python science tools help drive optimization testing tooling improve collaborate software engineers ml experts business stakeholders opportunity learn lead every single day multifunctional agile teams continuously experiment iterate deliver product objectives meet customers needs
393,"Bachelorâs degree in computer science, mathematics, statistics, economics, or other quantitative field 3+ years of relevant work experience in a role requiring application of analytic skills to integrate data into operational/business planning or advanced degree Strong experience with ETL development, data modeling, data warehousing, MySQL, and databases in a business environment with large-scale, complex datasets Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management as required.  Experience in gathering requirements and formulating business metrics for reporting.  Experienced working in a fast-paced, high-tech environment and comfortable navigating conflicting priorities and ambiguous problems Strong track record in converting data analysis into tangible and significant real-world changes.  Strong grasp of quantitative data analysis and statistics.  Python scripting experience AWS or Azure production development and managment The ability to exclaim Woot!   Design, develop and maintain scalable, automated, user-friendly systems, reports, dashboards, etc.  that will support our analytical and business needs Write SQL code to retrieve and analyze data from database tables  ex.  Redshift, MySQL, DBs , and learn and understand a broad range of Amazonâs data resources and know how, when, and which to use and which not to use Develop queries and visualizations for ad-hoc requests and projects, as well as ongoing reporting Create pipelines for automated Use analytical and statistical rigor to solve complex problems and drive business decisions.  Develop Machine Learning models Write scripts in Python for data processing   ",bachelors degree computer science mathematics statistics economics quantitative relevant role requiring application analytic integrate data operationalbusiness planning advanced degree etl development data modeling data warehousing mysql databases business largescale complex datasets advanced draw insights data clearly communicate stakeholders senior management gathering formulating business metrics reporting experienced fastpaced hightech comfortable navigating conflicting priorities ambiguous problems track record converting data analysis tangible significant realworld changes grasp quantitative data analysis statistics python scripting aws azure production development managment exclaim woot design develop maintain scalable automated userfriendly systems reports dashboards support analytical business needs write sql code retrieve analyze data database tables ex redshift mysql dbs learn understand broad range amazons data resources know use use develop queries visualizations adhoc requests projects well ongoing reporting create pipelines automated use analytical statistical rigor solve complex problems drive business decisions develop machine models write scripts python data processing,bachelors degree computer science mathematics statistics economics quantitative relevant role requiring application analytic integrate data operationalbusiness planning advanced etl development modeling warehousing mysql databases business largescale complex datasets draw insights clearly communicate stakeholders senior management gathering formulating metrics reporting experienced fastpaced hightech comfortable navigating conflicting priorities ambiguous problems track record converting analysis tangible significant realworld changes grasp python scripting aws azure production managment exclaim woot design develop maintain scalable automated userfriendly systems reports dashboards support analytical needs write sql code retrieve analyze database tables ex redshift dbs learn understand broad range amazons resources know use queries visualizations adhoc requests projects well ongoing create pipelines statistical rigor solve drive decisions machine models scripts processing
394,"Cloud experience is must  AWS-S3, Snowflake, Redshift, Big Query etc.   Experience with open source such as Hadoop, Spark, Kafka, Druid, Pilosa and Yarn/Kubernetes.  Experience in SQL, ETL Tools is required Are passionate about data, technology, & creative innovation.  Experience in working with Data Scientists to operationalize machine learning models.     ",cloud must awss snowflake redshift big query open source hadoop spark kafka druid pilosa yarnkubernetes sql etl tools passionate data technology creative innovation data scientists operationalize machine models,cloud must awss snowflake redshift big query open source hadoop spark kafka druid pilosa yarnkubernetes sql etl tools passionate data technology creative innovation scientists operationalize machine models
395,"5+ years of relevant experience regarding designing and implementing software systemsSoftware engineering skill in one or more high level languages  C , C++, Java, Python   5+ years of relevant experience regarding designing and implementing software systemsSoftware engineering skill in one or more high level languages  C , C++, Java, Python   ",relevant regarding designing implementing software systemssoftware engineering skill one level languages c c java python relevant regarding designing implementing software systemssoftware engineering skill one level languages c c java python,relevant regarding designing implementing software systemssoftware engineering skill one level languages c java python
396," BS or MS Degree in Computer Science or Data Science.  Experience developing with cloud-based technologies, including relational databases, data warehouse, big data  i. e.  Hadoop, Spark , orchestration/data pipeline tools.  Experience with telemetry and data mining eg  Azure Data Lake, Kusto, Hadoop and related big data systems Software Programming experience with C  Knowledge of an analysis tools such as R and Python Proficiency in Power BI Experience with Agile software development using the scrum methodology.    Lead the development of our data pipeline and reporting infrastructure including developing solutions for data collection, management and usage.  Help strategize and extend this system to handle Data Science models in platforms like Azure ML and/or DataBricks Working on a system that speaks natively to various data platforms, enabling individual users to rapidly explore their data and author insightful visualizations.  Partner with our extended team of PMs, service engineers, support delivery managers and engineers to ensure reporting requirements, delivery plans, engineering execution, risks and issues and support scenarios are well-understood and communicated.  Become a SME on multiple business processes and how related solutions are expressed in our services and technology and mentor other engineers.   ",bs ms degree computer science data science developing cloudbased technologies relational databases data warehouse big data e hadoop spark orchestrationdata pipeline tools telemetry data mining eg azure data lake kusto hadoop big data systems software programming c analysis tools r python proficiency power bi agile software development scrum methodology lead development data pipeline reporting infrastructure developing solutions data collection management usage help strategize extend handle data science models platforms like azure ml andor databricks speaks natively various data platforms enabling individual users rapidly explore data author insightful visualizations partner extended team pms service engineers support delivery managers engineers reporting delivery plans engineering execution risks issues support scenarios wellunderstood communicated become sme multiple business processes solutions expressed services technology mentor engineers,bs ms degree computer science data developing cloudbased technologies relational databases warehouse big e hadoop spark orchestrationdata pipeline tools telemetry mining eg azure lake kusto systems software programming c analysis r python proficiency power bi agile development scrum methodology lead reporting infrastructure solutions collection management usage help strategize extend handle models platforms like ml andor databricks speaks natively various enabling individual users rapidly explore author insightful visualizations partner extended team pms service engineers support delivery managers plans engineering execution risks issues scenarios wellunderstood communicated become sme multiple business processes expressed services technology mentor
397," At least 5-years of experience as a software development or data engineer At least 3-years of experience with SQL, Python and/or other data collection tools & reporting Experience with Pyspark or Scala is necessity  Databricks or Spark .  Advanced knowledge and skills with Azure, or similar cloud platforms.  Excellent collaboration skills to work on a team as well as independently  be self-reliant and resourceful  Excellent organization skills and able to multi-task and detailed oriented Excellent verbal and written communication skills  must be able to write clear and concise emails for any audience, etc.      ",least software development data engineer least sql python andor data collection tools reporting pyspark scala necessity databricks spark advanced azure similar cloud platforms collaboration team well independently selfreliant resourceful organization able multitask detailed oriented verbal written communication must able write clear concise emails audience,least software development data engineer sql python andor collection tools reporting pyspark scala necessity databricks spark advanced azure similar cloud platforms collaboration team well independently selfreliant resourceful organization able multitask detailed oriented verbal written communication must write clear concise emails audience
398," 8+ years of professional experience as a Data Engineer/BI Developer 4+ years of professional experience implementing and using Power BI Expertise in Microsoft Business Intelligence Stack  SSRS, SSAS, SSIS  and T-SQL using SQL Server , SSIS Well versed experience with SQL, Python and/or other data collection tools & reporting Extensive experience with PySpark or Scala is necessity  Databricks or Spark .  Advanced knowledge and skills with Azure, or similar cloud platforms.  Experience required with DAX/MDX Excellent collaboration skills to work on a team as well as independently  be self-reliant and resourceful  Excellent organization skills and able to multi-task and detailed oriented Excellent verbal and written communication skills  must be able to write clear and concise emails for any audience, etc.      ",professional data engineerbi developer professional implementing power bi expertise microsoft business intelligence stack ssrs ssas ssis tsql sql server ssis well versed sql python andor data collection tools reporting extensive pyspark scala necessity databricks spark advanced azure similar cloud platforms daxmdx collaboration team well independently selfreliant resourceful organization able multitask detailed oriented verbal written communication must able write clear concise emails audience,professional data engineerbi developer implementing power bi expertise microsoft business intelligence stack ssrs ssas ssis tsql sql server well versed python andor collection tools reporting extensive pyspark scala necessity databricks spark advanced azure similar cloud platforms daxmdx collaboration team independently selfreliant resourceful organization able multitask detailed oriented verbal written communication must write clear concise emails audience
399,"   Create and maintain optimal data pipeline architecture for the Run Research Lab  Lead and drive the re-structuring of the current data architecture, development and implementation of new data management projects & capabilities, data applications and data cleansing.   Collaborate with appropriate data owners and key stakeholders including Research, Assessment, Run Sights and Product Creation to identify and map data from the source environment to the target data environment Clean, prepare and optimize data at scale for ingestion and consumption including interfaces between Brooks and third-party systems to enable real time data consumption and preparation for analysis  Identify data quality gaps and work with data owners to develop solutions and close gaps.  Participate in on-going service delivery, including documentation and ownership of relevant change control requests  including evaluation, test, implementation, and verification .  Write code or use specialized development tools to create product features, enhance and/or customize software components  Anticipate, identify and solve issues concerning data management in the Lab to improve data quality.  Troubleshoot data issues and perform root cause analysis to proactively resolve product and operational issues  Build continuous integration, test-driven development and production deployment frameworks.  Drive collaborative reviews of design, code, test plans and data set implementation in support of maintaining data engineering standards.  Test developed programs and integration of data from various sources.  Liaise with enterprise data teams to ensure that development adheres to organizational architecture guidelines.  Participate in key architectural and technical decisions as they apply to the Run Research Lab Coordinate and conduct application testing  new support packages, releases, functionality and customizing  in close cooperation with the technology team.  Engage system owners to filter, size and prioritize business requests and drive towards appropriate decision points.  Establish consistent technical architecture & contribute to development policies, standards and conventions Maintain expert knowledge of development tools, technologies and related delivery methods.    Bachelorâs degree in computer science, statistics or applicable engineering fields with a focus in biomechanics and a research environment a plus.  3+ yearsâ experience with data management tools and industry standard relational database systems preferably in the lab based setting.   An expert in database technologies  SQL, Big Data frameworks  Hadoop, Spark , advanced data modelling, cloud platforms  AWS, Azure  as well as real-time  Kafka  and batch data integration frameworks Significant experience in writing programs to analyze biomechanical data is strongly preferred  matlab, visual 3D, Labview, ATL, Python, Jave, C/C++  Advanced knowledge and experience in use of biomechanics systems for analyzing running/walking gait  3D mocap systems  Motion Analysis, Vicon, Qualysis , Visual 3D, plantar pressure systems  Novel  Experience in algorithms, especially in the field of AI and machine learning Experienced in Agile/ Scrum methodologies and collaboration with cross functional teams Strong project management and analytical skills Ability to work cross functionally in a fast paced, dynamic environment Curious and open minded; always open for a challenge, inventive, creative.  Ability to challenge the status quo â always looking at improving our products and processes while also displaying a willingness to dive into the details.  Unwavering demonstration of Brooksâ corporate values  Serve People, Lead Thought, Compete as a Team, Have Integrity, Be Active, Have Fun! A passion for the running enthusiast and active lifestyle Travel 5% of the time",create maintain optimal data pipeline architecture run research lab lead drive restructuring current data architecture development implementation data management projects capabilities data applications data cleansing collaborate appropriate data owners key stakeholders research assessment run sights product creation identify map data source target data clean prepare optimize data scale ingestion consumption interfaces brooks thirdparty systems enable real time data consumption preparation analysis identify data gaps data owners develop solutions close gaps participate ongoing service delivery documentation ownership relevant change control requests evaluation test implementation verification write code use specialized development tools create product features enhance andor customize software components anticipate identify solve issues concerning data management lab improve data troubleshoot data issues perform root cause analysis proactively resolve product operational issues build continuous integration testdriven development production deployment frameworks drive collaborative reviews design code test plans data set implementation support maintaining data engineering standards test developed programs integration data various sources liaise enterprise data teams development adheres organizational architecture guidelines participate key architectural technical decisions apply run research lab coordinate conduct application testing support packages releases functionality customizing close cooperation technology team engage owners filter size prioritize business requests drive towards appropriate decision points establish consistent technical architecture contribute development policies standards conventions maintain expert development tools technologies delivery methods bachelors degree computer science statistics applicable engineering fields focus biomechanics research plus data management tools industry standard relational database systems preferably lab based setting expert database technologies sql big data frameworks hadoop spark advanced data modelling cloud platforms aws azure well realtime kafka batch data integration frameworks significant writing programs analyze biomechanical data strongly matlab visual labview atl python jave cc advanced use biomechanics systems analyzing runningwalking gait mocap systems motion analysis vicon qualysis visual plantar pressure systems novel algorithms especially ai machine experienced agile scrum methodologies collaboration cross functional teams project management analytical cross functionally fast paced dynamic curious open minded always open challenge inventive creative challenge status quo always looking improving products processes also displaying willingness dive details unwavering demonstration brooks corporate values serve people lead thought compete team integrity active fun passion running enthusiast active lifestyle travel time,create maintain optimal data pipeline architecture run research lab lead drive restructuring current development implementation management projects capabilities applications cleansing collaborate appropriate owners key stakeholders assessment sights product creation identify map source target clean prepare optimize scale ingestion consumption interfaces brooks thirdparty systems enable real time preparation analysis gaps develop solutions close participate ongoing service delivery documentation ownership relevant change control requests evaluation test verification write code use specialized tools features enhance andor customize software components anticipate solve issues concerning improve troubleshoot perform root cause proactively resolve operational build continuous integration testdriven production deployment frameworks collaborative reviews design plans set support maintaining engineering standards developed programs various sources liaise enterprise teams adheres organizational guidelines architectural technical decisions apply coordinate conduct application testing packages releases functionality customizing cooperation technology team engage filter size prioritize business towards decision points establish consistent contribute policies conventions expert technologies methods bachelors degree computer science statistics applicable fields focus biomechanics plus industry standard relational database preferably based setting sql big hadoop spark advanced modelling cloud platforms aws azure well realtime kafka batch significant writing analyze biomechanical strongly matlab visual labview atl python jave cc analyzing runningwalking gait mocap motion vicon qualysis plantar pressure novel algorithms especially ai machine experienced agile scrum methodologies collaboration cross functional project analytical functionally fast paced dynamic curious open minded always challenge inventive creative status quo looking improving products processes also displaying willingness dive details unwavering demonstration corporate values serve people thought compete integrity active fun passion running enthusiast lifestyle travel
400,"3+ years experience data engineering development using scala/python Experience designing and implementing large, scalable services Experience with automating & deploying micro-services Experience working with third-party APIs and creating robust APIs Know when and where to optimize and cache Comfortable working with large data sets and distributed systems  datastores, frameworks  Experience with streaming / realtime technologies  Kinesis, Kafka, Spark streaming etc  Deep understanding of MySQL, Postgres, Redshift, relational databases etc.  Understand when to use NOSQL databases / other distributed ""big data"" technologies Experience with AWS technologies Startup work experience a major plus!    ",data engineering development scalapython designing implementing scalable services automating deploying microservices thirdparty apis creating robust apis know optimize cache comfortable data sets distributed systems datastores frameworks streaming realtime technologies kinesis kafka spark streaming deep understanding mysql postgres redshift relational databases understand use nosql databases distributed big data technologies aws technologies startup major plus,data engineering development scalapython designing implementing scalable services automating deploying microservices thirdparty apis creating robust know optimize cache comfortable sets distributed systems datastores frameworks streaming realtime technologies kinesis kafka spark deep understanding mysql postgres redshift relational databases understand use nosql big aws startup major plus
401,"An educational background that has facilitated your technology career-surprise us! 10+ years of professional software experience Passion for building high-quality software and systems that deliver business value Passion about system design, with interests and experience across the architecture from virtual machine and language internals to distributed systems performance and analysis Fluency with the fundamentals of algorithms and data structures   Architect and lead the implementation of distributed systems solutions to data engineering problems.  Partner with management in hiring and team building Analyze business needs, and design and build high-quality solutions Balance resources, requirements, and complexity Collaborate with customer representatives, product and project management teams Evangelize technologies, solutions, and best practices Contribute new ideas to a larger community of high-caliber professionals   ",educational background facilitated technology careersurprise us professional software passion building highquality software systems deliver business value passion design interests across architecture virtual machine language internals distributed systems performance analysis fluency fundamentals algorithms data structures architect lead implementation distributed systems solutions data engineering problems partner management hiring team building analyze business needs design build highquality solutions balance resources complexity collaborate customer representatives product project management teams evangelize technologies solutions best practices contribute ideas larger community highcaliber professionals,educational background facilitated technology careersurprise us professional software passion building highquality systems deliver business value design interests across architecture virtual machine language internals distributed performance analysis fluency fundamentals algorithms data structures architect lead implementation solutions engineering problems partner management hiring team analyze needs build balance resources complexity collaborate customer representatives product project teams evangelize technologies best practices contribute ideas larger community highcaliber professionals
402,"  Experience with performing system backups, scheduling tape rotations, and using Veeam backup solution Understanding of the operation and management of Ciscoâs Nexus switches and patching network devices Experience and understanding of UPS systems, HVAC systems, and other physical infrastructure associated with temperature control and power systems   ",performing backups scheduling tape rotations veeam backup solution understanding operation management ciscos nexus switches patching network devices understanding ups systems hvac systems physical infrastructure associated temperature control power systems,performing backups scheduling tape rotations veeam backup solution understanding operation management ciscos nexus switches patching network devices ups systems hvac physical infrastructure associated temperature control power
403, Enrolled in a college or university Returning for at least one more term following the internship Available to work full-time hours for 10-12 weeks at the indicated office during the summer Authorized to work in the U. S.      ,enrolled college university returning least one term following internship available fulltime hours weeks indicated office summer authorized u,enrolled college university returning least one term following internship available fulltime hours weeks indicated office summer authorized u
404,"   Design strategies for database systems as well as set standards for operations, programming and security.  Develop physical modeling standards and strategies to support OLTP and Data Warehouse domains.  Ensures physical database features and capabilities are incorporated into data model designs to optimize performance.  Designs deliverables that are defined with appropriate database platform and security context.  Perform problem-solving of application issues and production errors, including high level critical production issues that require immediate attention.  Design and code a high volume of SQL Queries, stored procedures, and SSIS packages Monitor and perform performance tuning on stored procedures and ETL jobs Monitor and analyze SQL Server production metrics    Strong knowledge and experience in data modeling, relational database design, optimization for performance, T-SQL, stored procedures, SSIS packages Extensive knowledge of SQL Server including the writing of complex stored procedures, functions and SSIS packages as well as basic knowledge of database administration Strong understanding and experience of database development methodologies  Agile and Scrum  Expert level knowledge of SQL administration, engineering, and monitoring tools Demonstrated expertise in performance tuning including both query and server optimization Expertise in understanding complex business needs, analyzing, designing and developing solutions Advanced knowledge of relational database design, testing and implementation Advanced communication and professional skills and the ability to establish relationships across business units Ability to participate in an on-call support rotation and provide non-standard work hour support Extensive SQL Server performance tuning and troubleshooting experience Hands-on development experience in SQL Server 2012/2016 Experience with SSIS Financial technology domain knowledge would be a plus Experience with . NET, MVC, C  HTML5 CSS3 AJAX jQuery IIS and Java script would be a plus Computer Science Degree Highly Desired ",design strategies database systems well set standards operations programming security develop physical modeling standards strategies support oltp data warehouse domains ensures physical database features capabilities incorporated data model designs optimize performance designs deliverables defined appropriate database platform security context perform problemsolving application issues production errors level critical production issues require immediate attention design code volume sql queries stored procedures ssis packages monitor perform performance tuning stored procedures etl jobs monitor analyze sql server production metrics data modeling relational database design optimization performance tsql stored procedures ssis packages extensive sql server writing complex stored procedures functions ssis packages well basic database administration understanding database development methodologies agile scrum expert level sql administration engineering monitoring tools demonstrated expertise performance tuning query server optimization expertise understanding complex business needs analyzing designing developing solutions advanced relational database design testing implementation advanced communication professional establish relationships across business units participate oncall support rotation nonstandard hour support extensive sql server performance tuning troubleshooting handson development sql server ssis financial technology domain would plus net mvc c html css ajax jquery iis java script would plus computer science degree highly desired,design strategies database systems well set standards operations programming security develop physical modeling support oltp data warehouse domains ensures features capabilities incorporated model designs optimize performance deliverables defined appropriate platform context perform problemsolving application issues production errors level critical require immediate attention code volume sql queries stored procedures ssis packages monitor tuning etl jobs analyze server metrics relational optimization tsql extensive writing complex functions basic administration understanding development methodologies agile scrum expert engineering monitoring tools demonstrated expertise query business needs analyzing designing developing solutions advanced testing implementation communication professional establish relationships across units participate oncall rotation nonstandard hour troubleshooting handson financial technology domain would plus net mvc c html css ajax jquery iis java script computer science degree highly desired
405,"Experience working with cloud as infrastructure  Google/AWS/Azure  or other cloud platform based on IaaS and PaaS Solutions 2+ years working with and demonstrating a solid understanding of open source data management components and services including Hadoop, Spark, Flume, Kafka and Hive Experience with Google BigQuery, App Engine, Dataflow Proficiency in SQL, R, Python, Java, Java Script and Web Services, and experience in visualization tools like Tableau, Looker, MicroStrategy and PowerBI NoSQL DB  Dynamo DB/Mongo DB knowledge is helpful  Experience with Code Management/CICD tools like GITHub, Jenkins is useful Prior experience with automation is helpful Prior experience working in a Linux/Unix environment is helpful Prior experience working in an enterprise technology environment is helpful Understanding of AI/ML services  TensorFlow, Keras, Rekognition, and/or PyTorch Strong written and verbal communication skills  Proficient understanding of distributed computing principles Experience developing data storage processing and streaming flows using technologies like Spark, HDFS, Yarn and Hadoop Experience with various messaging systems, such as Kafka or RabbitMQ Good knowledge of Big Data querying tools, such as Pig or Hive Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with integration of data from multiple data sources Ability to solve any ongoing issues with operating the cluster Experience with scripting languages and automation of processes Proven track record of development accomplishments in highly collaborative environment Translate complex functional and technical requirements into detailed design Be passionate about solving customer problems and develop solutions that result in a passionate customer/community following  ",cloud infrastructure googleawsazure cloud platform based iaas paas solutions demonstrating solid understanding open source data management components services hadoop spark flume kafka hive google bigquery app engine dataflow proficiency sql r python java java script web services visualization tools like tableau looker microstrategy powerbi nosql db dynamo dbmongo db helpful code managementcicd tools like github jenkins useful prior automation helpful prior linuxunix helpful prior enterprise technology helpful understanding aiml services tensorflow keras rekognition andor pytorch written verbal communication proficient understanding distributed computing principles developing data storage processing streaming flows technologies like spark hdfs yarn hadoop various messaging systems kafka rabbitmq good big data querying tools pig hive good understanding lambda architecture along advantages drawbacks integration data multiple data sources solve ongoing issues operating cluster scripting languages automation processes proven track record development accomplishments highly collaborative translate complex functional technical detailed design passionate solving customer problems develop solutions result passionate customercommunity following,cloud infrastructure googleawsazure platform based iaas paas solutions demonstrating solid understanding open source data management components services hadoop spark flume kafka hive google bigquery app engine dataflow proficiency sql r python java script web visualization tools like tableau looker microstrategy powerbi nosql db dynamo dbmongo helpful code managementcicd github jenkins useful prior automation linuxunix enterprise technology aiml tensorflow keras rekognition andor pytorch written verbal communication proficient distributed computing principles developing storage processing streaming flows technologies hdfs yarn various messaging systems rabbitmq good big querying pig lambda architecture along advantages drawbacks integration multiple sources solve ongoing issues operating cluster scripting languages processes proven track record development accomplishments highly collaborative translate complex functional technical detailed design passionate solving customer problems develop result customercommunity following
406, 5+ years of experience in core JAVA and SQL    ,core java sql,core java sql
407," Client services or programming experience with responsibility for scoping and delivering technical projects Experience with full-stack web application development using modern Javascript frameworks  NodeJS  Demonstrated experience building, testing, and troubleshooting web applications built around web services and RESTful APIs Knowledge in database architecture, system environments, SQL, and other data integration tools Experience with UNIX / Linux command line Bonus points for experience with any of the following  Go, Java, Node. js, React, Bigtable, MongoDB, PostgreSQL, ElasticSearch, AWS, Docker Experience working in Agile development environments Project management expertise with an eye for detail and organized thought process Good speaker with the ability to build rapport with key stakeholders inside and outside the organization Experience in a fast-paced, high growth environment where decision making and swift execution command a high premium    Work Experience  3+ years of experience with client services or engineering, preferably in a B2B SaaS company Education  An undergraduate degree or equivalent work experience Skills  Proficiency in web services, APIs, HTTP, JavaScript, ETL, SQL, Linux/Unix.  Comfortable in customer facing positions.  ",client services programming responsibility scoping delivering technical projects fullstack web application development modern javascript frameworks nodejs demonstrated building testing troubleshooting web applications built around web services restful apis database architecture environments sql data integration tools unix linux command line bonus points following go java node js react bigtable mongodb postgresql elasticsearch aws docker agile development environments project management expertise eye detail organized thought process good speaker build rapport key stakeholders inside outside organization fastpaced growth decision making swift execution command premium client services engineering preferably bb saas company education undergraduate degree proficiency web services apis http javascript etl sql linuxunix comfortable customer facing positions,client services programming responsibility scoping delivering technical projects fullstack web application development modern javascript frameworks nodejs demonstrated building testing troubleshooting applications built around restful apis database architecture environments sql data integration tools unix linux command line bonus points following go java node js react bigtable mongodb postgresql elasticsearch aws docker agile project management expertise eye detail organized thought process good speaker build rapport key stakeholders inside outside organization fastpaced growth decision making swift execution premium engineering preferably bb saas company education undergraduate degree proficiency http etl linuxunix comfortable customer facing positions
408," Experience in Java, Kotlin, JVM based languages  Scala, Clojure, Groovy, JRuby  SQL experience  Postgresql is a plus  Distributed Computing experience Excellent understanding of computer science fundamentals, data structures, and algorithms Expertise required in object-oriented design methodology and large scale application development in an object oriented language Understanding of distributed systems and service oriented architecture Large scale data processing experience Functional programming experience RabbitMQ    Build APIs and design documents Work closely with various business partners  firmware team, UI/UX  Architecture design Large scale data processing to understand user behavior and help define new product features   Bachelor's degree in Computer Science or related technical discipline 7+ years of industry experience",java kotlin jvm based languages scala clojure groovy jruby sql postgresql plus distributed computing understanding computer science fundamentals data structures algorithms expertise objectoriented design methodology scale application development object oriented language understanding distributed systems service oriented architecture scale data processing functional programming rabbitmq build apis design documents closely various business partners firmware team uiux architecture design scale data processing understand user behavior help define product features bachelors degree computer science technical discipline industry,java kotlin jvm based languages scala clojure groovy jruby sql postgresql plus distributed computing understanding computer science fundamentals data structures algorithms expertise objectoriented design methodology scale application development object oriented language systems service architecture processing functional programming rabbitmq build apis documents closely various business partners firmware team uiux understand user behavior help define product features bachelors degree technical discipline industry
409,Experience analyzing Big Data Experience utilizing Unix/Linux systems for installation and support Experience researching a collection of transit data to identify discrepancies Experience working in the public sector    ,analyzing big data utilizing unixlinux systems installation support researching collection transit data identify discrepancies public sector,analyzing big data utilizing unixlinux systems installation support researching collection transit identify discrepancies public sector
410,"   Define and calculate metrics to be analyzed Develop tools for electronic data collection Query, merge and extract data across sources Maintain data refresh and update pipelines  ",define calculate metrics analyzed develop tools electronic data collection query merge extract data across sources maintain data refresh update pipelines,define calculate metrics analyzed develop tools electronic data collection query merge extract across sources maintain refresh update pipelines
411," Extensive knowledge of SQL Server including the writing of complex stored procedures, functions and SSIS packages as well as basic knowledge of database administration Strong understanding and experience of database development methodologies  Agile and Scrum  Expert level knowledge of SQL administration, engineering, and monitoring tools Demonstrated expertise in performance tuning including both query and server optimization Strong knowledge and experience in data modeling, relational database design, optimization for performance, T-SQL, stored procedures, SSIS packages Expertise in understanding complex business needs, analyzing, designing and developing solutions Advanced knowledge of relational database design, testing and implementation Advanced communication and professional skills and the ability to establish relationships across business units Ability to participate in an on-call support rotation and provide non-standard work hour support Extensive SQL Server performance tuning and troubleshooting experience Hands-on development experience in SQL Server 2012/2016 Experience with SSIS Financial technology domain knowledge would be a plus Experience with . NET, MVC, C  HTML5 CSS3 AJAX jQuery IIS and Java script would be a plus Computer Science Degree Highly Desired   Design strategies for database systems as well as set standards for operations, programming and security.  Definition and governance of all structured and unstructured physical data models, data entities, supporting artifacts and metadata repositories.  Develops physical modeling standards and strategies.  Executes design sessions to gather requirements, review, approve, and communicate design artifacts with stakeholders.  Creates and manages physical models to support Enterprise Data Warehouse domains that include Operational Data Stores, Staging, Integrated EDW, Data Mart, Business, Reference/Master Data and Extract Transform Load  ETL  environments.  Ensures physical database features and capabilities are incorporated into data model designs to optimize performance.  Designs deliverables that are defined with appropriate database platform and security context.  Design and manage models using various Data Modeling tools.  Maintains optimal Enterprise Architecture alignment with Solution Architecture, Technical Leads, Business Analysis, Infrastructure, and PMO roles.  Design, Implementation and maintenance of database solutions, management of data access, and resolving database performance and capacity issues.  Develop, implement, administrate, and maintain policies and procedures for ensuring the security and integrity of our databases.  Perform problem-solving of application issues and production errors, including high level critical production issues that require immediate attention.  Design and code a high volume of SQL Queries, stored procedures, and SSIS packages Implement processes and procedures for the development and release of products/projects that facilitate high quality and rapid deployment Provide technical documentation as needed Monitor and perform performance tuning on stored procedures and ETL jobs Monitor and analyze SQL Server production metrics Support complex service impacting issues with little to no supervision Participate in an on-call support rotation and provide non-standard work hour support Trouble shooting and resolving data issues Perform other duties as assigned.   ",extensive sql server writing complex stored procedures functions ssis packages well basic database administration understanding database development methodologies agile scrum expert level sql administration engineering monitoring tools demonstrated expertise performance tuning query server optimization data modeling relational database design optimization performance tsql stored procedures ssis packages expertise understanding complex business needs analyzing designing developing solutions advanced relational database design testing implementation advanced communication professional establish relationships across business units participate oncall support rotation nonstandard hour support extensive sql server performance tuning troubleshooting handson development sql server ssis financial technology domain would plus net mvc c html css ajax jquery iis java script would plus computer science degree highly desired design strategies database systems well set standards operations programming security definition governance structured unstructured physical data models data entities supporting artifacts metadata repositories develops physical modeling standards strategies executes design sessions gather review approve communicate design artifacts stakeholders creates manages physical models support enterprise data warehouse domains include operational data stores staging integrated edw data mart business referencemaster data extract transform load etl environments ensures physical database features capabilities incorporated data model designs optimize performance designs deliverables defined appropriate database platform security context design manage models various data modeling tools maintains optimal enterprise architecture alignment solution architecture technical leads business analysis infrastructure pmo roles design implementation maintenance database solutions management data access resolving database performance capacity issues develop implement administrate maintain policies procedures ensuring security integrity databases perform problemsolving application issues production errors level critical production issues require immediate attention design code volume sql queries stored procedures ssis packages implement processes procedures development release productsprojects facilitate rapid deployment technical documentation needed monitor perform performance tuning stored procedures etl jobs monitor analyze sql server production metrics support complex service impacting issues little supervision participate oncall support rotation nonstandard hour support trouble shooting resolving data issues perform duties assigned,extensive sql server writing complex stored procedures functions ssis packages well basic database administration understanding development methodologies agile scrum expert level engineering monitoring tools demonstrated expertise performance tuning query optimization data modeling relational design tsql business needs analyzing designing developing solutions advanced testing implementation communication professional establish relationships across units participate oncall support rotation nonstandard hour troubleshooting handson financial technology domain would plus net mvc c html css ajax jquery iis java script computer science degree highly desired strategies systems set standards operations programming security definition governance structured unstructured physical models entities supporting artifacts metadata repositories develops executes sessions gather review approve communicate stakeholders creates manages enterprise warehouse domains include operational stores staging integrated edw mart referencemaster extract transform load etl environments ensures features capabilities incorporated model designs optimize deliverables defined appropriate platform context manage various maintains optimal architecture alignment solution technical leads analysis infrastructure pmo roles maintenance management access resolving capacity issues develop implement administrate maintain policies ensuring integrity databases perform problemsolving application production errors critical require immediate attention code volume queries processes release productsprojects facilitate rapid deployment documentation needed monitor jobs analyze metrics service impacting little supervision trouble shooting duties assigned
412,"Real passion for coding  If you have a Github profile, thatâs awesome! We would love to check it out!  Understanding of distributed systems and distributed computation Working knowledge in at least 2 of  Scala, Java, Python, or Go-Lang Working knowledge of data Apache Spark ecosystem technologies like Spark, Kafka, Hive, Presto, Oozie, Pig, Hue, Zeppelin Demonstrated working knowledge of data modeling, data-warehouse, data-mart and data-lake Unit, Integration, and Load testing Developing REST APIs Git Maven, SBT, and/or Gradle Unix/Linux Docker containers building and deployment Working experience of AWS Excellent communication and collaboration skills  Design, build and implement Hadoop/Spark batch jobs Build and optimize performance of Spark, Kafka, ELK, and whatever else makes sense for real-time pipelines Design and architect high quality data-lake, data-warehouse, and data-marts data models Enable and implement Data Science workflows and advanced machine learning algorithms Build and optimize performance of ElasticSearch cluster and relevance Build and maintain data pipelines orchestration Develop and cultivate expertise in current and new technologies and tools Collaborate with other software engineers and cross-functional teams Share new ideas with a larger community of highly experienced technologists Ability to prioritize tasks, requirements, and complexity Mentor junior data engineers on best practices BS in Computer Science or related field with 7+ years of experience ",real passion coding github profile thats awesome would love check understanding distributed systems distributed computation least scala java python golang data apache spark ecosystem technologies like spark kafka hive presto oozie pig hue zeppelin demonstrated data modeling datawarehouse datamart datalake unit integration load testing developing rest apis git maven sbt andor gradle unixlinux docker containers building deployment aws communication collaboration design build implement hadoopspark batch jobs build optimize performance spark kafka elk whatever else makes sense realtime pipelines design architect datalake datawarehouse datamarts data models enable implement data science workflows advanced machine algorithms build optimize performance elasticsearch cluster relevance build maintain data pipelines orchestration develop cultivate expertise current technologies tools collaborate software engineers crossfunctional teams share ideas larger community highly experienced technologists prioritize tasks complexity mentor junior data engineers best practices bs computer science,real passion coding github profile thats awesome would love check understanding distributed systems computation least scala java python golang data apache spark ecosystem technologies like kafka hive presto oozie pig hue zeppelin demonstrated modeling datawarehouse datamart datalake unit integration load testing developing rest apis git maven sbt andor gradle unixlinux docker containers building deployment aws communication collaboration design build implement hadoopspark batch jobs optimize performance elk whatever else makes sense realtime pipelines architect datamarts models enable science workflows advanced machine algorithms elasticsearch cluster relevance maintain orchestration develop cultivate expertise current tools collaborate software engineers crossfunctional teams share ideas larger community highly experienced technologists prioritize tasks complexity mentor junior best practices bs computer
413,"  An undergraduate or graduate degree  BS/MS/PhD  in STEM majors  science, technology, engineering, and math  comparable and 4+ yearsâ relevant experience.  Experience designing, fabricating, and testing complex data & control systems Ability to develop and review complex electrical schematics.  Knowledge of noise reduction, grounding, filtering, and other techniques necessary to ensure quality data.  Able to work in a fast-paced and intense startup environment.     ",undergraduate graduate degree bsmsphd stem majors science technology engineering math comparable relevant designing fabricating testing complex data control systems develop review complex electrical schematics noise reduction grounding filtering techniques necessary data able fastpaced intense startup,undergraduate graduate degree bsmsphd stem majors science technology engineering math comparable relevant designing fabricating testing complex data control systems develop review electrical schematics noise reduction grounding filtering techniques necessary able fastpaced intense startup
414," Experience with Electrical Engineering.   Experience with Electrical Engineering.   Gathers, defines and documents system level requirements to support flight control and mission requirements definition.  Performs and documents analysis and design of Command and Data Handling systems to validate or assess the system design or models.  Resolves technical issues and ensuring the spacecraft and system are flight worthy prior to delivery.  Performing trade studies to meet program requirements.  Performing analyses to evaluate and optimize total system performance to meet customer operation requirements.  Develops solutions and disposition of issues to assure customer satisfaction.  Designs trade studies to meet program requirements.   Experience with Electrical Engineering.  ",electrical engineering electrical engineering gathers defines documents level support flight control mission definition performs documents analysis design command data handling systems validate assess design models resolves technical issues ensuring spacecraft flight worthy prior delivery performing trade meet program performing analyses evaluate optimize total performance meet customer operation develops solutions disposition issues assure customer satisfaction designs trade meet program electrical engineering,electrical engineering gathers defines documents level support flight control mission definition performs analysis design command data handling systems validate assess models resolves technical issues ensuring spacecraft worthy prior delivery performing trade meet program analyses evaluate optimize total performance customer operation develops solutions disposition assure satisfaction designs
415,"1+ years experience preferred High School Diploma You perform all essential job functions, including walking, standing, bending, stooping, climbing, lifting and manual dexterity, with or without reasonable accommodation You are available to work days/nights/weekends/holidays, if needed and/or required You can lift heavy equipment/items up to 50 pounds   You will perform site inspections and monitor the building and IBX alarms Performs preventative maintenance on site infrastructure  e. g.  maintenance of primary infrastructures , or leads vendors You undertake repairs and corrective maintenance You operate critical infrastructure under the supervision of more senior technical staff Completion of site logs and data gathering issuing for basic permits, such as MOPs and scripts You respond to all on-site incidents and acts as required You complete routine work requests and circuit installations You provide assistance during critical maintenance activities You are able to effectively collaborate within the department and provide recommendations to peers for general maintenance activities You carry out basic infrastructure projects   ",school diploma perform essential job functions walking standing bending stooping climbing lifting manual dexterity without reasonable accommodation available daysnightsweekendsholidays needed andor lift heavy equipmentitems pounds perform site inspections monitor building ibx alarms performs preventative maintenance site infrastructure e g maintenance primary infrastructures leads vendors undertake repairs corrective maintenance operate critical infrastructure supervision senior technical staff completion site logs data gathering issuing basic permits mops scripts respond onsite incidents acts complete routine requests circuit installations assistance critical maintenance activities able effectively collaborate within department recommendations peers general maintenance activities carry basic infrastructure projects,school diploma perform essential job functions walking standing bending stooping climbing lifting manual dexterity without reasonable accommodation available daysnightsweekendsholidays needed andor lift heavy equipmentitems pounds site inspections monitor building ibx alarms performs preventative maintenance infrastructure e g primary infrastructures leads vendors undertake repairs corrective operate critical supervision senior technical staff completion logs data gathering issuing basic permits mops scripts respond onsite incidents acts complete routine requests circuit installations assistance activities able effectively collaborate within department recommendations peers general carry projects
416,"   BS in Computer Science or Related Field  MS preferred  Excellent technical, written, and verbal interpersonal communication skills 5+ years of backend engineering experience building products from ideation to launch  Deep knowledge of the AWS stack and infrastructure Athena Redshift/Lambda ECS/Docker/ECR S3 RDS Kinesis Firehose Application Load Balancing Cloudwatch and monitoring Experience in high volume data processing via batch and streaming Experience with Data Warehouse design and query issues Experience with the following languages/frameworks Python Go Shell Scripting PostgreSQL Apache Airflow  good to have  Aerospike  good to have  Terraform ",bs computer science ms technical written verbal interpersonal communication backend engineering building products ideation launch deep aws stack infrastructure athena redshiftlambda ecsdockerecr rds kinesis firehose application load balancing cloudwatch monitoring volume data processing via batch streaming data warehouse design query issues following languagesframeworks python go shell scripting postgresql apache airflow good aerospike good terraform,bs computer science ms technical written verbal interpersonal communication backend engineering building products ideation launch deep aws stack infrastructure athena redshiftlambda ecsdockerecr rds kinesis firehose application load balancing cloudwatch monitoring volume data processing via batch streaming warehouse design query issues following languagesframeworks python go shell scripting postgresql apache airflow good aerospike terraform
417,"     Experience with statistical analysis of data and variability  Ex.  Monte Carlo simulation, Bayesian network, Minitab, JMP, excel, etc.   Exposure to microfabrication processes, microelectromechanical systems  MEMS , or similar work experience.  Experience with technical documentation  Ex.  laboratory notebooks, publications, engineering reports, IQ/OQ/PQâs, Standard Operating Procedures, test protocols, etc.  ",statistical analysis data variability ex monte carlo simulation bayesian network minitab jmp excel exposure microfabrication processes microelectromechanical systems mems similar technical documentation ex laboratory notebooks publications engineering reports iqoqpqs standard operating procedures test protocols,statistical analysis data variability ex monte carlo simulation bayesian network minitab jmp excel exposure microfabrication processes microelectromechanical systems mems similar technical documentation laboratory notebooks publications engineering reports iqoqpqs standard operating procedures test protocols
418," Graduate degree in computer science.  Active involvement in the open source community.  Experience in transportation, logistics, and supply chain industries.  Experience in venture-backed startups or other hyper-growth environments.     ",graduate degree computer science active involvement open source community transportation logistics supply chain industries venturebacked startups hypergrowth environments,graduate degree computer science active involvement open source community transportation logistics supply chain industries venturebacked startups hypergrowth environments
419,"   Build and manage efficient and reliable real-time data pipelines from disparate data sources Design, develop and launch data ingestion and storage systems with high availability and reliability that can scale Drive the advancement of data infrastructure by developing and implementing underlying logic and structure for how data is set up, cleaned and stored Take an integral role in designing and implementing a data lake strategy Build and manage a universal semantic layer over the data lake Architect, launch and manage automated extraction & transformation processes Build scalable data aggregation layer from queues and batches of data for data visualization Collaborate with development teams on design, architecture, and expansion of infrastructure    BS from an accredited university in Computer Science, Engineering, Math or related field 8+ years of experience in building data pipelines, data architecture, data modeling & data governance Proficient working with SQL/NoSQL databases and MPP/columnar data warehouse solutions  Redshift, BigQuery, Snowflake etc.   Experience with AWS environments  Redshift, EC2, Data Pipeline, S3, RDS, Glue, Spectrum, Dynamodb, Lambda Proficient working with Python, bash or other scripting languages Must have experience working with large data sets ",build manage efficient reliable realtime data pipelines disparate data sources design develop launch data ingestion storage systems availability reliability scale drive advancement data infrastructure developing implementing underlying logic structure data set cleaned stored take integral role designing implementing data lake strategy build manage universal semantic layer data lake architect launch manage automated extraction transformation processes build scalable data aggregation layer queues batches data data visualization collaborate development teams design architecture expansion infrastructure bs accredited university computer science engineering math building data pipelines data architecture data modeling data governance proficient sqlnosql databases mppcolumnar data warehouse solutions redshift bigquery snowflake aws environments redshift ec data pipeline rds glue spectrum dynamodb lambda proficient python bash scripting languages must data sets,build manage efficient reliable realtime data pipelines disparate sources design develop launch ingestion storage systems availability reliability scale drive advancement infrastructure developing implementing underlying logic structure set cleaned stored take integral role designing lake strategy universal semantic layer architect automated extraction transformation processes scalable aggregation queues batches visualization collaborate development teams architecture expansion bs accredited university computer science engineering math building modeling governance proficient sqlnosql databases mppcolumnar warehouse solutions redshift bigquery snowflake aws environments ec pipeline rds glue spectrum dynamodb lambda python bash scripting languages must sets
420,"  Build data pipelines to feed Machine Learning models for language analysis and translation Work closely with Machine Learning Scientist to explore new data sources and create new capabilities and models Establish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation Collaborate to create robust large-scale, production-ready applications which leverage distributed computing and dynamic provisioning Write efficient, scalable code Build SDL Research next-generation products and platforms   ",build data pipelines feed machine models language analysis translation closely machine scientist explore data sources create capabilities models establish scalable efficient automated processes scale data analyses model development model validation model implementation collaborate create robust largescale productionready applications leverage distributed computing dynamic provisioning write efficient scalable code build sdl research nextgeneration products platforms,build data pipelines feed machine models language analysis translation closely scientist explore sources create capabilities establish scalable efficient automated processes scale analyses model development validation implementation collaborate robust largescale productionready applications leverage distributed computing dynamic provisioning write code sdl research nextgeneration products platforms
421,"  Expert proficiency in Agile development process Advanced degree in Computer Science and/or Math Excellent ability to prioritize and communication in a fast pace environment Bonus  Experience in predictive analysis and machine learning  Develop and deploy platforms, services, abstractions, and frameworks that enable Data Science to iterate and launch data models into production Design and implement database and storage solutions that fit the needs for both data transaction and reporting Mentor engineers, data scientists, and analysts on best practices and code efficiency    Strong Computer Science fundamentals 4+ years experience working on backend software using modern languages and frameworks  Java, Scala, Python  Strong database experience  MySQL, PostgreSQL, NoSQL  Strong familiarity AWS  EC2, S3, RDS, ELB  Experience with Hadoop data and underlying file systems  HDFS, HBase, Cassandra, Hive  Working knowledge in API development for mobile/web use ",expert proficiency agile development process advanced degree computer science andor math prioritize communication fast pace bonus predictive analysis machine develop deploy platforms services abstractions frameworks enable data science iterate launch data models production design implement database storage solutions fit needs data transaction reporting mentor engineers data scientists analysts best practices code efficiency computer science fundamentals backend software modern languages frameworks java scala python database mysql postgresql nosql familiarity aws ec rds elb hadoop data underlying file systems hdfs hbase cassandra hive api development mobileweb use,expert proficiency agile development process advanced degree computer science andor math prioritize communication fast pace bonus predictive analysis machine develop deploy platforms services abstractions frameworks enable data iterate launch models production design implement database storage solutions fit needs transaction reporting mentor engineers scientists analysts best practices code efficiency fundamentals backend software modern languages java scala python mysql postgresql nosql familiarity aws ec rds elb hadoop underlying file systems hdfs hbase cassandra hive api mobileweb use
422," Dedication to Carbon Lighthouseâs environmental mission Bachelors in Engineering/Science or 2 year technical school 2+ years work experience or applicable lab/prototyping experience Demonstrable field work and testing experience, such as  controls & programming, construction, environmental sampling, installation & commissioning, operations & maintenance, or similar Mechanical aptitude and ability to work with hands and tools Excellent question asker and communicator, and enjoys working on teams Excellent organization, attention to detail, and time management skills Demonstrable experience solving complex and real-world problems  Able to adapt solutions to changing field conditions and constraints Able to create a solution, fail, and iterate Exercise initiative and make sound decisions with imperfect information Ability to travel to local and national sites  ~5-20 days per month  Ability to work in industrial and mechanical spaces, with exposure to heat, noise, heights, and confined spaces Ability to lift 40 pounds, and carry 20 pounds up a ladder Must be able to work in the US, this role is not eligible for visa sponsorship    Frequent site travel  up to 75%  to mechanical and industrial spaces to assess feasibility of solutions and manage contractors Lead and participate in field-based sensor deployment, functional testing, equipment measurements, and other field data collection needs Manage gear and sensor inventory, calibration, conditions, and needs Work with a variety of Building Automation Systems  BAS/BMS  to assess functionality and limitations, set up and download trends, and identify sequences of operation Support simultaneous complex tasks and projects across multiple client sites Support Engineers in acquiring data, developing proposals, and implementing solutions Apply real world judgment in assessing risks and uncertainties in the field Cultivate effective relationships with contractors, facility engineers, and clients Develop new field processes and tools that reduce time, improve contractor relationships, or increase energy savings in delivered solutions Work in a cross-functional team environment where asking questions and challenging assumptions is necessary due to complex and multi-disciplinary problems Stay current with established safety procedures.  Exemplify CL safety culture both internally and externally.    ",dedication carbon lighthouses environmental mission bachelors engineeringscience year technical school applicable labprototyping demonstrable testing controls programming construction environmental sampling installation commissioning operations maintenance similar mechanical aptitude hands tools question asker communicator enjoys teams organization attention detail time management demonstrable solving complex realworld problems able adapt solutions changing conditions constraints able create solution fail iterate exercise initiative make sound decisions imperfect information travel local national sites days per month industrial mechanical spaces exposure heat noise heights confined spaces lift pounds carry pounds ladder must able us role eligible visa sponsorship frequent site travel mechanical industrial spaces assess feasibility solutions manage contractors lead participate fieldbased sensor deployment functional testing equipment measurements data collection needs manage gear sensor inventory calibration conditions needs variety building automation systems basbms assess functionality limitations set download trends identify sequences operation support simultaneous complex tasks projects across multiple client sites support engineers acquiring data developing proposals implementing solutions apply real world judgment assessing risks uncertainties cultivate effective relationships contractors facility engineers clients develop processes tools reduce time improve contractor relationships increase energy savings delivered solutions crossfunctional team asking questions challenging assumptions necessary due complex multidisciplinary problems stay current established safety procedures exemplify cl safety culture internally externally,dedication carbon lighthouses environmental mission bachelors engineeringscience year technical school applicable labprototyping demonstrable testing controls programming construction sampling installation commissioning operations maintenance similar mechanical aptitude hands tools question asker communicator enjoys teams organization attention detail time management solving complex realworld problems able adapt solutions changing conditions constraints create solution fail iterate exercise initiative make sound decisions imperfect information travel local national sites days per month industrial spaces exposure heat noise heights confined lift pounds carry ladder must us role eligible visa sponsorship frequent site assess feasibility manage contractors lead participate fieldbased sensor deployment functional equipment measurements data collection needs gear inventory calibration variety building automation systems basbms functionality limitations set download trends identify sequences operation support simultaneous tasks projects across multiple client engineers acquiring developing proposals implementing apply real world judgment assessing risks uncertainties cultivate effective relationships facility clients develop processes reduce improve contractor increase energy savings delivered crossfunctional team asking questions challenging assumptions necessary due multidisciplinary stay current established safety procedures exemplify cl culture internally externally
423," Degree in Computer Science or a related field or 5+ years of professional experience in developing ETL and data warehouse solutions In depth knowledge of how to write and optimize SQL statements Deep familiarity with distributed processing  Map Reduce, MPP, etc.   3+ years experience with schema design  logical and physical  Strong experience with data integration tool sets Experience with cloud solutions  AWS, Redshift, Snowflake, other  is a must Strong programming  Java/C  or related  and scripting skills  Python/Javascript  is a plus Experience with workflow management tools  Airflow, Luigi etc  helpful Ability to quickly learn complex domains Strong attention to detail with excellent analytical, problem-solving, and communication skills   Degree in Computer Science or a related field or 5+ years of professional experience in developing ETL and data warehouse solutions In depth knowledge of how to write and optimize SQL statements Deep familiarity with distributed processing  Map Reduce, MPP, etc.   3+ years experience with schema design  logical and physical  Strong experience with data integration tool sets Experience with cloud solutions  AWS, Redshift, Snowflake, other  is a must Strong programming  Java/C  or related  and scripting skills  Python/Javascript  is a plus Experience with workflow management tools  Airflow, Luigi etc  helpful Ability to quickly learn complex domains Strong attention to detail with excellent analytical, problem-solving, and communication skills   Analyze, design, develop, test and implement data warehouse and business intelligence solutions with emphasis on data quality Design highly scalable ETL processes with complex data transformations Gather and document business requirements and translate into technical architecture/design Ability to understand and document data flows in and between different systems and map data from a data source to target tables in a data warehouse Work closely with other engineers to enhance infrastructure, improve reliability and efficiency Make smart engineering and product decisions based on data analysis and collaboration Act as an in-house data expert and make recommendations regarding standards for code quality and timeliness   ",degree computer science professional developing etl data warehouse solutions depth write optimize sql statements deep familiarity distributed processing map reduce mpp schema design logical physical data integration tool sets cloud solutions aws redshift snowflake must programming javac scripting pythonjavascript plus workflow management tools airflow luigi helpful quickly learn complex domains attention detail analytical problemsolving communication degree computer science professional developing etl data warehouse solutions depth write optimize sql statements deep familiarity distributed processing map reduce mpp schema design logical physical data integration tool sets cloud solutions aws redshift snowflake must programming javac scripting pythonjavascript plus workflow management tools airflow luigi helpful quickly learn complex domains attention detail analytical problemsolving communication analyze design develop test implement data warehouse business intelligence solutions emphasis data design highly scalable etl processes complex data transformations gather document business translate technical architecturedesign understand document data flows different systems map data data source target tables data warehouse closely engineers enhance infrastructure improve reliability efficiency make smart engineering product decisions based data analysis collaboration act inhouse data expert make recommendations regarding standards code timeliness,degree computer science professional developing etl data warehouse solutions depth write optimize sql statements deep familiarity distributed processing map reduce mpp schema design logical physical integration tool sets cloud aws redshift snowflake must programming javac scripting pythonjavascript plus workflow management tools airflow luigi helpful quickly learn complex domains attention detail analytical problemsolving communication analyze develop test implement business intelligence emphasis highly scalable processes transformations gather document translate technical architecturedesign understand flows different systems source target tables closely engineers enhance infrastructure improve reliability efficiency make smart engineering product decisions based analysis collaboration act inhouse expert recommendations regarding standards code timeliness
424," Bachelorsâ Degree in related technology field or equivalent experience.    Monitors and takes correction action on Middleware technologies.  â¢ Responsible for operations activities of Database, ERP, BI, ETL, Data Warehouse.  â¢ Respond, prioritize, and escalate incidents or service requests as necessary according to EIS processes and procedures.  â¢ Develop and implement database operational procedures â¢ Perform the loading of production data and any other issues regarding data loads.  â¢ Continue to provide a pre-determined and agreed level of services to support the minimum business requirements following an interruption to the Platform operations.  â¢ Support application development functions.  â¢ Maintain service level agreements, policies and procedures.  â¢ Assist with application support functions, including coordinating vendor assisted support.  â¢ Participate in problem management task forces and perform root cause troubleshooting.  â¢ Implement monitoring solutions as defined by Senior Staff Member â¢ Implement backup solutions as defined by Senior Staff Members â¢ Ensure adequate capacity is available at all times to meet the requirements of the operations and business through capacity planning and management.  â¢ Maintains existing databases, ERP, BI, ETL, Data Warehouse based on specifications.  â¢ Establishes documentation procedures and standards and backups.  â¢ Manages security of Middleware technologies structures.  â¢ Responsible for triage of performance and tuning of Middleware technologies.  â¢ Provides on-call support 24/7.  â¢ Experience in data virtulization using Delphix.  â¢ Experience in ETL using Attunity, Talend, Informatica.     Bachelorsâ Degree in related technology field or equivalent experience.   ",bachelors degree technology monitors takes correction action middleware technologies responsible operations activities database erp bi etl data warehouse respond prioritize escalate incidents service requests necessary according eis processes procedures develop implement database operational procedures perform loading production data issues regarding data loads continue predetermined agreed level services support minimum business following interruption platform operations support application development functions maintain service level agreements policies procedures assist application support functions coordinating vendor assisted support participate problem management task forces perform root cause troubleshooting implement monitoring solutions defined senior staff member implement backup solutions defined senior staff members adequate capacity available times meet operations business capacity planning management maintains existing databases erp bi etl data warehouse based specifications establishes documentation procedures standards backups manages security middleware technologies structures responsible triage performance tuning middleware technologies provides oncall support data virtulization delphix etl attunity talend informatica bachelors degree technology,bachelors degree technology monitors takes correction action middleware technologies responsible operations activities database erp bi etl data warehouse respond prioritize escalate incidents service requests necessary according eis processes procedures develop implement operational perform loading production issues regarding loads continue predetermined agreed level services support minimum business following interruption platform application development functions maintain agreements policies assist coordinating vendor assisted participate problem management task forces root cause troubleshooting monitoring solutions defined senior staff member backup members adequate capacity available times meet planning maintains existing databases based specifications establishes documentation standards backups manages security structures triage performance tuning provides oncall virtulization delphix attunity talend informatica
425,  Proficient with Unix commands and comfortable working on the command line Adept using a scripting language like Python or Ruby to process text files Experience with relational and non-relational databases    ,proficient unix commands comfortable command line adept scripting language like python ruby process text files relational nonrelational databases,proficient unix commands comfortable command line adept scripting language like python ruby process text files relational nonrelational databases
426,"   Leverage your software development and data engineering skills to impact our business by taking ownership of key projects requiring coding and data pipelines Collaborate with product managers, software engineers and data engineers to design, implement, and deliver successful data solutions Define technical requirements and implementation details for data solutions Design, build and optimize performant databases, data models, integrations and ETL pipelines in RDBMS and NoSQL environments Maintain detailed documentation of your work and changes to support data quality and governance Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers Be an active participant and advocate of agile/scrum practices to ensure health and process improvements for your team    5+ years of experience designing and delivering large scale, 24/7, mission-critical data pipelines and features using modern big data architectures 3+ years of Scala 3+ years of Python 3+ years of Spark 2+ years of experience with AWS data ecosystem  Redshift, EMR, Glue, Athena, . . .   Deep knowledge in various ETL/ELT tools and concepts, data modeling, SQL, query performance optimization Experience with building stream processing applications using Kinesis or Kafka Experience with workflow management tools  Airflow, Oozie, Azkaban, Luigi, etc.   Comfortable working in Linux environment Ability to thrive in an agile, entrepreneurial start-up environment ",leverage software development data engineering impact business taking ownership key projects requiring coding data pipelines collaborate product managers software engineers data engineers design implement deliver successful data solutions define technical implementation details data solutions design build optimize performant databases data models integrations etl pipelines rdbms nosql environments maintain detailed documentation changes support data governance operational efficiency solutions meet slas support commitment customers active participant advocate agilescrum practices health process improvements team designing delivering scale missioncritical data pipelines features modern big data architectures scala python spark aws data ecosystem redshift emr glue athena deep various etlelt tools concepts data modeling sql query performance optimization building stream processing applications kinesis kafka workflow management tools airflow oozie azkaban luigi comfortable linux thrive agile entrepreneurial startup,leverage software development data engineering impact business taking ownership key projects requiring coding pipelines collaborate product managers engineers design implement deliver successful solutions define technical implementation details build optimize performant databases models integrations etl rdbms nosql environments maintain detailed documentation changes support governance operational efficiency meet slas commitment customers active participant advocate agilescrum practices health process improvements team designing delivering scale missioncritical features modern big architectures scala python spark aws ecosystem redshift emr glue athena deep various etlelt tools concepts modeling sql query performance optimization building stream processing applications kinesis kafka workflow management airflow oozie azkaban luigi comfortable linux thrive agile entrepreneurial startup
427,"3+ years experience SQL 2008, 2012, 2014, 2017.  2 or more years working in high-transaction environments is required Working knowledge of different index types and how they are used  columnstore, full-text, filtered, indexes with include columns Basic Execution Plan understanding SSIS/SSRS experience is a strong plus Experience with performance tuning and optimization, using native monitoring and troubleshooting tools and techniques, including complex queries as well as procedure and indexing strategies Excellent written and verbal communication Adaptable team-player with a focus on results and value delivery Able to organize and plan work independently Participates in various technology POCs Understanding of OLTP vs OLAP environments Working knowledge of relational database internals  locking, consistency, serialization, recovery paths  NoSQL is a plus MCTS, MCITP, and/or MVP certifications are a plus    ",sql hightransaction environments different index types used columnstore fulltext filtered indexes include columns basic execution plan understanding ssisssrs plus performance tuning optimization native monitoring troubleshooting tools techniques complex queries well procedure indexing strategies written verbal communication adaptable teamplayer focus results value delivery able organize plan independently participates various technology pocs understanding oltp vs olap environments relational database internals locking consistency serialization recovery paths nosql plus mcts mcitp andor mvp certifications plus,sql hightransaction environments different index types used columnstore fulltext filtered indexes include columns basic execution plan understanding ssisssrs plus performance tuning optimization native monitoring troubleshooting tools techniques complex queries well procedure indexing strategies written verbal communication adaptable teamplayer focus results value delivery able organize independently participates various technology pocs oltp vs olap relational database internals locking consistency serialization recovery paths nosql mcts mcitp andor mvp certifications
428," Experience working with Spark, Hadoop, or other big data processing platform in high-volume environments A solid and demonstrable understanding of ETL workflows and data warehousing Experience with high-performance, low-latency, distributed systems SQL fluency and an understanding of relational data models Experience optimizing queries and developing stored procedure Building out Spark apps to support new product development Building data models to support new functionality Build data pipelines to analyze, scrub, and integrate first and third party data Productionalizing data science models Coordinating data models with other engineering teams Tuning and optimizing Spark apps to get the most out of the quickly evolving platform    You will be involved in the design, development, deployment, and testing processes for new products.  Our platform provides disruptive insights and optimizes the efficacy of our client's ad campaigns.  You will be a core member of our engineering team and will have a major impact on new initiatives.  We're mindful of how the time and space complexities of the small parts greatly affect our services at scale and we need your help to build responsive systems.  You will help build, test, benchmark, and tune our distributed systems to ensure that our product is scalable, responsive, robust, cost-effective, and correct.    ",spark hadoop big data processing platform highvolume environments solid demonstrable understanding etl workflows data warehousing highperformance lowlatency distributed systems sql fluency understanding relational data models optimizing queries developing stored procedure building spark apps support product development building data models support functionality build data pipelines analyze scrub integrate first third party data productionalizing data science models coordinating data models engineering teams tuning optimizing spark apps get quickly evolving platform involved design development deployment testing processes products platform provides disruptive insights optimizes efficacy clients ad campaigns core member engineering team major impact initiatives mindful time space complexities small parts greatly affect services scale need help build responsive systems help build test benchmark tune distributed systems product scalable responsive robust costeffective correct,spark hadoop big data processing platform highvolume environments solid demonstrable understanding etl workflows warehousing highperformance lowlatency distributed systems sql fluency relational models optimizing queries developing stored procedure building apps support product development functionality build pipelines analyze scrub integrate first third party productionalizing science coordinating engineering teams tuning get quickly evolving involved design deployment testing processes products provides disruptive insights optimizes efficacy clients ad campaigns core member team major impact initiatives mindful time space complexities small parts greatly affect services scale need help responsive test benchmark tune scalable robust costeffective correct
429," Collaborate with data product managers, data architects, and data engineers to design, implement and deliver successful data solutions Define technical requirements and implementation details for the underlying data warehouse and data marts.  Develop and optimize performant database, data model, integration and ETL in RDBMS and NoSQL environments.  Maintain detailed documentation of your work and changes to support data quality and governance.  Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers.  Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team.   Collaborate with data product managers, data architects, and data engineers to design, implement and deliver successful data solutions Define technical requirements and implementation details for the underlying data warehouse and data marts.  Develop and optimize performant database, data model, integration and ETL in RDBMS and NoSQL environments.  Maintain detailed documentation of your work and changes to support data quality and governance.  Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers.  Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team.    ",collaborate data product managers data architects data engineers design implement deliver successful data solutions define technical implementation details underlying data warehouse data marts develop optimize performant database data model integration etl rdbms nosql environments maintain detailed documentation changes support data governance operational efficiency solutions meet slas support commitment customers active participant advocate agilescrum practice health process improvements team collaborate data product managers data architects data engineers design implement deliver successful data solutions define technical implementation details underlying data warehouse data marts develop optimize performant database data model integration etl rdbms nosql environments maintain detailed documentation changes support data governance operational efficiency solutions meet slas support commitment customers active participant advocate agilescrum practice health process improvements team,collaborate data product managers architects engineers design implement deliver successful solutions define technical implementation details underlying warehouse marts develop optimize performant database model integration etl rdbms nosql environments maintain detailed documentation changes support governance operational efficiency meet slas commitment customers active participant advocate agilescrum practice health process improvements team
430," Degree in Computer Science 6 + year's building enterprise software applications 5+ years' experience in custom ETL design, implementation, and maintenance 3+ years experience with Spark/PySpark or equivalent distributed processing systems 3 + years experience with AWS Services  Athena, Glue, Redshift, DynamoDB, ECS, S3  and python.  Experience with Service Oriented Architecture and Microservices Experience in the complete Software Development Life Cycle  SDLC  Experience with Agile Software Development  SCRUM  Good understanding on relational databases  Postgres, MySql, etc.   Good understanding on NoSQL databases  DynamoDB, MongoDB, etc.   Familiarity with container orchestration services  ECS, Kubernetes, etc.   Familiarity with serverless compute platforms  Lambda, Cloud Functions, etc.   Drive to analyze data to identify deliverables, anomalies, and gaps Accountable, Curious and Organized Experience in data analytics tools such as Tableau a good to have Strong communication skills to collaborate with various teams    Work closely with Business Intelligence and Data Science teams to understand the data needs, define, scope and plan new product features Design, build, deploy new data models and complex ETL pipelines into production with PySpark and AWS Glue Design and implement serverless microservices with AWS Lambda, API Gateway and Fargate Be accountable for operational efficiency and be proactive in monitoring data pipelines Make smart platform and engineering decisions based on data analysis and collaboration Deliver quality scalable, efficient and reusable software Ensure unit test coverage and promote code quality Research new tools and technologies applicable to new and existing use cases Identify areas to improve architecture, application design and scalability Provide input on software architecture, data storage, API and user interface design Mentor more junior team members   ",degree computer science building enterprise software applications custom etl design implementation maintenance sparkpyspark distributed processing systems aws services athena glue redshift dynamodb ecs python service oriented architecture microservices complete software development life cycle sdlc agile software development scrum good understanding relational databases postgres mysql good understanding nosql databases dynamodb mongodb familiarity container orchestration services ecs kubernetes familiarity serverless compute platforms lambda cloud functions drive analyze data identify deliverables anomalies gaps accountable curious organized data analytics tools tableau good communication collaborate various teams closely business intelligence data science teams understand data needs define scope plan product features design build deploy data models complex etl pipelines production pyspark aws glue design implement serverless microservices aws lambda api gateway fargate accountable operational efficiency proactive monitoring data pipelines make smart platform engineering decisions based data analysis collaboration deliver scalable efficient reusable software unit test coverage promote code research tools technologies applicable existing use cases identify areas improve architecture application design scalability input software architecture data storage api user interface design mentor junior team members,degree computer science building enterprise software applications custom etl design implementation maintenance sparkpyspark distributed processing systems aws services athena glue redshift dynamodb ecs python service oriented architecture microservices complete development life cycle sdlc agile scrum good understanding relational databases postgres mysql nosql mongodb familiarity container orchestration kubernetes serverless compute platforms lambda cloud functions drive analyze data identify deliverables anomalies gaps accountable curious organized analytics tools tableau communication collaborate various teams closely business intelligence understand needs define scope plan product features build deploy models complex pipelines production pyspark implement api gateway fargate operational efficiency proactive monitoring make smart platform engineering decisions based analysis collaboration deliver scalable efficient reusable unit test coverage promote code research technologies applicable existing use cases areas improve application scalability input storage user interface mentor junior team members
431," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.  Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive .  Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.  Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions.  Up to petabytes in scale.  Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ",expertise least one following domain areas data warehouse modernization building complete data warehouse solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming data processing software beam airflow hadoop spark hive data migration migrating data stores reliable scalable cloudbased stores strategies near zerodowntime backup restore disaster recovery building productiongrade data backup restore disaster recovery solutions petabytes scale writing software one languages python java scala go building productiongrade data solutions relational nosql systems monitoringalerting capacity planning performance tuning technical consulting customerfacing role,expertise least one following domain areas data warehouse modernization building complete solutions technical architectures starsnowflake schema designs infrastructure components etlelt pipelines reportinganalytic tools must handson batch streaming processing software beam airflow hadoop spark hive migration migrating stores reliable scalable cloudbased strategies near zerodowntime backup restore disaster recovery productiongrade petabytes scale writing languages python java scala go relational nosql systems monitoringalerting capacity planning performance tuning consulting customerfacing role
432,"  Capability to fully understand the operational missions of both projects and their science team.  Linux proficiency, including use of screen Use of server monitoring tools  Zabbix  Extensive experience with scripting language and capability to create/edit scripts to  1  control job flow and  2  monitor data archives.  Knowledge of Oracle database query language, SQL Knowledge of Docker containers Use of JIRA for submitting and receiving processing requests Excellent written and verbal communication skills    Capability to fully understand the operational missions of both projects and their science team.  Linux proficiency, including use of screen Use of server monitoring tools  Zabbix  Extensive experience with scripting language and capability to create/edit scripts to  1  control job flow and  2  monitor data archives.  Knowledge of Oracle database query language, SQL Knowledge of Docker containers Use of JIRA for submitting and receiving processing requests Excellent written and verbal communication skills  ",capability fully understand operational missions projects science team linux proficiency use screen use server monitoring tools zabbix extensive scripting language capability createedit scripts control job flow monitor data archives oracle database query language sql docker containers use jira submitting receiving processing requests written verbal communication capability fully understand operational missions projects science team linux proficiency use screen use server monitoring tools zabbix extensive scripting language capability createedit scripts control job flow monitor data archives oracle database query language sql docker containers use jira submitting receiving processing requests written verbal communication,capability fully understand operational missions projects science team linux proficiency use screen server monitoring tools zabbix extensive scripting language createedit scripts control job flow monitor data archives oracle database query sql docker containers jira submitting receiving processing requests written verbal communication
433,"Bachelorâs degree in computer science, software/computer engineering, applied mathematics, or physics statistics.  5+ years of experience in a Data Engineer role.  Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases.  Experience building and optimizing âbig dataâ data pipelines, architectures and data sets.  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.  Strong analytic skills related to working with unstructured datasets.  Build processes supporting data transformation, data structures, metadata, dependency and workload management.  Develop set processes for data mining, data modeling, and data production.  A successful history of manipulating, processing and extracting value from large disconnected datasets.  Working knowledge of message queuing, stream processing, and highly scalable âbig dataâ data stores.  Strong project management and organizational skills.  Experience supporting and working with cross-functional teams in a dynamic environment.  Ensure that all systems meet the business/company requirements as well as industry practices.  Integrate up-and-coming data management and software engineering technologies into existing data structures.  Recommend different ways to constantly improve data reliability and quality.    Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.  Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and as well as AWS and Azure-based data technologies.  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.  Work with stakeholders including the executive, operations, and business development teams to assist with data-related technical issues and support their data infrastructure needs.  Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.  Work with data and analytics experts to strive for greater functionality in our data systems.    ",bachelors degree computer science softwarecomputer engineering applied mathematics physics statistics data engineer role advanced sql relational databases query authoring sql well familiarity variety databases building optimizing big data data pipelines architectures data sets performing root cause analysis internal external data processes answer specific business questions identify opportunities improvement analytic unstructured datasets build processes supporting data transformation data structures metadata dependency workload management develop set processes data mining data modeling data production successful history manipulating processing extracting value disconnected datasets message queuing stream processing highly scalable big data data stores project management organizational supporting crossfunctional teams dynamic systems meet businesscompany well industry practices integrate upandcoming data management software engineering technologies existing data structures recommend different ways constantly improve data reliability create maintain optimal data pipeline architecture assemble complex data sets meet functional nonfunctional business identify design implement internal process improvements automating manual processes optimizing data delivery redesigning infrastructure greater scalability build infrastructure optimal extraction transformation loading data wide variety data sources sql well aws azurebased data technologies build analytics tools utilize data pipeline actionable insights customer acquisition operational efficiency key business performance metrics stakeholders executive operations business development teams assist datarelated technical issues support data infrastructure needs keep data separated secure across national boundaries multiple data centers aws regions create data tools analytics data scientist team members assist building optimizing product innovative industry leader data analytics experts strive greater functionality data systems,bachelors degree computer science softwarecomputer engineering applied mathematics physics statistics data engineer role advanced sql relational databases query authoring well familiarity variety building optimizing big pipelines architectures sets performing root cause analysis internal external processes answer specific business questions identify opportunities improvement analytic unstructured datasets build supporting transformation structures metadata dependency workload management develop set mining modeling production successful history manipulating processing extracting value disconnected message queuing stream highly scalable stores project organizational crossfunctional teams dynamic systems meet businesscompany industry practices integrate upandcoming software technologies existing recommend different ways constantly improve reliability create maintain optimal pipeline architecture assemble complex functional nonfunctional design implement process improvements automating manual delivery redesigning infrastructure greater scalability extraction loading wide sources aws azurebased analytics tools utilize actionable insights customer acquisition operational efficiency key performance metrics stakeholders executive operations development assist datarelated technical issues support needs keep separated secure across national boundaries multiple centers regions scientist team members product innovative leader experts strive functionality
434," Bachelor's degree in Computer Science or comparable field 8+ years experience in Python and SQL 8+ years experience in Java, Scala, or similar OO experience Experience with data analysis, processing, and validation 5+ years experience with Spark, Hadoop, or Databricks Professional experience with open source ETL frameworks such as Airflow, Luigi, or similar Knowledge within a diverse set of public cloud technologies  AWS RDS, S3, EC2, Lambda, Google Cloud Big Query, Google Cloud Bigtable, etc.  For this role, you'll find success through craft expertise, a collaborative spirit, and decision-making that prioritizes your fellow Rioters, who are the customers of your work.  Being a dedicated fan of games is not necessary for this position!    Automate and maintain batch pipelines that collect and process data to improve the player experience, drive game understanding, and report business metrics.  Design and build tools that provide confidence in our data quality-- from data QA processes for publishing new events, to systems for automatic detection of unexpected changes in our data's volume and distributional properties.  Partner with software engineers to build data-driven feedback loops into our game server, client, and backend services by defining data models and contributing to telemetry implementations.  Work with data customers  product owners, game designers, analysts, and data scientists  to identify important questions and then provide the services, datasets, and tools that empower them to find the answers.  Help other developers select data technologies that provide efficient insights with a high return on investment.  Be the face of data best-practices to other developers around the product by advocating for data considerations early in the product life cycle and educating teammates on how to get the most out of our ecosystem.  Mentor and coach data and software engineers and collaborate with other technologists to increase the Data Discipline's capabilities and influence across the product.    ",bachelors degree computer science comparable python sql java scala similar oo data analysis processing validation spark hadoop databricks professional open source etl frameworks airflow luigi similar within diverse set public cloud technologies aws rds ec lambda google cloud big query google cloud bigtable role youll find success craft expertise collaborative spirit decisionmaking prioritizes fellow rioters customers dedicated fan games necessary position automate maintain batch pipelines collect process data improve player drive game understanding report business metrics design build tools confidence data data qa processes publishing events systems automatic detection unexpected changes datas volume distributional properties partner software engineers build datadriven feedback loops game server client backend services defining data models contributing telemetry implementations data customers product owners game designers analysts data scientists identify important questions services datasets tools empower find answers help developers select data technologies efficient insights return investment face data bestpractices developers around product advocating data considerations early product life cycle educating teammates get ecosystem mentor coach data software engineers collaborate technologists increase data disciplines capabilities influence across product,bachelors degree computer science comparable python sql java scala similar oo data analysis processing validation spark hadoop databricks professional open source etl frameworks airflow luigi within diverse set public cloud technologies aws rds ec lambda google big query bigtable role youll find success craft expertise collaborative spirit decisionmaking prioritizes fellow rioters customers dedicated fan games necessary position automate maintain batch pipelines collect process improve player drive game understanding report business metrics design build tools confidence qa processes publishing events systems automatic detection unexpected changes datas volume distributional properties partner software engineers datadriven feedback loops server client backend services defining models contributing telemetry implementations product owners designers analysts scientists identify important questions datasets empower answers help developers select efficient insights return investment face bestpractices around advocating considerations early life cycle educating teammates get ecosystem mentor coach collaborate technologists increase disciplines capabilities influence across
435,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform.  Multi-cloud experience a plus.    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",minimum previous consulting client service delivery google gcp devops gcp platform multicloud plus proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,minimum previous consulting client service delivery google gcp devops platform multicloud plus proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
436,"   Design, maintenance, and/or administration with Apache Cassandra, including data modeling and CQL scripting.  Work with DataStax Enterprise Graph, including Gremlin query language.  Develop data models, and plan and write high performing queries.  Help analyze data access patterns and identify hotspots and bottlenecks.  Plan, coordinate, and administer database/DSE systems, including base definition, structure, documentation, requirements, operational guidelines and protection.  You'll be doing some administrator tasks for Cassandra, including configuration of Cassandra cluster with mutli-data centers.  Your tasks will also include performance tuning for memory, troubleshooting, and node rebuild/backup    5+ years of related experience Past DB production support experience is helpful Prior experience in the following areas is strongly preferred  Adding /removing a datacenter Adding/removing nodes Replication, database schema, database administration and handling user/application security using Cassandra required managing storage capacity of systems Experience with AWS and/or Azure is required Unix/Shell/Java scripting and DB automation experience is required Experience with Microsoft . NET Framework  C , . NET Core  is a plus.  Knowledge of both Windows and Linux Operating Systems is a plus Ability to quickly learn new technologies and business processes/functions is helpful Strong analytical skills to determine effective approaches to business solutions is important Understanding of database best practices and the ability to promote those best practices in the organization is very important Bachelorâs degree in Computer Science or related technical field is required Experience supporting mission critical, customer facing applications is required ",design maintenance andor administration apache cassandra data modeling cql scripting datastax enterprise graph gremlin query language develop data models plan write performing queries help analyze data access patterns identify hotspots bottlenecks plan coordinate administer databasedse systems base definition structure documentation operational guidelines protection youll administrator tasks cassandra configuration cassandra cluster mutlidata centers tasks also include performance tuning memory troubleshooting node rebuildbackup past db production support helpful prior following areas strongly adding removing datacenter addingremoving nodes replication database schema database administration handling userapplication security cassandra managing storage capacity systems aws andor azure unixshelljava scripting db automation microsoft net framework c net core plus windows linux operating systems plus quickly learn technologies business processesfunctions helpful analytical determine effective approaches business solutions important understanding database best practices promote best practices organization important bachelors degree computer science technical supporting mission critical customer facing applications,design maintenance andor administration apache cassandra data modeling cql scripting datastax enterprise graph gremlin query language develop models plan write performing queries help analyze access patterns identify hotspots bottlenecks coordinate administer databasedse systems base definition structure documentation operational guidelines protection youll administrator tasks configuration cluster mutlidata centers also include performance tuning memory troubleshooting node rebuildbackup past db production support helpful prior following areas strongly adding removing datacenter addingremoving nodes replication database schema handling userapplication security managing storage capacity aws azure unixshelljava automation microsoft net framework c core plus windows linux operating quickly learn technologies business processesfunctions analytical determine effective approaches solutions important understanding best practices promote organization bachelors degree computer science technical supporting mission critical customer facing applications
437," Bachelorâs degree in Computer Science or related field.  3+ years experience working with AWS and Redshift 3+ years experience in Business Intelligence / Data Engineering / Data Warehousing roles working with multiple disparate, complex, large datasets 3+ years experience in one of Python, Java, Scala, or a similar programming language 5+ years experience working with SQL   Work with engineering and business stakeholders to understand data requirements Help shape the growth of in-house data resources to support business Intelligence, analytics, and data science Define, design, model, implement, and operate large, evolving, structured and unstructured datasets Take action with quality, performance, scalability, and maintainability in mind Interact and integrate with internal and external teams and systems to extract, transform, and load data from a wide variety of sources Implement secure and auditable data infrastructure as required Help execute the roll out of modern business intelligence and analytics tools and visualizations Assist with ad hoc data investigations and analysis Train users on data capabilities and best practices  ",bachelors degree computer science aws redshift business intelligence data engineering data warehousing roles multiple disparate complex datasets one python java scala similar programming language sql engineering business stakeholders understand data help shape growth inhouse data resources support business intelligence analytics data science define design model implement operate evolving structured unstructured datasets take action performance scalability maintainability mind interact integrate internal external teams systems extract transform load data wide variety sources implement secure auditable data infrastructure help execute roll modern business intelligence analytics tools visualizations assist ad hoc data investigations analysis train users data capabilities best practices,bachelors degree computer science aws redshift business intelligence data engineering warehousing roles multiple disparate complex datasets one python java scala similar programming language sql stakeholders understand help shape growth inhouse resources support analytics define design model implement operate evolving structured unstructured take action performance scalability maintainability mind interact integrate internal external teams systems extract transform load wide variety sources secure auditable infrastructure execute roll modern tools visualizations assist ad hoc investigations analysis train users capabilities best practices
438," Bachelorâs degree in Computer Science or a related field 5+ years of experience in Software Engineering / Data Engineering / Data Warehousing roles working in high traffic, fault tolerant, and highly available environments 5+ years experience with SQL skills 5+ years experience with Big Data Technologies  Hadoop, Hive, Hbase, Pig, Spark, etc.   2+ years of experience with Streaming platforms like Kafka or Kinesis 2+ years of experience with Elasticsearch, Logstash, Splunk or similar technologies 2+ years of experience with Scala, Golang, Python, Java, Ruby or a similar programming language   Work with engineering and business stakeholders to understand data requirements Help shape the growth of transformative in-house big data resources and capabilities Drive design, model, implement, and operate large, evolving, structured and unstructured datasets Evaluate and implement efficient distributed storage and query techniques Interact and integrate with internal and external teams and systems to extract, transform, and load data from a wide variety of sources Implement secure and auditable data infrastructure as required Train users on data capabilities and best practices  ",bachelors degree computer science software engineering data engineering data warehousing roles traffic fault tolerant highly available environments sql big data technologies hadoop hive hbase pig spark streaming platforms like kafka kinesis elasticsearch logstash splunk similar technologies scala golang python java ruby similar programming language engineering business stakeholders understand data help shape growth transformative inhouse big data resources capabilities drive design model implement operate evolving structured unstructured datasets evaluate implement efficient distributed storage query techniques interact integrate internal external teams systems extract transform load data wide variety sources implement secure auditable data infrastructure train users data capabilities best practices,bachelors degree computer science software engineering data warehousing roles traffic fault tolerant highly available environments sql big technologies hadoop hive hbase pig spark streaming platforms like kafka kinesis elasticsearch logstash splunk similar scala golang python java ruby programming language business stakeholders understand help shape growth transformative inhouse resources capabilities drive design model implement operate evolving structured unstructured datasets evaluate efficient distributed storage query techniques interact integrate internal external teams systems extract transform load wide variety sources secure auditable infrastructure train users best practices
439," 4+ years of industry experience building highly scalable data pipelines  batch and/or streaming  utilizing Spark, Hive, Presto or other open source frameworks. architecture.  Python and shell scripting experience for automation and data manipulation.  Prior experience utilizing dashboarding tools such as Tableau, Superset or similar.  Experience translating ambiguous business needs to highly scalable data models and datasets.  Background in software engineering - able to write elegant, scalable and maintainable code.  Strong SQL skills    Collaborate with product and engineering teams in multiple projects building forward thinking, innovative data solutions that up-level our features and get results in a data driven way.  Build highly scalable resilient data pipelines and models which produce high quality datasets.  Deliver visualizations that distill clear, actionable insights from large, complex datasets.  Improve our tooling by building generic data features such as data quality and anomaly detection and drive overall improvements in our data infrastructure.  Drive ideas and projects through deep understanding of our data and how it applies in the broader sense of the organization.    ",industry building highly scalable data pipelines batch andor streaming utilizing spark hive presto open source frameworks architecture python shell scripting automation data manipulation prior utilizing dashboarding tools tableau superset similar translating ambiguous business needs highly scalable data models datasets background software engineering able write elegant scalable maintainable code sql collaborate product engineering teams multiple projects building forward thinking innovative data solutions uplevel features get results data driven way build highly scalable resilient data pipelines models produce datasets deliver visualizations distill clear actionable insights complex datasets improve tooling building generic data features data anomaly detection drive overall improvements data infrastructure drive ideas projects deep understanding data applies broader sense organization,industry building highly scalable data pipelines batch andor streaming utilizing spark hive presto open source frameworks architecture python shell scripting automation manipulation prior dashboarding tools tableau superset similar translating ambiguous business needs models datasets background software engineering able write elegant maintainable code sql collaborate product teams multiple projects forward thinking innovative solutions uplevel features get results driven way build resilient produce deliver visualizations distill clear actionable insights complex improve tooling generic anomaly detection drive overall improvements infrastructure ideas deep understanding applies broader sense organization
440," 3-5 years of hands-on experience in âbig-dataâ technologies.  BS or MS in Computer Science or a related degree.  Hands on experience and strong proficiency with either Python, Java, or Scala.  Familiarity with software engineering practices for the full software development life cycle, including coding standards, code reviews, build processes, testing, and operations.  Familiarity of the fundamentals of distributed data processing, data modeling and ETL.  Ability to extract data from multiple data sources and load them into a centralized data warehouse to facilitate unified reporting.    Build production grade live data processing systems.  Implement custom data pipelines streaming large amounts of sensor and location data.  Ensure data is processed correctly and efficiently.  Attend daily stand-ups led by TPM.  Write clean testable code.   ",handson bigdata technologies bs ms computer science degree hands proficiency either python java scala familiarity software engineering practices full software development life cycle coding standards code reviews build processes testing operations familiarity fundamentals distributed data processing data modeling etl extract data multiple data sources load centralized data warehouse facilitate unified reporting build production grade live data processing systems implement custom data pipelines streaming amounts sensor location data data processed correctly efficiently attend daily standups led tpm write clean testable code,handson bigdata technologies bs ms computer science degree hands proficiency either python java scala familiarity software engineering practices full development life cycle coding standards code reviews build processes testing operations fundamentals distributed data processing modeling etl extract multiple sources load centralized warehouse facilitate unified reporting production grade live systems implement custom pipelines streaming amounts sensor location processed correctly efficiently attend daily standups led tpm write clean testable
441,"Requires a Bachelor's Degree  BA, BS  in Computer Science, Computer Engineering, Aeronautical or other related Engineering discipline; Minimum of to five  5  years of experience with custom software applications and data analysis.  Basic understanding of aviation and/or avionics system is a plus.  Thorough knowledge of Microsoft Office application tools  Outlook, Word, PowerPoint, Advanced knowledge of Microsoft Excel  Pivot tables .  Experience working with Tableau or any other Data Analysis/Visualization software is highly desired.  Programming skills are highly desired.  Experience working with SQL databases software highly preferred.  Solid understanding of Tableau strongly preferred.  Knowledgeable experience in C++, web applications, HTML JQuery, CSS is a plus.     Support deployment, integration and maintenance of Controlsâ software products within clientsâ networking environments.  Program simple applications  ACMS, AirFASE  using Teledyne Controls tools.  Support all in-service issues with deployed systems.  Work with tools such as SQL, Tableau, MatLab, etc.  to extract meaningful information.  Resolve challenging, potentially high impact customer situations with a high level of tact and understanding.  Understand customer needs and identify possible new services opportunities.  Share best practices with fellow team members to enhance the quality and efficiency of the services deployment and support process.  Create reports using data provided by our Services Customers and analyzed by our software platforms.  Effectively interact with the engineering teams to provide solutions to complex technical issues.  Must be able to travel domestically and internationally to customer locations.  Requires a Bachelor's Degree  BA, BS  in Computer Science, Computer Engineering, Aeronautical or other related Engineering discipline; Minimum of to five  5  years of experience with custom software applications and data analysis.  Basic understanding of aviation and/or avionics system is a plus.  Thorough knowledge of Microsoft Office application tools  Outlook, Word, PowerPoint, Advanced knowledge of Microsoft Excel  Pivot tables .  Experience working with Tableau or any other Data Analysis/Visualization software is highly desired.  Programming skills are highly desired.  Experience working with SQL databases software highly preferred.  Solid understanding of Tableau strongly preferred.  Knowledgeable experience in C++, web applications, HTML JQuery, CSS is a plus.   ",requires bachelors degree ba bs computer science computer engineering aeronautical engineering discipline minimum five custom software applications data analysis basic understanding aviation andor avionics plus thorough microsoft office application tools outlook word powerpoint advanced microsoft excel pivot tables tableau data analysisvisualization software highly desired programming highly desired sql databases software highly solid understanding tableau strongly knowledgeable c web applications html jquery css plus support deployment integration maintenance controls software products within clients networking environments program simple applications acms airfase teledyne controls tools support inservice issues deployed systems tools sql tableau matlab extract meaningful information resolve challenging potentially impact customer situations level tact understanding understand customer needs identify possible services opportunities share best practices fellow team members enhance efficiency services deployment support process create reports data provided services customers analyzed software platforms effectively interact engineering teams solutions complex technical issues must able travel domestically internationally customer locations requires bachelors degree ba bs computer science computer engineering aeronautical engineering discipline minimum five custom software applications data analysis basic understanding aviation andor avionics plus thorough microsoft office application tools outlook word powerpoint advanced microsoft excel pivot tables tableau data analysisvisualization software highly desired programming highly desired sql databases software highly solid understanding tableau strongly knowledgeable c web applications html jquery css plus,requires bachelors degree ba bs computer science engineering aeronautical discipline minimum five custom software applications data analysis basic understanding aviation andor avionics plus thorough microsoft office application tools outlook word powerpoint advanced excel pivot tables tableau analysisvisualization highly desired programming sql databases solid strongly knowledgeable c web html jquery css support deployment integration maintenance controls products within clients networking environments program simple acms airfase teledyne inservice issues deployed systems matlab extract meaningful information resolve challenging potentially impact customer situations level tact understand needs identify possible services opportunities share best practices fellow team members enhance efficiency process create reports provided customers analyzed platforms effectively interact teams solutions complex technical must able travel domestically internationally locations
442,"  Undergraduate degree in STEM major  science, technology, engineering, and math  and at least 3 years of engineering experience Willing to travel as required  up to 20%  Demonstrated track record of completing hardware/software projects within time and budget constraints Experience designing, fabricating, and testing complex data & control systems Ability to develop and review complex electrical schematics.  Knowledge of noise reduction, grounding, filtering, and other techniques necessary to ensure quality data.  Demonstrated competency in at least 2 programming languages Able to work in a fast-paced and intense startup environment.     ",undergraduate degree stem major science technology engineering math least engineering willing travel demonstrated track record completing hardwaresoftware projects within time budget constraints designing fabricating testing complex data control systems develop review complex electrical schematics noise reduction grounding filtering techniques necessary data demonstrated competency least programming languages able fastpaced intense startup,undergraduate degree stem major science technology engineering math least willing travel demonstrated track record completing hardwaresoftware projects within time budget constraints designing fabricating testing complex data control systems develop review electrical schematics noise reduction grounding filtering techniques necessary competency programming languages able fastpaced intense startup
443,"At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations. Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node. js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc. Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc. 5+ years of hands on experience in programming languages such as Java, c , node. js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc. Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc. Bachelors or higher degree in Computer Science or a related discipline.  DevOps on an AWS platform.  Multi-cloud experience a plus. Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills",least consulting client service delivery amazon aws aws least developing data ingestion data processing analytical pipelines big data relational databases nosql data warehouse solutionsextensive providing practical direction within aws native hadoopexperience private public cloud architectures proscons migration considerations minimum handson aws big data technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming technologies kafka kinesis nifi extensive handson implementing data migration data processing aws services vpcsg ec autoscaling cloudformation lakeformation dms kinesis kafka nifi cdc processing redshift snowflake rds aurora neptune dynamodb hive nosql cloudtrail cloudwatch docker lambda sparkglue sage maker aiml api gw hands programming languages java c node js python pyspark spark sql unix shellperl scripting minimum rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline devops aws platform multicloud plus developing deploying etl solutions aws tools like talend informatica matillionstrong java c spark pyspark unix shellperl scriptingiot eventdriven microservices containerskubernetes cloud proven build manage foster teamoriented proven creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management,least consulting client service delivery amazon aws developing data ingestion processing analytical pipelines big relational databases nosql warehouse solutionsextensive providing practical direction within native hadoopexperience private public cloud architectures proscons migration considerations minimum handson technologies java node js c python sql ec lambda sparksparksql hivemr pig oozie streaming kafka kinesis nifi extensive implementing services vpcsg autoscaling cloudformation lakeformation dms cdc redshift snowflake rds aurora neptune dynamodb hive cloudtrail cloudwatch docker sparkglue sage maker aiml api gw hands programming languages pyspark spark unix shellperl scripting rdbms experienceexperience hadoop file formats compression techniquesexperience devops tools gitlabs jenkins codebuild coepipeline codedeploy bachelors higher degree computer science discipline platform multicloud plus deploying etl solutions like talend informatica matillionstrong scriptingiot eventdriven microservices containerskubernetes proven build manage foster teamoriented creatively analytically problemsolving desire information systems communication written oral interpersonal leadership management
444,"At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ",least consulting client service delivery azure devops azure platform proven build manage foster teamoriented,least consulting client service delivery azure devops platform proven build manage foster teamoriented
445," 5+ years of experience leading a companies data infrastructure Desire to work in a startup environment Demonstrated ability to design and implement algorithms and methods, especially in collaborative software and data analysis development environments  i. e.  source control, code and analysis reviews, API use and definition .     Develop pipelines for continuous integration/deployment of data infrastructure and full-stack applications on AWS.  Building pipelines/systems with scalability, performance, and security in mind.    ",leading companies data infrastructure desire startup demonstrated design implement algorithms methods especially collaborative software data analysis development environments e source control code analysis reviews api use definition develop pipelines continuous integrationdeployment data infrastructure fullstack applications aws building pipelinessystems scalability performance security mind,leading companies data infrastructure desire startup demonstrated design implement algorithms methods especially collaborative software analysis development environments e source control code reviews api use definition develop pipelines continuous integrationdeployment fullstack applications aws building pipelinessystems scalability performance security mind
446,"  Develop and automate data pipelines using Hadoop/Spark to model large data sets Performing algorithm development and implementation in production systems Developing software in Java, Python, Scala or scripting programming language Improve existing data frameworks within the data lake to handle anticipated growth and new objectives Collaborate with business teams to create monetizable product Expose and deliver aggregated/customer-centric data through reports, visualization products, and RESTful APIs Manage space allocations / data partitioning across our data lakes Increase the capabilities of our reporting/analytics platforms to support business insight for internal and external users Maintain data integrity by enhancing our ability to remove content generated by undesirable actors such as bots, scrapers, and pen testers   3+ years of Java experience working with unstructured data and perform raw text processing 2+ years of experience in data analysis and the ability to translate raw, technical data into actionable insight 1+ year of hands on experience using Hadoop/Spark Comfortable with Unix shell or other scripting languages Demonstrate clear understanding of web analytics and tracking Comfortable working with open-source tools and have the self learning ability to get tools to work with little instructions Understanding of software development best practices and revision control  git  ",develop automate data pipelines hadoopspark model data sets performing algorithm development implementation production systems developing software java python scala scripting programming language improve existing data frameworks within data lake handle anticipated growth objectives collaborate business teams create monetizable product expose deliver aggregatedcustomercentric data reports visualization products restful apis manage space allocations data partitioning across data lakes increase capabilities reportinganalytics platforms support business insight internal external users maintain data integrity enhancing remove content generated undesirable actors bots scrapers pen testers java unstructured data perform raw text processing data analysis translate raw technical data actionable insight year hands hadoopspark comfortable unix shell scripting languages demonstrate clear understanding web analytics tracking comfortable opensource tools self get tools little instructions understanding software development best practices revision control git,develop automate data pipelines hadoopspark model sets performing algorithm development implementation production systems developing software java python scala scripting programming language improve existing frameworks within lake handle anticipated growth objectives collaborate business teams create monetizable product expose deliver aggregatedcustomercentric reports visualization products restful apis manage space allocations partitioning across lakes increase capabilities reportinganalytics platforms support insight internal external users maintain integrity enhancing remove content generated undesirable actors bots scrapers pen testers unstructured perform raw text processing analysis translate technical actionable year hands comfortable unix shell languages demonstrate clear understanding web analytics tracking opensource tools self get little instructions best practices revision control git
447,"   Extensive knowledge and work experience in system engineering lifecycle activities and product including Configuration Management, Continuous Integration, Software and/or System Testing, Software Test Automation, Software Deployment.  In-depth knowledge of software architecture practices and extensive experience in the integration and deployment of large scale and highly complex software systems.  Excellent coding and scripting skills in languages such as Java, shell, Python, Perl, or Ruby Good knowledge of test automation practices, familiarity with test automation tools, practical experience developing test automation solutions, and software deployment methods Experience authoring automated tests using frameworks and tools, such as RobotFramework, TestNG, Selenium, Watir, TestComplete Working knowledge of software configuration management and issue tracking tools  such as GIT, SVN, AccuRev, CVS, JIRA  and/or test management tool  such as TestRail, qTest  with understanding of best practices Strong analytical and troubleshooting skills with extensive problem-solving experience in a distributed Unix/Linux environment where extensive knowledge of Unix/Linux environment is required Experience running test environments in AWS Strong written and verbal communication skills with experience in writing technical documents and presenting at product reviews Exceptional interpersonal skills and demonstrated solid technical leadership and teaming skills to effectively interface with key stakeholders and customers including scientists, project management, OPS Teams, Developers, and industry partners, including advising senior management.   ",extensive engineering lifecycle activities product configuration management continuous integration software andor testing software test automation software deployment indepth software architecture practices extensive integration deployment scale highly complex software systems coding scripting languages java shell python perl ruby good test automation practices familiarity test automation tools practical developing test automation solutions software deployment methods authoring automated tests frameworks tools robotframework testng selenium watir testcomplete software configuration management issue tracking tools git svn accurev cvs jira andor test management tool testrail qtest understanding best practices analytical troubleshooting extensive problemsolving distributed unixlinux extensive unixlinux running test environments aws written verbal communication writing technical documents presenting product reviews exceptional interpersonal demonstrated solid technical leadership teaming effectively interface key stakeholders customers scientists project management ops teams developers industry partners advising senior management,extensive engineering lifecycle activities product configuration management continuous integration software andor testing test automation deployment indepth architecture practices scale highly complex systems coding scripting languages java shell python perl ruby good familiarity tools practical developing solutions methods authoring automated tests frameworks robotframework testng selenium watir testcomplete issue tracking git svn accurev cvs jira tool testrail qtest understanding best analytical troubleshooting problemsolving distributed unixlinux running environments aws written verbal communication writing technical documents presenting reviews exceptional interpersonal demonstrated solid leadership teaming effectively interface key stakeholders customers scientists project ops teams developers industry partners advising senior
448,"  Develop and Maintain preventative maintenance schedule for all site equipment ensuring accurate technical/maintenance instructions and content.  Understand and document all maintenance contracts including scope of work and length of contracts.  Ensure no lapses in contracts and all PM occurs as scheduled.  Develop CEWA, schedule events, publish calendar, and manage all critical equipment maintenance events in strict accordance with CEWA program.  On call 24 hours a day 7 days a week.   Knowledge, Skills and Abilities Required  Ã¢â¬Â¢ Excellent verbal and written communication skills Ã¢â¬Â¢ Detail oriented Ã¢â¬Â¢ Proficient in MS Office applications Ã¢â¬Â¢ Proficient in AutoCAD Ã¢â¬Â¢ Proficient in building automation systems  BAS/BMS  Ã¢â¬Â¢ Flexible and adapts well to a constantly changing environment  Education and/or Experience  Ã¢â¬Â¢ 5+ years working in a data center environment Ã¢â¬Â¢ 2+ years managing projects with significant capital expenditures Ã¢â¬Â¢ BachelorÃ¢â¬â¢s Degree in engineering field preferred  Working Conditions Most work is done in an area where normal office noise is present.   - Disclaimer- The above statements are neither intended to be an all-inclusive list of the duties and responsibilities of the job described, nor are they intended to be a listing of all of the skills and abilities required to do the job.  Rather, they are intended only to describe the general nature of the job.  This job description is not a contract of employment, either express or implied.  Employment with INAP will be voluntarily entered into and your employment is considered at will.  INAP reserves the right to alter the job description at any time without notice.   ",develop maintain preventative maintenance schedule site equipment ensuring accurate technicalmaintenance instructions content understand document maintenance contracts scope length contracts lapses contracts pm occurs scheduled develop cewa schedule events publish calendar manage critical equipment maintenance events strict accordance cewa program call hours day days week abilities verbal written communication detail oriented proficient ms office applications proficient autocad proficient building automation systems basbms flexible adapts well constantly changing education andor data center managing projects significant capital expenditures bachelors degree engineering conditions done area normal office noise present disclaimer statements neither intended allinclusive list duties responsibilities job described intended listing abilities job rather intended describe general nature job job description contract employment either express implied employment inap voluntarily entered employment considered inap reserves right alter job description time without notice,develop maintain preventative maintenance schedule site equipment ensuring accurate technicalmaintenance instructions content understand document contracts scope length lapses pm occurs scheduled cewa events publish calendar manage critical strict accordance program call hours day days week abilities verbal written communication detail oriented proficient ms office applications autocad building automation systems basbms flexible adapts well constantly changing education andor data center managing projects significant capital expenditures bachelors degree engineering conditions done area normal noise present disclaimer statements neither intended allinclusive list duties responsibilities job described listing rather describe general nature description contract employment either express implied inap voluntarily entered considered reserves right alter time without notice
449,"Bachelor's degree in a related field or a combination of education and relevant experience 2-5 years software development experience Experience of data-oriented work within cultural heritage organizations, including transformation of JSON, CSV and/or XML formats Attention to detail combined with a focus on usability Excellent verbal and written communication skills, especially when interacting with non-technical stakeholders Proficiency in Python, or willingness to translate experience in equivalent language Proficiency in relational and document oriented databases Familiarity with Linked Open Data standards and technologies Familiarity with cultural heritage data standards Familiarity with engineering tools such as git and docker Familiarity with test driven and agile software development methodologies Familiarity with machine learning techniques With the Semantic Architect, work with technical and content stakeholders to understand data-oriented project requirements With the Semantic Architect, design and document the institution's data model and resulting APIs With other Data Engineers, ensure the accuracy of data transformation pipelines to migrate legacy datasets into Linked Open Usable Data  LOUD  within our ecosystem With other Data Engineers, design, implement and ensure the accuracy of validation and related services for data models Integrate external content services to enrich and reconcile our data Work in an agile way, including supporting testing, continuous integration and deployment Configure institutional LOUD data management instances built on the Arches Platform Assist software engineering teams by translating stakeholder requirements into feature requests  ",bachelors degree combination education relevant software development dataoriented within cultural heritage organizations transformation json csv andor xml formats attention detail combined focus usability verbal written communication especially interacting nontechnical stakeholders proficiency python willingness translate language proficiency relational document oriented databases familiarity linked open data standards technologies familiarity cultural heritage data standards familiarity engineering tools git docker familiarity test driven agile software development methodologies familiarity machine techniques semantic architect technical content stakeholders understand dataoriented project semantic architect design document institutions data model resulting apis data engineers accuracy data transformation pipelines migrate legacy datasets linked open usable data loud within ecosystem data engineers design implement accuracy validation services data models integrate external content services enrich reconcile data agile way supporting testing continuous integration deployment configure institutional loud data management instances built arches platform assist software engineering teams translating stakeholder feature requests,bachelors degree combination education relevant software development dataoriented within cultural heritage organizations transformation json csv andor xml formats attention detail combined focus usability verbal written communication especially interacting nontechnical stakeholders proficiency python willingness translate language relational document oriented databases familiarity linked open data standards technologies engineering tools git docker test driven agile methodologies machine techniques semantic architect technical content understand project design institutions model resulting apis engineers accuracy pipelines migrate legacy datasets usable loud ecosystem implement validation services models integrate external enrich reconcile way supporting testing continuous integration deployment configure institutional management instances built arches platform assist teams translating stakeholder feature requests
450,"     Bachelorâs Degree in Computer Science, Information Systems, or other related field as well as equivalent work experience.  Minimum 5 years of data engineering, ETL or software engineering experience with a focus on data extraction, transformation and publishing.  Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.  ",bachelors degree computer science information systems well minimum data engineering etl software engineering focus data extraction transformation publishing creating maintaining automated data pipelines data standards best practices maintain integrity security data adherence developed standards,bachelors degree computer science information systems well minimum data engineering etl software focus extraction transformation publishing creating maintaining automated pipelines standards best practices maintain integrity security adherence developed
451,"   Proficiency Legends Proficiency Level Generic Reference PL1 The associate has basic awareness and comprehension of the skill and is in the process of acquiring this skill through various channels.  PL2 The associate possesses working knowledge of the skill, and can actively and independently apply this skill in engagements and projects.  PL3 The associate has comprehensive, in-depth and specialized knowledge of the skill.  She / he has extensively demonstrated successful application of the skill in engagements or projects.  PL4 The associate can function as a subject matter expert for this skill.  The associate is capable of analyzing, evaluating and synthesizing solutions using the skill.    ",proficiency legends proficiency level generic reference pl associate basic awareness comprehension skill process acquiring skill various channels pl associate possesses skill actively independently apply skill engagements projects pl associate comprehensive indepth specialized skill extensively demonstrated successful application skill engagements projects pl associate function subject matter expert skill associate capable analyzing evaluating synthesizing solutions skill,proficiency legends level generic reference pl associate basic awareness comprehension skill process acquiring various channels possesses actively independently apply engagements projects comprehensive indepth specialized extensively demonstrated successful application function subject matter expert capable analyzing evaluating synthesizing solutions
452,"   Design, build and oversee the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources.  Establish and build processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed  cloud  structures, local databases, and other applicable storage forms as required.  Develop technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis.  Create and establish design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements.  Review internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs.     Bachelorâs degree or equivalent in Computer Science, Computer Engineering or related field, plus five  5  years of experience as a Software Engineer, Database Engineer, Systems Engineer or related occupation.  Must have experience in building and maintaining large-scale data-intensive systems using advanced SQL, ELT/ETL tools and concepts  including SSIS, Talend, Pentaho , OLTP, OLAP and MDM, multi-dimensional data modeling, and query performance optimizations; Participating in all phases of the software development life cycle  SDLC ; Analyzing source data using 3NF, DBA, and reporting and analytical tools; Working on scripting languages including Bash, Powershell, Python, and Ruby; Utilizing Microsoft BI Stack, . Net, C , and schedulers and source control software including TFS; and Experience with Amazon cloud technologies including Amazon RedShift, S3, Elastic MapReduce, and Hive.  ",design build oversee deployment operation technology architecture solutions software capture manage store utilize structured unstructured data internal external sources establish build processes structures based business technical channel data multiple inputs route appropriately store combination distributed cloud structures local databases applicable storage forms develop technical tools programming leverage artificial intelligence machine bigdata techniques cleanse organize transform data maintain defend update data structures integrity automated basis create establish design standards assurance processes software systems applications development compatibility operability data connections flows storage review internal external business product data operations activity suggests changes upgrades systems storage accommodate ongoing needs bachelors degree computer science computer engineering plus five software engineer database engineer systems engineer occupation must building maintaining largescale dataintensive systems advanced sql eltetl tools concepts ssis talend pentaho oltp olap mdm multidimensional data modeling query performance optimizations participating phases software development life cycle sdlc analyzing source data nf dba reporting analytical tools scripting languages bash powershell python ruby utilizing microsoft bi stack net c schedulers source control software tfs amazon cloud technologies amazon redshift elastic mapreduce hive,design build oversee deployment operation technology architecture solutions software capture manage store utilize structured unstructured data internal external sources establish processes structures based business technical channel multiple inputs route appropriately combination distributed cloud local databases applicable storage forms develop tools programming leverage artificial intelligence machine bigdata techniques cleanse organize transform maintain defend update integrity automated basis create standards assurance systems applications development compatibility operability connections flows review product operations activity suggests changes upgrades accommodate ongoing needs bachelors degree computer science engineering plus five engineer database occupation must building maintaining largescale dataintensive advanced sql eltetl concepts ssis talend pentaho oltp olap mdm multidimensional modeling query performance optimizations participating phases life cycle sdlc analyzing source nf dba reporting analytical scripting languages bash powershell python ruby utilizing microsoft bi stack net c schedulers control tfs amazon technologies redshift elastic mapreduce hive
453,"Qualifications  3-7 years  2 years min relevant experience in the role  experience, Bachelorâs Degree.     ",qualifications min relevant role bachelors degree,qualifications min relevant role bachelors degree
