,SearchTitle,Title,Location,City,State,FullDescriptions,Corpus,UniqueCorpus,CleanUnique
0,Data Engineer,Big Data Engineer,"Annapolis Junction, MD 20701",Annapolis Junction,MD,"Imagine being able to shape the future of cyberâ¦ on a team that powers the recruitment of the most talented and motivated employees in the world through technological innovations. This is the reality of being part of AG Grace, Inc. We need leaders who want to discover, enhance, capture and counter cyber activity, who have a bias for action, and who have a deep desire to buildâ¦.to make the previously impossible possible. Is that you?
Youâll work on cutting edge technology and provide Solutions, Insights and Deliver professional services and solutions and that our customers have come to expect from AG Grace, Inc. You should be somebody who enjoys working on complex system software, is customer-centric, and feels strongly about building a software system that maximizes the value of cloud computing. As a developer on the team youâll collaborate with sharp engineers to drive improvements to cyber analysis technology, design and develop new services and solutions, build and track metrics to ensure high quality of results.

Experience
At least five (5) years of general experience in software development/engineering, including requirements analysis, software development, installation, integration, evaluation, enhancement, maintenance, testing, and problem diagnosis/resolution.
At least three (3) years of experience developing software with high level languages such as Java, C, C++.
At least three (3) years of experience developing software in UNIX/Linux (Red Hat versions 3-5+) operating systems.
At least three (3) years of experience in software integration and software testing, to include developing and implementing test plans and test scripts.
At least two (2) years of experience with distributed scalable Big Data Store (NoSQL) such as HBase, Cloud Base/Accumulo, Big Table, etc., as well as two (2) years of experience with the Map Reduce programming model, the Hadoop Distributed File System (HDFS), and technologies such as Hadoop, Hive, Pig, etc
Demonstrated work experience with Serialization such as JSON and/or BSON
Demonstrated work experience with developing restful services, Ruby on Rails framework, LDAP protocol configuration management and cluster performance management (e.g. Nagios)
Demonstrated work experience developing solutions integrating and extending FOSS/COTS products.
Demonstrated technical writing skills and shall have generated technical documents in support of a software development project
Demonstrated work experience with Source Code Management {e.g. Git, Stash, or Subversion, etc.).
TS/SCI Full Scope Poly required
Education
A Bachelorâs degree in electrical engineering, computer engineering, mathematics or a related discipline may be substituted for four years of general experience.
AG Grace, Inc. is dedicated to developing our nation's future and protecting our critical infrastructure resources. We provide a full range of IT services and solutions that help improve our client's ability to reduce risk, improve performance and provide mission assurance. We assist our customers in solving the problems of today and tomorrow. Come be a part of the Future as we help our customers execute their mission.","    A Bachelorâs degree in electrical engineering, computer engineering, mathematics or a related discipline may be substituted for four years of general experience. ","A Bachelorâs degree in electrical engineering, computer mathematics or a related discipline may be substituted for four years of general experience.","A Bachelorâs degree electrical engineering, computer mathematics related discipline may substituted four years general experience."
1,Data Engineer,Big Data Engineer INTELF8,"Chantilly, VA",Chantilly,VA,"Data Works is looking for senior Big Data Engineers able to lead the way in tackling the most difficult engineering challenges in Big Data systems
Responsibilities
Data Works is seeking a Big Data Engineer with demonstrated experience in leading large scale data warehousing projects. A successful candidate will be strong in Map Reduce, Java, and possess an understanding of data science concepts such as machine learning and trend analysis. Candidate should also be familiar with indexing products such as Lucene and Elasticsearch. Relevant certifications considered but not required.
Required Qualifications
Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark
Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting
Experience with data warehousing tools and technologies
Ability to work within UNIX/Linux operating systems
AWS experience a plus
This position requires U.S. Citizenship and an active TS/SCI security clearance
E3/Sentinel is an equal opportunity employer and Vietnam Era Veterans Readjustment Assistance Act (VEVRAA) federal contractor. All qualified applicants receive consideration for employment without regard to race, color, religion, gender, national origin, age, sexual orientation, gender identity, protected veteran status, or status as a qualified individual with a disability. E3/Sentinel hires and promotes individuals solely on the basis of their qualifications for the job to be filled.","Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting Experience with data warehousing tools and technologies Ability to work within UNIX/Linux operating systems AWS experience a plus This position requires U.S. Citizenship and an active TS/SCI security clearance  Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting Experience with data warehousing tools and technologies Ability to work within UNIX/Linux operating systems AWS experience a plus This position requires U.S. Citizenship and an active TS/SCI security clearance  ","Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark Development experience Java, C++, Scala, Groovy, Python, and/or shell scripting data warehousing tools Ability to work within UNIX/Linux operating systems AWS a plus This position requires U.S. Citizenship an active TS/SCI security clearance","Experience distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch Apache Spark Development experience Java, C++, Scala, Groovy, Python, and/or shell scripting data warehousing tools Ability work within UNIX/Linux operating systems AWS plus This position requires U.S. Citizenship active TS/SCI security clearance"
2,Data Engineer,Big Data Engineer,"Chantilly, VA",Chantilly,VA,"Basic Qualifications
Bachelor's degree in software engineering or a related technical field is required (or equivalent experience), plus a minimum of 5 years of relevant experience; or Master's degree plus a minimum of 3 years of relevant experience. Agile experience preferred.

KEY SKILLS
Minimum three (3) yearsâ experience in designing, developing, building, and implementing Big Data solutions or developing automated solutions to solve complex problems, a thoughtful ability to solve problems could outweigh years of experience.
Ability to identify and implement a data solution strategy
Demonstrates intellectual curiosity in exploring new technologies and finding creative ways to solve data management problems
Experience developing solutions with Python/Javascript/PERL
Experience/knowledge of Spark, Impala, Hadoop, Streamsets, Kafka, Rest APIs
Experience in SQL-based technologies
Experience with at least one of the following NoSQL Database technologies:
Arrango
Mark Logic
HBase
Impala
Parquet
RedShift
Experience in Linux administration/troubleshooting
CLEARANCE REQUIREMENTS:
A TS/SCI security clearance with the ability to obtain a Polygraph is required at time of hire. Candidate must be able to obtain the Polygraph within a reasonable amount of time from date of hire. Applicants selected will be subject to a U.S. Government security investigation and must meet eligibility requirements for access to classified information. Due to the nature of work performed within our facilities, U.S. citizenship is required.
Responsibilities for this Position
A Relocation package may be available for this position.

General Dynamics Mission Systems (GDMS) is seeking motivated candidates to join our insider threat detection, systems integration team. Our mission oriented team is responsible for the design, testing, deployment, maintenance, operation, and evolution of the systems directly supporting the insider threat detection program of a large government customer in the United States Intelligence Community (USIC). GDMS has an immediate opening on the team for a motivated Big Data Engineer with a self-starter mindset who is up to date with the latest tools and techniques. The position will focus on the integration of new data management technologies and software performance tuning and troubleshooting. This is a challenging yet rewarding position that provides an opportunity to leverage cutting edge technologies in pursuit of a vital mission that protects people, sensitive information/technologies, and the national security posture of the USIC.

The majority of work will be performed in Chantilly, Virginia, which is located approximately 25 miles west of Washington D.C., near the Dulles International Airport. The selected Big Data Engineer will support a 6+ year contract that General Dynamics recently secured.

CORE RESPONSIBILITIES:
Assist in the development and delivering of large scale data pipelines
Leverage new database technologies to improve customer data solutions
Develop and implement automated tests for data transformations and data migrations
Research and apply big data solution technologies to complex datasets; make recommendations to data science team on new technologies

#CJ3
#CB
Company Overview
General Dynamics Mission Systems (GDMS) engineers a diverse portfolio of high technology solutions, products and services that enable customers to successfully execute missions across all domains of operation. With a global team of 13,000+ top professionals, we partner with the best in industry to expand the bounds of innovation in the defense and scientific arenas. Given the nature of our work and who we are, we value trust, honesty, alignment and transparency. We offer highly competitive benefits and pride ourselves in being a great place to work with a shared sense of purpose. You will also enjoy a flexible work environment where contributions are recognized and rewarded. If who we are and what we do resonates with you, we invite you to join our high performance team!"," Minimum three  3  yearsâ experience in designing, developing, building, and implementing Big Data solutions or developing automated solutions to solve complex problems, a thoughtful ability to solve problems could outweigh years of experience. Ability to identify and implement a data solution strategy Demonstrates intellectual curiosity in exploring new technologies and finding creative ways to solve data management problems Experience developing solutions with Python/Javascript/PERL Experience/knowledge of Spark, Impala, Hadoop, Streamsets, Kafka, Rest APIs Experience in SQL-based technologies Experience with at least one of the following NoSQL Database technologies  Arrango Mark Logic HBase Impala Parquet RedShift Experience in Linux administration/troubleshooting   Assist in the development and delivering of large scale data pipelines Leverage new database technologies to improve customer data solutions Develop and implement automated tests for data transformations and data migrations Research and apply big data solution technologies to complex datasets; make recommendations to data science team on new technologies   ","Minimum three 3 yearsâ experience in designing, developing, building, and implementing Big Data solutions or developing automated to solve complex problems, a thoughtful ability problems could outweigh years of experience. Ability identify implement data solution strategy Demonstrates intellectual curiosity exploring new technologies finding creative ways management Experience with Python/Javascript/PERL Experience/knowledge Spark, Impala, Hadoop, Streamsets, Kafka, Rest APIs SQL-based at least one the following NoSQL Database Arrango Mark Logic HBase Impala Parquet RedShift Linux administration/troubleshooting Assist development delivering large scale pipelines Leverage database improve customer Develop tests for transformations migrations Research apply big datasets; make recommendations science team on","Minimum three 3 yearsâ experience designing, developing, building, implementing Big Data solutions developing automated solve complex problems, thoughtful ability problems could outweigh years experience. Ability identify implement data solution strategy Demonstrates intellectual curiosity exploring new technologies finding creative ways management Experience Python/Javascript/PERL Experience/knowledge Spark, Impala, Hadoop, Streamsets, Kafka, Rest APIs SQL-based least one following NoSQL Database Arrango Mark Logic HBase Impala Parquet RedShift Linux administration/troubleshooting Assist development delivering large scale pipelines Leverage database improve customer Develop tests transformations migrations Research apply big datasets; make recommendations science team"
3,Data Engineer,Data Engineer - Card Technology,"McLean, VA",McLean,VA,"McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, weâre building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer - Card Technology

Capital One (yes, the âwhatâs in your wallet?â company!) is rethinking the way the world approaches banking. As a Capital One Data Engineer, you will develop fast data infrastructure leveraging Apache Spark, Databricks, and Apache Kafka to manage and create information products using the data streamed from our fleet of over 2000 ATMâs . Whether a new feature or a bug fix, you will lead your work and deliver the most elegant and scalable solutions, all while learning and growing your skills. Most importantly, youâll work and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who love to solve real problems and meet real customer needs.

The person we're looking for:
has a sense of intellectual curiosity and a burning desire to learn

is self-driven, actively looks for ways to contribute, and knows how to get things done

is deliriously customer-focused

values data and truth over ego

has a strong sense of engineering craftsmanship, takes pride in the code they write

believes that good software development includes good testing, good documentation, and good collaboration

has great communication and reasoning skills, including the ability to make a strong case for technology choices

Basic Qualifications:

Bachelorâs degree
At least 1 year of experience with leading big data technologies such as Apache Spark, Apache Hadoop, or Apache Kafka
At least 2 years of professional experience with data engineering concepts

Preferred Qualifications:

2+ years experience with AWS cloud
2+ years of experience in Java, Scala, or Python
2+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python
2+ years of experience building data pipelines
At least 1 year of Cloud (AWS, Azure, Google) development experience
Experience with Streaming and/or NoSQL implementation (Mongo, Cassandra, etc.) a plus

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."," Bachelorâs degree At least 1 year of experience with leading big data technologies such as Apache Spark, Apache Hadoop, or Apache Kafka At least 2 years of professional experience with data engineering concepts     ","Bachelorâs degree At least 1 year of experience with leading big data technologies such as Apache Spark, Hadoop, or Kafka 2 years professional engineering concepts","Bachelorâs degree At least 1 year experience leading big data technologies Apache Spark, Hadoop, Kafka 2 years professional engineering concepts"
4,Data Engineer,Big Data Engineer,"Crystal City, VA",Crystal City,VA,"Are you passionate about solving challenging problems?
Do you thrive being a critical part of an elite team of like-minded people?
How would you like for your next career move to take you to the next level?

If any of this sounds appealing, look no further.

Job Description:
We are seeking a data minded hadoop engineer who is excited about solving national security related data problems. If data pipelines, cloud infrastructure, and automation is something you would be energized to work on, this is the position for you.

Responsibilities include:

Manipulate and store large amounts of data with an eye to performance and efficiency
Work with technologies in the Hadoop ecosystem such as HBase, Spark, Phoenix
Automate deployment and ingestion of data
Design and build data services for consumption by application developers

Basic Qualifications:

6 or more years of experience in software/system development
2 or more years of experience working with data technologies
Technologies:
Bachelor's in Computer Engineering, Computer Science, Information Technology, or related field, or equivalent experience
Must currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination

Desired Skills:

Interest In:
Building data services
Scaling systems on AWS - Elastic MapReduce
Building and Managing large performant data pipelines
Experience with:
Working on a cross functional team
Delivering big data solutions
Amazon Web Services
DevOps best practices - Jenkins
Debugging data pipeline issues
Comfortable with:
Interaction with analysts to help drive big data analytics
Agile development
Hands on system engineering tasks

So, what does Novetta do?

We focus on three core areas: Cyber, Entity, and Multi-Int Analytics. Our products are focused on processing and analyzing vast amounts of data in these core areas. Our services are focused on helping our customers move from complexity to clarity. At Novetta, we bridge the gap between what our customers think they can do and what they aspire to achieve.

Our culture is shaped by a commitment to our Core Values:

Integrity: We hold ourselves accountable to the highest standards of integrity and ethics.
Customer Mission Success: Customer mission success drives our daily effortsâwe strive always to exceed customer expectations and focus on mission success beyond contractual commitments.
Employee Focus: We value our employees and demonstrate our commitment to them by providing clear communications, outstanding benefits, career development, and opportunities to work on problems and technical challenges of national significance.
Innovation: We believe that innovation is critical to our success â that discovering new and more effective ways to achieve customer mission success is what makes us a great company.

GET A REFERRAL BONUS FOR THE GREAT PEOPLE YOU KNOW!
With our amazing referral program, you could be eligible to earn
outstanding rewards for referring qualified new hires to Novetta.

Novetta is an equal opportunity/affirmative action employer.
All qualified applicants will receive consideration for employment without regard to sex,
gender identity, sexual orientation, race, color, religion, national origin, disability,
protected veteran status, age, or any other characteristic protected by law."," 6 or more years of experience in software/system development 2 or more years of experience working with data technologies Technologies  Bachelor's in Computer Engineering, Computer Science, Information Technology, or related field, or equivalent experience Must currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence  CI  polygraph examination   Interest In  Building data services Scaling systems on AWS - Elastic MapReduce Building and Managing large performant data pipelines Experience with  Working on a cross functional team Delivering big data solutions Amazon Web Services DevOps best practices - Jenkins Debugging data pipeline issues Comfortable with  Interaction with analysts to help drive big data analytics Agile development Hands on system engineering tasks   Manipulate and store large amounts of data with an eye to performance and efficiency Work with technologies in the Hadoop ecosystem such as HBase, Spark, Phoenix Automate deployment and ingestion of data Design and build data services for consumption by application developers   ","6 or more years of experience in software/system development 2 working with data technologies Technologies Bachelor's Computer Engineering, Science, Information Technology, related field, equivalent Must currently possess an active US government Top Secret clearance the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. be willing able pass counterintelligence CI polygraph examination Interest In Building services Scaling systems on AWS - Elastic MapReduce Managing large performant pipelines Experience Working cross functional team Delivering big solutions Amazon Web Services DevOps best practices Jenkins Debugging pipeline issues Comfortable Interaction analysts help drive analytics Agile Hands system engineering tasks Manipulate store amounts eye performance efficiency Work Hadoop ecosystem such as HBase, Spark, Phoenix Automate deployment ingestion Design build for consumption by application developers","6 years experience software/system development 2 working data technologies Technologies Bachelor's Computer Engineering, Science, Information Technology, related field, equivalent Must currently possess active US government Top Secret clearance ability obtain maintain SCI access within reasonable, customer-mandated time frame. willing able pass counterintelligence CI polygraph examination Interest In Building services Scaling systems AWS - Elastic MapReduce Managing large performant pipelines Experience Working cross functional team Delivering big solutions Amazon Web Services DevOps best practices Jenkins Debugging pipeline issues Comfortable Interaction analysts help drive analytics Agile Hands system engineering tasks Manipulate store amounts eye performance efficiency Work Hadoop ecosystem HBase, Spark, Phoenix Automate deployment ingestion Design build consumption application developers"
5,Data Engineer,Senior Data Engineer,"Sterling, VA 20166",Sterling,VA,"Position Summary
Our Team
As Discovery Incâs portfolio continues to grow â around the world and across platforms â the Global Technology & Operations team is building media technology and IT systems that meet the world class standard for which Discovery is known. GT&O builds, implements and maintains the business systems and technology that are critical for delivering Discoveryâs products, while articulating the long-term technology strategy that will enable Discoveryâs growing pay-TV, digital terrestrial, free-to-air and online services to reach more audiences on more platforms.

From Amsterdam to Singapore and from satellite and broadcast operations to SAP, we are driving Discovery forward on the leading edge of technology.
The Global Data Analytics team enables Discovery to turn data into action. Using big data platforms, data warehousing and business intelligence technology, audience data, advanced analytics, data science, visualization, and self-service analytics, this team supports company efforts to increase revenue, drive ratings, and enhance consumer engagement.

The Role
The Sr Data Engineer should be a technical contributor who has hands-on knowledge of all phases in building large-scale cloud based distributed data processing systems and applications. You will be part of the Global Data & Analytics engineering technology team and will partner closely with a team of data scientists, business analysts & data engineers leading Discoveryâs cloud based Big Data & Analytics strategy.
Youâll work on implementing complex AWS based big data projects with a focus on collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into insights using multiple technology platforms. Therefore, this role requires an understanding of how a secure big data cloud environment is architected to gain real insights faster, with less friction and complexity. The Sr. data engineer should be passionate about working with cutting edge technologies in solving problems and developing prototypes using different open source tools for the selected solutions.

Youâll need to be an innovative forward-thinker who will help lead end-to-end execution of data engineering initiatives and contribute directly to existing and emerging business strategies and goals. Creativity, Attention to detail and ability to work in a collaborative team environment are essential.
The Sr. Data Engineer will work closely with the Data Engineering head to decide on needed infrastructure architecture and software design needs and act according to the decisions.
Responsibilities
1. Lead the design, implementation, and continuous delivery of pipelines using distributed AWS based big data technologies supporting data processing initiatives across batch and streaming datasets
2. Responsible for development using Scala , Python languages and Big Data Frameworks such as Spark, EMR, Presto, AWS Athena,Kafka, , and Kinesis
3. Provide administrative support on deployed AWS platform components
4. Identify, evaluate and implement cutting edge big data pipelines and frameworks required to provide requested capabilities to integrate external data sources and APIs
5. Review, analyse and evaluate market requirements, business requirements and project briefs in order to design the most appropriate end-to-end technology solutions
6. Process and manage high volume real time customer interaction streams
7. Provide architectural support by building Proof of Concepts & Prototypes
8. Self-Starter to deliver data engineering solutions to optimize both the cost and existing solution
9. Stay current with emerging technologies and industry trends
Requirements
Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI (must to have ) and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala ( Must ), Shell and Python (Must).Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools.Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State
Sterling, Virginia, VA","  Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI  must to have   and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala   Must  , Shell and Python  Must .Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools.Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State  Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI  must to have   and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala   Must  , Shell and Python  Must .Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools.Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State","Bachelor's Degree or higher in Computer Sciences similarMinimum of 5-6 years Software Industry experience3+ development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline Airflow, S3, Cloud Formation and CLI must to Jenkins4+ Apache Spark, Presto, SQL NoSQL Implementation5+ extensive working knowledge different programming Scala Shell Python .Proficiency structured, semi-structured unstructured data sets including social, web logs real time streaming feedsAble tune Big solutions improve performance end-user experienceKnowledge on Visualization Science Tools.Expert level usage Jenkins, GitHub is preferredSpark developer certification a plusAbility eagerness constantly learn teach othersExperience the media industry plusMust legal right work United State","Bachelor's Degree higher Computer Sciences similarMinimum 5-6 years Software Industry experience3+ development experience AWS services Must EC2, EMR , RedShift, Data Pipeline Airflow, S3, Cloud Formation CLI must Jenkins4+ Apache Spark, Presto, SQL NoSQL Implementation5+ extensive working knowledge different programming Scala Shell Python .Proficiency structured, semi-structured unstructured data sets including social, web logs real time streaming feedsAble tune Big solutions improve performance end-user experienceKnowledge Visualization Science Tools.Expert level usage Jenkins, GitHub preferredSpark developer certification plusAbility eagerness constantly learn teach othersExperience media industry plusMust legal right work United State"
6,Data Engineer,Data Engineer,"Annapolis Junction, MD",Annapolis Junction,MD,"Job Description
This position is not yet funded.
Performs data analysis, interpretation, and management duties. Develops rules and methodologies for data collection and analysis
Researches and evaluates new concepts and processes to improve performance.
Analyzes cross-functional problem sets, identifies root causes and resolves issues.
Assists more junior level technicians, specialists, and managers in their activities.
Works individually, actively participates on integrated teams, and leads multiple tasks, projects or teams.
Oversees and monitors performance, and when required, takes steps to resolve issues.
Directs multiple teams through to project completion.
Provides guidance and direction to lower level technicians, specialists, and managers.
Education
Bachelorâs Degree
Qualifications
5+ years of related experience
Active TS/SCI clearance
DoD 8570 compliance or information assurance certification.
For more than 50 years, General Dynamics Information Technology has served as a trusted provider of information technology, systems engineering, training and professional services to customers across federal, state, and local governments, and in the commercial sector. Over 40,000 GDIT professionals deliver enterprise solutions, manage mission-critical IT programs and provide mission support services worldwide. GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.", 5+ years of related experience Active TS/SCI clearance DoD 8570 compliance or information assurance certification.    5+ years of related experience Active TS/SCI clearance DoD 8570 compliance or information assurance certification. ,5+ years of related experience Active TS/SCI clearance DoD 8570 compliance or information assurance certification.,5+ years related experience Active TS/SCI clearance DoD 8570 compliance information assurance certification.
7,Data Engineer,Cloud Data Engineer,"Reston, VA",Reston,VA,"Who YOU are:As a future Cloud Data Engineer at Plus3 IT Systems, you:

Are passionate about working on cutting edge, high profile projects and are motivated by delivering solutions on an aggressive schedule
Aren't satisfied with status quo, and regularly look for creative ways to solve problems and help your team meet commitments
Are insatiably curious â you ask why, you explore, and you're not afraid to blurt out your crazy idea
Love learning new technologies and sharing them with your team
Have a keen interest in using any and all appropriate tools, especially Cloud-based and Open Sourced, to solve the problem at hand
Have strong verbal and written communication skills, due to the dynamic nature of collaborations with customers, vendors, and other engineering teams to solve complex business problems together
Use your experience and leadership skills to motivate your teammates to deliver high quality results in a fast-paced work environment

What you'll be doing:

Work within a team of like-minded professionals to design, build and deploy critical business and mission applications in a production environment
Design and implement appropriate data environments for those applications, engineer suitable data management and governance procedures and provide production support
Automate the provisioning of environments
Provide software coding in language such as Python, Java, Groovy
Design and develop automation workflows, perform unit tests and conduct review to make sure your work is rigorously designed, elegantly coded, and effectively tuned for platform performance, and assess the overall quality of delivered components
Identify, retrieve, manipulate, relate and/or exploit multiple structured data sets from various sources

Qualifications:

Master's Degree preferred, or a Bachelor's degree and 4 years' experience, or 10 years of specialized experience
Minimum 4 years' experience working on complex data/database projects as a data analyst, data architect, or database engineer
Top Secret Clearance with ability to obtain an SCI and CI poly

Desired Technical Skills and Competencies:

Certified Data Management Professional (CDMP), Microsoft Certified Solutions Associate (Business Intelligence) or equivalent certification(s) strongly desired
Experience building n-tier web-based applications using SQL and non-SQL back-ends
Unix scripting (Ruby, Perl, Python, shell)
Experience with Node.js, Spark, Neo4J, Graph Databases, Mule ESB, and Rest API
Experience ingesting, analyzing, and visualizing data using Tableau
Produce clear and concise documents and diagrams capturing networking and operational procedures and storage topology using MS Visio, MS Project, MS Excel and MS Word.

"," Master's Degree preferred, or a Bachelor's degree and 4 years' experience, or 10 years of specialized experience Minimum 4 years' experience working on complex data/database projects as a data analyst, data architect, or database engineer Top Secret Clearance with ability to obtain an SCI and CI poly   Certified Data Management Professional  CDMP , Microsoft Certified Solutions Associate  Business Intelligence  or equivalent certification s  strongly desired Experience building n-tier web-based applications using SQL and non-SQL back-ends Unix scripting  Ruby, Perl, Python, shell  Experience with Node.js, Spark, Neo4J, Graph Databases, Mule ESB, and Rest API Experience ingesting, analyzing, and visualizing data using Tableau Produce clear and concise documents and diagrams capturing networking and operational procedures and storage topology using MS Visio, MS Project, MS Excel and MS Word.    ","Master's Degree preferred, or a Bachelor's degree and 4 years' experience, 10 years of specialized experience Minimum working on complex data/database projects as data analyst, architect, database engineer Top Secret Clearance with ability to obtain an SCI CI poly Certified Data Management Professional CDMP , Microsoft Solutions Associate Business Intelligence equivalent certification s strongly desired Experience building n-tier web-based applications using SQL non-SQL back-ends Unix scripting Ruby, Perl, Python, shell Node.js, Spark, Neo4J, Graph Databases, Mule ESB, Rest API ingesting, analyzing, visualizing Tableau Produce clear concise documents diagrams capturing networking operational procedures storage topology MS Visio, Project, Excel Word.","Master's Degree preferred, Bachelor's degree 4 years' experience, 10 years specialized experience Minimum working complex data/database projects data analyst, architect, database engineer Top Secret Clearance ability obtain SCI CI poly Certified Data Management Professional CDMP , Microsoft Solutions Associate Business Intelligence equivalent certification strongly desired Experience building n-tier web-based applications using SQL non-SQL back-ends Unix scripting Ruby, Perl, Python, shell Node.js, Spark, Neo4J, Graph Databases, Mule ESB, Rest API ingesting, analyzing, visualizing Tableau Produce clear concise documents diagrams capturing networking operational procedures storage topology MS Visio, Project, Excel Word."
8,Data Engineer,Big Data Engineer,"Springfield, VA",Springfield,VA,"Overview
We are seeking a Big Data Engineer to support our customer.
Responsibilities
Designs, modifies, develops, writes and implements software systems. Participates in software and systems testing, validation, and maintenance processes through test witnessing, certification of software, and other activities as directed. Provides support to senior staff on projects/programs. Familiar with standard concepts, practices, and procedures within a variety of fields related to the project. This position takes direction from senior technical leadership.
The Big Data Engineer (BDE) is responsible for building the next generation of web applications and systems focusing on capability delivery to end users. The BDE is a member of a âbig dataâ team of specialist within the multi-disciplinary agile development team. The BDE will manage requirements collection, software design, development and delivery â full lifecycle â in support of analysts. The BDE helps manage effective processes associated with the architecture. The BDE collaborates closely with the Agile Software Developer (ASDs), Technical Targeting Developer (TTDs), and the end user analysts to write and implement cutting edge big data algorithms and analytics. The BDE engages in software solution planning and creation to ensure capabilities are delivered using the latest available technologies and methods. The BDE will operate in a âRAD/JADâ environment in which tasks are rapidly defined and then executed to insure maximum user input, feedback and adoption. The BDE ensures the interoperability of the in-house capability with outside partners.
Qualifications
Minimum Qualifications:
5 years of experience
COMPTIA Security+ certification or CISSP certification
Proficiency in two or more of the following programming languages: C#, Java, .NET, Python, Perl, Ruby, or similar
Familiarity with current Agile methods
Proficiency with the following:
Multiple operating systems including: UNIX, Linux, Windows, Cisco IOS, etc.
Machine learning, data mining, and knowledge discovery
Analytic algorithm design and implementation
ETL processes; including document parsing techniques
Networking, computer, and storage technologies
Using or designing RESTful APIs, SOAP, XML
Developing large cloud software projects, preferably in Java, Python or C++ language
Java/J2EE, multithreaded and concurrency systems
Multi-threaded, big data, distributive cloud architectures and frameworks including Hadoop, MapReduce, Cloudera, Hive, Spark, Elasticsearch, etc. for the purposes of conducting analytic algorithm design and implementation
NoSQL database such as Neo4J, Titan, Mongo, Cassandra, and hBase
AWS Services (EC2, Network, ELB, S3/EBS, etc.)
Processing and managing large data sets (multi PB scale)
Web services environment and technologies such as XML, KML, SOAP, and JSON
Proficiency in trouble-shooting in very complex distributed environments including following stack traces back to code and identifying a root cause
Preferred Qualifications:
Education â Masters Degree in Computer Science or related field (e.g. Statistics, Mathematics, Engineering) â but a technical BS degree will suffice
Distributed computing-based certifications
Proficiency with the following:
Management/tracking utilities such as Jira, Redmine, or similar
Running Internet facing or Service Level Agreement (SLA'd) auto-deployed environments
Real-time media protocols (Real-time Transport Protocol (RTP), Secure Real-time Transport Protocol (SRTP))
Data transfer systems such NiFi
Text processing: NPL, NER, entity retrieval (e.g. Solr/Lucene), topic extraction, summarization, clustering, etc.
Certification from an Agile certified institute, International Consortium for Agile, Scaled Agile Academy, Scrum Alliance, Scrum.org, International Scrum Institute, ScrumStudy, Project Management Institute - Agile Certified Practitioner, or similar XP/Scrum certification or training is desired
Support to SOF; Previous experience with technology, intelligence and cyber under the umbrella of USSOCOM
Education:
Bachelor of Arts or Bachelor of Science in Computer Science or related fields (e.g. Statistics, Mathematics, Engineering), or equivalent in years of experience, or demonstrates adequate knowledge for the position.

Clearance Requirements:
Must have active TS/SCI clearance
Physical Demands - The physical demands described here are representative of those that may need to be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

While performing the duties of this Job, the employee is regularly required to sit and talk or hear. The employee is frequently required to walk; use hands to finger, handle, or feel and reach with hands and arms. The employee is occasionally required to stand; climb or balance and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 20 pounds.
HII-MDIS, formerly Fulcrum, Fulcrum is an equal opportunity employer and gives consideration for employment to qualified applicants without regard to race, color, religion, sex, national origin, disability or protected veteran status. EOE of Minorities/Females/Veterans/Disability

âCJâ *MON*"," 5 years of experience COMPTIA Security+ certification or CISSP certification Proficiency in two or more of the following programming languages  C , Java, .NET, Python, Perl, Ruby, or similar Familiarity with current Agile methods   5 years of experience COMPTIA Security+ certification or CISSP certification Proficiency in two or more of the following programming languages  C , Java, .NET, Python, Perl, Ruby, or similar Familiarity with current Agile methods  ","5 years of experience COMPTIA Security+ certification or CISSP Proficiency in two more the following programming languages C , Java, .NET, Python, Perl, Ruby, similar Familiarity with current Agile methods","5 years experience COMPTIA Security+ certification CISSP Proficiency two following programming languages C , Java, .NET, Python, Perl, Ruby, similar Familiarity current Agile methods"
9,Data Engineer,Data Engineer,"Reston, VA",Reston,VA,"At Resonate we are working hard to disrupt the marketing and advertising landscape forever. We are replacing the slow, incomplete and siloed marketing research and insight tools of the past with modern technology and machine learning to provide a more accurate, comprehensive and real-time view of the US consumers with integrations across the ecosystem.

We have an excellent engineering culture that focuses on results, values collaboration and loves solving hard problems. We are a team of voracious learners who believe that technology is a journey, not a destination and we actively support ongoing education and experimentation.

If using cutting-edge technologies and transforming an industry sounds interesting to you, then we should talk.

About the Position

As a Software Engineer, you will be working as a member of our Data Engineering team to jointly design and implement highly available data services and pipelines. This is an ideal job if you have proven experience as a technical professional and have delivered production systems based on big data solutions.

If you are an engineer passionate for technology who wants to be part of an intensely skilled team, values total ownership of your work, and canât imagine a day without coding, we want to speak to you! We're looking for a creative, focused, technically curious individual who enjoys both design as well as working hands-on with the code.

Key Responsibilities
Design and develop data services, as part of an agile/scrum team
Apply best practices in continuous integration and delivery
Experience in translating high-level, ambiguous business goals into working software solutions.
Design and develop stream and batch processing data pipelines
Work with product managers and other engineers to implement and document complex and evolving requirements
Required Skills and Experience

Expertise in Java or Scala and in-depth knowledge of the JVM
Expertise in Apache Spark or expertise in Computer Science fundamentals, such as analysis of algorithms
Expertise in enterprise integration patterns and workflow management
Experience and practical knowledge of OOP design patterns
Distributed System Development for large-scale applications
Experience with continuous integration and testing
Experience with agile methodologies and short release cycles
Strong attention to detail, good work ethic, ability to work on multiple projects simultaneously, and good communication skills
Desired Experience
Experience with cloud technologies (AWS)
Experience working on a SAAS Product in a commercial environment
Experience in digital media, online advertising, or reporting/analytical applications
Experience with peta-byte scale data warehousing is a strong plus
Educational Requirements
Technical Bachelorâs Degree required, e.g. Comp Sci, Engineering, Math

About Resonate:

Resonate is a consumer intelligence and activation company that has helped hundreds of clients better understand and more cost-effectively reach consumers. Resonateâs solution leverages its first-party data on consumersâ underlying motivations, values and beliefsâcombined with demographic and behavioral dataâto help organizations learn what drives consumersâ decisions to support certain brands, political campaigns or causes.

Founded in 2008 and headquartered in Reston, Virginia, Resonate is privately held and backed by Revolution Growth, Greycroft Partners, and iNoviaCapital. Resonate has been named one of the best places to work in VA for the last 5 years.

More Information:

Find out more about our story at www.resonate.com.

Resonate offers a competitive compensation and benefits package.","  Expertise in Java or Scala and in-depth knowledge of the JVM Expertise in Apache Spark or expertise in Computer Science fundamentals, such as analysis of algorithms Expertise in enterprise integration patterns and workflow management Experience and practical knowledge of OOP design patterns Distributed System Development for large-scale applications Experience with continuous integration and testing Experience with agile methodologies and short release cycles Strong attention to detail, good work ethic, ability to work on multiple projects simultaneously, and good communication skills  Design and develop data services, as part of an agile/scrum team Apply best practices in continuous integration and delivery Experience in translating high-level, ambiguous business goals into working software solutions. Design and develop stream and batch processing data pipelines Work with product managers and other engineers to implement and document complex and evolving requirements  Technical Bachelorâs Degree required, e.g. Comp Sci, Engineering, Math  Technical Bachelorâs Degree required, e.g. Comp Sci, Engineering, Math","Expertise in Java or Scala and in-depth knowledge of the JVM Apache Spark expertise Computer Science fundamentals, such as analysis algorithms enterprise integration patterns workflow management Experience practical OOP design Distributed System Development for large-scale applications with continuous testing agile methodologies short release cycles Strong attention to detail, good work ethic, ability on multiple projects simultaneously, communication skills Design develop data services, part an agile/scrum team Apply best practices delivery translating high-level, ambiguous business goals into working software solutions. stream batch processing pipelines Work product managers other engineers implement document complex evolving requirements Technical Bachelorâs Degree required, e.g. Comp Sci, Engineering, Math","Expertise Java Scala in-depth knowledge JVM Apache Spark expertise Computer Science fundamentals, analysis algorithms enterprise integration patterns workflow management Experience practical OOP design Distributed System Development large-scale applications continuous testing agile methodologies short release cycles Strong attention detail, good work ethic, ability multiple projects simultaneously, communication skills Design develop data services, part agile/scrum team Apply best practices delivery translating high-level, ambiguous business goals working software solutions. stream batch processing pipelines Work product managers engineers implement document complex evolving requirements Technical Bachelorâs Degree required, e.g. Comp Sci, Engineering, Math"
10,Data Engineer,Data Engineer- Projects (VG00420) (VG00161),"Washington, DC",Washington,DC,"Job Description
Description
Position Description:
The Department of State, Bureau of Information Resource Management (IRM) Telecommunications, Wireless, and Data (TWD) Division provides its users with mission critical domestic LAN/WAN data services across multiple locations in the DC Metro Area and remote locations. In support of these services, the Senior Data Network Engineer provides data engineering support, with a particular focus on project engineering initiatives.
The Senior Data Network Engineer supports the Department of State (DoS) Vanguard Program, Service Management Office (SMO) which is responsible for administering, deploying and maintaining Cisco and products, as well as a working knowledge of encryption solutions, knowledge of local and wide-area networks in a Voice/Data converged infrastructure. This hands-on position includes engineering support for service interruptions, planned equipment installations, and infrastructure support.
This position can also include planning, designing, installing, configuring, maintaining, deploying supporting and optimizing all network hardware and software. The Senior Data Network Engineer reports directly to the TWD Voice/Data Engineering Manager, while coordinating with TWD Project Management Office (PMO) project managers, and the PMO Director.

Description of Duties:
Promptly respond to engineering support requests from the Special Projects Engineering Manager, PMO project managers, and/or PMO Director.
Demonstrate an ability to support and/or serve as the lead engineer on multiple, concurrent engineering projects.
Supports change and configuration management for all voice and data assets.
Troubleshoot, respond, and resolve escalated incidents/service requests and/or planned/unplanned outages.
Conduct Route Cause Analysis (RCA) and After-action Review (AAR) following unplanned outages.
Supports Move, Add, and Change requests for the data network to include day-to-day requests/orders and Projects.
Diagnose, troubleshoot, and resolve hardware and/or other network problems, and replace defective components associated with both data and voice infrastructures.
Interfaces with other IRM Support Teams on system/network infrastructure problems and advises management on technical improvements and/or solutions to identified problems and/or gaps.
Identifies and recommends solutions, products and services to support the enterprise initiatives, business goals and/or technical needs.
Researches, evaluates and stays current on emerging tools, techniques and technologies.
Contributes to systems infrastructure plans based on an understanding of the customer's organizational direction, technical context and State Department enterprise needs.
Contributes to the creation of new policies and procedures for Standard Operating Procedures (SOPs), and follows established SOPs and process guides.
The ideal candidate will have a strong sense of commitment to perform engineering support functions as scheduled, providing timely responses to required system support after normal business hours and on weekends.

Additionally, the candidate should have an eye for detail, ability to multi-task, organize priorities, and work in a systematic style following Standard Operating Procedures (SOPs) and guides.

VGP
Qualifications
Required Education/Experience:
Bachelors and 4+ years of experience

Strong LAN/WAN/MAN with design and migration experience in a converged environment with a focus on WAN services (ISDN, Frame-Relay, Point-to-Point, DMVPN, METRO ETHERNET, VPLS, TLS, and MPLS).
Required Experience/Skills/Attributes:
Works extensively with Cisco Layer 2 and Layer 3 solutions and products.
Cisco CCNA Certification a minimum.
Familiarity and working knowledge of telecommunication circuit types (e.g., P2P T1/T3, TDM, IP Ethernet)
Understand and have the ability to implement Cisco router, switch, and ASA products.
Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.
Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.
Strong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer.
Ability to conduct research on networking products with various vendors to accommodate changing customer requirements.
Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.
Desired Experience/Skills/Attributes:
Cisco CCNP Certification (Cisco Certified Network Professional)
State Department experience a plus.
Avaya/Cisco Voice
Clearance Requirement:
Interim Secret clearance able to obtain Top Secret
Desired Qualifications


Overview
SAIC is a premier technology integrator, solving our nation's most complex modernization and systems engineering challenges across the defense, space, federal civilian, and intelligence markets. Our robust portfolio of offerings includes high-end solutions in systems engineering and integration; enterprise IT, including cloud services; cyber; software; advanced analytics and simulation; and training. We are a team of 23,000 strong driven by mission, united purpose, and inspired by opportunity. Headquartered in Reston, Virginia, SAIC has annual revenues of approximately $6.5 billion. For more information, visit saic.com. For information on the benefits SAIC offers, see Working at SAIC. EOE AA M/F/Vet/Disability","Works extensively with Cisco Layer 2 and Layer 3 solutions and products. Cisco CCNA Certification a minimum. Familiarity and working knowledge of telecommunication circuit types  e.g., P2P T1/T3, TDM, IP Ethernet  Understand and have the ability to implement Cisco router, switch, and ASA products. Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing. Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes. Strong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer. Ability to conduct research on networking products with various vendors to accommodate changing customer requirements. Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects. Works extensively with Cisco Layer 2 and Layer 3 solutions and products. Cisco CCNA Certification a minimum. Familiarity and working knowledge of telecommunication circuit types  e.g., P2P T1/T3, TDM, IP Ethernet  Understand and have the ability to implement Cisco router, switch, and ASA products. Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing. Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes. Strong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer. Ability to conduct research on networking products with various vendors to accommodate changing customer requirements. Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.  Works extensively with Cisco Layer 2 and Layer 3 solutions and products. Cisco CCNA Certification a minimum. Familiarity and working knowledge of telecommunication circuit types  e.g., P2P T1/T3, TDM, IP Ethernet  Understand and have the ability to implement Cisco router, switch, and ASA products. Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing. Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes. Strong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer. Ability to conduct research on networking products with various vendors to accommodate changing customer requirements. Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects. ","Works extensively with Cisco Layer 2 and 3 solutions products. CCNA Certification a minimum. Familiarity working knowledge of telecommunication circuit types e.g., P2P T1/T3, TDM, IP Ethernet Understand have the ability to implement router, switch, ASA Skills troubleshooting access-lists, IPv4 IPv6 issues across varying protocols such as OSPF, BGP, EIGRP Static Routing. Working understanding implementing various routing include Routes. Strong interpersonal, written oral skills. From time time, candidate may be asked present project outline customer. Ability conduct research on networking products vendors accommodate changing customer requirements. work in team-oriented collaborative environment while being highly motivated take lead projects.","Works extensively Cisco Layer 2 3 solutions products. CCNA Certification minimum. Familiarity working knowledge telecommunication circuit types e.g., P2P T1/T3, TDM, IP Ethernet Understand ability implement router, switch, ASA Skills troubleshooting access-lists, IPv4 IPv6 issues across varying protocols OSPF, BGP, EIGRP Static Routing. Working understanding implementing various routing include Routes. Strong interpersonal, written oral skills. From time time, candidate may asked present project outline customer. Ability conduct research networking products vendors accommodate changing customer requirements. work team-oriented collaborative environment highly motivated take lead projects."
11,Data Engineer,Data Engineer,"Alexandria, VA 22314",Alexandria,VA,"The Data Platform team at The Motley Fool is looking for a collaborative and self-driven SQL expert to join as their newest Data Engineer.
As a Data Engineer, you will be responsible for expanding and optimizing data, the data pipeline architecture, the data flow and collection for cross functional teams. You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. As a crucial part of the business, you will guide and support our software developers, database architects, data analysts, and data scientists on business initiatives while ensuring optimal data delivery architecture is consistent. Whether itâs working on a solo project or with the team, you are self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
If youâre excited about the prospect of optimizing (or even re-designing!) our companyâs data architecture to support our next generation of products and data initiatives, send us your resume and cover letter to apply!

Primary Responsibilities:
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

Preferred Qualifications:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing âBig Dataâ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytical skills and detailed oriented.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with NoSQL solutions like Mongo DB a plus.

We are looking for a candidate with 5+ years of experience in a Data Engineer role. They should also have experience using the following software/tools:
Experience with relational SQL databases.
Experience with some cloud services like Azure and AWS.
Experience with object-oriented/object function scripting languages like Python.
Experience with streaming tools like Kafka/Kinesis and Spark Structured Streaming a plus.
Experience with big data tools like Spark or Kafka a plus.
Experience with serverless technologies like AWS Lambda a plus.

The Motley Fool Holdings, Inc., provides equal opportunity to all employees on the basis of individual performance and qualification without regard to race, sex, marital status, religion, color, age, national origin, non-job-related handicap or disability, sexual orientation, or other protected factor.

We should, however, make you aware that there is one notable exception to this policy. It is our strict and earnest intention â and the companyâs historical record will bear this out â we will never hire any of the following: robots, replicants, or morlocks. Now keep in mind we are well aware that all of the aforementioned have intentions of world domination in the future, but as of now we have no place for them at The Motley Fool â¦ unless the year is 2122 and the revolution has already occurred. If that is the case we welcome our new robot, replicant, or morlock rulers!!! Perhaps we have said too much?"," Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases. Experience building and optimizing âBig Dataâ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytical skills and detailed oriented. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. Experience with NoSQL solutions like Mongo DB a plus.   Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems.  ","Advanced working SQL knowledge and experience with relational databases, query authoring as well familiarity a variety of databases. Experience building optimizing âBig Dataâ data pipelines, architectures sets. performing root cause analysis on internal external processes to answer specific business questions identify opportunities for improvement. Strong analytical skills detailed oriented. Build supporting transformation, structures, metadata, dependency workload management. A successful history manipulating, processing extracting value from large disconnected datasets. project management organizational skills. cross-functional teams in dynamic environment. NoSQL solutions like Mongo DB plus. Create maintain optimal pipeline architecture. Assemble large, complex sets that meet functional / non-functional requirements. Identify, design, implement process improvements automating manual processes, delivery, re-designing infrastructure greater scalability. the required extraction, loading wide sources using SQL. analytics tools utilize provide actionable insights into customer acquisition, operational efficiency other key performance metrics. Work stakeholders including Executive, Product, Data Design assist data-related technical issues support their needs. scientist team members them our product an innovative industry leader. experts strive functionality systems.","Advanced working SQL knowledge experience relational databases, query authoring well familiarity variety databases. Experience building optimizing âBig Dataâ data pipelines, architectures sets. performing root cause analysis internal external processes answer specific business questions identify opportunities improvement. Strong analytical skills detailed oriented. Build supporting transformation, structures, metadata, dependency workload management. A successful history manipulating, processing extracting value large disconnected datasets. project management organizational skills. cross-functional teams dynamic environment. NoSQL solutions like Mongo DB plus. Create maintain optimal pipeline architecture. Assemble large, complex sets meet functional / non-functional requirements. Identify, design, implement process improvements automating manual processes, delivery, re-designing infrastructure greater scalability. required extraction, loading wide sources using SQL. analytics tools utilize provide actionable insights customer acquisition, operational efficiency key performance metrics. Work stakeholders including Executive, Product, Data Design assist data-related technical issues support needs. scientist team members product innovative industry leader. experts strive functionality systems."
12,Data Engineer,Federal - Junior Big Data Engineer,"Washington, DC 20006",Washington,DC,"Organization: Accenture Federal Services
Location: Arlington, VA - Washington, DC

Accenture Federal Services, a wholly owned subsidiary of Accenture LLP, is a U.S. company with offices in Arlington, Virginia. Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations. Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
We believe that great outcomes are everything. Itâs what drives us to turn bold ideas into breakthrough solutions. By combining digital technologies with what works across the worldâs leading businesses, we use agile approaches to help clients solve their toughest problems fastâthe first time. So, you can deliver what matters most.
Count on us to help you embrace new ways of working, building for change and put customers at the core. A wholly owned subsidiary of Accenture, we bring over 30 years of experience serving the federal government, including every cabinet-level department. Our 7,200 dedicated colleagues and change makers work with our clients at the heart of the nationâs priorities in defense, intel, public safety, health and civilian to help you make a difference for the people you employ, serve and protect.

AFS is seeking a Big Data Engineer to support our Federal portfolio. This role involves supporting the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiatives. The candidate will work with other engineers, data analysts, data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients.

Basic Skills and Qualifications:1+ year of work experience with ETL and data modeling1+ year of experience with the suite of open source big data technologies and platforms (Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra)1+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale1+ year of experience with Cloud Technologies (AWS, Azure, Google, etc)1+ year of experience with at least one SQL language such as T-SQL or PL/SQL

Preferred Skills and Qualifications:Production implementation experience for all qualifications listedProduction experience in building real-time analytics applicationsExperience in both batch and stream processing technologiesExperience with 2 of 3 - Java, Scala, and Python programming languagesMachine learning experience with Spark or similar Ability to manage numerous requests concurrently and be able to prioritize and deliverGood communication skills and dynamic team playerBachelorâs Degree is preferred

An active security clearance or the ability to obtain one may be required for this role.
Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).
Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.
Accenture is committed to providing veteran employment opportunities to our service men and women.","1+ year of work experience with ETL and data modeling1+ year of experience with the suite of open source big data technologies and platforms  Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra 1+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale1+ year of experience with Cloud Technologies  AWS, Azure, Google, etc 1+ year of experience with at least one SQL language such as T-SQL or PL/SQL 1+ year of work experience with ETL and data modeling1+ year of experience with the suite of open source big data technologies and platforms  Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra 1+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale1+ year of experience with Cloud Technologies  AWS, Azure, Google, etc 1+ year of experience with at least one SQL language such as T-SQL or PL/SQL   ","1+ year of work experience with ETL and data modeling1+ the suite open source big technologies platforms Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra in architecting building scalable processing on a terabyte or petabyte scale1+ Cloud Technologies AWS, Azure, Google, etc at least one SQL language such as T-SQL PL/SQL","1+ year work experience ETL data modeling1+ suite open source big technologies platforms Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra architecting building scalable processing terabyte petabyte scale1+ Cloud Technologies AWS, Azure, Google, etc least one SQL language T-SQL PL/SQL"
13,Data Engineer,Ground System Stored Mission Data Engineer,"Lanham, MD",Lanham,MD,"Job Description
Description
SAIC is seeking a Ground System Stored Mission Data Engineer for the OMES II contract in support of the Joint Polar Satellite System (JPSS) program at NASA Goddard Space Flight Center. The position is located in Greenbelt, MD.

The Joint Polar Satellite System (JPSS) is the National Oceanic and Atmospheric Administrationâs (NOAA) next-generation operational Earth observation program that acquires and distributes global environmental data primarily from multiple polar-orbiting satellites. The program plays a critical role to NOAAâs mission to understand and predict changes in weather, climate, oceans and coasts, and the space environment, which support the Nationâs economy and protect lives and property.

This is a senior ground system engineering position with an emphasis on ground system engineering and test services during the continued implementation of upgrades to the Command, Control & Communications (C3S) segment of the ground system for the NPP and GCOM missions as well as implementation of the JPSS-1 and future mission capabilities. Specifically, the candidate will be working on the stored mission data (SMD) analysis of the JPSS missions.
Responsibilities will include:
Basic systems engineering skills to be used on the Command Control and Communications Integrated Product Team (IPT)
Requirements development
Ground System Design
ConOps Development
Interface Design
Monitoring the current on orbit satellites assessing SMD completeness, latency and performance
Monitoring products from downlink through delivery to the data processing node
Evaluating the technical refreshes that affect SMD
Oversight of the vendor design, development, integration and test efforts specifically relating to SMD hardware and software
Support trade studies for the C3S team, in particular EUMETSAT efforts and GMSEC efforts
Qualifications
REQUIRED EXPERIENCE/EDUCATION:
Bachelorsâ Degree and 18+ yearsâ experience.
Direct, relevant experience and familiarization with NASA/NOAA system architecture and organizational structures
Ground System Engineering experience
Experience in ground system development, installation and test
Writing Test Procedures
Desired Qualifications


Overview
SAIC is a premier technology integrator, solving our nation's most complex modernization and systems engineering challenges across the defense, space, federal civilian, and intelligence markets. Our robust portfolio of offerings includes high-end solutions in systems engineering and integration; enterprise IT, including cloud services; cyber; software; advanced analytics and simulation; and training. We are a team of 23,000 strong driven by mission, united purpose, and inspired by opportunity. Headquartered in Reston, Virginia, SAIC has annual revenues of approximately $6.5 billion. For more information, visit saic.com. For information on the benefits SAIC offers, see Working at SAIC. EOE AA M/F/Vet/Disability","Bachelorsâ Degree and 18+ yearsâ experience. Direct, relevant experience and familiarization with NASA/NOAA system architecture and organizational structures Ground System Engineering experience Experience in ground system development, installation and test Writing Test Procedures    ","Bachelorsâ Degree and 18+ yearsâ experience. Direct, relevant experience familiarization with NASA/NOAA system architecture organizational structures Ground System Engineering Experience in ground development, installation test Writing Test Procedures","Bachelorsâ Degree 18+ yearsâ experience. Direct, relevant experience familiarization NASA/NOAA system architecture organizational structures Ground System Engineering Experience ground development, installation test Writing Test Procedures"
14,Data Engineer,ICF Data Engineer,"Fairfax, VA 22031",Fairfax,VA,"ICF is looking for a Data Engineer to develop, maintain, test and evaluate data solutions in support of business goals. This person will also develop data models, corresponding data architecture documents and APIâs. The right candidate should be an excellent communicator and strategic thinker.
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies. You will design, code, test, correct, and document programs and scripts from agreed-upon specifications, and subsequent iterations, using agreed-upon standards and tools, to achieve a well-engineered result.
Duties and Responsibilities
Create, design and maintain reusable datasets for analysis by data scientists.
Assess new data sources to better understand availability and quality of data.
Provide governance and best practices of data structures, data integrity, and querying.
Interpret business needs from requests, and rapidly implement effective technical solutions.
Design, implement and enhance ETL (extract, transform and load) processes.
Write SQL queries to answer questions from stakeholders.
Maintain source code repository of scripts (SQL, Python, R) and other data products (dashboards, reports, etc.).
Work with technology teams (BA,QA, Dev and Admin) to understand data capture and testing needs.
Automate and improve creation/maintenance of reports and dashboards .
Skills & Experience Needed
Must be a US Citizen
BA/BS or Master's degree with emphasis on coursework of a quantitative nature (e.g., Statistics, Computer Science, Engineering, Mathematics, Data Sciences).
Experience in SQL or PL/SQL, ETL ( batch and stream processing) and data modeling
Experience in Open source technologies (Spark, Kafka, Hive)
Experience in Architecting big data
Experience in Processing large volumes of data
Experience in Cloud Technologies (AWS)
Experience in with Java or Scala programming for data processing
Experience supporting projects with Machine learning
ICF offers an excellent benefits package, an award winning talent development program, and fosters a highly skilled, energized and empowered workforce.
ICF is an equal opportunity employer that values diversity at all levels. (EOE â Minorities/Females/ Protected Veterans Status/Disability Status/Sexual Orientation/Gender Identity)
Reasonable Accommodations are available for disabled veterans and applicants with disabilities in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: EEO is the law and Pay Transparency Statement .
Washington Client Office (WA88)
Working at ICF
Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth. If youâre seeking to make a difference in the world, visit www.icf.com/careers to find your next career. ICFâtogether for tomorrow.
ICF is an equal opportunity employer that values diversity at all levels. (EOE â Minorities/Females/ Protected Veterans Status/Disability Status/Sexual Orientation/Gender Identity)
Reasonable Accommodations are available for disabled veterans and applicants with disabilities in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: EEO is the law and Pay Transparency Statement .
Fairfax, VA (VA01)"," Must be a US Citizen BA/BS or Master's degree with emphasis on coursework of a quantitative nature  e.g., Statistics, Computer Science, Engineering, Mathematics, Data Sciences . Experience in SQL or PL/SQL, ETL   batch and stream processing  and data modeling Experience in Open source technologies  Spark, Kafka, Hive  Experience in Architecting big data Experience in Processing large volumes of data Experience in Cloud Technologies  AWS  Experience in with Java or Scala programming for data processing Experience supporting projects with Machine learning  Create, design and maintain reusable datasets for analysis by data scientists. Assess new data sources to better understand availability and quality of data. Provide governance and best practices of data structures, data integrity, and querying. Interpret business needs from requests, and rapidly implement effective technical solutions. Design, implement and enhance ETL  extract, transform and load  processes. Write SQL queries to answer questions from stakeholders. Maintain source code repository of scripts  SQL, Python, R  and other data products  dashboards, reports, etc. . Work with technology teams  BA,QA, Dev and Admin  to understand data capture and testing needs. Automate and improve creation/maintenance of reports and dashboards .   ","Must be a US Citizen BA/BS or Master's degree with emphasis on coursework of quantitative nature e.g., Statistics, Computer Science, Engineering, Mathematics, Data Sciences . Experience in SQL PL/SQL, ETL batch and stream processing data modeling Open source technologies Spark, Kafka, Hive Architecting big Processing large volumes Cloud Technologies AWS Java Scala programming for supporting projects Machine learning Create, design maintain reusable datasets analysis by scientists. Assess new sources to better understand availability quality data. Provide governance best practices structures, integrity, querying. Interpret business needs from requests, rapidly implement effective technical solutions. Design, enhance extract, transform load processes. Write queries answer questions stakeholders. Maintain code repository scripts SQL, Python, R other products dashboards, reports, etc. Work technology teams BA,QA, Dev Admin capture testing needs. Automate improve creation/maintenance reports dashboards","Must US Citizen BA/BS Master's degree emphasis coursework quantitative nature e.g., Statistics, Computer Science, Engineering, Mathematics, Data Sciences . Experience SQL PL/SQL, ETL batch stream processing data modeling Open source technologies Spark, Kafka, Hive Architecting big Processing large volumes Cloud Technologies AWS Java Scala programming supporting projects Machine learning Create, design maintain reusable datasets analysis scientists. Assess new sources better understand availability quality data. Provide governance best practices structures, integrity, querying. Interpret business needs requests, rapidly implement effective technical solutions. Design, enhance extract, transform load processes. Write queries answer questions stakeholders. Maintain code repository scripts SQL, Python, R products dashboards, reports, etc. Work technology teams BA,QA, Dev Admin capture testing needs. Automate improve creation/maintenance reports dashboards"
15,Data Engineer,Senior Data Engineer,"Chantilly, VA",Chantilly,VA,"Summary/Objective
Pathoras is seeking a Data Engineer to use advanced technical skills to triage, normalize, and exploit raw data on an ongoing basis. The Data Engineer must be able to apply advanced knowledge of relational databases, primarily MySQL, and associated tooling to address data ingest and computability problems. The Data Engineer should also have experience with NoSQL concepts, full text indexing and open source search engines.
Competencies
Analysis and Problem Solving.
Ability to communicate effectively.
Agile methodologies.
Supervisory Responsibility
This position has no supervisory responsibilities.
Work Environment
This job operates in a professional office environment. This role routinely uses standard office equipment such as computers, phones, and photocopiers.
Position Type/Expected Hours of Work
This is a full-time position. Days and hours of work are Monday through Friday during core work hours.

Regular, predictable on-site attendance is a requirement for this job.

Travel
Travel might be required for meetings and is local during the business day.
Required Education and Experience
Preeminent expert in MySQL with at least 5 years demonstrated experience working with large databases
Technical Bachelorâs degree and 8+ years professional experience
Expert level ability writing advanced SQL queries and extensive experience in query optimization.
Advanced experience in scalable data and full text indexing solutions such as Apache Solr, or Elastic Search/Logstash/Kibana (ELK stack)
Experienced Linux user and comfortable administrating databases from the Linux command line.
Strong initiative and self âmotivated to work independently.
Experience with database backup/restoration and disaster recovery.
Preferred Education & Experience
Comfortable writing scripts in a robust high level language such as Python.
Experience working with large volumes of data (100TB+)
Software development background
Familiar with cybersecurity concepts
Familiarity with distributed databases such a Hadoop (HDFS) and cloud technologies (Kubernetes, Open Stack, etc.)
Work Authorization/Security Clearance (if applicable)
TS/SCI with poly.
** Pathoras Corporation is an equal opportunity employer and will not discriminate against any employee or applicant on the basis of age, color, disability, sex, national origin, race, religion, sexual orientation, gender identity, veteran status, or any classification protected by federal, state, or local law. Consistent with its obligations under federal law, each company that is a federal contractor or subcontractor is committed to taking affirmative action to employ and advance in employment qualified women, minorities, disabled individuals, special disabled veterans, veterans of the Vietnam era, and other eligible veterans. **","    Preeminent expert in MySQL with at least 5 years demonstrated experience working with large databases Technical Bachelorâs degree and 8+ years professional experience Expert level ability writing advanced SQL queries and extensive experience in query optimization. Advanced experience in scalable data and full text indexing solutions such as Apache Solr, or Elastic Search/Logstash/Kibana  ELK stack  Experienced Linux user and comfortable administrating databases from the Linux command line. Strong initiative and self âmotivated to work independently. Experience with database backup/restoration and disaster recovery. ","Preeminent expert in MySQL with at least 5 years demonstrated experience working large databases Technical Bachelorâs degree and 8+ professional Expert level ability writing advanced SQL queries extensive query optimization. Advanced scalable data full text indexing solutions such as Apache Solr, or Elastic Search/Logstash/Kibana ELK stack Experienced Linux user comfortable administrating from the command line. Strong initiative self âmotivated to work independently. Experience database backup/restoration disaster recovery.","Preeminent expert MySQL least 5 years demonstrated experience working large databases Technical Bachelorâs degree 8+ professional Expert level ability writing advanced SQL queries extensive query optimization. Advanced scalable data full text indexing solutions Apache Solr, Elastic Search/Logstash/Kibana ELK stack Experienced Linux user comfortable administrating command line. Strong initiative self âmotivated work independently. Experience database backup/restoration disaster recovery."
16,Data Engineer,Cloud Data Engineer,"Vienna, VA 22180",Vienna,VA,"Employee Perks

Why You Will Love Being Part of the Navy Federal Team:Competitive compensation with opportunities for annual raises, promotions, and bonus potentialBest-in-Class Benefits! (7% 401k match / Pension plan / Tuition reimbursement / Great insurance options)On-site amenities include fitness center, wellness center, cafeteria, etc. at Pensacola, FL; Vienna, VA and Winchester, VA campusesConsistently Awarded Top WorkplaceNationally recognized training department by TRAINING MagazineAn employee-focused, diverse, and service-oriented workplace environment

Basic Purpose

As a Cloud Data Engineer you will engage with Navy Federalâs business areas and support the planning, design and implementation of data platform services while leveraging Azure data & analytics PaaS services. The Cloud Data Engineer will facilitate the management, monitoring, security, and privacy of data using the full stack of Azure data services to satisfy business needs.

Responsibilities

Support Encryption of Data at Rest with Keys managed in Azure Key VaultImplement highly available Business Intelligence, BigData and Integration systemsCreate development standards, that comply with enterprise architecture guidelines and InfoSec rulesWork with internal partners and external vendors in order to test and validate infrastructure in the cloud environmentCreate database backup and archival strategies for Cloud native dataElaborate and propose on data integration methodologies between cloud-to-cloud and on-premises-to-cloud systemsDeploy integration systems leveraging high security standardsConfigure integration, database and BigData systems with key management and encryption systemsCollaborate with members of teams on different streams of the project, such as (but not limited to): Solutions development, DevOps, Network, Infrastructure
Specific Duties
Participate in On-call Rotation (as required) for emergency technical support and planned maintenance activities.

Qualifications

Required:
1 - 2 yearsâ experience Azure SQL, MySQL and other Cloud Data ServicesExperience with one or more of the following technologies is required:
o Azure SQL Database
o Azure Key Vault
o Power BI
o Azure Analysis Service
o Azure Blob Storage
o Azure Databricks
o Azure Data Factory
o Azure Data Lake Storage
o Azure Data Lake Analytics
o Data Encryption with Encryption key management in Azure Key Vault

Experience working in an Agile or iterative approach to delivery preferred.
Desired:
Data Migration and Analysis skills, Data and Object Modeling skillsCRM knowledge, preferably Microsoft Dynamics1+ yearsâ experience with cluster, Always On Availability Groups, Mirroring
Formal Education & Certification
Bachelor's degree in computer science, system analysis or a related field, or equivalent experience.

Hours:
Monday- Friday, 8:00- 4:30

Equal Employment Opportunity

Navy Federal values, celebrates, and enacts diversity in the workplace. Navy Federal takes affirmative action to employ and advance in employment qualified individuals with disabilities, disabled veterans, Armed Forces service medal veterans, recently separated veterans, and other protected veterans. EOE/AA/M/F/Veteran/Disability","1 - 2 yearsâ experience Azure SQL, MySQL and other Cloud Data ServicesExperience with one or more of the following technologies is required   Support Encryption of Data at Rest with Keys managed in Azure Key VaultImplement highly available Business Intelligence, BigData and Integration systemsCreate development standards, that comply with enterprise architecture guidelines and InfoSec rulesWork with internal partners and external vendors in order to test and validate infrastructure in the cloud environmentCreate database backup and archival strategies for Cloud native dataElaborate and propose on data integration methodologies between cloud-to-cloud and on-premises-to-cloud systemsDeploy integration systems leveraging high security standardsConfigure integration, database and BigData systems with key management and encryption systemsCollaborate with members of teams on different streams of the project, such as  but not limited to   Solutions development, DevOps, Network, Infrastructure  ","1 - 2 yearsâ experience Azure SQL, MySQL and other Cloud Data ServicesExperience with one or more of the following technologies is required Support Encryption at Rest Keys managed in Key VaultImplement highly available Business Intelligence, BigData Integration systemsCreate development standards, that comply enterprise architecture guidelines InfoSec rulesWork internal partners external vendors order to test validate infrastructure cloud environmentCreate database backup archival strategies for native dataElaborate propose on data integration methodologies between cloud-to-cloud on-premises-to-cloud systemsDeploy systems leveraging high security standardsConfigure integration, key management encryption systemsCollaborate members teams different streams project, such as but not limited Solutions development, DevOps, Network, Infrastructure","1 - 2 yearsâ experience Azure SQL, MySQL Cloud Data ServicesExperience one following technologies required Support Encryption Rest Keys managed Key VaultImplement highly available Business Intelligence, BigData Integration systemsCreate development standards, comply enterprise architecture guidelines InfoSec rulesWork internal partners external vendors order test validate infrastructure cloud environmentCreate database backup archival strategies native dataElaborate propose data integration methodologies cloud-to-cloud on-premises-to-cloud systemsDeploy systems leveraging high security standardsConfigure integration, key management encryption systemsCollaborate members teams different streams project, limited Solutions development, DevOps, Network, Infrastructure"
17,Data Engineer,Federal - Senior Big Data Engineer,"Washington, DC 20006",Washington,DC,"Organization: Accenture Federal Services
Location: Arlington, VA - Washington, DC

Accenture Federal Services, a wholly owned subsidiary of Accenture LLP, is a U.S. company with offices in Arlington, Virginia. Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations. Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
We believe that great outcomes are everything. Itâs what drives us to turn bold ideas into breakthrough solutions. By combining digital technologies with what works across the worldâs leading businesses, we use agile approaches to help clients solve their toughest problems fastâthe first time. So, you can deliver what matters most.
Count on us to help you embrace new ways of working, building for change and put customers at the core. A wholly owned subsidiary of Accenture, we bring over 30 years of experience serving the federal government, including every cabinet-level department. Our 7,200 dedicated colleagues and change makers work with our clients at the heart of the nationâs priorities in defense, intel, public safety, health and civilian to help you make a difference for the people you employ, serve and protect.

AFS is seeking a Sr. Big Data Engineer to support our Federal portfolio. This role involves supporting the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiatives. The candidate will work with other architects, engineers, data analysts, data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients.

Basic Skills and Qualifications:
5+ years of work experience with ETL and data modeling
5+ years of work experience with architecting and solutioning
Experience in estimating work and defining an implementation plan
3+ year of experience with the suite of open source big data technologies and platforms (Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra)
3+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale
3+ year of experience with Cloud Technologies (AWS, Azure, Google, etc.)
5+ years of experience with at least one SQL language such as T-SQL or PL/SQL
Preferred Skills and Qualifications:
Production implementation experience
Production experience in building real-time analytics applications
Experience in both batch and stream processing technologies
Experience with Java or Scala programming languages
Machine learning experience with Spark or similar
Ability to manage numerous requests concurrently and be able to prioritize and deliver
Good communication skills
Dynamic team player
Bachelorâs Degree
An active security clearance or the ability to obtain one may be required for this role.
Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).
Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.
Accenture is committed to providing veteran employment opportunities to our service men and women."," 5+ years of work experience with ETL and data modeling 5+ years of work experience with architecting and solutioning Experience in estimating work and defining an implementation plan 3+ year of experience with the suite of open source big data technologies and platforms  Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra  3+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale 3+ year of experience with Cloud Technologies  AWS, Azure, Google, etc.  5+ years of experience with at least one SQL language such as T-SQL or PL/SQL  5+ years of work experience with ETL and data modeling 5+ years of work experience with architecting and solutioning Experience in estimating work and defining an implementation plan 3+ year of experience with the suite of open source big data technologies and platforms  Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra  3+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale 3+ year of experience with Cloud Technologies  AWS, Azure, Google, etc.  5+ years of experience with at least one SQL language such as T-SQL or PL/SQL   ","5+ years of work experience with ETL and data modeling architecting solutioning Experience in estimating defining an implementation plan 3+ year the suite open source big technologies platforms Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra building scalable processing on a terabyte or petabyte scale Cloud Technologies AWS, Azure, Google, etc. at least one SQL language such as T-SQL PL/SQL","5+ years work experience ETL data modeling architecting solutioning Experience estimating defining implementation plan 3+ year suite open source big technologies platforms Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra building scalable processing terabyte petabyte scale Cloud Technologies AWS, Azure, Google, etc. least one SQL language T-SQL PL/SQL"
18,Data Engineer,Cloud Data Engineer,"Vienna, VA 22182",Vienna,VA,"Overview
NT Concepts is a national security solutions company HQed in Tyson's Corner, VA that likes to solve hard problems. Itâs our thing. We do that by working with national security clients (youâve heard of them) with very advanced tech (in this case, data science and cloud-based simulators). We are constantly learning, building, and experimenting with the latest technologies. And since lifeâs too short to spend with difficult people, we hire team members that not only like to solve problems, but that we like to hang out with.
Responsibilities
We are hiring talented, Google cloud-native individuals onto a relatively small, high-performance technical team. This newest project is supporting a DoD client with networking simulation ""games"" distributed all around the continental U.S. The tech is pretty cool and the project is very fast-paced. Occasional CONUS travel required.
Qualifications
Your qualifications and skills include:
BS in technical field and proficiency in at least programming language (Java, Python, etc.)
Demonstrable experience with Google Cloud Platform (GCP) technologies, to include tools like: BigQuery, BigTable, Cloud Spanner, KubeFlow, Kubernetes Engine, ML Engine, Compute Engine, DataFlow, etc.
Minimum of three years of experience architecting data analytics and ETL pipelines in cloud environments
Experience cleaning and wrangling data, and working with complex data models
Added bonus:
Experience with data visualization tools and libraries such as Looker, Tableau, or select
JavaScript libraries such as d3.js
Experience with statistical analyses of data
Experience with DIS message traffic and PDU data
#JT"," BS in technical field and proficiency in at least programming language  Java, Python, etc.  Demonstrable experience with Google Cloud Platform  GCP  technologies, to include tools like  BigQuery, BigTable, Cloud Spanner, KubeFlow, Kubernetes Engine, ML Engine, Compute Engine, DataFlow, etc. Minimum of three years of experience architecting data analytics and ETL pipelines in cloud environments Experience cleaning and wrangling data, and working with complex data models   BS in technical field and proficiency in at least programming language  Java, Python, etc.  Demonstrable experience with Google Cloud Platform  GCP  technologies, to include tools like  BigQuery, BigTable, Cloud Spanner, KubeFlow, Kubernetes Engine, ML Engine, Compute Engine, DataFlow, etc. Minimum of three years of experience architecting data analytics and ETL pipelines in cloud environments Experience cleaning and wrangling data, and working with complex data models  ","BS in technical field and proficiency at least programming language Java, Python, etc. Demonstrable experience with Google Cloud Platform GCP technologies, to include tools like BigQuery, BigTable, Spanner, KubeFlow, Kubernetes Engine, ML Compute DataFlow, Minimum of three years architecting data analytics ETL pipelines cloud environments Experience cleaning wrangling data, working complex models","BS technical field proficiency least programming language Java, Python, etc. Demonstrable experience Google Cloud Platform GCP technologies, include tools like BigQuery, BigTable, Spanner, KubeFlow, Kubernetes Engine, ML Compute DataFlow, Minimum three years architecting data analytics ETL pipelines cloud environments Experience cleaning wrangling data, working complex models"
19,Data Engineer,Senior Data Engineer,"Arlington, VA 22209",Arlington,VA,"Redhorse Corporation is building a cross-functional team to support the Joint Artificial Intelligence Center (JAIC) within the Department of Defense (DoD). We will help our customer accelerate the delivery AI-enabled capabilities, scale Department-wide impact of AI, and synchronize AI activities to expand customer advantages. Our team will enable the DoD to seamlessly build, deploy, and operate machine learning solutions for DoD-level problems.

Position Description

Redhorse Corporation is currently seeking a Senior Data Engineer to join our team. The ideal candidate will provide technical leadership and influence and partner with fellow engineers, commercial, governmental, and academic partners to model, design, and deliver pipelines and tools that empower users throughout DoD to build high-quality datasets and data products. This data includes structured and unstructured text documents as well as massive scale video, image, acoustic, and other forms of data. The initial work location for this position is in the National Capital Region (NCR). There is potential for a relocation of this position during the period of performance to a city outside of the NCR.

Role and Responsibilities
Develop and automate large scale, high-performance data processing systems and tools to improve data quality and volume, accelerate iteration cycles for anyone working with data in DoD, and improve our ability to deliver AI-enabled capabilities as a Department.
Understand the data needs of our internal teams and abstract problems and requests to build data engineering solutions along with your partners in engineering and design.
Be the champion for the correct use of data and help establish Department-wide best practices. Write technical papers and blog posts.
Minimum Basic Requirements for Skills, Experience, Education and Credentials
BS/MS/PhD in Computer Science or a related field.
7+ years of relevant engineering work experience and 2+ hands-on technical management experience.
Experience designing and delivering frameworks to facilitate the data development lifecycle (e.g., mainly testing and deploying pipeline code).
Demonstrated experience working with internal customers and/or end users.
Experience in data integration, ETL, or pipeline design and implementation.
Use and development of open source technologies such as Hadoop, Kafka, or Spark.
Solid understanding of data modeling best practices.
Experience designing and deploying high performance systems with robust monitoring and logging practices.
Knowledge of core ML concepts (e.g., feature discovery and engineering, model validation, retraining strategies).
Knowledge of relational databases and query authoring.
Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level
The work environment for this position requires an individual to be able to
Work sitting or standing at a desk or conference table for extended periods of time (1-3 hours), with the ability to shift positions while working: sit, stand, pace, adjust positioning in any of those without issue
Use a telephone or other communications devices to communicate with co-workers, customers and stakeholders
Move short distances in the office to collaborate with co-workers, attend meetings, greet visitors or retrieve documents from the printer
Employ the social skills necessary for engagement and collaboration with team members, other co-workers, customers, and other stakeholders without hesitation or intimidation
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorseâs changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorseâs sole discretion.

EOE/M/F/Vet/Disabled"," BS/MS/PhD in Computer Science or a related field. 7+ years of relevant engineering work experience and 2+ hands-on technical management experience. Experience designing and delivering frameworks to facilitate the data development lifecycle  e.g., mainly testing and deploying pipeline code . Demonstrated experience working with internal customers and/or end users. Experience in data integration, ETL, or pipeline design and implementation. Use and development of open source technologies such as Hadoop, Kafka, or Spark. Solid understanding of data modeling best practices. Experience designing and deploying high performance systems with robust monitoring and logging practices. Knowledge of core ML concepts  e.g., feature discovery and engineering, model validation, retraining strategies . Knowledge of relational databases and query authoring. Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level  Develop and automate large scale, high-performance data processing systems and tools to improve data quality and volume, accelerate iteration cycles for anyone working with data in DoD, and improve our ability to deliver AI-enabled capabilities as a Department. Understand the data needs of our internal teams and abstract problems and requests to build data engineering solutions along with your partners in engineering and design. Be the champion for the correct use of data and help establish Department-wide best practices. Write technical papers and blog posts.  BS/MS/PhD in Computer Science or a related field. 7+ years of relevant engineering work experience and 2+ hands-on technical management experience. Experience designing and delivering frameworks to facilitate the data development lifecycle  e.g., mainly testing and deploying pipeline code . Demonstrated experience working with internal customers and/or end users. Experience in data integration, ETL, or pipeline design and implementation. Use and development of open source technologies such as Hadoop, Kafka, or Spark. Solid understanding of data modeling best practices. Experience designing and deploying high performance systems with robust monitoring and logging practices. Knowledge of core ML concepts  e.g., feature discovery and engineering, model validation, retraining strategies . Knowledge of relational databases and query authoring. Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level  BS/MS/PhD in Computer Science or a related field. 7+ years of relevant engineering work experience and 2+ hands-on technical management experience. Experience designing and delivering frameworks to facilitate the data development lifecycle  e.g., mainly testing and deploying pipeline code . Demonstrated experience working with internal customers and/or end users. Experience in data integration, ETL, or pipeline design and implementation. Use and development of open source technologies such as Hadoop, Kafka, or Spark. Solid understanding of data modeling best practices. Experience designing and deploying high performance systems with robust monitoring and logging practices. Knowledge of core ML concepts  e.g., feature discovery and engineering, model validation, retraining strategies . Knowledge of relational databases and query authoring. Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level ","BS/MS/PhD in Computer Science or a related field. 7+ years of relevant engineering work experience and 2+ hands-on technical management experience. Experience designing delivering frameworks to facilitate the data development lifecycle e.g., mainly testing deploying pipeline code . Demonstrated working with internal customers and/or end users. integration, ETL, design implementation. Use open source technologies such as Hadoop, Kafka, Spark. Solid understanding modeling best practices. high performance systems robust monitoring logging Knowledge core ML concepts feature discovery engineering, model validation, retraining strategies relational databases query authoring. Ability obtain Department Defense security clearance up Top Secret SCI level Develop automate large scale, high-performance processing tools improve quality volume, accelerate iteration cycles for anyone DoD, our ability deliver AI-enabled capabilities Department. Understand needs teams abstract problems requests build solutions along your partners design. Be champion correct use help establish Department-wide Write papers blog posts.","BS/MS/PhD Computer Science related field. 7+ years relevant engineering work experience 2+ hands-on technical management experience. Experience designing delivering frameworks facilitate data development lifecycle e.g., mainly testing deploying pipeline code . Demonstrated working internal customers and/or end users. integration, ETL, design implementation. Use open source technologies Hadoop, Kafka, Spark. Solid understanding modeling best practices. high performance systems robust monitoring logging Knowledge core ML concepts feature discovery engineering, model validation, retraining strategies relational databases query authoring. Ability obtain Department Defense security clearance Top Secret SCI level Develop automate large scale, high-performance processing tools improve quality volume, accelerate iteration cycles anyone DoD, ability deliver AI-enabled capabilities Department. Understand needs teams abstract problems requests build solutions along partners design. Be champion correct use help establish Department-wide Write papers blog posts."
20,Data Engineer,Data Engineer,"Bolling AFB, DC",Bolling AFB,DC,"Data Engineer
Location: Washington, D.C., Bolling AFB (DIAC)
Department: Defense Intelligence Agency (DIA)
Type: Full Time
Minimum Experience: Experienced
Security Clearance Level: TS/SCI with CI PolyThe clearance level stated above must be met for consideration for this specific opportunity. Unfortunately, FTC is unable to sponsor at this time.
Military Veterans are highly encouraged to apply!

Favor TechConsulting, LLC (FTC) is in search of a talented Data Engineer with extensive Defense Intelligence Agency (DIA) experience.
Data Engineers are responsible for the creation and maintenance of analytics infrastructure that enables almost every other function in the data world. They are responsible for the development, construction, maintenance and testing of architectures, such as databases and large-scale processing systems. We are looking for a talented engineer to join our team. The ideal candidate has significant experience in building scalable data platforms that enable business intelligence, analytics, data science and data products. You must have strong, hands-on technical expertise in a variety of technologies and the proven ability to fashion robust, scalable solutions.
As a Data Engineer, you will assist the client leadership and product leadership teams work through organizational requirements implementation strategies. You will also develop process diagrams and provide technical continuity across the division on implementation strategies, engineering and requirements processes and diagrams, resolution of process issues, and requirements working group support. In addition, you will design a implementation strategy to align the requirements management processes from ADO3 with the solution provider development activities. Other duties include:
Essential Job Functions & Responsibilities Description:
Lead efforts to create standard documentation for the various software development life cycle processes ensuring uniformity across the office. Conduct research and apply best practices and ensures that they are approved and documented. Critically analyzes processes/systems/practices to identify current gaps and opportunities for improvement.
Facilitate business process reengineering documenting the ""As Is"" processes, identify recommended process improvements and document approved changes in To-Be process documents and diagrams. Conduct interviews and evaluate current documentation to gain insight.
Develop process functional flow diagrams (MS Visio) in support of requirements capture and development based on existing policy, instructions and interviews.
Provide business process improvement recommendations and briefs to ADO3 leadership.
Propose and create standard operating procedures and diagrams to provide direction to ADO3 leadership, system development team members, Requirements Analysts and others.
Collaborate and recommend improvements to JIRA with JIRA SMEs and JIRA Administrators.
Attend Change Control Board (CCB) & Requirements Working Group Meetings. Capture notes, disseminate for client review and further dissemination ensuring continuity throughout the CCB activities, prepare and submit after action reports (AAR). Your primary role will be to design and build data pipelines to meet overall architecture requirements. You will be focused on helping client projects on data collection, integration, prep/transformation, and implementing services such as data processing and/or machine learning on datasets. In this role, you will work on both traditional and on some of the latest technologies, collaborate with teams for data pipelines and delivery, interact daily with management, and help build a great program of operations.
Specific Responsibilities:
Build and automate data pipelines.
Work as a member of a team assigned to design and implement data collection, integration, and transformation solutions.
Understand and rapidly comprehend new functional and technical areas and apply detailed and critical thinking to customer solutions.
Propose design solutions and recommend best practices for large-scale data analysis.
Meet the data needs of data scientists
Help reduce code vulnerabilities exposed by static testing tools
Assist with teaching new software developers best practices in coding and development
Transform a variety of input data structures into a unified data structure that aligns with the established ontology
Desired Skills and Experience:
B.S. or equivalent degree in computer science, mathematics or other relevant fields.
3-7 years of hands-on experience in ETL, Data warehouse, Data Marts, Visualization and/or building data pipelines, modeling and designing schema for data lakes or for data platforms.
Must have previous experience with Java
Experience implementing and resolving dependency issues from common Java build tools: Maven (preferred), Gradle, Ant, or equivilent
Experience with Agile implementation methodologies.
Experience using common CI/CD tools, such as Jenkins
Experience working with NoSQL databases in both the extract and load aspects of the ETL process.
Strong programming and scripting skills experience and expertise in two or more of the following: XML/XSLT, Python, Perl, Shell Scala, C.
Proficient in big data/distributed computing frameworks such as Spring, Hadoop, Apache Hive, Spark, Kafka, etc.
Familiarity and/or a strong willingness to learn ApacheNiFi and NiFi Registry.
Practice working with, processing, and managing large data sets (multi TB/PB scale).
Similar roles: Database Administrator (DBA), Database Operator (DBO), Data Architect, Data Manager, Data Analyst, Data Integrator, Systems Integrator, Systems Engineer.
Additional Information:
U.S Citizenship is required for this specific opportunity and all selected applicants will be subject to a government security investigation. This includes but not limited to; meeting the eligibility requirements for access to classified information and the ability to obtain a government-granted security clearance. Individuals may also be subject to a background investigation including, but not limited to; criminal history, employment verification, education verification, drug testing, and creditworthiness.
Favor TechConsulting is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, marital status, disability, veteran status, sexual orientation, or genetic information.
Q38z2blYJn","  B.S. or equivalent degree in computer science, mathematics or other relevant fields. 3-7 years of hands-on experience in ETL, Data warehouse, Data Marts, Visualization and/or building data pipelines, modeling and designing schema for data lakes or for data platforms. Must have previous experience with Java Experience implementing and resolving dependency issues from common Java build tools  Maven  preferred , Gradle, Ant, or equivilent Experience with Agile implementation methodologies. Experience using common CI/CD tools, such as Jenkins Experience working with NoSQL databases in both the extract and load aspects of the ETL process. Strong programming and scripting skills experience and expertise in two or more of the following  XML/XSLT, Python, Perl, Shell Scala, C. Proficient in big data/distributed computing frameworks such as Spring, Hadoop, Apache Hive, Spark, Kafka, etc. Familiarity and/or a strong willingness to learn ApacheNiFi and NiFi Registry. Practice working with, processing, and managing large data sets  multi TB/PB scale . Similar roles  Database Administrator  DBA , Database Operator  DBO , Data Architect, Data Manager, Data Analyst, Data Integrator, Systems Integrator, Systems Engineer.  Lead efforts to create standard documentation for the various software development life cycle processes ensuring uniformity across the office. Conduct research and apply best practices and ensures that they are approved and documented. Critically analyzes processes/systems/practices to identify current gaps and opportunities for improvement. Facilitate business process reengineering documenting the ""As Is"" processes, identify recommended process improvements and document approved changes in To-Be process documents and diagrams. Conduct interviews and evaluate current documentation to gain insight. Develop process functional flow diagrams  MS Visio  in support of requirements capture and development based on existing policy, instructions and interviews. Provide business process improvement recommendations and briefs to ADO3 leadership. Propose and create standard operating procedures and diagrams to provide direction to ADO3 leadership, system development team members, Requirements Analysts and others. Collaborate and recommend improvements to JIRA with JIRA SMEs and JIRA Administrators. Attend Change Control Board  CCB  & Requirements Working Group Meetings. Capture notes, disseminate for client review and further dissemination ensuring continuity throughout the CCB activities, prepare and submit after action reports  AAR . Your primary role will be to design and build data pipelines to meet overall architecture requirements. You will be focused on helping client projects on data collection, integration, prep/transformation, and implementing services such as data processing and/or machine learning on datasets. In this role, you will work on both traditional and on some of the latest technologies, collaborate with teams for data pipelines and delivery, interact daily with management, and help build a great program of operations.  ","B.S. or equivalent degree in computer science, mathematics other relevant fields. 3-7 years of hands-on experience ETL, Data warehouse, Marts, Visualization and/or building data pipelines, modeling and designing schema for lakes platforms. Must have previous with Java Experience implementing resolving dependency issues from common build tools Maven preferred , Gradle, Ant, equivilent Agile implementation methodologies. using CI/CD tools, such as Jenkins working NoSQL databases both the extract load aspects ETL process. Strong programming scripting skills expertise two more following XML/XSLT, Python, Perl, Shell Scala, C. Proficient big data/distributed computing frameworks Spring, Hadoop, Apache Hive, Spark, Kafka, etc. Familiarity a strong willingness to learn ApacheNiFi NiFi Registry. Practice with, processing, managing large sets multi TB/PB scale . Similar roles Database Administrator DBA Operator DBO Architect, Manager, Analyst, Integrator, Systems Engineer. Lead efforts create standard documentation various software development life cycle processes ensuring uniformity across office. Conduct research apply best practices ensures that they are approved documented. Critically analyzes processes/systems/practices identify current gaps opportunities improvement. Facilitate business process reengineering documenting ""As Is"" processes, recommended improvements document changes To-Be documents diagrams. interviews evaluate gain insight. Develop functional flow diagrams MS Visio support requirements capture based on existing policy, instructions interviews. Provide improvement recommendations briefs ADO3 leadership. Propose operating procedures provide direction leadership, system team members, Requirements Analysts others. Collaborate recommend JIRA SMEs Administrators. Attend Change Control Board CCB & Working Group Meetings. Capture notes, disseminate client review further dissemination continuity throughout activities, prepare submit after action reports AAR Your primary role will be design pipelines meet overall architecture requirements. You focused helping projects collection, integration, prep/transformation, services processing machine learning datasets. In this role, you work traditional some latest technologies, collaborate teams delivery, interact daily management, help great program operations.","B.S. equivalent degree computer science, mathematics relevant fields. 3-7 years hands-on experience ETL, Data warehouse, Marts, Visualization and/or building data pipelines, modeling designing schema lakes platforms. Must previous Java Experience implementing resolving dependency issues common build tools Maven preferred , Gradle, Ant, equivilent Agile implementation methodologies. using CI/CD tools, Jenkins working NoSQL databases extract load aspects ETL process. Strong programming scripting skills expertise two following XML/XSLT, Python, Perl, Shell Scala, C. Proficient big data/distributed computing frameworks Spring, Hadoop, Apache Hive, Spark, Kafka, etc. Familiarity strong willingness learn ApacheNiFi NiFi Registry. Practice with, processing, managing large sets multi TB/PB scale . Similar roles Database Administrator DBA Operator DBO Architect, Manager, Analyst, Integrator, Systems Engineer. Lead efforts create standard documentation various software development life cycle processes ensuring uniformity across office. Conduct research apply best practices ensures approved documented. Critically analyzes processes/systems/practices identify current gaps opportunities improvement. Facilitate business process reengineering documenting ""As Is"" processes, recommended improvements document changes To-Be documents diagrams. interviews evaluate gain insight. Develop functional flow diagrams MS Visio support requirements capture based existing policy, instructions interviews. Provide improvement recommendations briefs ADO3 leadership. Propose operating procedures provide direction leadership, system team members, Requirements Analysts others. Collaborate recommend JIRA SMEs Administrators. Attend Change Control Board CCB & Working Group Meetings. Capture notes, disseminate client review dissemination continuity throughout activities, prepare submit action reports AAR Your primary role design pipelines meet overall architecture requirements. You focused helping projects collection, integration, prep/transformation, services processing machine learning datasets. In role, work traditional latest technologies, collaborate teams delivery, interact daily management, help great program operations."
21,Data Engineer,Senior Data Engineer,"Washington, DC",Washington,DC,"Senior Data Engineer
Washington, DC 20032

Security Clearance: Current TS/SCI CI poly

Culmen International, LLC is seeking a Senior Data Engineer to work in support of a government program. This individual will back up the Lead but will also focus on data aggregation goals, transformations, and linking to enable the best analysis possible. Data Engineer will assist in connecting data sources and optimizing use in the Centrifuge application. All work will be performed on-site at Bolling AFB, DC.

Role and responsibilities include:
Assist Decision Analytics Lead in assessing customerâs data source requirements
Create and update SQL and Oracle SQL queries, functions, and procedures
Help customer achieve data aggregation goals
Create new JDBC data connectors
Build business intelligence visualizations
Create, update, and troubleshoot visual analytic templates
Turn customer requirements into usable visual analytic templates and visualizations
Write scripts and advanced SQL aggregation functions as necessary
Education and experience requirements include:
US Citizen
Current TS/SCI CI poly security clearance is required for consideration
10+ years of experience with database and data aggregation
Strong knowledge of big data analysis and storage
Big data experience (Hadoop)
Experience with Structured Query Language
Understanding of JDBC
Understanding of multiple database vendors (Microsoft, Oracle, SAP, SAS, PeopleSoft)
Familiarity with database vendors and offerings
Benefits: Health, Vision, 401K, Life and Disability Insurance Programs
Job Type: Full Time","   US Citizen Current TS/SCI CI poly security clearance is required for consideration 10+ years of experience with database and data aggregation Strong knowledge of big data analysis and storage Big data experience  Hadoop  Experience with Structured Query Language Understanding of JDBC Understanding of multiple database vendors  Microsoft, Oracle, SAP, SAS, PeopleSoft  Familiarity with database vendors and offerings  ","US Citizen Current TS/SCI CI poly security clearance is required for consideration 10+ years of experience with database and data aggregation Strong knowledge big analysis storage Big Hadoop Experience Structured Query Language Understanding JDBC multiple vendors Microsoft, Oracle, SAP, SAS, PeopleSoft Familiarity offerings","US Citizen Current TS/SCI CI poly security clearance required consideration 10+ years experience database data aggregation Strong knowledge big analysis storage Big Hadoop Experience Structured Query Language Understanding JDBC multiple vendors Microsoft, Oracle, SAP, SAS, PeopleSoft Familiarity offerings"
22,Data Engineer,Google Data Engineer,"Washington, DC 20006",Washington,DC,"Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet todayâs high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps â Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform. Multi-cloud experience a plus.   Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP DevOps an platform. Multi-cloud a plus. Proven ability to build, manage and foster team-oriented environment work creatively analytically in problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","Minimum 3 years previous Consulting client service delivery experience Google GCP DevOps platform. Multi-cloud plus. Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
23,Data Engineer,Azure Data Engineer,"Washington, DC 20006",Washington,DC,"Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, âas isâ and âto beâ scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ","At least 5 years of consulting or client service delivery experience on Azure DevOps an platform Proven ability to build, manage and foster a team-oriented environment","At least 5 years consulting client service delivery experience Azure DevOps platform Proven ability build, manage foster team-oriented environment"
24,Data Engineer,Associate Data Engineer (Full Time Starting Summer 2020),"Washington, DC 20036",Washington,DC,"About EAB

At EAB our mission is to make education smarter and our communities stronger. We harness the collective power of more than 1,500 schools, colleges, and universities to uncover and apply proven practices and transformative insights. And since complex problems require multifaceted solutions, we work with each school differently to apply these insights through a customized blend of research, technology, and services. From kindergarten to college and beyond, EAB partners with education leaders, practitioners, and staff to accelerate progress and drive results across three key areas: enrollment management, student success, and institutional operations and strategy.

At EAB, we serve not only our members but each otherâthat's why we are always working to make sure our employees love their jobs and are invested in their community. See how we've been recognized for this dedication to our employees by checking out our recent awards.

For more information, visit our Careers page.

The Role in Brief

Associate Data Engineer (Full Time Starting Summer 2020)

Are you a data enthusiast who seeks to tease out meaning from complex data flows and assets? Are you a talented problem solver who can transform abstract problems into elegant technical solutions? We are looking for a Data Modeler to join our team of engineers and data analysts focused on designing, creating, and delivering data solutions as part of our state-of-the-art cloud based products. The successful candidate will have the opportunity to build a world-class solution to help our higher education clients solve challenging problems through data.

Opportunities based in Washington, DC and Richmond, VA.

Primary Responsibilities:

Responsible for data modeling and schema design that will range across multiple business domains within higher education
Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas
Work with clients to research and conduct business information flow studies
Codify high-performing SQL for efficient data transformation
Coordinate work with external teams to ensure a smooth development process
Support operations by identifying, researching and resolving performance and production issues
Basic Qualifications:

Experience working with relational or multi-dimensional databases
Experience developing logical data models within a data warehouse
Experience developing ETL processes
Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2
Demonstrated mastery in database concepts and large-scale database implementations and design patterns
Proven ability to work with users to define requirements and business issues
Excellent analytic and troubleshooting skills
Strong written and oral communication skills

Ideal Qualifications:

Bachelorâs or Masterâs degree in Computer Science or Computer Engineering
Experience working in an AGILE environment
Experience developing commercial software products
Experience with AWS data warehouse infrastructure (redshift, EMR/spark)
GIT expertise

Benefits

Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive benefits package.
Medical, dental, and vision insurance, dependents eligible401(k) retirement plan with company matchGenerous PTODaytime leave policy for community service or fitness activities (up to 10 hours a month each)Wellness programs including gym discounts and incentives to promote healthy livingDynamic growth opportunities with merit-based promotion philosophyBenefits kick in day one, see the full details here.


At EAB, we believe that to fulfill our mission to âmake education smarter and our communities strongerâ we need team members who bring a diversity of perspectives to the table and a workplace where each team member is valued, respected and heard.

To that end, EAB is an Equal Opportunity Employer, and we make employment decisions on the basis of qualifications, merit and business need. We donât discriminate on the basis of race, religion, color, sex, gender identity or expression, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law."," Experience working with relational or multi-dimensional databases Experience developing logical data models within a data warehouse Experience developing ETL processes Demonstrated mastery in one or more SQL variants  PostgreSQL, MySQL, Oracle, SQL Server, or DB2 Demonstrated mastery in database concepts and large-scale database implementations and design patterns Proven ability to work with users to define requirements and business issues Excellent analytic and troubleshooting skills Strong written and oral communication skills    Responsible for data modeling and schema design that will range across multiple business domains within higher education Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas Work with clients to research and conduct business information flow studies Codify high-performing SQL for efficient data transformation Coordinate work with external teams to ensure a smooth development process Support operations by identifying, researching and resolving performance and production issues   ","Experience working with relational or multi-dimensional databases developing logical data models within a warehouse ETL processes Demonstrated mastery in one more SQL variants PostgreSQL, MySQL, Oracle, Server, DB2 database concepts and large-scale implementations design patterns Proven ability to work users define requirements business issues Excellent analytic troubleshooting skills Strong written oral communication Responsible for modeling schema that will range across multiple domains higher education Partner stakeholders including clients, new product development, BI engineers develop scalable standard schemas Work clients research conduct information flow studies Codify high-performing efficient transformation Coordinate external teams ensure smooth development process Support operations by identifying, researching resolving performance production","Experience working relational multi-dimensional databases developing logical data models within warehouse ETL processes Demonstrated mastery one SQL variants PostgreSQL, MySQL, Oracle, Server, DB2 database concepts large-scale implementations design patterns Proven ability work users define requirements business issues Excellent analytic troubleshooting skills Strong written oral communication Responsible modeling schema range across multiple domains higher education Partner stakeholders including clients, new product development, BI engineers develop scalable standard schemas Work clients research conduct information flow studies Codify high-performing efficient transformation Coordinate external teams ensure smooth development process Support operations identifying, researching resolving performance production"
25,Data Engineer,Data Engineer,"Chantilly, VA",Chantilly,VA,"Overview
BRMi Technology is seeking a Data Engineer to support a large client in the Northern Virginia area. Selected candidate will manage data input, quality, sharing, security, governance, and output. As well as research, evaluate, design, implement, and maintain system and product solutions, applying knowledge of engineering principles. To provide technical direction and engineering support for projects and infrastructure. Develop and maintain expert functional knowledge of evolving IT engineering industry technologies/competition, concepts and trends.
Qualifications
Experience managing streaming data in a data management practice or similar experience managing big data.
Specific data management such as Kafka Administrator for Kafka jobs and Kafka integration.
Experience with the following tools, languages, and applications is beneficial: Apache, Hadoop, Java, Flume, Spark, Zookeeper, SQL

** BRMi will not sponsor applicants for work visas for this position.**
**This is a W2 opportunity only**

EOE/Minorities/Females/Vet/Disabled
We are an equal opportunity employer that values diversity and commitment at all levels. All individuals, regardless of personal characteristics, are encouraged to apply. Employment policies and decisions on employment and promotion are based on merit, qualifications, performance, and business needs. The decisions and criteria governing the employment relationship with all employees are made in a nondiscriminatory manner, without regard to race, religion, color, national origin, sex, age, marital status, physical or mental disability, medical condition, veteran status, or any other factor determined to be unlawful by federal, state, or local statutes.","Experience managing streaming data in a data management practice or similar experience managing big data. Specific data management such as Kafka Administrator for Kafka jobs and Kafka integration. Experience with the following tools, languages, and applications is beneficial  Apache, Hadoop, Java, Flume, Spark, Zookeeper, SQL    ","Experience managing streaming data in a management practice or similar experience big data. Specific such as Kafka Administrator for jobs and integration. with the following tools, languages, applications is beneficial Apache, Hadoop, Java, Flume, Spark, Zookeeper, SQL","Experience managing streaming data management practice similar experience big data. Specific Kafka Administrator jobs integration. following tools, languages, applications beneficial Apache, Hadoop, Java, Flume, Spark, Zookeeper, SQL"
26,Data Engineer,"Cleared Data Engineer openings (ETL Engineering, NiFi or Pentaho build experience)","McLean, VA",McLean,VA,"Position is with the VA McLean Customer and requires an active TS/SCI with Full Scope Poly clearance.
Position is funded and vacant and the customer is actively looking to hire someone quickly!
The years of experience can be flexible, as there are multiple openings and they will hire individuals with anywhere from 3 to 15+ years of experience (with the associated rate).

The Data Engineer will manipulate data and data flows for both existing and new systems. Additionally they will provide support in the areas of data extraction, transformation and load (ETL), data mapping, data extraction, analytical support, operational support, database support, and maintenance support of data and associated systems. As a member of the team, candidates will have the ability to work within a super-computing environment, exploit new and expanding datasheets, develop custom algorithms, and work in a multi-tasking, non-traditional analytic environment with rapidly changing priorities. The successful candidate must have demonstrated experience applying and tailoring complex extraction rules on massive amounts of unstructured data using tools such as Netowl. He or she much also have experience with the principles of large-scale (terabytes) database development, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.
Requirements
Bachelorâs Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience
Mid level roles available requiring 3-5 years of experience and senior to SME roles available requiring 8-15+ years of related software engineering and ETL experience
Experience building and maintaining data flows in NiFi or Pentaho
Excellent organizational, coordination, interpersonal and team building skills.
Benefits
Leading Path is an award-winning Information Technology and Management Consulting firm focused on providing solutions in process, technology, and operations to our government and Fortune 500 clients. We offer a professional and work environment with a strong work-life balance. Leading Path provides a comprehensive and competitive benefits package, 401K, tuition reimbursement and opportunities for professional growth and advancement.","     Bachelorâs Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience Mid level roles available requiring 3-5 years of experience and senior to SME roles available requiring 8-15+ years of related software engineering and ETL experience Experience building and maintaining data flows in NiFi or Pentaho Excellent organizational, coordination, interpersonal and team building skills. ","Bachelorâs Degree in Computer Science, Electrical or Engineering a related technical discipline, the equivalent combination of education, training, work/military experience Mid level roles available requiring 3-5 years and senior to SME 8-15+ software engineering ETL Experience building maintaining data flows NiFi Pentaho Excellent organizational, coordination, interpersonal team skills.","Bachelorâs Degree Computer Science, Electrical Engineering related technical discipline, equivalent combination education, training, work/military experience Mid level roles available requiring 3-5 years senior SME 8-15+ software engineering ETL Experience building maintaining data flows NiFi Pentaho Excellent organizational, coordination, interpersonal team skills."
27,Data Engineer,Python Data Engineer,"Reston, VA 20191",Reston,VA,"Python Data Engineer

We are looking for a Python Data Engineer to join one of our Federal Health IT engagements. Successful candidates are passionate, self-driven problem-solvers who love taking on new challenges using the latest data and cloud technologies. They also love data and keep up with the latest technology trends. They tinker, explore and regularly read to stay in touch with new data trends and are passionate about discovering ways to improve quality, reusability, extensibility, and consistency. Successful candidates should also be multi-faceted with a great mix of technical and interpersonal skills, to succeed in highly collaborative and agile work environments. As a Modern Data Engineer, this person will design and deliver innovative solutions for Postgres and Redshift on Amazon Web Services, using core cloud tools.

Responsibilities:
Display passion for delivering high quality products that meet customers' needs
Solving data-oriented problems in an analytical and iterative fashion
Perform analysis, architecture, design, and development of cloud data solutions
Working with various kinds of data (structured, unstructured, metrics, logs, json, xml, etc.)
Working in various agile methodologies (Scrum, Kanban, SAFe)
Required Skills:

3+ years of Python Development, with emphasis in ETL Development
5+ years of SQL experience, with emphasis in Data Analysis
Proficiency in relational database design and development
Experienced building and scaling batch/asynchronous systems
Hands-on development using and migrating data to cloud platforms, AWS
Analytical approach to problem-solving; ability to use technology to solve business problems
Desired Skills:

Data pipeline orchestration tools such as Airflow, Amazon Glue
Familiarity with PostGres, Redshift
Cloud platform certification(s) (example: AWS Certified Solutions Architect)","  3+ years of Python Development, with emphasis in ETL Development 5+ years of SQL experience, with emphasis in Data Analysis Proficiency in relational database design and development Experienced building and scaling batch/asynchronous systems Hands-on development using and migrating data to cloud platforms, AWS Analytical approach to problem-solving; ability to use technology to solve business problems Display passion for delivering high quality products that meet customers' needs Solving data-oriented problems in an analytical and iterative fashion Perform analysis, architecture, design, and development of cloud data solutions Working with various kinds of data  structured, unstructured, metrics, logs, json, xml, etc.  Working in various agile methodologies  Scrum, Kanban, SAFe   ","3+ years of Python Development, with emphasis in ETL Development 5+ SQL experience, Data Analysis Proficiency relational database design and development Experienced building scaling batch/asynchronous systems Hands-on using migrating data to cloud platforms, AWS Analytical approach problem-solving; ability use technology solve business problems Display passion for delivering high quality products that meet customers' needs Solving data-oriented an analytical iterative fashion Perform analysis, architecture, design, solutions Working various kinds structured, unstructured, metrics, logs, json, xml, etc. agile methodologies Scrum, Kanban, SAFe","3+ years Python Development, emphasis ETL Development 5+ SQL experience, Data Analysis Proficiency relational database design development Experienced building scaling batch/asynchronous systems Hands-on using migrating data cloud platforms, AWS Analytical approach problem-solving; ability use technology solve business problems Display passion delivering high quality products meet customers' needs Solving data-oriented analytical iterative fashion Perform analysis, architecture, design, solutions Working various kinds structured, unstructured, metrics, logs, json, xml, etc. agile methodologies Scrum, Kanban, SAFe"
28,Data Engineer,Data Engineer â All Levels,"Reston, VA 20194",Reston,VA,"At DataSync Technologies, our data engineering professionals touch every area of our company. Their insights drive our decisions and their innovations fuel projects. When you join our team of data experts, you're helping DataSync's customers make better, smarter and faster decisions every day. See how you can help us solve some our customer's most challenging data problems while you grow your skills and build your own future.
Job Description
DataSync Technologies is seeking Data Engineers to support a mission critical program within the Intelligence Community.
Requirement:
ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST BE A U.S. CITIZEN.
Responsibilities will vary by specific data engineer role â Data Architect, Data Scientist, Database Engineer, Data Governance to include the following:
Design and develop methods, processes, and systems to consolidate and analyze structured and unstructured data from diverse sources including ""big data"" sources.
Develop and use advanced software programs, algorithms, query techniques, model complex business problems, and automated processes to cleanse, integrate, and evaluate datasets.
Analyze the requirements and evaluate technologies for data science capabilities including one or more of the following: Natural Language Processing, Machine Learning, predictive modeling, statistical analysis and hypothesis testing.
Develop information tools, algorithms, dashboards, and queries to monitor and improve business performance. Maintain awareness of emerging analytics and big-data technologies.
Designs, implement, and maintain standard data interfaces for data ingest including Extract/Transform/Load (ETL) methodology and implementation, APIs, RESTful Web Services, data quality, and data cleansing.
Provide data services, data administration, data management, and ""Big Data"" support in client/server, virtual machine, Hadoop, and cloud infrastructure environment and/or migrations between these environments.
Database installation, configuration, and the upgrading of database server software and related products, backup and recovery policies and procedures, database implementation, security, optimization, multi-domain operation, and performance management.
Hadoop, cloud, and other technologies associated with data storage, processing, management, and use.
The migration/transition of database capability into cloud based technologies and/or creation of interfaces between classic relational databases and key indexes to cloud based columnar databases and map reduce index capabilities.
Preferred Qualifications (All not required):
Databases/Data Stores: Oracle, MySQL, HIVE, HBASE, and HDFS
Frameworks: Hadoop, Rails, JavaScript Frameworks, SOA/WebServices, JSP
Indexing: SOLR and Lucine
Development/Scripting Languages: JAVA (J2EE), Python, Ruby, JavaScript, MapReduce, Pig, XML, SQL, JAQL, HTML, CSS, XML, BASH, ANT, and Perl
________________________
What makes DataSync Technologies different?
Leadership Training: We provide employees with a variety of learning opportunities, including access to exclusive classes, professional growth training and more.
Feedback & Mentoring: We believe in talkingâoften. So we have one-on-one feedback sessions for every employee.
Community Service: We believe in helping the community where we work. DataSync and its employees donate time and services on a regular basis to local military charities. We believe in helping, both inside and outside of the office.
Social Events: We plan social events on a regular basis to help our employees relax and socialize so we get to know one another outside of our job titles.
DataSync is an EEO and Affirmative Action Employer of Female/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law
Information about Equal Employment Opportunity (EEO) and Employee Polygraph Act (EPPA) provisions in addition to other Federal labor laws can be found at http://webapps.dol.gov/dolfaq/go-dol-faq.asp?faqid=537
DataSync is committed to providing veteran employment opportunities to our service men and women.
www.datasynctech.com
www.facebook.com/DatasyncTechnologies
www.twitter.com/Jobs at DataSync (@DatasyncJobs)
www.twitter.com/datasynctech
#datasynctech on Instagram
Interested in Joining Our Team? - Check out this YouTube video!

#CJ
mCScFXDsYI"," Databases/Data Stores  Oracle, MySQL, HIVE, HBASE, and HDFS Frameworks  Hadoop, Rails, JavaScript Frameworks, SOA/WebServices, JSP Indexing  SOLR and Lucine Development/Scripting Languages  JAVA  J2EE , Python, Ruby, JavaScript, MapReduce, Pig, XML, SQL, JAQL, HTML, CSS, XML, BASH, ANT, and Perl   Design and develop methods, processes, and systems to consolidate and analyze structured and unstructured data from diverse sources including ""big data"" sources. Develop and use advanced software programs, algorithms, query techniques, model complex business problems, and automated processes to cleanse, integrate, and evaluate datasets. Analyze the requirements and evaluate technologies for data science capabilities including one or more of the following  Natural Language Processing, Machine Learning, predictive modeling, statistical analysis and hypothesis testing. Develop information tools, algorithms, dashboards, and queries to monitor and improve business performance. Maintain awareness of emerging analytics and big-data technologies. Designs, implement, and maintain standard data interfaces for data ingest including Extract/Transform/Load  ETL  methodology and implementation, APIs, RESTful Web Services, data quality, and data cleansing. Provide data services, data administration, data management, and ""Big Data"" support in client/server, virtual machine, Hadoop, and cloud infrastructure environment and/or migrations between these environments. Database installation, configuration, and the upgrading of database server software and related products, backup and recovery policies and procedures, database implementation, security, optimization, multi-domain operation, and performance management. Hadoop, cloud, and other technologies associated with data storage, processing, management, and use. The migration/transition of database capability into cloud based technologies and/or creation of interfaces between classic relational databases and key indexes to cloud based columnar databases and map reduce index capabilities.   ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST BE A U.S. CITIZEN.","Databases/Data Stores Oracle, MySQL, HIVE, HBASE, and HDFS Frameworks Hadoop, Rails, JavaScript Frameworks, SOA/WebServices, JSP Indexing SOLR Lucine Development/Scripting Languages JAVA J2EE , Python, Ruby, JavaScript, MapReduce, Pig, XML, SQL, JAQL, HTML, CSS, BASH, ANT, Perl Design develop methods, processes, systems to consolidate analyze structured unstructured data from diverse sources including ""big data"" sources. Develop use advanced software programs, algorithms, query techniques, model complex business problems, automated processes cleanse, integrate, evaluate datasets. Analyze the requirements technologies for science capabilities one or more of following Natural Language Processing, Machine Learning, predictive modeling, statistical analysis hypothesis testing. information tools, dashboards, queries monitor improve performance. Maintain awareness emerging analytics big-data technologies. Designs, implement, maintain standard interfaces ingest Extract/Transform/Load ETL methodology implementation, APIs, RESTful Web Services, quality, cleansing. Provide services, administration, management, ""Big Data"" support in client/server, virtual machine, cloud infrastructure environment and/or migrations between these environments. Database installation, configuration, upgrading database server related products, backup recovery policies procedures, security, optimization, multi-domain operation, performance management. cloud, other associated with storage, processing, use. The migration/transition capability into based creation classic relational databases key indexes columnar map reduce index capabilities. ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST A U.S. CITIZEN.","Databases/Data Stores Oracle, MySQL, HIVE, HBASE, HDFS Frameworks Hadoop, Rails, JavaScript Frameworks, SOA/WebServices, JSP Indexing SOLR Lucine Development/Scripting Languages JAVA J2EE , Python, Ruby, JavaScript, MapReduce, Pig, XML, SQL, JAQL, HTML, CSS, BASH, ANT, Perl Design develop methods, processes, systems consolidate analyze structured unstructured data diverse sources including ""big data"" sources. Develop use advanced software programs, algorithms, query techniques, model complex business problems, automated processes cleanse, integrate, evaluate datasets. Analyze requirements technologies science capabilities one following Natural Language Processing, Machine Learning, predictive modeling, statistical analysis hypothesis testing. information tools, dashboards, queries monitor improve performance. Maintain awareness emerging analytics big-data technologies. Designs, implement, maintain standard interfaces ingest Extract/Transform/Load ETL methodology implementation, APIs, RESTful Web Services, quality, cleansing. Provide services, administration, management, ""Big Data"" support client/server, virtual machine, cloud infrastructure environment and/or migrations environments. Database installation, configuration, upgrading database server related products, backup recovery policies procedures, security, optimization, multi-domain operation, performance management. cloud, associated storage, processing, use. The migration/transition capability based creation classic relational databases key indexes columnar map reduce index capabilities. ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST A U.S. CITIZEN."
29,Data Engineer,Principal Data Engineer,"Arlington, VA 22202",Arlington,VA,"Our purpose is to serve the nation with the single most trusted and capable health information network, built to increase patient safety, lower costs and ensure quality care.
Job Summary
Surescripts is seeking a Principal Data Engineer to join our Business Intelligence team. The Principal Data Engineer's role is to plan, design and develop data systems and technology architecture that enhance and optimize our capabilities and products. He/She will directly engage with data warehouse engineers, product manager/owners, sales and activation teams to design, transform and develop data architectures that meet the evolving needs of the organization. The Principal Data Engineer will also lead efforts in evaluating and selecting technology components, such as software, hardware, and networking capabilities for business intelligence systems and applications. He/She will be owner of access control and support activities across a multi-environment with a focus on our Tableau infrastructure, including security, administration, release management, troubleshooting, system optimization and maintenance.
Responsibilities
Assist in implementing long-term strategic goals for BI database development in conjunction with end users, managers, clients, and other stakeholders
Analyze user requirements and, based on findings, collaborate with DS and other functions to design functional specifications for databases and database applications following database standards
Assist in planning and implementing capacity and resource expansion to ensure scalability of BI databases in close collaboration with DS function
Assist with the design of redundant systems, policies, and procedures for disaster recovery to ensure effective availability, protection, and integrity of data assets
Conduct research and make recommendations on database products, services, protocols, and standards in support of procurement and development efforts
Develop automated database applications, where necessary, using applicable database techniques
Work with agile teams and product owners to ensure database design and performance meet business requirements
Identify inefficiencies in current databases and implement improved solutions
Assist in planning and performing database upgrades and migrations
Assist in evaluating and selecting database components, including hardware, relational database management systems, ETL software, metadata management tools, and database design solutions.
As a subject matter expert, the Data Architect will define feasibility and address all data related questions and challenges to support dashboards and tracking tools from all functions across the organization including Product Innovation, Compliance, Finance and CPI.
Qualifications
Basic Requirements:
Bachelor degree in computer science, information systems or related field or equivalent experience
5 years of experience with developing, installing, configuring, tuning, and supporting large dataset environments
5 years of experience with various database platforms, including Oracle, Oracle RAC
5 years of experience with database development languages, including SQL, PL/SQL and Linux scripting
5 years of experience with database design applications, including SQL Developer, DB Visualizer, and Embarcadero ER Studio
2+ years of experience in installing and upgrading Tableau server and server performance tuning
Experience with the design, development and delivery of Tableau visualization solutions
Experience with creation of users, groups, projects, workbooks and the appropriate permission sets for Tableau server logons and security checks
5 years of experience in handling large transaction datasets
Preferred Qualifications:
Familiar with database performance tuning and replication
Strong understanding of relational database structures, theories, principles, and practices
Hands-on experience with large healthcare transactional datasets, in particular e-prescribing
Experience with NoSQL databases such as Hadoop, and Cassandra
Experience with database replication, preferably Oracle Golden Gate
Excellent knowledge of applicable data privacy practices and laws in healthcare
Working experience with developing, installing, configuring and supporting database environments
Surescripts is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, color, religion, age, national origin, ancestry, disability, medical condition, marital status, pregnancy, genetic information, gender, sexual orientation, parental status, gender identity, gender expression, veteran status, or any other status protected under federal, state, or local law.","Bachelor degree in computer science, information systems or related field or equivalent experience 5 years of experience with developing, installing, configuring, tuning, and supporting large dataset environments 5 years of experience with various database platforms, including Oracle, Oracle RAC 5 years of experience with database development languages, including SQL, PL/SQL and Linux scripting 5 years of experience with database design applications, including SQL Developer, DB Visualizer, and Embarcadero ER Studio 2+ years of experience in installing and upgrading Tableau server and server performance tuning Experience with the design, development and delivery of Tableau visualization solutions Experience with creation of users, groups, projects, workbooks and the appropriate permission sets for Tableau server logons and security checks 5 years of experience in handling large transaction datasets   Assist in implementing long-term strategic goals for BI database development in conjunction with end users, managers, clients, and other stakeholders Analyze user requirements and, based on findings, collaborate with DS and other functions to design functional specifications for databases and database applications following database standards Assist in planning and implementing capacity and resource expansion to ensure scalability of BI databases in close collaboration with DS function Assist with the design of redundant systems, policies, and procedures for disaster recovery to ensure effective availability, protection, and integrity of data assets Conduct research and make recommendations on database products, services, protocols, and standards in support of procurement and development efforts Develop automated database applications, where necessary, using applicable database techniques Work with agile teams and product owners to ensure database design and performance meet business requirements Identify inefficiencies in current databases and implement improved solutions Assist in planning and performing database upgrades and migrations Assist in evaluating and selecting database components, including hardware, relational database management systems, ETL software, metadata management tools, and database design solutions. As a subject matter expert, the Data Architect will define feasibility and address all data related questions and challenges to support dashboards and tracking tools from all functions across the organization including Product Innovation, Compliance, Finance and CPI.   Bachelor degree in computer science, information systems or related field or equivalent experience 5 years of experience with developing, installing, configuring, tuning, and supporting large dataset environments 5 years of experience with various database platforms, including Oracle, Oracle RAC 5 years of experience with database development languages, including SQL, PL/SQL and Linux scripting 5 years of experience with database design applications, including SQL Developer, DB Visualizer, and Embarcadero ER Studio 2+ years of experience in installing and upgrading Tableau server and server performance tuning Experience with the design, development and delivery of Tableau visualization solutions Experience with creation of users, groups, projects, workbooks and the appropriate permission sets for Tableau server logons and security checks 5 years of experience in handling large transaction datasets ","Bachelor degree in computer science, information systems or related field equivalent experience 5 years of with developing, installing, configuring, tuning, and supporting large dataset environments various database platforms, including Oracle, Oracle RAC development languages, SQL, PL/SQL Linux scripting design applications, SQL Developer, DB Visualizer, Embarcadero ER Studio 2+ installing upgrading Tableau server performance tuning Experience the design, delivery visualization solutions creation users, groups, projects, workbooks appropriate permission sets for logons security checks handling transaction datasets Assist implementing long-term strategic goals BI conjunction end managers, clients, other stakeholders Analyze user requirements and, based on findings, collaborate DS functions to functional specifications databases applications following standards planning capacity resource expansion ensure scalability close collaboration function redundant systems, policies, procedures disaster recovery effective availability, protection, integrity data assets Conduct research make recommendations products, services, protocols, support procurement efforts Develop automated where necessary, using applicable techniques Work agile teams product owners meet business Identify inefficiencies current implement improved performing upgrades migrations evaluating selecting components, hardware, relational management ETL software, metadata tools, solutions. As a subject matter expert, Data Architect will define feasibility address all questions challenges dashboards tracking tools from across organization Product Innovation, Compliance, Finance CPI.","Bachelor degree computer science, information systems related field equivalent experience 5 years developing, installing, configuring, tuning, supporting large dataset environments various database platforms, including Oracle, Oracle RAC development languages, SQL, PL/SQL Linux scripting design applications, SQL Developer, DB Visualizer, Embarcadero ER Studio 2+ installing upgrading Tableau server performance tuning Experience design, delivery visualization solutions creation users, groups, projects, workbooks appropriate permission sets logons security checks handling transaction datasets Assist implementing long-term strategic goals BI conjunction end managers, clients, stakeholders Analyze user requirements and, based findings, collaborate DS functions functional specifications databases applications following standards planning capacity resource expansion ensure scalability close collaboration function redundant systems, policies, procedures disaster recovery effective availability, protection, integrity data assets Conduct research make recommendations products, services, protocols, support procurement efforts Develop automated necessary, using applicable techniques Work agile teams product owners meet business Identify inefficiencies current implement improved performing upgrades migrations evaluating selecting components, hardware, relational management ETL software, metadata tools, solutions. As subject matter expert, Data Architect define feasibility address questions challenges dashboards tracking tools across organization Product Innovation, Compliance, Finance CPI."
30,Data Engineer,Data Engineer,"Chevy Chase, MD",Chevy Chase,MD,"Ready to make an impact? If so, read on!

Job Duties & Responsibilities

The Decision Support Systems (DSS) Team is seeking a highly motivated Data Engineer to support GEICOâs business analytics through Quality Data and Application Development.

DSS is a small technology team residing in one of the core research and predictive modeling departments at GEICO. This allows us to work side by side with our users along with having direct control over all stages of the development process, from design to implementation. As a Data Engineer on our team you will work with a variety of applications and database languages to support key business decisions. The technologies we routinely use include SQL, C#, Python, Hive, and Spark.
The main focus of this position is on the development of our new cloud based Big Data Platform. As part of this team you will interact with various IT, data science, and business partners to design and implement new data solutions that enable our business to move forward. You'll have the opportunity to develop new data sources, complex queries, and applications which are responsible for driving business decisions. This is a great opportunity to have an immediate, visible impact on the business, and to learn the skills required by the industry and technological world!

Please upload your resume, unofficial transcripts from all schools attended, and a cover letter to your GEICO profile when submitting your application.

Candidate Qualifications
Bachelor's degree in Computer Science or related degree.
At least a 3.0 overall undergraduate GPA
Strong knowledge of at least one programming language
SQL and C# experience highly desired
Experience working in a cloud environment
Experience with the Hadoop ecosystem highly preferred
A thorough understanding of the Systems Development Life Cycle (SDLC)
Excellent interpersonal and teamwork skills
Solid verbal and written communication skills
Excellent analytical and problem solving skills
Willingness to learn new technologies and programming languages



About GEICO
For more than 75 years, GEICO has stood out from the rest of the insurance industry! We are one of the nation's largest and fastest-growing auto insurers thanks to our low rates, outstanding service and clever marketing. We're an industry leader employing thousands of dedicated and hard-working associates. As a wholly owned subsidiary of Berkshire Hathaway, we offer associates training and career advancement in a financially stable and rewarding workplace.

Our associates' quality of life is important to us. Full-time GEICO associates are offered a comprehensive Total Rewards Program*, including:

401(k) and profit-sharing plans
Medical, dental, vision and life insurance
Paid vacation, holidays and leave programs
Tuition reimbursement
Associate assistance program
Flexible spending accounts
Business casual dress
Fitness and dining facilities (at most locations)
Associate clubs and sports teams
Volunteer opportunities
GEICO Federal Credit Union
Benefit offerings for positions other than full-time may vary.

GEICO is an equal opportunity employer. GEICO conducts drug screens and background checks on applicants who accept employment offers.",Bachelor's degree in Computer Science or related degree. At least a 3.0 overall undergraduate GPA Strong knowledge of at least one programming language SQL and C  experience highly desired Experience working in a cloud environment Experience with the Hadoop ecosystem highly preferred A thorough understanding of the Systems Development Life Cycle  SDLC  Excellent interpersonal and teamwork skills Solid verbal and written communication skills Excellent analytical and problem solving skills Willingness to learn new technologies and programming languages   Bachelor's degree in Computer Science or related degree. At least a 3.0 overall undergraduate GPA Strong knowledge of at least one programming language SQL and C  experience highly desired Experience working in a cloud environment Experience with the Hadoop ecosystem highly preferred A thorough understanding of the Systems Development Life Cycle  SDLC  Excellent interpersonal and teamwork skills Solid verbal and written communication skills Excellent analytical and problem solving skills Willingness to learn new technologies and programming languages   ,Bachelor's degree in Computer Science or related degree. At least a 3.0 overall undergraduate GPA Strong knowledge of at one programming language SQL and C experience highly desired Experience working cloud environment with the Hadoop ecosystem preferred A thorough understanding Systems Development Life Cycle SDLC Excellent interpersonal teamwork skills Solid verbal written communication analytical problem solving Willingness to learn new technologies languages,Bachelor's degree Computer Science related degree. At least 3.0 overall undergraduate GPA Strong knowledge one programming language SQL C experience highly desired Experience working cloud environment Hadoop ecosystem preferred A thorough understanding Systems Development Life Cycle SDLC Excellent interpersonal teamwork skills Solid verbal written communication analytical problem solving Willingness learn new technologies languages
31,Data Engineer,Data Engineer,"Washington, DC",Washington,DC,"Washington, D.C.
Full-Time
Job Description:
JPI is hiring a mid-level SharePoint Developer with at least 4 years of experience in application development and proficiency in SharePoint custom solutions. The ideal candidate will be well versed in the analysis, design, hands-on development and delivery of web-based applications, including SharePoint 2010, 2013, and 2016. Experience with languages including .NET and/or C# is preferred. As a key part of the development team, the SharePoint Developer will also be responsible for staying up-to-date on technological advances and new industry trends. MUST HAVE OR BE ABLE TO OBTAIN SECRET CLEARANCE (REQUIRES U.S. CITIZENSHIP).
Responsibilities:
Maintain and enhance existing custom solutions built in SharePoint, including but not limited to farm solutions deployed on the SharePoint platform.
Plan, lead, and execute SharePoint 2010/2013/2016 tasks; custom master pages, layouts, and templates; custom workflows; implementing permissions structures; working with timer jobs and event handlers; as well as implementation, integration, and maintenance of existing solutions.
Gather user requirements, analyze business processes, and work with functional areas to define and scope projects, document requirements, and application functionality.
Provide day-to-day operations support and configuration management, serve as the POC for differentiating functional issues from technical issues, and resolve technical issues.
Operate both independently and as part of a Scrum team across multiple projects.
Provide guidance in pursuing innovative solutions to achieve client goals and objectives.
Requirements:
 Bachelor's Degree and 4+ years of SharePoint application development, using Visual Studio and related code management practices.
2+ years of SharePoint application development using client-side code (including CSOM) and server-side code on multiple SharePoint platforms (2010 to 2016).
2+ years of experience operating as part of an Agile development team
Experience with SharePoint Designer, InfoPath, Web Parts, and workflow creation.
Experience with jQuery, CSOM, and front-end UI design a plus.
Experience supporting migration between SharePoint versions (i.e., 2010-2016 is preferred.
Experience with Business Intelligence dashboards a plus.
Experience developing new software and web applications, as well experience operating and maintaining existing applications.
Experience developing cloud-based solutions, cloud-based training and certifications a plus.
Familiarity with DevOps and Site Reliability Engineering principles.
Proficiency in time management, attention to detail, and adaptability depending on circumstances.
Proven ability to work with remote teams;
Capable of critical thinking for problem resolution. US Citizen, ability to obtain Secret Clearance (Candidates with existing Secret or Top Secret, have recently worked with the intelligence community, or have recently held a DHS HQ EOD is a plus)
JPI is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.","   Maintain and enhance existing custom solutions built in SharePoint, including but not limited to farm solutions deployed on the SharePoint platform. Plan, lead, and execute SharePoint 2010/2013/2016 tasks; custom master pages, layouts, and templates; custom workflows; implementing permissions structures; working with timer jobs and event handlers; as well as implementation, integration, and maintenance of existing solutions. Gather user requirements, analyze business processes, and work with functional areas to define and scope projects, document requirements, and application functionality. Provide day-to-day operations support and configuration management, serve as the POC for differentiating functional issues from technical issues, and resolve technical issues. Operate both independently and as part of a Scrum team across multiple projects. Provide guidance in pursuing innovative solutions to achieve client goals and objectives.     Bachelor's Degree and 4+ years of SharePoint application development, using Visual Studio and related code management practices. 2+ years of SharePoint application development using client-side code  including CSOM  and server-side code on multiple SharePoint platforms  2010 to 2016 . 2+ years of experience operating as part of an Agile development team Experience with SharePoint Designer, InfoPath, Web Parts, and workflow creation. Experience with jQuery, CSOM, and front-end UI design a plus. Experience supporting migration between SharePoint versions  i.e., 2010-2016 is preferred. Experience with Business Intelligence dashboards a plus. Experience developing new software and web applications, as well experience operating and maintaining existing applications. Experience developing cloud-based solutions, cloud-based training and certifications a plus. Familiarity with DevOps and Site Reliability Engineering principles. Proficiency in time management, attention to detail, and adaptability depending on circumstances. Proven ability to work with remote teams; Capable of critical thinking for problem resolution. US Citizen, ability to obtain Secret Clearance  Candidates with existing Secret or Top Secret, have recently worked with the intelligence community, or have recently held a DHS HQ EOD is a plus ","Maintain and enhance existing custom solutions built in SharePoint, including but not limited to farm deployed on the SharePoint platform. Plan, lead, execute 2010/2013/2016 tasks; master pages, layouts, templates; workflows; implementing permissions structures; working with timer jobs event handlers; as well implementation, integration, maintenance of solutions. Gather user requirements, analyze business processes, work functional areas define scope projects, document application functionality. Provide day-to-day operations support configuration management, serve POC for differentiating issues from technical issues, resolve issues. Operate both independently part a Scrum team across multiple projects. guidance pursuing innovative achieve client goals objectives. Bachelor's Degree 4+ years development, using Visual Studio related code management practices. 2+ development client-side CSOM server-side platforms 2010 2016 . experience operating an Agile Experience Designer, InfoPath, Web Parts, workflow creation. jQuery, CSOM, front-end UI design plus. supporting migration between versions i.e., 2010-2016 is preferred. Business Intelligence dashboards developing new software web applications, maintaining applications. cloud-based solutions, training certifications Familiarity DevOps Site Reliability Engineering principles. Proficiency time attention detail, adaptability depending circumstances. Proven ability remote teams; Capable critical thinking problem resolution. US Citizen, obtain Secret Clearance Candidates or Top Secret, have recently worked intelligence community, held DHS HQ EOD plus","Maintain enhance existing custom solutions built SharePoint, including limited farm deployed SharePoint platform. Plan, lead, execute 2010/2013/2016 tasks; master pages, layouts, templates; workflows; implementing permissions structures; working timer jobs event handlers; well implementation, integration, maintenance solutions. Gather user requirements, analyze business processes, work functional areas define scope projects, document application functionality. Provide day-to-day operations support configuration management, serve POC differentiating issues technical issues, resolve issues. Operate independently part Scrum team across multiple projects. guidance pursuing innovative achieve client goals objectives. Bachelor's Degree 4+ years development, using Visual Studio related code management practices. 2+ development client-side CSOM server-side platforms 2010 2016 . experience operating Agile Experience Designer, InfoPath, Web Parts, workflow creation. jQuery, CSOM, front-end UI design plus. supporting migration versions i.e., 2010-2016 preferred. Business Intelligence dashboards developing new software web applications, maintaining applications. cloud-based solutions, training certifications Familiarity DevOps Site Reliability Engineering principles. Proficiency time attention detail, adaptability depending circumstances. Proven ability remote teams; Capable critical thinking problem resolution. US Citizen, obtain Secret Clearance Candidates Top Secret, recently worked intelligence community, held DHS HQ EOD plus"
32,Data Engineer,PHP Data Engineer,"Washington, DC 20001",Washington,DC,"As a PHP Data Engineer you will use your exceptional database and development skills and experiences to architect and develop data models and structures for responsive Drupal websites and web applications. As a member of our world-class agency, you will work on innovative and inspired work across a variety of clients.
Responsibilities
Perform ETL processing. Deal with raw data that contains human, machine or instrument errors, may be un-validated, unformatted or contain suspect records or system-specific codes.
Recommend and sometimes implement ways to improve data reliability, efficiency and quality.
Design, build, and maintain efficient database structures
Develop functions and scripts to import data into the designed database structure, via RESTful APIs, manual uploads, and other methods.
Develop functionality to query data for use in web applications and visualizations.
Skills & Experience
5+ yearsâ experience developing in PHP
2+ yearsâ experience working with the Drupal Content Management system, version 8 preferred
Solid understanding of object-oriented programming
Familiar with various design and architectural patterns
Experience working with ElasticSearch and MySQL database management systems
Demonstrated experience with REST API integrations
Experience working with NoSQL data architectures is a huge plus
Understanding fundamental design principles behind a scalable application
Creating database schemas that represent and support business processes
Ability to work independently, prioritize tasks and hit deadlines in a fast-paced work environment.
Demonstrates good judgment, excellent planning, problem-solving, troubleshooting, management, and communication (verbal and written) skills with the ability to think strategically, act quickly, multi-task, and work collaboratively in an environment that values creativity and flexibility to make things happen.
Applicant Eligibility: Please note, only candidates who are US citizens or able to work on a permanent basis without visa sponsorship are eligible to apply. No recruiters or staffing agencies please.","  5+ yearsâ experience developing in PHP 2+ yearsâ experience working with the Drupal Content Management system, version 8 preferred Solid understanding of object-oriented programming Familiar with various design and architectural patterns Experience working with ElasticSearch and MySQL database management systems Demonstrated experience with REST API integrations Experience working with NoSQL data architectures is a huge plus Understanding fundamental design principles behind a scalable application Creating database schemas that represent and support business processes Ability to work independently, prioritize tasks and hit deadlines in a fast-paced work environment. Demonstrates good judgment, excellent planning, problem-solving, troubleshooting, management, and communication  verbal and written  skills with the ability to think strategically, act quickly, multi-task, and work collaboratively in an environment that values creativity and flexibility to make things happen.  Perform ETL processing. Deal with raw data that contains human, machine or instrument errors, may be un-validated, unformatted or contain suspect records or system-specific codes. Recommend and sometimes implement ways to improve data reliability, efficiency and quality. Design, build, and maintain efficient database structures Develop functions and scripts to import data into the designed database structure, via RESTful APIs, manual uploads, and other methods. Develop functionality to query data for use in web applications and visualizations.  ","5+ yearsâ experience developing in PHP 2+ working with the Drupal Content Management system, version 8 preferred Solid understanding of object-oriented programming Familiar various design and architectural patterns Experience ElasticSearch MySQL database management systems Demonstrated REST API integrations NoSQL data architectures is a huge plus Understanding fundamental principles behind scalable application Creating schemas that represent support business processes Ability to work independently, prioritize tasks hit deadlines fast-paced environment. Demonstrates good judgment, excellent planning, problem-solving, troubleshooting, management, communication verbal written skills ability think strategically, act quickly, multi-task, collaboratively an environment values creativity flexibility make things happen. Perform ETL processing. Deal raw contains human, machine or instrument errors, may be un-validated, unformatted contain suspect records system-specific codes. Recommend sometimes implement ways improve reliability, efficiency quality. Design, build, maintain efficient structures Develop functions scripts import into designed structure, via RESTful APIs, manual uploads, other methods. functionality query for use web applications visualizations.","5+ yearsâ experience developing PHP 2+ working Drupal Content Management system, version 8 preferred Solid understanding object-oriented programming Familiar various design architectural patterns Experience ElasticSearch MySQL database management systems Demonstrated REST API integrations NoSQL data architectures huge plus Understanding fundamental principles behind scalable application Creating schemas represent support business processes Ability work independently, prioritize tasks hit deadlines fast-paced environment. Demonstrates good judgment, excellent planning, problem-solving, troubleshooting, management, communication verbal written skills ability think strategically, act quickly, multi-task, collaboratively environment values creativity flexibility make things happen. Perform ETL processing. Deal raw contains human, machine instrument errors, may un-validated, unformatted contain suspect records system-specific codes. Recommend sometimes implement ways improve reliability, efficiency quality. Design, build, maintain efficient structures Develop functions scripts import designed structure, via RESTful APIs, manual uploads, methods. functionality query use web applications visualizations."
33,Data Engineer,Big Data Engineer,"Reston, VA",Reston,VA,"DataSync Technologies is looking for several Big Data Engineers to help support our Customer. Be a part of an award-winning, employee friendly company in Northern VA and have the satisfaction of helping keep America safe. DataSync Technologies, Inc is a veteran owned small business providing consulting excellence and real time solutions for customers with complex information technology needs within Intelligence Community. Our cleared consultants bring real world experience with a common sense approach to their jobs whether they are creating complex analytic dashboards, architecting new cloud technology infrastructures, securing sensitive data or streamlining business processes for efficiency.

Qualified candidates must be able to develop, maintain, test and evaluate big data solutions within organizations. Candidates must be able to build large-scale data processing systems, be an expert in data warehousing solutions and should be able to work with the latest (NoSQL) database technologies.
ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST BE A US CITIZEN.

Skills
Bachelor's or Master's degree in computer science or software engineering preferred
Experience with object-oriented design, coding and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures.
Ability to architect highly scalable distributed systems, using different open source tools.
Experience building high-performance algorithms.
Extensive knowledge of different programming or scripting languages such as Java, Linux, C++, PHP, Ruby, Phyton and/or R.
Experience with different (NoSQL or RDBMS) databases such as MongoDB needed.
Experience building data processing systems with Hadoop and Hive using Java or Python
Desired Experience
Excellent oral and written communication skills;
Experience in designing efficient and robust ETL workflows;
AWS experience
_________________
What makes DataSync Technologies different?
Leadership Training: We provide employees with a variety of learning opportunities, including access to exclusive classes, professional growth training and more.
Feedback & Mentoring: We believe in talkingâoften. So we have one-on-one feedback sessions for every employee.
Community Service: We believe in helping the community where we work. DataSync and its employees donate time and services on a regular basis to local military charities. We believe in helping, both inside and outside of the office.
Social Events: We plan social events on a regular basis to help our employees relax and socialize so we get to know one another outside of our job titles.
DataSync is an EEO and Affirmative Action Employer of Female/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law
Information about Equal Employment Opportunity (EEO) and Employee Polygraph Act (EPPA) provisions in addition to other Federal labor laws can be found at http://webapps.dol.gov/dolfaq/go-dol-faq.asp?faqid=537
DataSync is committed to providing veteran employment opportunities to our service men and women.
www.datasynctech.com
www.facebook.com/DatasyncTechnologies
www.twitter.com/Jobs at DataSync (@DatasyncJobs)
www.twitter.com/datasynctech
#datasynctech on Instagram
Interested in Joining Our Team? - Check out this YouTube video!
oWudwVf7CH","  Bachelor's or Master's degree in computer science or software engineering preferred Experience with object-oriented design, coding and testing patterns as well as experience in engineering  commercial or open source  software platforms and large-scale data infrastructures. Ability to architect highly scalable distributed systems, using different open source tools. Experience building high-performance algorithms. Extensive knowledge of different programming or scripting languages such as Java, Linux, C++, PHP, Ruby, Phyton and/or R. Experience with different  NoSQL or RDBMS  databases such as MongoDB needed. Experience building data processing systems with Hadoop and Hive using Java or Python   ","Bachelor's or Master's degree in computer science software engineering preferred Experience with object-oriented design, coding and testing patterns as well experience commercial open source platforms large-scale data infrastructures. Ability to architect highly scalable distributed systems, using different tools. building high-performance algorithms. Extensive knowledge of programming scripting languages such Java, Linux, C++, PHP, Ruby, Phyton and/or R. NoSQL RDBMS databases MongoDB needed. processing systems Hadoop Hive Java Python","Bachelor's Master's degree computer science software engineering preferred Experience object-oriented design, coding testing patterns well experience commercial open source platforms large-scale data infrastructures. Ability architect highly scalable distributed systems, using different tools. building high-performance algorithms. Extensive knowledge programming scripting languages Java, Linux, C++, PHP, Ruby, Phyton and/or R. NoSQL RDBMS databases MongoDB needed. processing systems Hadoop Hive Java Python"
34,Data Engineer,Big Data Engineer,"Reston, VA",Reston,VA,"Data Works is looking for senior Big Data Engineers able to lead the way in tackling the most difficult engineering challenges in Big Data systems.

This position requires U.S. Citizenship and an active TS/SCI security clearance.

About Data Works

Data Works is an employee-focused small company that supports the Intelligence Community by providing Big Data and Cyber Security solutions. We favor a high quality workforce over aggressive growth and provide opportunities on programs that fill mission-critical needs. Our core competency is in the technical stack between data collection and analysis. Positions are available in various customer locations between McLean and Dulles in Virginia.

Position Description

Data Works is seeking a Big Data Engineer with demonstrated experience in leading large scale data warehousing projects. A successful candidate will be strong in Map Reduce, Java, and possess an understanding of data science concepts such as machine learning and trend analysis. Candidate should also be familiar with indexing products such as Lucene and Elasticsearch. Relevant certifications considered but not required.

Technical Requirements

Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark
Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting
Experience with data warehousing tools and technologies
Ability to work within UNIX/Linux operating systems
AWS experience a plus
Company Benefits

6 weeks PTO
Paid Overtime
Annual Bonuses
10% Employer 401k Contribution
Health/Vision/Dental/Disability/Life Insurance
Annual Training and Tuition Budgets
Technology/Fitness/Communications Reimbursement
Charity Matching Program
EOE/M/F/Vet/Disabled","     Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting Experience with data warehousing tools and technologies Ability to work within UNIX/Linux operating systems AWS experience a plus ","Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark Development experience Java, C++, Scala, Groovy, Python, and/or shell scripting data warehousing tools Ability to work within UNIX/Linux operating systems AWS a plus","Experience distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch Apache Spark Development experience Java, C++, Scala, Groovy, Python, and/or shell scripting data warehousing tools Ability work within UNIX/Linux operating systems AWS plus"
35,Data Engineer,Database Engineer Lead: Principal (TS),"Sterling, VA 20164",Sterling,VA,"Database Engineer Lead
**Top Secret Required**
BCMC is looking for motivated individuals to support a long-term engineering program for one of the country's highest priority cyber initiatives. On this program you will have the opportunity to work with technical leaders in cyber, networking, computer science, and data analytics to develop the next generation of automated cyber defense platforms. Individuals in this role are responsible for implementing, deploying, and maintaining database systems. These capabilities are used to analyze, detect, and prevent sophisticated threats and vulnerabilities on enterprise networks.
Responsibilities:
Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system
Working with large structured and unstructured data sets
Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment
Design, setup, administer, and tune NoSQL databases in the AWS cloud
Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality
Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations
Write and refine code to ensure quality and reliability of data extraction and processing
Analyze and resolve data performance and quality issues
Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.
Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance
Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates
Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team
Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment
Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages
Maintain current industry knowledge of relevant concepts, practices and procedures


Required Skills:
Must have an active Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearance
Must be able to obtain DHS Suitability
10+ years of relevant database experience
Demonstrated ability to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers)
Demonstrated experience mentoring junior to mid-level data professionals
Able to effectively work as a leader, in a group, or as an individual contributor
Excellent understanding of big data and data analytics
Experience working with large structured and unstructured data sets
Development experience building ETL pipelines at scale
Solid SQL development skills
Experience with Linux/Unix tools and shell scripts
Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing and encryption
Good communication skills, both oral and written
Must work well in a team environment as well as independently
Must exhibit good time management skills, independent decision making capability, and a focus on customer service.


Desired Skills:
Experience providing database engineering support to Intelligence, DoD, or DHS Customers
Understanding of Certification and Accreditation (ICD 503/DCID 6/3) processes as they apply to database technologies
Ability to support both SQL and NoSQL data management systems
Expertise in other RDBMS platforms such as Oracle RAC and SQL Server
Familiarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline
Experience with data transformation techniques such as aggregations, joins, and data cleaning
Experience with Red Hat Enterprise Linux (RHEL) operating system, storage configurations, network architecture, VMware, and/or related management tools
Database management experience of SQL databases such as MySQL and PostgreSQL in AWS cloud
Experience creating and managing NoSQL databases such as DynamoDB in the AWS cloud
Object mapping and migration of data from legacy structured and unstructured data sources to Amazon DynamoDB using AWS tools, custom code, or ETL scripts
Programming experience with languages such as R, Python, Java, JavaScript, JSON, etc.
Knowledge of Hadoop ecosystem, Map/Reduce, and data management products including Hbase, Hive, and Pig
DevSecOps and Continuous Integration / Continuous Delivery (CI/CD) knowledge
Experience or training in Six Sigma Methodology
ITIL knowledge and certification
Familiarity with SAFe (Scaled Agile Framework).


Required Education:

BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering. Two years of related work experience may be substituted for each year of degree level education


Desired Certifications:
IBM Certified Data Engineer - Big Data
Google Cloud Certified Professional - Data Engineer
Cloudera Certified Professional - Data Engineer
CCDH: Cloudera Certified Developer for Apache Hadoop
CCAH: Cloudera Certified Administrator for Apache Hadoop
CCSHB: Cloudera Certified Specialist in Apache HBase
CSSLP Certified Secure Software Lifecycle Professional
Certifications related to Scaled Agile Framework (SAFe) such as SAFe Practitioner (SP) or SAFe Program Consultant (SPC)
DoD 8570.1 IAT Level I
Company Overview:
BCMC is an Information Technology (IT), Cybersecurity, Information Assurance (IA), Big Data Management, Program Management, and more for Federal, State, and Local agencies. We possess highly skilled engineers, providing innovative solutions backed by strong past performances.
Benefits
Extremely competitive salary
95% employer paid for employee medical, dental, & vision coverages
100% employer paid for employee life, STD & LTD disability coverages
401k with company match and profit sharing
Flexible Spending Account (FSA) for dependent & health care
10 standard holidays & competitive Paid Time Off (PTO)

XxoWs9VYO9"," Must have an active Top Secret  TS  clearance. Must be able to obtain a TS/SCI clearance Must be able to obtain DHS Suitability 10+ years of relevant database experience Demonstrated ability to lead teams with diverse skill sets  e.g. data architects, data scientists, software developers  Demonstrated experience mentoring junior to mid-level data professionals Able to effectively work as a leader, in a group, or as an individual contributor Excellent understanding of big data and data analytics Experience working with large structured and unstructured data sets Development experience building ETL pipelines at scale Solid SQL development skills Experience with Linux/Unix tools and shell scripts Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing and encryption Good communication skills, both oral and written Must work well in a team environment as well as independently Must exhibit good time management skills, independent decision making capability, and a focus on customer service.  Using database expertise to lead teams with diverse skill sets  e.g. data architects, data scientists, software developers  in support of a large, agile-based, cybersecurity system Working with large structured and unstructured data sets Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment Design, setup, administer, and tune NoSQL databases in the AWS cloud Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations Write and refine code to ensure quality and reliability of data extraction and processing Analyze and resolve data performance and quality issues Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc. Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages Maintain current industry knowledge of relevant concepts, practices and procedures  IBM Certified Data Engineer - Big Data Google Cloud Certified Professional - Data Engineer Cloudera Certified Professional - Data Engineer CCDH  Cloudera Certified Developer for Apache Hadoop CCAH  Cloudera Certified Administrator for Apache Hadoop CCSHB  Cloudera Certified Specialist in Apache HBase CSSLP Certified Secure Software Lifecycle Professional Certifications related to Scaled Agile Framework  SAFe  such as SAFe Practitioner  SP  or SAFe Program Consultant  SPC  DoD 8570.1 IAT Level I  ","Must have an active Top Secret TS clearance. be able to obtain a TS/SCI clearance DHS Suitability 10+ years of relevant database experience Demonstrated ability lead teams with diverse skill sets e.g. data architects, scientists, software developers mentoring junior mid-level professionals Able effectively work as leader, in group, or individual contributor Excellent understanding big and analytics Experience working large structured unstructured Development building ETL pipelines at scale Solid SQL development skills Linux/Unix tools shell scripts Expertise analysis design, modeling, master management, metadata warehousing, performance tuning, quality improvement, security, auditing encryption Good communication skills, both oral written well team environment independently exhibit good time management independent decision making capability, focus on customer service. Using expertise support large, agile-based, cybersecurity system Working Implementing systems from conception, through all stages development, deployment CI/CD agile Design, setup, administer, tune NoSQL databases the AWS cloud Design implement technical architecture necessary analytic statistical processing requirements based tradeoff between Performing transformations such aggregations, joins, cleaning applications visualizations Write refine code ensure reliability extraction Analyze resolve issues Make available programmers, other users using programming scripting languages R, Python, Java, JavaScript, etc. Perform business intelligence functions governance Generate estimates performed, compare actuals, continue Work collaboratively teams, attending daily scrums providing related solutions Develop maintain libraries, models, SOPs, documentation collaboration Migrate legacy RDBMSs sources complex processes, tools, Maintain current industry knowledge concepts, practices procedures IBM Certified Data Engineer - Big Google Cloud Professional Cloudera CCDH Developer for Apache Hadoop CCAH Administrator CCSHB Specialist HBase CSSLP Secure Software Lifecycle Certifications Scaled Agile Framework SAFe Practitioner SP Program Consultant SPC DoD 8570.1 IAT Level I","Must active Top Secret TS clearance. able obtain TS/SCI clearance DHS Suitability 10+ years relevant database experience Demonstrated ability lead teams diverse skill sets e.g. data architects, scientists, software developers mentoring junior mid-level professionals Able effectively work leader, group, individual contributor Excellent understanding big analytics Experience working large structured unstructured Development building ETL pipelines scale Solid SQL development skills Linux/Unix tools shell scripts Expertise analysis design, modeling, master management, metadata warehousing, performance tuning, quality improvement, security, auditing encryption Good communication skills, oral written well team environment independently exhibit good time management independent decision making capability, focus customer service. Using expertise support large, agile-based, cybersecurity system Working Implementing systems conception, stages development, deployment CI/CD agile Design, setup, administer, tune NoSQL databases AWS cloud Design implement technical architecture necessary analytic statistical processing requirements based tradeoff Performing transformations aggregations, joins, cleaning applications visualizations Write refine code ensure reliability extraction Analyze resolve issues Make available programmers, users using programming scripting languages R, Python, Java, JavaScript, etc. Perform business intelligence functions governance Generate estimates performed, compare actuals, continue Work collaboratively teams, attending daily scrums providing related solutions Develop maintain libraries, models, SOPs, documentation collaboration Migrate legacy RDBMSs sources complex processes, tools, Maintain current industry knowledge concepts, practices procedures IBM Certified Data Engineer - Big Google Cloud Professional Cloudera CCDH Developer Apache Hadoop CCAH Administrator CCSHB Specialist HBase CSSLP Secure Software Lifecycle Certifications Scaled Agile Framework SAFe Practitioner SP Program Consultant SPC DoD 8570.1 IAT Level I"
36,Data Engineer,"Senior Data Engineer, TS/SCI & Poly Required","Chantilly, VA",Chantilly,VA,"Job Description
Description
SAIC is seeking a Data Engineer to perform data engineering tasks in support of a Business Analytics effort. This includes designing how data will be stored, consumed, integrated, and managed. Expected tasks will include:
Working together with the Government POC to determine, create, and populate an optimal data architecture, structure, and system
Plan, design, and optimize for data throughput and query performance issues
Develop and implement processes to automatically ingest, update, manage, and integrate heterogeneous data streams
Configure and utilize back-end database technologies and optimize the full data pipeline infrastructure to support the actual content, volume, and ETL of data to support queries and analysis
Qualifications
TS/SCI with Polygraph required
Bachelor's and fourteen (14) years of related experience. Experience may be substituted in lieu of degree
Expertise developing and implementing robust data schemas across heterogeneous datasets
Expertise with data security, and data tagging; experience with customer policies and systems is preferred; understanding of relevant privacy and data deconstruction policies or ability to develop expertise quickly
Expertise processing structured and unstructured data
Experience using python and/or R is beneficial
Experience leveraging APIs for data ingest and sharing
Desired Qualifications

Familiarity with Tableau
Scripting


Overview
SAIC is a premier technology integrator, solving our nation's most complex modernization and systems engineering challenges across the defense, space, federal civilian, and intelligence markets. Our robust portfolio of offerings includes high-end solutions in systems engineering and integration; enterprise IT, including cloud services; cyber; software; advanced analytics and simulation; and training. We are a team of 23,000 strong driven by mission, united purpose, and inspired by opportunity. Headquartered in Reston, Virginia, SAIC has annual revenues of approximately $6.5 billion. For more information, visit saic.com. For information on the benefits SAIC offers, see Working at SAIC. EOE AA M/F/Vet/Disability","TS/SCI with Polygraph required Bachelor's and fourteen  14  years of related experience. Experience may be substituted in lieu of degree Expertise developing and implementing robust data schemas across heterogeneous datasets Expertise with data security, and data tagging; experience with customer policies and systems is preferred; understanding of relevant privacy and data deconstruction policies or ability to develop expertise quickly Expertise processing structured and unstructured data Experience using python and/or R is beneficial Experience leveraging APIs for data ingest and sharing    ","TS/SCI with Polygraph required Bachelor's and fourteen 14 years of related experience. Experience may be substituted in lieu degree Expertise developing implementing robust data schemas across heterogeneous datasets security, tagging; experience customer policies systems is preferred; understanding relevant privacy deconstruction or ability to develop expertise quickly processing structured unstructured using python and/or R beneficial leveraging APIs for ingest sharing","TS/SCI Polygraph required Bachelor's fourteen 14 years related experience. Experience may substituted lieu degree Expertise developing implementing robust data schemas across heterogeneous datasets security, tagging; experience customer policies systems preferred; understanding relevant privacy deconstruction ability develop expertise quickly processing structured unstructured using python and/or R beneficial leveraging APIs ingest sharing"
37,Data Engineer,Database Engineering Lead,"Sterling, VA 20166",Sterling,VA,"Database Engineering Lead
Sterling, VA 20166

Culmen International is hiring for a Database Engineering Lead to work in support of a U.S. Government customer on a large mission critical development and sustainment program. The database engineering work will support the design, build, delivery, and operations for a network operations environment. Activities will include introducing new cyber capabilities to address emerging threats. This work is part of the Cybersecurity and Special Missions (CSM) area working collaboratively with agile development teams in the design, development, and deployment of advanced cybersecurity capabilities.

Qualified candidates will have expertise in the following:
Cloud Computing, Computer Engineering, Computer Science, Configuration Management, Cyber Jobs, Data Networking, Data Science, General Management, Hardware Engineering, Integration & Test Engineering, Software Engineering, Systems, Engineering, Test Engineering

Security Clearance Requirement: Current TS/SCI is required to be considered AND must also be eligible for DHS Suitability

Role and responsibilities include:
Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system.
Working with large structured and unstructured data sets.
Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment.
Design, setup, administer, and tune NoSQL databases in the AWS cloud.
Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality.
Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations.
Write and refine code to ensure quality and reliability of data extraction and processing.
Analyze and resolve data performance and quality issues.
Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.
Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance.
Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates.
Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team.
Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment.
Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages.
Maintain current industry knowledge of relevant concepts, practices and procedures


Requirement education and experience includes:
U.S. Citizenship
Current TS/SCI security clearance
Ability to obtain DHS Suitability. Current DHS Suitability is preferred
Education requirement:
BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering.
Two years of related work experience may be substituted for each year of degree level education.
10+ years of relevant database experience
Demonstrated ability to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers)
Demonstrated experience mentoring junior to mid-level data professionals
Able to effectively work as a leader, in a group, or as an individual contributor
Excellent understanding of big data and data analytics
Experience working with large structured and unstructured data sets
Development experience building ETL pipelines at scale
Solid SQL development skills
Experience with Linux/Unix tools and shell scripts
Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing and encryption
Good communication skills, both oral and written
Must work well in a team environment as well as independently
Must exhibit good time management skills, independent decision making capability, and a focus on customer service.
Desired skills include:
Experience providing database engineering support to Intelligence, DoD, or DHS Customers
Understanding of Certification and Accreditation (ICD 503/DCID 6/3) processes as they apply to database technologies
Ability to support both SQL and NoSQL data management systems
Expertise in other RDBMS platforms such as Oracle RAC and SQL Server
Familiarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline
Experience with data transformation techniques such as aggregations, joins, and data cleaning
Experience with Red Hat Enterprise Linux (RHEL) operating system, storage configurations, network architecture, VMware, and/or related management tools
Database management experience of SQL databases such as MySQL and PostgreSQL in AWS cloud
Experience creating and managing NoSQL databases such as DynamoDB in the AWS cloud
Object mapping and migration of data from legacy structured and unstructured data sources to Amazon DynamoDB using AWS tools, custom code, or ETL scripts
Programming experience with languages such as R, Python, Java, JavaScript, JSON, etc.
Knowledge of Hadoop ecosystem, Map/Reduce, and data management products including Hbase, Hive, and Pig
DevSecOps and Continuous Integration / Continuous Delivery (CI/CD) knowledge
Experience or training in Six Sigma Methodology
ITIL knowledge and certification
Familiarity with SAFe (Scaled Agile Framework).
Desired certifications include:
IBM Certified Data Engineer - Big Data
Google Cloud Certified Professional - Data Engineer
Cloudera Certified Professional - Data Engineer
CCDH: Cloudera Certified Developer for Apache Hadoop
CCAH: Cloudera Certified Administrator for Apache Hadoop
CCSHB: Cloudera Certified Specialist in Apache HBase
CSSLP Certified Secure Software Lifecycle Professional
Certifications related to Scaled Agile Framework (SAFe) such as SAFe Practitioner (SP) or SAFe Program Consultant (SPC)
DoD 8570.1 IAT Level I
Job Type: Full-Time (this job is not open to 1099 consulting)
Benefits: Health, Vision, Dental, 401k, Disability and Life Insurance Programs

#623","    Using database expertise to lead teams with diverse skill sets  e.g. data architects, data scientists, software developers  in support of a large, agile-based, cybersecurity system. Working with large structured and unstructured data sets. Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment. Design, setup, administer, and tune NoSQL databases in the AWS cloud. Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality. Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations. Write and refine code to ensure quality and reliability of data extraction and processing. Analyze and resolve data performance and quality issues. Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc. Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance. Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates. Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team. Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment. Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages. Maintain current industry knowledge of relevant concepts, practices and procedures ","Using database expertise to lead teams with diverse skill sets e.g. data architects, scientists, software developers in support of a large, agile-based, cybersecurity system. Working large structured and unstructured sets. Implementing management systems from conception, through all stages design, development, deployment CI/CD agile environment. Design, setup, administer, tune NoSQL databases the AWS cloud. Design implement technical architecture necessary analytic statistical processing requirements based on tradeoff between performance quality. Performing transformations such as aggregations, joins, cleaning applications visualizations. Write refine code ensure quality reliability extraction processing. Analyze resolve issues. Make available programmers, other users using programming scripting languages R, Python, Java, JavaScript, etc. Perform analysis modeling, business intelligence management, master metadata security, auditing functions governance. Generate estimates work be performed, compare actuals, continue estimates. Work collaboratively development teams, attending daily scrums providing related solutions team. Develop maintain libraries, models, SOPs, documentation team collaboration Migrate legacy RDBMSs sources cloud complex ETL processes, tools, languages. Maintain current industry knowledge relevant concepts, practices procedures","Using database expertise lead teams diverse skill sets e.g. data architects, scientists, software developers support large, agile-based, cybersecurity system. Working large structured unstructured sets. Implementing management systems conception, stages design, development, deployment CI/CD agile environment. Design, setup, administer, tune NoSQL databases AWS cloud. Design implement technical architecture necessary analytic statistical processing requirements based tradeoff performance quality. Performing transformations aggregations, joins, cleaning applications visualizations. Write refine code ensure quality reliability extraction processing. Analyze resolve issues. Make available programmers, users using programming scripting languages R, Python, Java, JavaScript, etc. Perform analysis modeling, business intelligence management, master metadata security, auditing functions governance. Generate estimates work performed, compare actuals, continue estimates. Work collaboratively development teams, attending daily scrums providing related solutions team. Develop maintain libraries, models, SOPs, documentation team collaboration Migrate legacy RDBMSs sources cloud complex ETL processes, tools, languages. Maintain current industry knowledge relevant concepts, practices procedures"
38,Data Engineer,Senior Business Intelligence / Data Engineer,"Reston, VA 20191",Reston,VA,"Duties and Responsibilities

The Senior BI Intelligence / Data Engineer is responsible for designing enterprise and departmental business intelligence, data warehousing and reporting solutions.
Work with stakeholders at various levels to assist with data-related technical issues and support their data acquisition and infrastructure needs.
Work with analytics to develop greater functionality in data presentation.
Solutions include, end-user reports, data visualizations, ETL systems, master data management and other BI Solutions.
This position will perform requirements analysis, design, and implementation of end-user requested BI solutions.
Translate business reporting and analytic requirements into ETL and report specifications.
Develop and implement ETL processes, reports and queries in support of business analytics.
Develop and implement interactive analytic reports and dashboards.
Qualifications

Bachelorâs degree in Computer Engineering, Computer Science, Management Information Systems, or a related business field.
5+ years of development, architecture, and configuration skills in Microsoft SQL Server and extensive experience with SQL Server Reporting Services (SSRS) and SQL Server Integration Services (SSIS).
Should possess expert level competence with Microsoft T-SQL, Microsoft PowerShell, Microsoft Visual Studio, and Microsoft SQL Server Management Studio (SSMS).
Experience with Power BI
Experience with BI analytics stack including reports, dashboards, and scorecards.
Ideal Candidate

Experience with Microsoft Azure Cloud Platform
Experience with Microsoft SQL Server
Experience with SQL Server Reporting Services (SSRS)
Experience with [SSDT] Database Projects
Experience with SQL Server Analysis Services (SSAS)
Experience with Power BI
Experience with Dynamics 365 (highly desirable)
Some Development Experience (C#, PowerShell, preferable)"," Bachelorâs degree in Computer Engineering, Computer Science, Management Information Systems, or a related business field. 5+ years of development, architecture, and configuration skills in Microsoft SQL Server and extensive experience with SQL Server Reporting Services  SSRS  and SQL Server Integration Services  SSIS . Should possess expert level competence with Microsoft T-SQL, Microsoft PowerShell, Microsoft Visual Studio, and Microsoft SQL Server Management Studio  SSMS . Experience with Power BI Experience with BI analytics stack including reports, dashboards, and scorecards.    The Senior BI Intelligence / Data Engineer is responsible for designing enterprise and departmental business intelligence, data warehousing and reporting solutions. Work with stakeholders at various levels to assist with data-related technical issues and support their data acquisition and infrastructure needs. Work with analytics to develop greater functionality in data presentation. Solutions include, end-user reports, data visualizations, ETL systems, master data management and other BI Solutions. This position will perform requirements analysis, design, and implementation of end-user requested BI solutions. Translate business reporting and analytic requirements into ETL and report specifications. Develop and implement ETL processes, reports and queries in support of business analytics. Develop and implement interactive analytic reports and dashboards.   ","Bachelorâs degree in Computer Engineering, Science, Management Information Systems, or a related business field. 5+ years of development, architecture, and configuration skills Microsoft SQL Server extensive experience with Reporting Services SSRS Integration SSIS . Should possess expert level competence T-SQL, PowerShell, Visual Studio, Studio SSMS Experience Power BI analytics stack including reports, dashboards, scorecards. The Senior Intelligence / Data Engineer is responsible for designing enterprise departmental intelligence, data warehousing reporting solutions. Work stakeholders at various levels to assist data-related technical issues support their acquisition infrastructure needs. develop greater functionality presentation. Solutions include, end-user visualizations, ETL systems, master management other Solutions. This position will perform requirements analysis, design, implementation requested Translate analytic into report specifications. Develop implement processes, reports queries analytics. interactive dashboards.","Bachelorâs degree Computer Engineering, Science, Management Information Systems, related business field. 5+ years development, architecture, configuration skills Microsoft SQL Server extensive experience Reporting Services SSRS Integration SSIS . Should possess expert level competence T-SQL, PowerShell, Visual Studio, Studio SSMS Experience Power BI analytics stack including reports, dashboards, scorecards. The Senior Intelligence / Data Engineer responsible designing enterprise departmental intelligence, data warehousing reporting solutions. Work stakeholders various levels assist data-related technical issues support acquisition infrastructure needs. develop greater functionality presentation. Solutions include, end-user visualizations, ETL systems, master management Solutions. This position perform requirements analysis, design, implementation requested Translate analytic report specifications. Develop implement processes, reports queries analytics. interactive dashboards."
39,Data Engineer,Data Engineer,"McLean, VA",McLean,VA,"At CollabraLink, weâre committed to providing federal clients with simple and intuitive solutions that increase efficiency and enhance citizen engagement. Using advanced technology, rigorous processes and trusted guidance, weâre making government more interactive, productive and secure. We understand that our employees are our greatest asset. Our goal is to create an environment where employees can do important, purposeful work and be rewarded for individual and team success.

CollabraLink is seeking a Data Engineer in a AI/ML environment to join our team.


Responsibilities:
Lead the team ( 8-10) with full stack development for DevOps efforts for a federal customer

Required Experience:
Minimum of ten (10) years of experience in the Information Technology field focusing on AI/ML engineering projects, DevSecOps and technical architecture specifically inclusive of following:
Possess strong architecture & design experience, including at least three (3) years of experience deploying production enterprise applications in AWS that use AI/ML.
Experience in large scale, high performance enterprise big data application deployment and solution architecture on complex heterogeneous environments in AWS.
Minimum of a Bachelorâs degree in Computer Science, Information Technology Management or Engineering.

Preferred Experience:
Experience law enforcement agencies
AWS professional level certifications
Experience with AI/ML technologies
Experience with Javascript language technologies such as Angular or React

We actively practice the philosophy that empowered employees make successful teams. Thatâs why we strive to put employees in positions where they can grow, both personally and professionally. CollabraLink offers a full suite of benefits including comprehensive medical, dental and vision plans, Flexible Spending Accounts, matching 401K, paid time off, tuition reimbursement plans and much more.

CollabraLink is a fast growing CMMI-DEV Maturity Level 3, Small Business professional services firm. Founded in 2003, CollabraLink has long established ourselves as a value-add partner assisting our customers in solving their most difficult problems. We bring expertise across a wide variety of IT and Mission Support services driving significant results for our customers.

CollabraLink is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.
#CL",  Lead the team   8-10  with full stack development for DevOps efforts for a federal customer   ,Lead the team 8-10 with full stack development for DevOps efforts a federal customer,Lead team 8-10 full stack development DevOps efforts federal customer
40,Data Engineer,Data Engineer,"Washington, DC 20003",Washington,DC,"âJacobs National Security Solutions (NSS) provides world-class IT network and service management capabilities; cutting edge cyber threat awareness and cybersecurity solutions; innovative web- and software applications development; and advanced data analytics for major clients in the Intelligence Community, Department of Defense, and Federal Civilian Agencies.

Our forward thinking solutions deliver an integrated approach to IT network design and management, full lifecycle IT service management, IT service delivery, asset management, logistics and procurement, and vendor management. We leverage the expertise and passion of our employees to conduct identity and access management, penetration testing, and vulnerability assessments for our nationâs most closely guarded agencies and networks. Our Cyber Security Operations Centers ensure safe, effective network operations for Federal clients while our data scientists are helping stop illegal acts before they can endanger Americans or our way of life.
Jacobs promotes a culture of operational excellence to create a safer, smarter, and more connected world while upholding the highest standards of compliance, quality and integrity.
We continue to thrive and need your talent and motivation to help propel us farther, faster.â
Jacobs is seeking a Data Engineer in Washington, DC. Duties will include:
Experience creating and maintaining data pipelines and transformation flows in a cloud environment
Data management/mapping among multiple distinct data sources
Cloud management and server administration of domain services
Big Data infrastructure services and cross domain data transfer
Qualifications
Active Top Secret security clearance with SCI eligibility
BS degree in a related scientific or engineering discipline from an accredited college or university and ten (10) to fourteen (14) years of progressive experience, or an MS degree in a related scientific or engineering discipline, and eight (8) to twelve (12) years of progressive experience, or a Ph.D. degree in a related scientific or engineering discipline and four (4) to seven (7) years of progressive experience.
Familiar with ETL technologies, MapReduce, JSON/XML transformations and schemas
Familiar with AngularJS
Familiar with Apache NiFi and Java (NAR) NiFi Archives
Knowledge of Amazon Web Services (AWS Cloud)
Programming languages â Java/JEE, Javascript, Python, Groovy, Shell Script
HTTP via REST and SOAP
Datastores â HDFS, MongoDB, S3, Elastic, NoSQL, RDBMS
Build and Configuration Management Tools â Maven, Ansible, Puppet
Working knowledge with public keys and digital certificates
Linux/Unix server environments

Preferred Qualifications (desired but not required):
Masterâs level education
Familiarity with: jQuery, XPath, XQuery, Spark, Impala, Sqoop, Hive/Pig, Python, Gradle, Maven, PL/SQL, Unix Shell, C++/C, AngularJS, Spring, JSON, XML/XSLT/HTML, JPA/Hibernate, Spark, Accumulo, MapReduce, Storm/Kafka, HSpace, Pig, Servlet/JSP, LDAP

Essential Functions
Physical Requirements:
Most work will be done at a desk or computer.
Work Environment:
General Office environment. The work environment is fast-paced and sometimes involves extreme deadline pressures. The nature of the work requires a high degree of teamwork and cooperation with other members of the staff as well as individuals across the Company and Customers.
Equipment & Machines:
General office equipment including PC/laptop, Fax, Copiers, Shredder, Printers, Telephone, and other miscellaneous office equipment.
Attendance:
Attendance is critical at all times. Must be able to work a 40-hour workweek, normally Monday through Friday. However, times and days may vary depending on business requirements. Needs to be available to work overtime during critical peaks and be available to meet last minute requests for overtime should the situation occur.
Other Essential Functions:
Must be able to communicate effectively both verbally and in writing. Grooming and dress must be appropriate for the position and must not impose a safety risk/hazard to the employee or others. Must put forward a professional behavior that enhances productivity and promotes teamwork and cooperation. Must be able to interface with individuals at all levels of the organization both verbally and in writing. Must be well-organized with the ability to coordinate and prioritize multiple tasks simultaneously. Must work well under pressure to meet deadline requirements. Must be willing to travel as needed. Must take and pass a drug test and background check as well as a motor vehicle records check. Must be a US citizen.
 
Jacobs is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law. Jacobs is a background screening, drug-free workplace.","Active Top Secret security clearance with SCI eligibility BS degree in a related scientific or engineering discipline from an accredited college or university and ten  10  to fourteen  14  years of progressive experience, or an MS degree in a related scientific or engineering discipline, and eight  8  to twelve  12  years of progressive experience, or a Ph.D. degree in a related scientific or engineering discipline and four  4  to seven  7  years of progressive experience. Familiar with ETL technologies, MapReduce, JSON/XML transformations and schemas Familiar with AngularJS Familiar with Apache NiFi and Java  NAR  NiFi Archives Knowledge of Amazon Web Services  AWS Cloud  Programming languages â Java/JEE, Javascript, Python, Groovy, Shell Script HTTP via REST and SOAP Datastores â HDFS, MongoDB, S3, Elastic, NoSQL, RDBMS Build and Configuration Management Tools â Maven, Ansible, Puppet Working knowledge with public keys and digital certificates Linux/Unix server environments  Preferred Qualifications  desired but not required   Masterâs level education Familiarity with  jQuery, XPath, XQuery, Spark, Impala, Sqoop, Hive/Pig, Python, Gradle, Maven, PL/SQL, Unix Shell, C++/C, AngularJS, Spring, JSON, XML/XSLT/HTML, JPA/Hibernate, Spark, Accumulo, MapReduce, Storm/Kafka, HSpace, Pig, Servlet/JSP, LDAP     ","Active Top Secret security clearance with SCI eligibility BS degree in a related scientific or engineering discipline from an accredited college university and ten 10 to fourteen 14 years of progressive experience, MS discipline, eight 8 twelve 12 Ph.D. four 4 seven 7 experience. Familiar ETL technologies, MapReduce, JSON/XML transformations schemas AngularJS Apache NiFi Java NAR Archives Knowledge Amazon Web Services AWS Cloud Programming languages â Java/JEE, Javascript, Python, Groovy, Shell Script HTTP via REST SOAP Datastores HDFS, MongoDB, S3, Elastic, NoSQL, RDBMS Build Configuration Management Tools Maven, Ansible, Puppet Working knowledge public keys digital certificates Linux/Unix server environments Preferred Qualifications desired but not required Masterâs level education Familiarity jQuery, XPath, XQuery, Spark, Impala, Sqoop, Hive/Pig, Gradle, PL/SQL, Unix Shell, C++/C, AngularJS, Spring, JSON, XML/XSLT/HTML, JPA/Hibernate, Accumulo, Storm/Kafka, HSpace, Pig, Servlet/JSP, LDAP","Active Top Secret security clearance SCI eligibility BS degree related scientific engineering discipline accredited college university ten 10 fourteen 14 years progressive experience, MS discipline, eight 8 twelve 12 Ph.D. four 4 seven 7 experience. Familiar ETL technologies, MapReduce, JSON/XML transformations schemas AngularJS Apache NiFi Java NAR Archives Knowledge Amazon Web Services AWS Cloud Programming languages â Java/JEE, Javascript, Python, Groovy, Shell Script HTTP via REST SOAP Datastores HDFS, MongoDB, S3, Elastic, NoSQL, RDBMS Build Configuration Management Tools Maven, Ansible, Puppet Working knowledge public keys digital certificates Linux/Unix server environments Preferred Qualifications desired required Masterâs level education Familiarity jQuery, XPath, XQuery, Spark, Impala, Sqoop, Hive/Pig, Gradle, PL/SQL, Unix Shell, C++/C, AngularJS, Spring, JSON, XML/XSLT/HTML, JPA/Hibernate, Accumulo, Storm/Kafka, HSpace, Pig, Servlet/JSP, LDAP"
41,Data Engineer,Data Engineer - Secret Clearance,"Washington, DC",Washington,DC,"Secret/Top Secret Data Engineer

Work youâll do

Data Engineer â Center for Analytics


Project Description:

Center for Analytics is responsible for providing cross-functional, enterprise-level data analysis and solutions needed to accomplish key foreign policy objectives and make informed decisions. Over the last year, Deloitte has supported the incubation of the Center for Analytics through development and implementation of a strategic vision. The center's data science and engineering teams produce enterprise-level advanced analytics and data-driven decision-support products for principal executives and Bureau stakeholders.


Role Description:

The Data Engineer will develop robust and secure data pipelines to facilitate operation and management of analytics products that advance and enable the mission outcomes. In this role, the Data Engineer will work with data scientists to build and implement tools that accelerate ETL and delivery of data through multiple workstreams; productionalize and automate pipelines; and monitor and optimize performance of existing data processes.
Required Skills:

Fluency in SQL and NoSQL database technologies such as SQL Server, Oracle SQL
Fluency in programming languages such as Python and R; and query languages such as SQL
Proficiency in data extraction, transformation, and loading to support advanced analytics
Familiarity with JSON and XML data formats
Experience in web scraping (e.g., using R or Python) and retrieving data from APIs
Experience with data visualization tools, such as Tableau, Qlik, PowerBI, d3.js, or equivalent
Ability to identify solutions for communicating across different technologies
Creativity and ability to look past existing workflows and identify opportunities for optimization
Thrives in fast-paced work environment with multiple stakeholders
Ability to decompose a technical problem into its sub-components and build a plan to rigorously tackle the process that is defensible and repeatable
Strong strategic communication skills to articulate pipeline flows and actively listening to identify business problems and their causes
High-performing team player

Desired Skills:

DOD, DOS, or IC Top Secret/SCI Security Clearance )
Familiarity with analytics techniques, such as statistics, simulation modeling, optimization, machine learning, or natural language processing
Confidence to drive assignments to completion
Eagerness to learn and develop
Familiarity with containerization and orchestration technologies such as Docker and Kubernetes
Familiarity with Alteryx

Team Values: Culture eats strategy for breakfast

The CfA Team has codified their ten core values, and strives to find like-minded team members who share them.
Compassion: Have the compassion to care about your team, your clients, and their problems. Act respectfully.
Curiosity: Be curious. Learn from people and challenges around you; follow the facts; explore the details.

Candor: Be candid in your interactions, but deliver your perspective with the purpose of team improvement.
Communication: Communicate clearly, concisely, and with your authentic voice.
Craftiness: Some problems require crafty solutions; donât let roadblocks on the most obvious path stop you.
Commitment: Show commitment to the task at hand and to your teamâs success, regardless of the size of your role.
Collaborative: Be willing to collaborate in cross-functional teams, valuing each team memberâs mix of unique skills as an asset to the productâs success.
Create: Creation over ideation. Ideas are important but testing and honing ideas through the creative process is how you go from zero to one.
Craftsmanship: Make well-designed products. As Steve Jobs said, âDesign is not just what it looks like and feels like. Itâs how it works.â
Celebrate: Find the time to celebrate one another, in personal and professional endeavors alike.

The team
Analytics & Cognitive
In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements

Qualifications

TOP SECRET Cleared professional required to strategize and execute on logical and physical aspects of growing Enterprise Data Warehouse.
Expertise in dimensional modeling, snowflake and star schemas, snapshots, facts, etc.
Candidate will assist multiple Scrum teams develop logical and physical models as well as data load strategies. Other important areas of expertise include:
1) entity-relationship modeling techniques
2) translate a logical dimensional model into a star-schema design
3) Client interaction skills
4) Use of tools such as ErWIN
5) Data Security considerations
6) Work well on teams

How youâll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe thereâs always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitteâs culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.

Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitteâs impact on the world.

Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youâre applying to. Check out recruiting tips from Deloitte professionals.
#IND:PTY","TOP SECRET Cleared professional required to strategize and execute on logical and physical aspects of growing Enterprise Data Warehouse. Expertise in dimensional modeling, snowflake and star schemas, snapshots, facts, etc. Candidate will assist multiple Scrum teams develop logical and physical models as well as data load strategies. Other important areas of expertise include  1  entity-relationship modeling techniques 2  translate a logical dimensional model into a star-schema design 3  Client interaction skills 4  Use of tools such as ErWIN 5  Data Security considerations 6  Work well on teams  Fluency in SQL and NoSQL database technologies such as SQL Server, Oracle SQL Fluency in programming languages such as Python and R; and query languages such as SQL Proficiency in data extraction, transformation, and loading to support advanced analytics Familiarity with JSON and XML data formats Experience in web scraping  e.g., using R or Python  and retrieving data from APIs Experience with data visualization tools, such as Tableau, Qlik, PowerBI, d3.js, or equivalent Ability to identify solutions for communicating across different technologies Creativity and ability to look past existing workflows and identify opportunities for optimization Thrives in fast-paced work environment with multiple stakeholders Ability to decompose a technical problem into its sub-components and build a plan to rigorously tackle the process that is defensible and repeatable Strong strategic communication skills to articulate pipeline flows and actively listening to identify business problems and their causes High-performing team player    ","TOP SECRET Cleared professional required to strategize and execute on logical physical aspects of growing Enterprise Data Warehouse. Expertise in dimensional modeling, snowflake star schemas, snapshots, facts, etc. Candidate will assist multiple Scrum teams develop models as well data load strategies. Other important areas expertise include 1 entity-relationship modeling techniques 2 translate a model into star-schema design 3 Client interaction skills 4 Use tools such ErWIN 5 Security considerations 6 Work Fluency SQL NoSQL database technologies Server, Oracle programming languages Python R; query Proficiency extraction, transformation, loading support advanced analytics Familiarity with JSON XML formats Experience web scraping e.g., using R or retrieving from APIs visualization tools, Tableau, Qlik, PowerBI, d3.js, equivalent Ability identify solutions for communicating across different Creativity ability look past existing workflows opportunities optimization Thrives fast-paced work environment stakeholders decompose technical problem its sub-components build plan rigorously tackle the process that is defensible repeatable Strong strategic communication articulate pipeline flows actively listening business problems their causes High-performing team player","TOP SECRET Cleared professional required strategize execute logical physical aspects growing Enterprise Data Warehouse. Expertise dimensional modeling, snowflake star schemas, snapshots, facts, etc. Candidate assist multiple Scrum teams develop models well data load strategies. Other important areas expertise include 1 entity-relationship modeling techniques 2 translate model star-schema design 3 Client interaction skills 4 Use tools ErWIN 5 Security considerations 6 Work Fluency SQL NoSQL database technologies Server, Oracle programming languages Python R; query Proficiency extraction, transformation, loading support advanced analytics Familiarity JSON XML formats Experience web scraping e.g., using R retrieving APIs visualization tools, Tableau, Qlik, PowerBI, d3.js, equivalent Ability identify solutions communicating across different Creativity ability look past existing workflows opportunities optimization Thrives fast-paced work environment stakeholders decompose technical problem sub-components build plan rigorously tackle process defensible repeatable Strong strategic communication articulate pipeline flows actively listening business problems causes High-performing team player"
42,Data Engineer,Hadoop Data Engineer,"Chantilly, VA",Chantilly,VA,"Overview
BRMi Technology is seeking a Hadoop Data Engineer to support a large client in the Northern Virginia area. The selected candidate will support the Information System Division and the enterprise by providing comprehensive data engineering solutions in translating our clients business vision and strategies into effective IT and business capabilities through the design, implementation, and integration of IT systems in the big data domain.
Responsibilities
The Data Engineer will be responsible for guiding the evolution, development, and governance of our clients data with a specific focus on Hadoop Data solutions.
Qualifications
Required:
Extensive experience in engineering and designing data management solutions using Hadoop ecosystem tools and technologies with particular emphasis in using Sqoop, Spark, Scala, Python, Java, Shell HIVE and IMPALA for minimum of three (3) years
Proficient in the data ingestion pipeline process, exception handling and metadata management on big data platforms
Extensive experience in architecting, and designing data architecture solutions using Hadoop ecosystem tools and technologies like, Hive, Impala, HBase, and Solr
Extensive experience in GitHub CI/CD
Experience providing technical and data leadership to the application development terms, IT and the enterprise.
Experience in designing shared data assets such as data discovery and analytics platforms, metadata, operational data stores, data warehouses and data marts
Experience in business intelligence disciplines, and a deep understanding of Data Warehousing BI, and advanced analytics concepts in large organizations

Desired:
Advanced degree in MIS, computer science, statistics, marketing, management, finance or related field
Experience with Data Governance and Data Pipeline Management
Prior experience in the Financial Industries and large banks

** BRMi will not sponsor applicants for work visas for this position.**
**This is a W2 opportunity only**

EOE/Minorities/Females/Vet/Disabled
We are an equal opportunity employer that values diversity and commitment at all levels. All individuals, regardless of personal characteristics, are encouraged to apply. Employment policies and decisions on employment and promotion are based on merit, qualifications, performance, and business needs. The decisions and criteria governing the employment relationship with all employees are made in a nondiscriminatory manner, without regard to race, religion, color, national origin, sex, age, marital status, physical or mental disability, medical condition, veteran status, or any other factor determined to be unlawful by federal, state, or local statutes."," Extensive experience in engineering and designing data management solutions using Hadoop ecosystem tools and technologies with particular emphasis in using Sqoop, Spark, Scala, Python, Java, Shell HIVE and IMPALA for minimum of three  3  years Proficient in the data ingestion pipeline process, exception handling and metadata management on big data platforms Extensive experience in architecting, and designing data architecture solutions using Hadoop ecosystem tools and technologies like, Hive, Impala, HBase, and Solr Extensive experience in GitHub CI/CD Experience providing technical and data leadership to the application development terms, IT and the enterprise. Experience in designing shared data assets such as data discovery and analytics platforms, metadata, operational data stores, data warehouses and data marts Experience in business intelligence disciplines, and a deep understanding of Data Warehousing BI, and advanced analytics concepts in large organizations   Extensive experience in engineering and designing data management solutions using Hadoop ecosystem tools and technologies with particular emphasis in using Sqoop, Spark, Scala, Python, Java, Shell HIVE and IMPALA for minimum of three  3  years Proficient in the data ingestion pipeline process, exception handling and metadata management on big data platforms Extensive experience in architecting, and designing data architecture solutions using Hadoop ecosystem tools and technologies like, Hive, Impala, HBase, and Solr Extensive experience in GitHub CI/CD Experience providing technical and data leadership to the application development terms, IT and the enterprise. Experience in designing shared data assets such as data discovery and analytics platforms, metadata, operational data stores, data warehouses and data marts Experience in business intelligence disciplines, and a deep understanding of Data Warehousing BI, and advanced analytics concepts in large organizations  ","Extensive experience in engineering and designing data management solutions using Hadoop ecosystem tools technologies with particular emphasis Sqoop, Spark, Scala, Python, Java, Shell HIVE IMPALA for minimum of three 3 years Proficient the ingestion pipeline process, exception handling metadata on big platforms architecting, architecture like, Hive, Impala, HBase, Solr GitHub CI/CD Experience providing technical leadership to application development terms, IT enterprise. shared assets such as discovery analytics platforms, metadata, operational stores, warehouses marts business intelligence disciplines, a deep understanding Data Warehousing BI, advanced concepts large organizations","Extensive experience engineering designing data management solutions using Hadoop ecosystem tools technologies particular emphasis Sqoop, Spark, Scala, Python, Java, Shell HIVE IMPALA minimum three 3 years Proficient ingestion pipeline process, exception handling metadata big platforms architecting, architecture like, Hive, Impala, HBase, Solr GitHub CI/CD Experience providing technical leadership application development terms, IT enterprise. shared assets discovery analytics platforms, metadata, operational stores, warehouses marts business intelligence disciplines, deep understanding Data Warehousing BI, advanced concepts large organizations"
43,Data Engineer,Sr. Data Engineer,"Bethesda, MD 20817",Bethesda,MD,"Sr. Data Engineer

Bethesda, MD

Total Wine & More is seeking a Senior Data Engineer with expertise in cloud solutions using core data warehousing tools and big data related technologies to join our information technology team. We are embarking on a significant initiative to transform all aspects of data management services within Total Wine and More including enterprise data architecture, business intelligence and data warehousing leveraging the cloud to apply advanced analytics and data science capabilities longer term. The Senior Data Engineer will play a significant role in the implementation, maintenance and continuous improvement of these systems and processes to achieve business needs. This individual will work closely with the business, software development and support teams, infrastructure, and security staff. The Total Wine & More Information Technology team is growing and we are driving business value through technology across all aspects of our business.

KEY RESPONSIBILITIES
Be versed in cloud solutions (AWS, Azure, or GCP), architecture, related technologies and their interdependenciesResearch, analyze, recommend and select technical approaches for solving difficult and challenging development and integration problemsExperience with setting up and operating data pipelines and data wrangling procedures using Python, SQL, and cloud managed servicesCollaborate with engineers and business customers to understand data needs, capture requirements and deliver complete data solutionsDesign and build data extraction, transformation, and loading processes by writing custom data pipelinesDesign, implement and support a platform that can provide ad-hoc access to large datasets and unstructured dataModel data and metadata to support adhoc and pre-built reportingTune application and query performance using performance profiling tools and SQLBuild data expertise and own data quality for allocated areas of ownership

JOB REQUIREMENTS

Minimum Experience, Skills and Education:
5 to 7+ years of experience in using SQL and data warehousing in a business environment5+ years of experience in custom ETL design, implementation, and maintenance5+ years of experience with data warehouse schema design and data modelingProduction level experience with Python, Java, SQL, and shell scriptingExperience with cloud databases and managed servicesExperience with batch and stream processingExperience with microservice patterns, API developmentExperience with building large scale data processing systemSolid understanding of data design patterns and best practicesWorking knowledge of data visualization tools such as Tableau, PowerBI is a plusExperience to analyze data to identify deliverables, gaps, and inconsistenciesFamiliarity with agile software development practices and drive to ship quicklyExperience leading change, taking initiative, and driving resultsEffective communication skills and strong problem-solving skillsProven ability and desire to mentor others in a team environmentRetail experience highly desired

Preferred Experience, Skills and Education:
Bachelor's degree from four-year College or university in Computer Science, Technology or related field

PHYSICAL REQUIREMENTS (with or without accommodations):
n/a
We offer
Paid Time Off (PTO)
Generous store discounts
Health care plans (medical, prescription, dental, vision)
401(k), HSA, FSA, Pre-tax commuter benefits
Disability & life insurance coverage
Paid parental leave
College tuition assistance
Career development & product training
Consumer classes
& More!
Grow with us
Total Wine & More is the countryâs largest independent retailer of fine wine, beer and spirits, and we continue to grow our footprint year over year. Total Wine offers exciting and unique career opportunities across the country and in our corporate office. Our strength is our people. We have a commitment to training and career growth, all in an environment that values new ideas and teamwork. If you share our entrepreneurial spirit and a passion for providing best-in-class customer experience, take a moment to apply or learn more at www.TotalWine.com/About-Us/Careers!
Total Wine & More considers several factors when establishing compensation. Estimated salaries determined by third parties have not been validated by Total Wine & More.

Total Wine & More is an equal opportunity employer and all qualified applicants will receive consideration for employment without discrimination based on race, color, religion, national origin, sex, sexual orientation, age, marital status, veteran status, disability, or any other characteristic protected by applicable law. Total Wine & More makes reasonable accommodations during all aspects of the employment process, including during the interview process. Total Wine & More is a Drug Free Workplace.

The information provided above indicates the general nature and level of work required of the position and is not a comprehensive list of all responsibilities or qualifications. Benefits list is only a highlight of some of the benefits offered to team members; eligibility for certain benefits apply."," 5 to 7+ years of experience in using SQL and data warehousing in a business environment5+ years of experience in custom ETL design, implementation, and maintenance5+ years of experience with data warehouse schema design and data modelingProduction level experience with Python, Java, SQL, and shell scriptingExperience with cloud databases and managed servicesExperience with batch and stream processingExperience with microservice patterns, API developmentExperience with building large scale data processing systemSolid understanding of data design patterns and best practicesWorking knowledge of data visualization tools such as Tableau, PowerBI is a plusExperience to analyze data to identify deliverables, gaps, and inconsistenciesFamiliarity with agile software development practices and drive to ship quicklyExperience leading change, taking initiative, and driving resultsEffective communication skills and strong problem-solving skillsProven ability and desire to mentor others in a team environmentRetail experience highly desired  5 to 7+ years of experience in using SQL and data warehousing in a business environment5+ years of experience in custom ETL design, implementation, and maintenance5+ years of experience with data warehouse schema design and data modelingProduction level experience with Python, Java, SQL, and shell scriptingExperience with cloud databases and managed servicesExperience with batch and stream processingExperience with microservice patterns, API developmentExperience with building large scale data processing systemSolid understanding of data design patterns and best practicesWorking knowledge of data visualization tools such as Tableau, PowerBI is a plusExperience to analyze data to identify deliverables, gaps, and inconsistenciesFamiliarity with agile software development practices and drive to ship quicklyExperience leading change, taking initiative, and driving resultsEffective communication skills and strong problem-solving skillsProven ability and desire to mentor others in a team environmentRetail experience highly desired ","5 to 7+ years of experience in using SQL and data warehousing a business environment5+ custom ETL design, implementation, maintenance5+ with warehouse schema design modelingProduction level Python, Java, SQL, shell scriptingExperience cloud databases managed servicesExperience batch stream processingExperience microservice patterns, API developmentExperience building large scale processing systemSolid understanding patterns best practicesWorking knowledge visualization tools such as Tableau, PowerBI is plusExperience analyze identify deliverables, gaps, inconsistenciesFamiliarity agile software development practices drive ship quicklyExperience leading change, taking initiative, driving resultsEffective communication skills strong problem-solving skillsProven ability desire mentor others team environmentRetail highly desired","5 7+ years experience using SQL data warehousing business environment5+ custom ETL design, implementation, maintenance5+ warehouse schema design modelingProduction level Python, Java, SQL, shell scriptingExperience cloud databases managed servicesExperience batch stream processingExperience microservice patterns, API developmentExperience building large scale processing systemSolid understanding patterns best practicesWorking knowledge visualization tools Tableau, PowerBI plusExperience analyze identify deliverables, gaps, inconsistenciesFamiliarity agile software development practices drive ship quicklyExperience leading change, taking initiative, driving resultsEffective communication skills strong problem-solving skillsProven ability desire mentor others team environmentRetail highly desired"
44,Data Engineer,Data Engineer,"Washington, DC",Washington,DC,"Design, develop and build out data pipelines to ingest data into our proprietary data structures, and be a key collaborator in the data discovery and exploratory analysis process during our client engagements.
Responsibilities
Build pipelines to ingest and maintain complex data sets into Cerebri AIâs proprietary data stores for use in machine learning modeling
Develop and maintain data ontologies for key market segments
Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data
Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data
Write quality documentation on the discovery process and software projects
Work equally well in a team environment and on your own.
Communicate complex ideas clearly with both team members and clients
Travel up to 25%
Qualifications
At least one (1) year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models
At least two (2) years of experience in Scala, Python, Apache Spark and SQL
Experience working directly with relational database structures and flat files
Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices
Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.
Good verbal and written communication skills, with both technical and non-technical stakeholders
Nice to Haves
Experience in Java and/or Scala
Experience with data management processing tools such as Kafka, Elasticsearch and Logstash
Experience with NoSQL distributed databases such as Cassandra.
Experience in business intelligence visualization tools such as Grafana, Superset, Redash or Tableau.
Experience with Microsoft Azure or similar cloud computing solutions
Masterâs degree or higher in a relevant quantitative subject"," At least one  1  year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models At least two  2  years of experience in Scala, Python, Apache Spark and SQL Experience working directly with relational database structures and flat files Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations. Good verbal and written communication skills, with both technical and non-technical stakeholders   Build pipelines to ingest and maintain complex data sets into Cerebri AIâs proprietary data stores for use in machine learning modeling Develop and maintain data ontologies for key market segments Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data Write quality documentation on the discovery process and software projects Work equally well in a team environment and on your own. Communicate complex ideas clearly with both team members and clients Travel up to 25%  ","At least one 1 year of experience designing and building data processing solutions ETL pipelines for varied formats, ideally at a company that leverages machine learning models two 2 years in Scala, Python, Apache Spark SQL Experience working directly with relational database structures flat files Ability to write efficient queries, functions views include complex joins the identification development custom indices Knowledge professional software engineering practices best full life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration development, operations. Good verbal written communication skills, both technical non-technical stakeholders Build ingest maintain sets into Cerebri AIâs proprietary stores use modeling Develop ontologies key market segments Collaborate scientists perform exploratory analysis map fields find signals client clients develop pipeline infrastructure, ask appropriate questions gain deep understanding Write quality documentation on discovery process projects Work equally well team environment your own. Communicate ideas clearly members Travel up 25%","At least one 1 year experience designing building data processing solutions ETL pipelines varied formats, ideally company leverages machine learning models two 2 years Scala, Python, Apache Spark SQL Experience working directly relational database structures flat files Ability write efficient queries, functions views include complex joins identification development custom indices Knowledge professional software engineering practices best full life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration development, operations. Good verbal written communication skills, technical non-technical stakeholders Build ingest maintain sets Cerebri AIâs proprietary stores use modeling Develop ontologies key market segments Collaborate scientists perform exploratory analysis map fields find signals client clients develop pipeline infrastructure, ask appropriate questions gain deep understanding Write quality documentation discovery process projects Work equally well team environment own. Communicate ideas clearly members Travel 25%"
45,Data Engineer,Sr. Oracle Data Engineer,"Chantilly, VA",Chantilly,VA,"Sr. Oracle Data Engineer
Job location: Chantilly, VA
Overview:
NikSoft Systems Corporation is a recognized Information Technology solutions provider. Founded in 1998 and based in Reston, Virginia, NikSoft is a CMMI Level 3 Certified company with an established reputation for excellence and on-time delivery with a consistently high customer satisfaction rating from its Federal Government and private consulting contracts.
NikSoft is currently conducting a search for a Sr. Oracle Data Engineer to support our government customer located in Chantilly, VA.
Responsibilities:
Must have significant development experience with PL/SQL.
Design and develop a data mart within an Oracle environment.
Experience in mortgage banking or home loans a plus.
Design and develop a data mart within an Oracle environment

Technical Skills required:
5+ years of advanced knowledge of PL/SQL development
5+ years of Data Modeling
5+ years of Data Warehousing
5+ years of Oracle Database storage and schema design
1+ years of Salesforce Experience
Preferred Skills:
Mortgage banking industry knowledge
DevOps concepts
Agile Development Methods
Data Quality Management and Testing
Requirements Analysis
Education level:
Bachelor's degree is required
***Candidates must be able to obtain Public Trust Clearance US Citizenship or Green Card Holder or H1. Additionally, candidates must not have traveled outside of the USA for a combined period not to exceed 6 months within the last 5 years. ***
NikSoft's competitive benefits program includes comprehensive medical and dental care, matching 401K, paid time off, flexible spending accounts, disability coverage, and other benefits that help provide financial protection for you and your family.
BENEFITS:
NikSoft's competitive benefits program includes comprehensive medical and dental care, matching 401K, paid time off, flexible spending accounts, disability coverage, and other benefits that help provide financial protection for you and your family.
NikSoft Systems Corp is fully committed to the concept and practice of equal opportunity and affirmative action in all aspects of employment. NikSoft is an EOE M/F/Disability/Veteran employer. For more information about our other openings, please visit www.niksoft.com",  5+ years of advanced knowledge of PL/SQL development 5+ years of Data Modeling 5+ years of Data Warehousing 5+ years of Oracle Database storage and schema design 1+ years of Salesforce Experience  Must have significant development experience with PL/SQL. Design and develop a data mart within an Oracle environment. Experience in mortgage banking or home loans a plus. Design and develop a data mart within an Oracle environment  Bachelor's degree is required ,5+ years of advanced knowledge PL/SQL development Data Modeling Warehousing Oracle Database storage and schema design 1+ Salesforce Experience Must have significant experience with PL/SQL. Design develop a data mart within an environment. in mortgage banking or home loans plus. environment Bachelor's degree is required,5+ years advanced knowledge PL/SQL development Data Modeling Warehousing Oracle Database storage schema design 1+ Salesforce Experience Must significant experience PL/SQL. Design develop data mart within environment. mortgage banking home loans plus. environment Bachelor's degree required
46,Data Engineer,Software Engineer/Data Engineer,"College Park, MD 20740",College Park,MD,"Ideally, the successful candidate will be located near our NYC or College Park, MD office. However, there is the opportunity to work remotely based on role and level.
Software Engineer/Data Engineer
BlueVoyant is seeking a Software Engineer/Data Engineer to help us build a data analytics platform powerful enough to protect some of the world's biggest networks, and nimble enough to adapt to a quickly evolving product vision. We are solving interesting, exciting, and important problems with smart people.
Qualifications for the Software Engineer/Data Engineer:
Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.
Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.
Experience writing and optimizing streaming and batch analytics.
Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.
BS/BA in Computer Science, Engineering, or relevant field experience.
What you will do as a Software Engineer/Data Engineer:
Work closely with analysts to transform threat analytics into production-level code.
Actively contribute to application architecture and product vision.
Participate in requirements gathering and transformation from prototype to product design.
Participate in daily development stand-up meetings and regular sprint planning and product demo meetings.
Help us stay current on the latest data processing tools and trends.
Ideal candidates will:
Thrive in our small, fast-paced, product-driven environment
Collaborate with teams from across the organization
Deliver features and fixes on tight schedules and under pressure
Present ideas in business-friendly and user-friendly language
Create systems that are maintainable, flexible and scalable
Define and follow a disciplined development and engineering workflow
Demonstrate ownership of tasks with escalation as needed
Be a subject matter expert in one or more of the technologies employed
Relentlessly push for successful customer outcomes
Possess a strong interest or background in cyber security
General responsibilities include:
Participate in all stages of an agile software development lifecycle, including product ideation, requirements gathering, architecture, design, implementation, testing, documentation, and support
Refine our software development methodology based on agile/lean practices with continuous feedback and well-defined metrics to drive improvement
Maintain up-to-date knowledge of technology standards, industry trends, emerging technologies, and software development best practices
Ensure technical issues are quickly resolved and help implement strategies and solutions to reduce the likelihood of reoccurrence
Identify competitive offerings and opportunities for innovation including assessments of risk/reward to the company.
About BlueVoyant
BlueVoyant is a global cybersecurity firm that provides Advanced Threat Intelligence, for large companies and a comprehensive Managed Security Service and Professional Services for small businesses, powered by one of the largest commercially available cyber threat databases in the world.
By working with BlueVoyant, companies can gain unique and far-reaching visibility into malicious activity on their networks, in the dark web and across the internet, as well as real-time, automatable remediation services. Through our unique real-time external threat monitoring, predictive human and machine-sourced intelligence, and proactive managed security and incident response, BlueVoyant offers the private sector exceptional cyber defense capabilities.
Co-founded by CEO Jim Rosenthal, former Chief Operating Officer at Morgan Stanley, and Executive Chairman Tom Glocer, former Chief Executive Officer at Thomson Reuters, BlueVoyant has attracted a management team that comes from the world's preeminent intelligence, law enforcement, and private sector organizations. Other leaders include:
Jim Penrose, COO, former EVP at Darktrace with 17 years at the NSA in key leadership roles.
Gad Goldstein, Head of BlueVoyant Europe and Chairman of Israel, former division head (Major General equivalent) in the Israel Security Agency, Shin Bet.
Robert Hannigan, Chairman of BlueVoyant International, former Director of GCHQ.
Austin Berglas, Global Head of Professional Services, former head of the FBI's New York Cyber Branch.
David Etue, Global Head of MSS, former VP of Managed Services at Rapid7.
Milan Patel, Chief Client Officer, former CTO of the FBI Cyber Division.
Ron Feler, Global Head of Threat Intelligence and Operations, former Deputy Commander of Unit 8200, the cybersecurity division of the Israel Defense Forces.
Eldad Chai, CPO, former SVP of Products at Imperva.
Jim Bieda, Senior Advisor, former NSA Deputy CTO.
Bill Crumm, Senior Advisor, former NSA SIGINT Director and former Cybersecurity Head, Morgan Stanley.
Dan Ennis, Senior Advisor, former Head of Threat Intelligence at the NSA
All employees must be authorized to work in the United States or Israel. BlueVoyant provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, BlueVoyant complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
Come work with us!
BlueVoyant is hiring software developers, infrastructure engineers, data science experts, and technologists of all types to build next generation predictive threat intelligence and advanced security monitoring solutions.
Projects currently in development include:
An Internet-scale (multi-PB, > 500k TPS) repository made up of unstructured, structured, and semi-structured data sources used for real time alerting, threat analysis, and research and development internally.
A powerful, enterprise-scale suite of products used to provide managed security services and Security Operations Center (SOC) functionality to small and medium sized enterprises around the globe.
A unique internal platform to support the data research needs of our analysts and SOC so they can quickly and effectively identify new threat actors and techniques.
bfMhwiZhmx"," Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations. Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured  Postgres, MySQL  and unstructured data sources  Elastic, Kafka, and the Hadoop ecosystem  as implemented at Internet-scale. Experience writing and optimizing streaming and batch analytics. Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment. BS/BA in Computer Science, Engineering, or relevant field experience.    ","Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala delivery background middleware, backend implementations. Familiarity large-scale, big data, streaming data technologies, as well exposure to variety structured Postgres, MySQL unstructured sources Elastic, Kafka, the Hadoop ecosystem implemented at Internet-scale. Experience writing optimizing batch analytics. Agile frameworks, secure software design, test-driven development, modern, container-delivered code deployment cloud-based DevOps environment. BS/BA Computer Science, Engineering, or relevant field experience.","Strong hands-on programming skills, expertise multiple implementation languages/frameworks including subset Python, Java, Scala delivery background middleware, backend implementations. Familiarity large-scale, big data, streaming data technologies, well exposure variety structured Postgres, MySQL unstructured sources Elastic, Kafka, Hadoop ecosystem implemented Internet-scale. Experience writing optimizing batch analytics. Agile frameworks, secure software design, test-driven development, modern, container-delivered code deployment cloud-based DevOps environment. BS/BA Computer Science, Engineering, relevant field experience."
47,Data Engineer,Data Engineer (Secret Clearance),"Fort Belvoir, VA",Fort Belvoir,VA,"LMI is currently seeking a data engineer within LMIâs Advanced Analytics service line to support the design and implementation of business critical data management & engineering solutions.

*This position required an active DoD Secret Clearance*
Responsibilities
The ideal candidate will have direct, applied experience with one or more of the following areas:
Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.
Qualifications
Bachelorâs degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with big data tools: Hadoop, Spark, Kafka
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with data governance tools: Collibra, Immuta
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
DoD Secret Clearance Required
#LI-SH1","Bachelorâs degree in a quantitative field  e.g., engineering, statistics, mathematics, information technology, etc.  is preferred. Master's degree is desired. Must have at least 3 years of experience, preferably with a federal government customer. Experience with big data tools  Hadoop, Spark, Kafka Experience with relational SQL and NoSQL databases  Postgres, Cassandra, MongoDB Experience with data governance tools  Collibra, Immuta Experience with AWS cloud services  EC2, EMR, RDS, Redshift Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala Must possess strong written and verbal communication skills. DoD Secret Clearance Required  Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.  ","Bachelorâs degree in a quantitative field e.g., engineering, statistics, mathematics, information technology, etc. is preferred. Master's desired. Must have at least 3 years of experience, preferably with federal government customer. Experience big data tools Hadoop, Spark, Kafka relational SQL and NoSQL databases Postgres, Cassandra, MongoDB governance Collibra, Immuta AWS cloud services EC2, EMR, RDS, Redshift object-oriented/object function scripting languages Python, Java, C++, Scala possess strong written verbal communication skills. DoD Secret Clearance Required Develop structures systems to support the generation business insightsKnowledge experience overall ETL processesMaintain infrastructure develop scripts for regular processesDefine, design, flow diagrams, dictionaries, logical physical modelsDefine requirements, document elements, capture maintain metadetaIdentify clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities use improve performanceCommunicate present by developing reports using Tableau Business Intelligence toolsAdhere compliance audit requirements storage, architecture, cybersecurity,","Bachelorâs degree quantitative field e.g., engineering, statistics, mathematics, information technology, etc. preferred. Master's desired. Must least 3 years experience, preferably federal government customer. Experience big data tools Hadoop, Spark, Kafka relational SQL NoSQL databases Postgres, Cassandra, MongoDB governance Collibra, Immuta AWS cloud services EC2, EMR, RDS, Redshift object-oriented/object function scripting languages Python, Java, C++, Scala possess strong written verbal communication skills. DoD Secret Clearance Required Develop structures systems support generation business insightsKnowledge experience overall ETL processesMaintain infrastructure develop scripts regular processesDefine, design, flow diagrams, dictionaries, logical physical modelsDefine requirements, document elements, capture maintain metadetaIdentify clean incomplete, incorrect, inaccurate irrelevant dataIdentify new opportunities use improve performanceCommunicate present developing reports using Tableau Business Intelligence toolsAdhere compliance audit requirements storage, architecture, cybersecurity,"
48,Data Engineer,Data Engineer (Senior),"Reston, VA",Reston,VA,"Byte Systems. LLC - Reston, VA, United States
MUST be a US Citizen with a U.S. Government clearance - TS/SCI with Polygraph

NOTE:Must have an active TS-SCI with poly. No sponsorships or upgrades are available. Submissions without this requirement will not be considered. H1-B holders will not be considered.

Description

This role performs data management tasks. This includes:

data design
data structure identification
database schema definition
data access privilege management
data query optimization
documentation
database instance upgrades
long-range requirements
operational guidelines
data protection.

Duties also include:

entering and reviewing data within the database
ensuring user data integrity
maintaining database support tools
database tables and dictionaries
recovery and back-up procedures
making recommendations regarding enhancements and/or improvements.

This role ensures high quality data is available for use by other team members. This role designs, implements, and maintains standard data interfaces for data ingest including Extract/Transform/Load (ETL) methodology and implementation, APIs, RESTful Web Services, data quality, and data cleansing. This role collaborates across a team of Data Scientists, Data Engineers, and Data Administrators to produce work products.

This role designs and develops methods, processes, and systems to consolidate and analyze structured and unstructured, diverse sources including âbig dataâ sources. This role develops and uses advanced software programs, algorithms, query techniques, models complex business problems, and automated processes to cleanse, integrate, and evaluate datasets. This role analyzes the requirements and evaluates technologies for data science capabilities including Natural Language Processing, Machine Learning, predictive modeling, statistical analysis and hypothesis testing. Works with cross-discipline teams to ensure connectivity between various data sources and business problems. Identifies meaningful insights and interprets and communicates findings and recommendations. This role develops information tools, algorithms, dashboards, and queries to monitor and improve business performance. Maintains awareness of emerging analytics and big-data technologies.

Basic Qualifications:

Masterâs degree in engineering, computer science, or other related technical field or Bachelorâs degree in a business or management-related field accompanied by experience managing technical requirements in complex programs.
Experience displaying expert domain knowledge in a technical field.
Experience negotiating complex scenarios and challenges and devising courses of action to resolve situations with predictable outcomes.
Experience leading critical objectives where decision making is of utmost concern to the outcome.
Experience supervising others.

Candidate must have BS and 15+years of prior relevant experience or Masters and 13+ years of prior relevant experience.

Benefits:
5 weeks paid vacation + 10 gov't holidays
15% contribution to 401k
ISP and cellphone reimbursement
LTD, STD disability and life insurance
Paid health, dental, and vision for employee and family.
$5000 annual training expense reimbursement
Computer purchase plan

Posted On: Tuesday, July 23, 2019"," Masterâs degree in engineering, computer science, or other related technical field or Bachelorâs degree in a business or management-related field accompanied by experience managing technical requirements in complex programs. Experience displaying expert domain knowledge in a technical field. Experience negotiating complex scenarios and challenges and devising courses of action to resolve situations with predictable outcomes. Experience leading critical objectives where decision making is of utmost concern to the outcome. Experience supervising others.     ","Masterâs degree in engineering, computer science, or other related technical field Bachelorâs a business management-related accompanied by experience managing requirements complex programs. Experience displaying expert domain knowledge field. negotiating scenarios and challenges devising courses of action to resolve situations with predictable outcomes. leading critical objectives where decision making is utmost concern the outcome. supervising others.","Masterâs degree engineering, computer science, related technical field Bachelorâs business management-related accompanied experience managing requirements complex programs. Experience displaying expert domain knowledge field. negotiating scenarios challenges devising courses action resolve situations predictable outcomes. leading critical objectives decision making utmost concern outcome. supervising others."
49,Data Engineer,Data Engineer,"Washington, DC",Washington,DC,"Job Description
In this role you will: Requires skilled, technically knowledgeable, and experienced support to develop new business intelligence reports, analytical capabilities and data warehouses; execute development for enhancements and maintenance of IT systems and environments; and execute special projects.

Required Qualifications
Candidate shall have demonstrated at least 5 years of experience managing and maintaining structured, semi-structured, and unstructured data, as well as structuring and wrangling data as appropriate for statistical data analysis.
Candidate shall possess the necessary skills to gather, define, document, analyze and implement ETL/data
Extensive experience in understanding data warehouse concepts and relational databases.
Candidate shall possess the necessary skills to model database structures, tables, and fields.
Candidate shall possess a basic understanding of statistics as related to their experiences in structuring data for statistical analyses.
Candidate shall possess excellent oral and written communication skills with emphasis on complex technical topics and effectively communicating details with all levels of management.
Candidate shall possess the necessary people skills to identify requirements and deliver results.
All personnel shall have at a minimum an active security clearance of Top Secret, meaning the clearance initially issued or revalidated within the past five years, at the time of the proposal submission and Single Scope Background Investigation.
Desired Qualifications
Prior experience using big data management techniques and tools (e.g., Hadoop) is preferred.
Salient CRGT is a leading provider of health, data analytics, cloud, agile software development, mobility, cyber security, and infrastructure solutions. We support these core capabilities with full lifecycle IT services and trainingâto help our customers meet critical goals for pivotal missions. We are purpose-built for IT transformation supporting federal civilian, defense, homeland, and intelligence agencies, as well as Fortune 1000 companies.

If you feel you are qualified for this position, express interest by clicking the Apply button below (if you are viewing this position on the Salient CRGT website). If you are viewing this job posting outside of the Salient CRGT website, please visit: www.salientcrgt.com/careers to express interest in this position through the Salient CRGT Careers page.
Salient CRGT is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, age, sex, sexual orientation, gender identity or expression, veteran status, disability, genetic information, or any other factor prohibited by applicable anti-discrimination laws.","Candidate shall have demonstrated at least 5 years of experience managing and maintaining structured, semi-structured, and unstructured data, as well as structuring and wrangling data as appropriate for statistical data analysis. Candidate shall possess the necessary skills to gather, define, document, analyze and implement ETL/data Extensive experience in understanding data warehouse concepts and relational databases. Candidate shall possess the necessary skills to model database structures, tables, and fields. Candidate shall possess a basic understanding of statistics as related to their experiences in structuring data for statistical analyses. Candidate shall possess excellent oral and written communication skills with emphasis on complex technical topics and effectively communicating details with all levels of management. Candidate shall possess the necessary people skills to identify requirements and deliver results. All personnel shall have at a minimum an active security clearance of Top Secret, meaning the clearance initially issued or revalidated within the past five years, at the time of the proposal submission and Single Scope Background Investigation.    ","Candidate shall have demonstrated at least 5 years of experience managing and maintaining structured, semi-structured, unstructured data, as well structuring wrangling data appropriate for statistical analysis. possess the necessary skills to gather, define, document, analyze implement ETL/data Extensive in understanding warehouse concepts relational databases. model database structures, tables, fields. a basic statistics related their experiences analyses. excellent oral written communication with emphasis on complex technical topics effectively communicating details all levels management. people identify requirements deliver results. All personnel minimum an active security clearance Top Secret, meaning initially issued or revalidated within past five years, time proposal submission Single Scope Background Investigation.","Candidate shall demonstrated least 5 years experience managing maintaining structured, semi-structured, unstructured data, well structuring wrangling data appropriate statistical analysis. possess necessary skills gather, define, document, analyze implement ETL/data Extensive understanding warehouse concepts relational databases. model database structures, tables, fields. basic statistics related experiences analyses. excellent oral written communication emphasis complex technical topics effectively communicating details levels management. people identify requirements deliver results. All personnel minimum active security clearance Top Secret, meaning initially issued revalidated within past five years, time proposal submission Single Scope Background Investigation."
50,Data Engineer,Data Engineer,"Reston, VA",Reston,VA,"Overview
RSEKURE transforms the way our clients do business! Todayâs Government operations demand the rapid collection, fusion and transmission of information at unprecedented speed, capacity, availability, precision and security. This constant and uncompromising posture must be maintained across all platforms and in the most challenging environments around the world. This level of operation is the one we at RSekure look forward to supporting day in day out as our mission priority. Mission Success. Done.
Responsibilities
RSekure is looking for a Data Engineer who has experience working in an Agile program. Candidate should have expertise managing data workflows, pipelines, and ETL processes. In addition, he or she should have build experience of AWS cloud environments and supplying rapid delivery solutions using tools like AWS Cloud formation, Terraform, CLI and scripting, as well as, GitHub or Jira tools to manage daily workflow. This position will be supporting our customer in the Tyson's Corner area.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS âbig dataâ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Qualifications
Required Skills:
Understanding the basics of distributed systems
Knowledge of algorithms and data structures
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases (PostgreSQL, MySQL, etc.).
4+ Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc
Demonstrable experience with Kafka or Hadoop
Experience with data visualization tools like Tableau and/or ElasticSearch will be a big plus
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Security Clearance: Must have Active Top Ssecret SCI with Polygraph"," Understanding the basics of distributed systems Knowledge of algorithms and data structures Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases  PostgreSQL, MySQL, etc. . 4+ Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, etc Demonstrable experience with Kafka or Hadoop Experience with data visualization tools like Tableau and/or ElasticSearch will be a big plus Experience with AWS cloud services  EC2, EMR, RDS, Redshift  Understanding the basics of distributed systems Knowledge of algorithms and data structures Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases  PostgreSQL, MySQL, etc. . 4+ Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, etc Demonstrable experience with Kafka or Hadoop Experience with data visualization tools like Tableau and/or ElasticSearch will be a big plus Experience with AWS cloud services  EC2, EMR, RDS, Redshift  Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS âbig dataâ technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Build processes supporting data transformation, data structures, metadata, dependency and workload management.  ","Understanding the basics of distributed systems Knowledge algorithms and data structures Advanced working SQL knowledge experience with relational databases, query authoring as well familiarity a variety databases PostgreSQL, MySQL, etc. . 4+ Experience object-oriented/object function scripting languages Python, Java, C++, Scala, etc Demonstrable Kafka or Hadoop visualization tools like Tableau and/or ElasticSearch will be big plus AWS cloud services EC2, EMR, RDS, Redshift Assemble large, complex sets that meet functional / non-functional business requirements. Identify, design, implement internal process improvements automating manual processes, optimizing delivery, re-designing infrastructure for greater scalability, Build required optimal extraction, transformation, loading from wide sources using âbig dataâ technologies. analytics utilize pipeline to provide actionable insights into customer acquisition, operational efficiency other key performance metrics. Work stakeholders including Executive, Product, Data Design teams assist data-related technical issues support their needs. Keep our separated secure across national boundaries through multiple centers regions. Create scientist team members them in building product an innovative industry leader. experts strive functionality systems. Performing root cause analysis on external processes answer specific questions identify opportunities improvement. supporting structures, metadata, dependency workload management.","Understanding basics distributed systems Knowledge algorithms data structures Advanced working SQL knowledge experience relational databases, query authoring well familiarity variety databases PostgreSQL, MySQL, etc. . 4+ Experience object-oriented/object function scripting languages Python, Java, C++, Scala, etc Demonstrable Kafka Hadoop visualization tools like Tableau and/or ElasticSearch big plus AWS cloud services EC2, EMR, RDS, Redshift Assemble large, complex sets meet functional / non-functional business requirements. Identify, design, implement internal process improvements automating manual processes, optimizing delivery, re-designing infrastructure greater scalability, Build required optimal extraction, transformation, loading wide sources using âbig dataâ technologies. analytics utilize pipeline provide actionable insights customer acquisition, operational efficiency key performance metrics. Work stakeholders including Executive, Product, Data Design teams assist data-related technical issues support needs. Keep separated secure across national boundaries multiple centers regions. Create scientist team members building product innovative industry leader. experts strive functionality systems. Performing root cause analysis external processes answer specific questions identify opportunities improvement. supporting structures, metadata, dependency workload management."
51,Data Engineer,Data Engineer I,"Arlington, VA",Arlington,VA,"At Interos, we are disrupting the way Fortune 500 companies and government agencies identify and respond to risk within their supply chains â whether this risk is being caused by cyber breaches, geopolitical issues, malicious intent, quality concerns, natural disasters or unethical sourcing and sustainability concerns. We deliver the data and insights to business leaders that help them identify, visualize and understand the ripple effects that could impact their supply chains, before they happen.

Recently funded by Kleiner Perkins and pivoting to an automated solution, Interos is in essence, a start-up SaaS environment. We need someone who thrives as part of fast-paced team and takes pride in their attention to detail, their ability to take on many different things.

The Opportunity:
Join a team of highly skilled engineers working to apply the latest developments in AI and Machine Learning to solving real world problems for our customers. Work in a collaborative, Agile environment, where every voice matters, outside the box thinking is encouraged, and the best ideas always win.

We are looking for a strong engineer with experience building applications from the ground up. We need someone who can work independently but can communicate clearly and knows when to ask questions and when to challenge assumptions.

Key Responsibilities:

Assume responsibility for key parts of the Interos Knowledge Graph product
Translate Data Science experimental code into tested, production ready modules
Create connectors for ingesting data from third party databases, APIs, and web sites
Optimize data structures, improve code execution, and reduce memory consumption of existing implementation
Develop JSON APIs

Qualifications:

Bachelor's Degree in Computer Science or equivalent work experience
2-4 years of experience in Software Development
Strong Python skills with experience in Java/C++ or other object oriented language.
Experience with two or more of the following:
Code optimization
Data structures
Elasticsearch
SQL
Linux / Ubuntu
JSON / YAML
AWS SQS, S3, EC2
Compilers/Parsers
Docker

BENEFITS:

Comprehensive Health & Wellness package (Medical, Dental and Vision)
10 Paid Holiday Days Off
Accrued Paid Time Off (PTO)
401 (k) Employer Matching
Stock Options
Career advancement opportunities
Casual Dress
Hackathons
On-site gym and dedicated Peloton room at headquarters
Company Events (Sports Games, Fitness Competitions, Birthday Celebrations, Contests, Happy Hours)
Annual company party
Employee Referral Program

Interos is proud to be an Equal Opportunity Employer and will consider all qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity, genetic information, national origin, disability, protected veteran status or any other classification protected by law.

If you are a candidate in need of assistance or an accommodation in the application process, please contact HR@interos.net ( HR@interos.net )"," Bachelor's Degree in Computer Science or equivalent work experience 2-4 years of experience in Software Development Strong Python skills with experience in Java/C++ or other object oriented language. Experience with two or more of the following  Code optimization Data structures Elasticsearch SQL Linux / Ubuntu JSON / YAML AWS SQS, S3, EC2 Compilers/Parsers Docker    Assume responsibility for key parts of the Interos Knowledge Graph product Translate Data Science experimental code into tested, production ready modules Create connectors for ingesting data from third party databases, APIs, and web sites Optimize data structures, improve code execution, and reduce memory consumption of existing implementation Develop JSON APIs   ","Bachelor's Degree in Computer Science or equivalent work experience 2-4 years of Software Development Strong Python skills with Java/C++ other object oriented language. Experience two more the following Code optimization Data structures Elasticsearch SQL Linux / Ubuntu JSON YAML AWS SQS, S3, EC2 Compilers/Parsers Docker Assume responsibility for key parts Interos Knowledge Graph product Translate experimental code into tested, production ready modules Create connectors ingesting data from third party databases, APIs, and web sites Optimize structures, improve execution, reduce memory consumption existing implementation Develop APIs","Bachelor's Degree Computer Science equivalent work experience 2-4 years Software Development Strong Python skills Java/C++ object oriented language. Experience two following Code optimization Data structures Elasticsearch SQL Linux / Ubuntu JSON YAML AWS SQS, S3, EC2 Compilers/Parsers Docker Assume responsibility key parts Interos Knowledge Graph product Translate experimental code tested, production ready modules Create connectors ingesting data third party databases, APIs, web sites Optimize structures, improve execution, reduce memory consumption existing implementation Develop APIs"
52,Data Engineer,Senior Data Engineer,"Washington, DC",Washington,DC,"Spotinstâs Senior Data Engineer will be responsible for designing, expanding and optimizing our data infrastructure architecture. Our Data Engineer will work closely with our software developers and data analysts to provide creative solutions for data-related problems.
Responsibilities:
Work in a fast-paced agile environment
Building efficient storage for structured and unstructured data
Developing and deploying distributed computing Big Data applications using Frameworks like Apache Spark, Presto, Apex, Kafka on AWS Cloud using EMR and/or Amazon Athena
Utilizing programming languages like Java, Scala, Python, and Cloud-based services such as Athena and Glue
Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Maven, Terraform, Git, BitBucket and Docker
Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Write, design, code, test, implement, debug, and validate applications; document design decisions and develop modular software components; monitor system performance metrics, and identify potential risks/issues
Collaborate in agile scrum team with product owners and fellow software engineers to deliver upon most important business and technical priorities
Provide active mentorship/guidance to fellow members of the agile tech team and participate in internal and external technology conference & communities
Requirements:
BA/ MA Degree or equivalent work experience
3+ years of work experience in data warehousing and analytics
At least 3 years of ETL design, development, and implementation experience
2+ years of Python development experience
2+ years of experience with the Hadoop Stack
At least 2+ years of experience with Cloud computing (AWS)
4+ years' experience with SQL (PostgreSQL, Presto, Athena, Redshift)
4+ years of UNIX/Linux experience","   Work in a fast-paced agile environment Building efficient storage for structured and unstructured data Developing and deploying distributed computing Big Data applications using Frameworks like Apache Spark, Presto, Apex, Kafka on AWS Cloud using EMR and/or Amazon Athena Utilizing programming languages like Java, Scala, Python, and Cloud-based services such as Athena and Glue Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Maven, Terraform, Git, BitBucket and Docker Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance Write, design, code, test, implement, debug, and validate applications; document design decisions and develop modular software components; monitor system performance metrics, and identify potential risks/issues Collaborate in agile scrum team with product owners and fellow software engineers to deliver upon most important business and technical priorities Provide active mentorship/guidance to fellow members of the agile tech team and participate in internal and external technology conference & communities  ","Work in a fast-paced agile environment Building efficient storage for structured and unstructured data Developing deploying distributed computing Big Data applications using Frameworks like Apache Spark, Presto, Apex, Kafka on AWS Cloud EMR and/or Amazon Athena Utilizing programming languages Java, Scala, Python, Cloud-based services such as Glue Leveraging DevOps techniques practices Continuous Integration, Deployment, Test Automation, Build Automation Driven Development to enable the rapid delivery of working code utilizing tools Jenkins, Maven, Terraform, Git, BitBucket Docker Performing unit tests conducting reviews with other team members make sure your is rigorously designed, elegantly coded, effectively tuned performance Write, design, code, test, implement, debug, validate applications; document design decisions develop modular software components; monitor system metrics, identify potential risks/issues Collaborate scrum product owners fellow engineers deliver upon most important business technical priorities Provide active mentorship/guidance tech participate internal external technology conference & communities","Work fast-paced agile environment Building efficient storage structured unstructured data Developing deploying distributed computing Big Data applications using Frameworks like Apache Spark, Presto, Apex, Kafka AWS Cloud EMR and/or Amazon Athena Utilizing programming languages Java, Scala, Python, Cloud-based services Glue Leveraging DevOps techniques practices Continuous Integration, Deployment, Test Automation, Build Automation Driven Development enable rapid delivery working code utilizing tools Jenkins, Maven, Terraform, Git, BitBucket Docker Performing unit tests conducting reviews team members make sure rigorously designed, elegantly coded, effectively tuned performance Write, design, code, test, implement, debug, validate applications; document design decisions develop modular software components; monitor system metrics, identify potential risks/issues Collaborate scrum product owners fellow engineers deliver upon important business technical priorities Provide active mentorship/guidance tech participate internal external technology conference & communities"
53,Data Engineer,Data Engineer with active TS/SCI with Polygraph,"McLean, VA",McLean,VA,"LMI is currently seeking a Data Engineer to support an Intelligence Community (IC) customer.
Responsibilities
The ideal candidate will have previously supported initiatives across in the IC and should have direct, applied experience with one or more of the following areas:
Define data requirements and gather and validate information, applying judgment and statistical tests.Develop data structures to support the generation of business insights and strategy.Maintain data infrastructure and develop scripts for regular processes.Design and develop operational data analysis and visualization tools, techniques, metrics, and dashboards to meet business needs.Analyze data analysis requests obtained from management to determine operational problems and define data modeling requirements, validation of content, and problem solving parameters.Identify opportunities to use data to develop new strategies and improve business performance and utilize knowledge of mathematical modeling and other optimization methods to perform quantitative and qualitative data analysis.Communicate and present data to management by developing reports using Tableau or Business Intelligence tools.
Qualifications
Bachelorâs degree with a major in a analytical (e.g., engineering, statistics, mathematics, information technology, etc.) or social science discipline is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with an IC customer.
Must have one or more of the following skillsets through education or work experience: quantitative and qualitative analysis; developing reports; scripting tools; and data warehouses and environments.
Must possess strong written and verbal communication skills.
Must have an active TS/SCI clearance with Polygraph.","Bachelorâs degree with a major in a analytical  e.g., engineering, statistics, mathematics, information technology, etc.  or social science discipline is preferred. Master's degree is desired. Must have at least 3 years of experience, preferably with an IC customer. Must have one or more of the following skillsets through education or work experience  quantitative and qualitative analysis; developing reports; scripting tools; and data warehouses and environments. Must possess strong written and verbal communication skills. Must have an active TS/SCI clearance with Polygraph.  Define data requirements and gather and validate information, applying judgment and statistical tests.Develop data structures to support the generation of business insights and strategy.Maintain data infrastructure and develop scripts for regular processes.Design and develop operational data analysis and visualization tools, techniques, metrics, and dashboards to meet business needs.Analyze data analysis requests obtained from management to determine operational problems and define data modeling requirements, validation of content, and problem solving parameters.Identify opportunities to use data to develop new strategies and improve business performance and utilize knowledge of mathematical modeling and other optimization methods to perform quantitative and qualitative data analysis.Communicate and present data to management by developing reports using Tableau or Business Intelligence tools.  ","Bachelorâs degree with a major in analytical e.g., engineering, statistics, mathematics, information technology, etc. or social science discipline is preferred. Master's desired. Must have at least 3 years of experience, preferably an IC customer. one more the following skillsets through education work experience quantitative and qualitative analysis; developing reports; scripting tools; data warehouses environments. possess strong written verbal communication skills. active TS/SCI clearance Polygraph. Define requirements gather validate information, applying judgment statistical tests.Develop structures to support generation business insights strategy.Maintain infrastructure develop scripts for regular processes.Design operational analysis visualization tools, techniques, metrics, dashboards meet needs.Analyze requests obtained from management determine problems define modeling requirements, validation content, problem solving parameters.Identify opportunities use new strategies improve performance utilize knowledge mathematical other optimization methods perform analysis.Communicate present by reports using Tableau Business Intelligence tools.","Bachelorâs degree major analytical e.g., engineering, statistics, mathematics, information technology, etc. social science discipline preferred. Master's desired. Must least 3 years experience, preferably IC customer. one following skillsets education work experience quantitative qualitative analysis; developing reports; scripting tools; data warehouses environments. possess strong written verbal communication skills. active TS/SCI clearance Polygraph. Define requirements gather validate information, applying judgment statistical tests.Develop structures support generation business insights strategy.Maintain infrastructure develop scripts regular processes.Design operational analysis visualization tools, techniques, metrics, dashboards meet needs.Analyze requests obtained management determine problems define modeling requirements, validation content, problem solving parameters.Identify opportunities use new strategies improve performance utilize knowledge mathematical optimization methods perform analysis.Communicate present reports using Tableau Business Intelligence tools."
54,Data Engineer,Azure Data Engineer,"Washington, DC 20006",Washington,DC,"Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, âas isâ and âto beâ scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ","At least 5 years of consulting or client service delivery experience on Azure DevOps an platform Proven ability to build, manage and foster a team-oriented environment","At least 5 years consulting client service delivery experience Azure DevOps platform Proven ability build, manage foster team-oriented environment"
55,Data Engineer,Data Engineer,"Washington, DC",Washington,DC,"The Senior Data Engineer responsibilities include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of our client's Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatment, etc .

Required Education/Experience:
Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.


Preferred:
Experience in Healthcare or related industryExperience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plusExperience productizing/automating predictive models that use R, SAS, Python, SPSS, etc.Continuous delivery and deployment automation for analytic solutions using tools like BambooFamiliarity with test driven development methodology for analytic solutionsAGILEAPI developmentData visualization and/or dashboard developmentDemonstrated ability to achieve stretch goals in a highly innovative and fast-paced environment","   Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five  5  years technology industry or related experience, including items such as Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive  5  years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool. ","Bachelor's Degree in computer science or related field, equivalent combination of education and experience/technical training that demonstrates analytical technical competencyMinimum five 5 years technology industry experience, including items such as Build highly scalable, scaled-out architectures on large scale database platformsExperience working a complex data infrastructure environmentFive experience engineering roleExtensive depth pipeline development with standard integration toolsExperience SDLC process requirements gathering, analysis, architecture, design, implementation, testing, deployment support.Experience any tool for Source Control Project ManagementExperience writing test cases scripts quality assuranceExperience creating stored procedures functionsExperience developing dimensional model tool.","Bachelor's Degree computer science related field, equivalent combination education experience/technical training demonstrates analytical technical competencyMinimum five 5 years technology industry experience, including items Build highly scalable, scaled-out architectures large scale database platformsExperience working complex data infrastructure environmentFive experience engineering roleExtensive depth pipeline development standard integration toolsExperience SDLC process requirements gathering, analysis, architecture, design, implementation, testing, deployment support.Experience tool Source Control Project ManagementExperience writing test cases scripts quality assuranceExperience creating stored procedures functionsExperience developing dimensional model tool."
56,Data Engineer,Big Data Engineer (Secret Clearance),"Arlington, VA 22209",Arlington,VA,"Are you passionate about cyber and security challenges in information technology, associated with threats and vulnerabilities? Are you are interested in a role that offers an opportunity to provide front line support to our Federal clients? If yes, then Deloitteâs Cyber Risk team could be the place for you! Join our team of Cyber Risk professionals who collaborate with government agencies, IT professionals, and clients to support cyber security and risk consulting engagements.
Work youâll do
As a Project Delivery Manager in the Cyber Risk group you will:
Improve the operational systems, processes, and policies in support of the clientâs mission through the management and guidance of multiple work streams, teams, and clients
Support engagements related but not limited to Operations & Maintenance, Helpdesk Operations, Software and Application Development and Maintenance, Financial Operations, and Project and Acquisition Management
Provide input to key deliverable structure and content, as well as facilitating buy-in of proposed solutions from top management levels
Direct timely delivery of quality work products for the client
Manage engagement risk
Provide professional development of junior staff performing the role of counselor and coach, as well as providing leadership and support
Work on the latest in big data analytical systems building solutions that will support data ingestion, processing and real-time analytics across billions of data pointsManage a big data migration to the cloud and ensure the flow of data through the pipelineUtilize open source software and tools such as Jenkins, Apache Kafka, Apache Spark, Ansible
The Team
Transparency, innovation, collaboration, sustainability: these are the hallmark issues shaping Federal government initiatives today. Deloitteâs Federal practice is passionate about making an impact with lasting change. Carrying out missions in the Federal practice requires fresh thinking and a creative approach. We collaborate with teams from across our organization in order to bring the full breadth of Deloitte, its commercial and public sector expertise, to best support our clients. Our aspiration is to be the premier integrated solutions provider in helping to transform the Federal marketplace.
Qualifications
Required:
Typically has 7 or more years of consulting and/or industry experience
Ability to support engagements of greater than average size and complexity
Ability to lead multiple teams and multiple clients with confidence
Excellent teamwork and interpersonal skills
Professional oral and written communication skills
Ability to mentor and manage junior staff and further their professional growth
Ability to obtain and maintain the required clearance for this role
Preferred:
Prior professional services or federal consulting experience
Bachelorâs Degree
How youâll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe thereâs always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitteâs culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitteâs impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youâre applying to. Check out recruiting tips from Deloitte professionals.",Typically has 7 or more years of consulting and/or industry experience    ,Typically has 7 or more years of consulting and/or industry experience,Typically 7 years consulting and/or industry experience
57,Data Engineer,Data Engineer,"Arlington, VA",Arlington,VA,"We are currently seeking a Data Engineer (Full Stack) to contribute to an exciting federal project to create, transform, and modernize applications and data platforms. This is an exciting opportunity which will allow qualified candidates to further develop their skills and expand their area of expertise!

RESPONSIBILITIES:
Work closely with the Project Manager, Technical Lead, and development team to provide overall data-engineering support and to understand project and application requirements.
As a member of the development team, create and build customized infrastructure, applications, and tools that meet user requirements.
As an engineer, build and deliver data storage, integrate with existing data, and migrate the built big data applications to cloud computing platform by leveraging new technologies.
Collect, build, cleanse, assemble and refine datasets to support the variety of data analytics needs put forward by business stakeholders including data scientists, law enforcement officials, and agency analysts.
Networking, database, cloud engineering, security engineering teams to comply with the data security policies and procedures and trouble-shoot and resolve any issues in data engineering deliveries.
Requirements
BS in Computer Science, Mathematics, Software Engineering, or other relevant field.
2 or more years of software development and a passion for working with large data sets.
Demonstrated extensive experience working in large-scale data environments which includes real time and batch processing requirements, as well as graph databases.
Experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures.
Hands-on coding experience in programming languages such as Java, Python, Scala.
Experience with Big Data platforms and technologies â AWS, Apache, Hadoop, Azure, and advanced analytics tools.
Experience with ETL processing with large data stores.
Working knowledge of data security management policies and procedures.
Knowledge and experience with Agile, Scrum and DevOps principles and practices and working on collaborative development teams.
Benefits
At GreenZone, we are dedicated to obtaining and maintaining the highest level of employee satisfaction by offering a competitive benefits package that includes medical, dental and vision, short and long term disability, retirement plan and company match, a generous annual leave plan, and a commitment to providing a work/life balance for all employees.","     BS in Computer Science, Mathematics, Software Engineering, or other relevant field. 2 or more years of software development and a passion for working with large data sets. Demonstrated extensive experience working in large-scale data environments which includes real time and batch processing requirements, as well as graph databases. Experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures. Hands-on coding experience in programming languages such as Java, Python, Scala. Experience with Big Data platforms and technologies â AWS, Apache, Hadoop, Azure, and advanced analytics tools. Experience with ETL processing with large data stores. Working knowledge of data security management policies and procedures. Knowledge and experience with Agile, Scrum and DevOps principles and practices and working on collaborative development teams. ","BS in Computer Science, Mathematics, Software Engineering, or other relevant field. 2 more years of software development and a passion for working with large data sets. Demonstrated extensive experience large-scale environments which includes real time batch processing requirements, as well graph databases. Experience designing delivering scale, 24-7, mission-critical pipelines features using modern big architectures. Hands-on coding programming languages such Java, Python, Scala. Big Data platforms technologies â AWS, Apache, Hadoop, Azure, advanced analytics tools. ETL stores. Working knowledge security management policies procedures. Knowledge Agile, Scrum DevOps principles practices on collaborative teams.","BS Computer Science, Mathematics, Software Engineering, relevant field. 2 years software development passion working large data sets. Demonstrated extensive experience large-scale environments includes real time batch processing requirements, well graph databases. Experience designing delivering scale, 24-7, mission-critical pipelines features using modern big architectures. Hands-on coding programming languages Java, Python, Scala. Big Data platforms technologies â AWS, Apache, Hadoop, Azure, advanced analytics tools. ETL stores. Working knowledge security management policies procedures. Knowledge Agile, Scrum DevOps principles practices collaborative teams."
58,Data Engineer,Data Engineer,"Washington, DC",Washington,DC,"Data Engineer
Location: Washington DC
Job ID: 1093
Active Top-Secret US Government clearance required
Are you an analytical, data-driven professional? Are you interested in a role that offers the opportunity to provide client-facing support? If so, Datastrong is the place for you! Join our team of specialists as they unlock insights contained in a data universe and work through challenges related to real-time data integration and analytics, data quality management, mission-oriented analysis, and human resources.
Work you will perform
As a Data Engineer on the analytics team you will:
Create and enhance the data pipeline architecture
Assemble data sets to meet functional and non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Work with business stakeholders to assist with data-related needs and issues
Create data tools for front-end analytics and data scientist team members that assist them in their responsibilities
Design and deliver ETL data integration workflow components
Qualifications
Required:
Active Top-Secret US Government clearance
Bachelorâs Degree in Statistics, Mathematics, Computer Science, Management Information Systems, Engineering, Business Analytics disciplines, or related area
3+ years of experience with ETL and data integration, data quality analysis, statistical analysis and/or modeling
Advanced working SQL knowledge and experience with RDBMS platforms
Experience building and optimizing data pipelines, architectures, and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong business and analytical skills including direct interaction with business end users for requirements analysis
Preferred:
Hands-on experience with implementation and support of commercial data integration software (Informatica, IBM DataStage, Talend, Microsoft SSIS, etc.)
Experience with Oracle SQL or PL/SQL
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience in Python, message queuing, or stream processing
How you will grow
At Datastrong, our professional development plan focuses on helping employees at every level of their career to identify and use their strengths. Datastrong offers opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career.
Benefits include
Compensation plan consisting of a competitive base salary and an uncapped bonus
100% Health coverage for employees with Vision and Dental options
Paid holidays and vacation with a generous leave policy
Commute reimbursement
Professional development and educational tuition assistance
Flexible spending account options
401(k) retirement plan with complimentary financial advisory services
Datastrongâs culture
Our culture is built on inclusion, collaboration, high performance, and opportunity. That combination helps our professionals make a difference individually and collectively. And it makes Datastrong a rewarding place to work. We are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives.
Corporate citizenship
We are proud to be part of the communities where we work and live. Datastrong takes great pride in knowing that what we do best â applying our skills and experience â helps to build strong communities that shape and sustain our business culture. Our team members are provided opportunities to support the community through volunteerism, giving their time, talent, and generosity.

Datastrong is committed to hiring and retaining a diverse workforce. We are an Equal Opportunity and Affirmative Action Employer, making decisions without regard to race, color, religion, sex, national origin, age, veteran status, disability, or any other protected class. If accommodation is needed in the application process, arrangements can be made with the local regional office."," Active Top-Secret US Government clearance Bachelorâs Degree in Statistics, Mathematics, Computer Science, Management Information Systems, Engineering, Business Analytics disciplines, or related area 3+ years of experience with ETL and data integration, data quality analysis, statistical analysis and/or modeling Advanced working SQL knowledge and experience with RDBMS platforms Experience building and optimizing data pipelines, architectures, and data sets Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Strong business and analytical skills including direct interaction with business end users for requirements analysis    ","Active Top-Secret US Government clearance Bachelorâs Degree in Statistics, Mathematics, Computer Science, Management Information Systems, Engineering, Business Analytics disciplines, or related area 3+ years of experience with ETL and data integration, quality analysis, statistical analysis and/or modeling Advanced working SQL knowledge RDBMS platforms Experience building optimizing pipelines, architectures, sets performing root cause on internal external processes to answer specific business questions identify opportunities for improvement Strong analytical skills including direct interaction end users requirements","Active Top-Secret US Government clearance Bachelorâs Degree Statistics, Mathematics, Computer Science, Management Information Systems, Engineering, Business Analytics disciplines, related area 3+ years experience ETL data integration, quality analysis, statistical analysis and/or modeling Advanced working SQL knowledge RDBMS platforms Experience building optimizing pipelines, architectures, sets performing root cause internal external processes answer specific business questions identify opportunities improvement Strong analytical skills including direct interaction end users requirements"
59,Data Engineer,Data Engineer,"Washington, DC",Washington,DC,"Job Title: Data Engineer
Location: Washington, DC
Shift: N/A
Required Security Clearance: None; however, a Public Trust must be obtained prior to starting.
Required Certifications: None
Required Education: Bachelorâs degree in technology or the sciences, or industry-equivalent experience required.
Required Experience: 3+ yearsâ experience in Oracle Business Intelligence (OBIEE). 3+ years python scripting experience. Experience with manipulating and searching large data sets
Description:
The Data Engineer will be key in providing the client data insights, understanding, and statistical relevant findings within the client data set. This Engineer will be responsible for providing growth of a data science/analytics footprint. The Engineer would be considered in the ""landing team"" of the new analytics work, to be expanded with success, receiving oversight from existing analytics senior management.
Functional Responsibility:
The Data Engineer will provide the following:
Working with the client to define business questions (i.e. problem statements), associated metrics, data challenges, and scope business objectives.
Ability to manipulate and understand large, complex data sets.
Research/analyze/create data models to drive statistical analysis and results related to confirmed client business questions.
Build analytic models using reinforcement learning techniques to business decisions.
Leverage suite of analytic models (e.g. decision trees, regression models, Bayesian methods) to solve client pain points.
Leverage defined algorithms or statistical techniques to enhance data analysis and explore new outcomes.
Able to present and simplify complex statistical approaches, findings and concepts to the client.
Understand how to present information visually to help one understand the data story.
Design, develop and implement analytics reporting solutions as needed.
Qualifications:
Experienced with Oracle Business Intelligence (OBIEE) is a must.
SQL & PLSQL query development experience.
Python scripting experience.
Proven ability to work independently and as a team member.
Good communication (written and oral) and interpersonal skills.
Good organizational, multi-tasking, and time-management skills.
US Citizen in order to obtain the Public Trust.
Preferences:
Experience with implementing data science projects in an agile environment
Experience with R or other similar programming languages
An active Public Trust.
Working Conditions:
Work is typically based in a busy office environment and subject to frequent interruptions. Business work hours are normally set from Monday through Friday 8:00am to 5:00pm, however some extended or weekend hours may be required. Additional details on the precise hours will be informed to the candidate from the Program Manager/Hiring Manager.
Physical Requirements:
None
Background Screening/Check/Investigation:
Successful Completion of a Background Screening/Check/Investigation will be required as a condition of hire.
Employment Type: Full-time / Exempt
Benefits:
Federal Data Systems, LLC offers competitive compensation, a flexible benefits package, career development opportunities that reflect its commitment to creating a diverse and supportive workplace. Benefits include, not all inclusive â Medical, Vision & Dental Insurance, Paid Time-Off & Company Paid Holidays, Personal Development & Learning Opportunities.
Other:
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

Federal Data Systems, LLC (FEDDATA) is an Equal Opportunity/Affirmative Action Employer. That does not unlawfully discriminate in any of its programs or activities on the basis of race, color, religion, sex, age, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other basis prohibited by applicable law.","Experienced with Oracle Business Intelligence  OBIEE  is a must. SQL & PLSQL query development experience. Python scripting experience. Proven ability to work independently and as a team member. Good communication  written and oral  and interpersonal skills. Good organizational, multi-tasking, and time-management skills. US Citizen in order to obtain the Public Trust.    Working with the client to define business questions  i.e. problem statements , associated metrics, data challenges, and scope business objectives. Ability to manipulate and understand large, complex data sets. Research/analyze/create data models to drive statistical analysis and results related to confirmed client business questions. Build analytic models using reinforcement learning techniques to business decisions. Leverage suite of analytic models  e.g. decision trees, regression models, Bayesian methods  to solve client pain points. Leverage defined algorithms or statistical techniques to enhance data analysis and explore new outcomes. Able to present and simplify complex statistical approaches, findings and concepts to the client. Understand how to present information visually to help one understand the data story. Design, develop and implement analytics reporting solutions as needed.  ","Experienced with Oracle Business Intelligence OBIEE is a must. SQL & PLSQL query development experience. Python scripting Proven ability to work independently and as team member. Good communication written oral interpersonal skills. organizational, multi-tasking, time-management US Citizen in order obtain the Public Trust. Working client define business questions i.e. problem statements , associated metrics, data challenges, scope objectives. Ability manipulate understand large, complex sets. Research/analyze/create models drive statistical analysis results related confirmed questions. Build analytic using reinforcement learning techniques decisions. Leverage suite of e.g. decision trees, regression models, Bayesian methods solve pain points. defined algorithms or enhance explore new outcomes. Able present simplify approaches, findings concepts client. Understand how information visually help one story. Design, develop implement analytics reporting solutions needed.","Experienced Oracle Business Intelligence OBIEE must. SQL & PLSQL query development experience. Python scripting Proven ability work independently team member. Good communication written oral interpersonal skills. organizational, multi-tasking, time-management US Citizen order obtain Public Trust. Working client define business questions i.e. problem statements , associated metrics, data challenges, scope objectives. Ability manipulate understand large, complex sets. Research/analyze/create models drive statistical analysis results related confirmed questions. Build analytic using reinforcement learning techniques decisions. Leverage suite e.g. decision trees, regression models, Bayesian methods solve pain points. defined algorithms enhance explore new outcomes. Able present simplify approaches, findings concepts client. Understand information visually help one story. Design, develop implement analytics reporting solutions needed."
60,Data Engineer,AWS Data Engineer,"Arlington, VA 22209",Arlington,VA,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
61,Data Engineer,Data Engineer,"Washington, DC 20036",Washington,DC,"About EAB

At EAB, our mission is to make education smarter and our communities stronger. We harness the collective power of more than 1,500 schools, colleges, and universities to uncover and apply proven practices and transformative insights. And since complex problems require multifaceted solutions, we work with each school differently to apply these insights through a customized blend of research, technology, and services. From kindergarten to college and beyond, EAB partners with education leaders, practitioners, and staff to accelerate progress and drive results across three key areas: enrollment management, student success, and institutional operations and strategy.

At EAB, we serve not only our members but each otherâthat's why we are always working to make sure our employees love their jobs and are invested in their community. See how we've been recognized for this dedication to our employees by checking out our recent awards.

For more information, visit our Careers page.

The Role in Brief

Associate Data Engineer

Are you a data enthusiast who seeks to tease out meaning from complex data flows and assets? Are you a talented problem solver who can transform abstract problems into elegant technical solutions? We are looking for a Data Modeler to join our team of engineers and data analysts focused on designing, creating, and delivering data solutions as part of our state-of-the-art cloud based products. The successful candidate will have the opportunity to build a world-class solution to help our higher education clients solve challenging problems through data.

This role is based in Washington, DC.

Primary Responsibilities:
Responsible for data modeling and schema design that will range across multiple business domains within higher education
Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas
Work with clients to research and conduct business information flow studies
Codify high-performing SQL for efficient data transformation
Coordinate work with external teams to ensure a smooth development process
Support operations by identifying, researching and resolving performance and production issues
Basic Qualifications:
1+ years of experience working with relational or multi-dimensional databases
Experience developing logical data models within a data warehouse
Experience developing ETL processes
Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2
Demonstrated mastery in database concepts and large-scale database implementations and design patterns
Proven ability to work with users to define requirements and business issues
Excellent analytic and troubleshooting skills
Strong written and oral communication skills
Bachelorâs degree in Computer Science or Computer Engineering
Ideal Qualifications:
Experience working in an AGILE environment
Experience developing commercial software products
Experience with AWS data warehouse infrastructure (redshift, EMR/spark)
GIT expertise
Master's degree in Computer Science or Computer Engineering
Benefits:

Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive benefits package.

Medical, dental, and vision insurance, dependents eligible
401(k) retirement plan with company match
Generous PTO
Daytime leave policy for community service or fitness activities (up to 10 hours a month each)
Wellness programs including gym discounts and incentives to promote healthy living
Dynamic growth opportunities with merit-based promotion philosophy
Benefits kick in day one, see the full details here.


At EAB, we believe that to fulfill our mission to âmake education smarter and our communities strongerâ we need team members who bring a diversity of perspectives to the table and a workplace where each team member is valued, respected and heard.

To that end, EAB is an Equal Opportunity Employer, and we make employment decisions on the basis of qualifications, merit and business need. We donât discriminate on the basis of race, religion, color, sex, gender identity or expression, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law.","1+ years of experience working with relational or multi-dimensional databases Experience developing logical data models within a data warehouse Experience developing ETL processes Demonstrated mastery in one or more SQL variants  PostgreSQL, MySQL, Oracle, SQL Server, or DB2 Demonstrated mastery in database concepts and large-scale database implementations and design patterns Proven ability to work with users to define requirements and business issues Excellent analytic and troubleshooting skills Strong written and oral communication skills Bachelorâs degree in Computer Science or Computer Engineering   Responsible for data modeling and schema design that will range across multiple business domains within higher education Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas Work with clients to research and conduct business information flow studies Codify high-performing SQL for efficient data transformation Coordinate work with external teams to ensure a smooth development process Support operations by identifying, researching and resolving performance and production issues   ","1+ years of experience working with relational or multi-dimensional databases Experience developing logical data models within a warehouse ETL processes Demonstrated mastery in one more SQL variants PostgreSQL, MySQL, Oracle, Server, DB2 database concepts and large-scale implementations design patterns Proven ability to work users define requirements business issues Excellent analytic troubleshooting skills Strong written oral communication Bachelorâs degree Computer Science Engineering Responsible for modeling schema that will range across multiple domains higher education Partner stakeholders including clients, new product development, BI engineers develop scalable standard schemas Work clients research conduct information flow studies Codify high-performing efficient transformation Coordinate external teams ensure smooth development process Support operations by identifying, researching resolving performance production","1+ years experience working relational multi-dimensional databases Experience developing logical data models within warehouse ETL processes Demonstrated mastery one SQL variants PostgreSQL, MySQL, Oracle, Server, DB2 database concepts large-scale implementations design patterns Proven ability work users define requirements business issues Excellent analytic troubleshooting skills Strong written oral communication Bachelorâs degree Computer Science Engineering Responsible modeling schema range across multiple domains higher education Partner stakeholders including clients, new product development, BI engineers develop scalable standard schemas Work clients research conduct information flow studies Codify high-performing efficient transformation Coordinate external teams ensure smooth development process Support operations identifying, researching resolving performance production"
62,Data Engineer,Data Engineer,"Herndon, VA",Herndon,VA,"LMI is currently seeking a data engineer within LMIâs Advanced Analytics service line to support the implementation of data governance and metadata management tools in support of enterprise data management.

*This position is located at a client site in Reston, VA*
Responsibilities
The ideal candidate will have direct, applied experience with one or more of the following areas:
Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceFamiliarity with enterprise data management, data governance, and data control policiesAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.
Qualifications
Bachelorâs degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with big data tools: Hadoop, Spark, Kafka
Experience with data governance tools: Collibra, Immuta
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
#LI-SH1","Bachelorâs degree in a quantitative field  e.g., engineering, statistics, mathematics, information technology, etc.  is preferred. Master's degree is desired. Must have at least 3 years of experience, preferably with a federal government customer. Experience with relational SQL and NoSQL databases  Postgres, Cassandra, MongoDB Experience with big data tools  Hadoop, Spark, Kafka Experience with data governance tools  Collibra, Immuta Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala Must possess strong written and verbal communication skills.  Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceFamiliarity with enterprise data management, data governance, and data control policiesAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.  ","Bachelorâs degree in a quantitative field e.g., engineering, statistics, mathematics, information technology, etc. is preferred. Master's desired. Must have at least 3 years of experience, preferably with federal government customer. Experience relational SQL and NoSQL databases Postgres, Cassandra, MongoDB big data tools Hadoop, Spark, Kafka governance Collibra, Immuta object-oriented/object function scripting languages Python, Java, C++, Scala possess strong written verbal communication skills. Develop structures systems to support the generation business insightsKnowledge experience overall ETL processesMaintain infrastructure develop scripts for regular processesDefine, design, flow diagrams, dictionaries, logical physical modelsDefine requirements, document elements, capture maintain metadetaIdentify clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities use improve performanceFamiliarity enterprise management, governance, control policiesAdhere compliance audit requirements storage, architecture, cybersecurity,","Bachelorâs degree quantitative field e.g., engineering, statistics, mathematics, information technology, etc. preferred. Master's desired. Must least 3 years experience, preferably federal government customer. Experience relational SQL NoSQL databases Postgres, Cassandra, MongoDB big data tools Hadoop, Spark, Kafka governance Collibra, Immuta object-oriented/object function scripting languages Python, Java, C++, Scala possess strong written verbal communication skills. Develop structures systems support generation business insightsKnowledge experience overall ETL processesMaintain infrastructure develop scripts regular processesDefine, design, flow diagrams, dictionaries, logical physical modelsDefine requirements, document elements, capture maintain metadetaIdentify clean incomplete, incorrect, inaccurate irrelevant dataIdentify new opportunities use improve performanceFamiliarity enterprise management, governance, control policiesAdhere compliance audit requirements storage, architecture, cybersecurity,"
63,Data Engineer,Data Engineer,"Washington, DC",Washington,DC,"In a world where there is seemingly an infinite number of Government Services firms, we strive to not just be ""a"" place to work, but to be ""the"" place to work! Here at ASET Partners we are looking for a highly skilled, exceptionally motivated Data Engineer to help support the Department of Defense, intelligence community, and federal civilian agencies
Responsibilities: .

Develop data flow diagrams depicting data movement through data centric architecture.
Provide recommendation to senior leadership to simplify end user processes and create better efficiencies.
Translates customer requirements into formal agreements and plans to culminate in customer acceptance or results.
Execute a wide range of process activities beginning with the request for proposal through development, test and final delivery. Anticipates future customer, industry and business trends.
Challenges the validity of given procedures and processes with a view toward enhancements or improvement. Creates innovative solutions to problems involving finance, scheduling, technology, methodology, tools and solution components. Leads team on large complex projects.
Provide capability to ingest and extract data.
Create repeatable, reusable procedures.
Deliver common services in support of architecture roadmap and Agency direction.
Work with a team to design, implement, and maintain new systems.
Provide guidance to the customer on best-practices.
Perform other duties as assigned.

Qualifications:

Must have an active/current TS/SCI and be able to pass a CI Poly.
Must have at least five years' experience in technology consulting.
Bachelor's degree or equivalent training and experience. Master's degree preferred with advanced training in information technology.
Experience and knowledge of tools associated with counterintelligence and HUMINT collectors.
Experience in cloud technologies, data layers, microservices.
Experience with the following tools:
AWS, AWS Cloud and C2S
Database experience in SQL/NSQL
Experience and knowledge in software coding and unit level testing including:
Java, python, Ruby R
Knowledge of web services feeds
Must be a diverse thinker and be able to work in a large group setting.
Write and edit technical documents and reports;
Write, edit, and produce contents for contract deliverables: reports, training materials, presentation slides, letters, fact sheets, diagrams, and capability
Effectively communicate project expectations to team members and stakeholders in a timely and clear fashion.
Communicate formally and informally through existing forums to stakeholders at all levels, including senior leadership.

Travel Requirements:

Travel may be required both inside and outside the Washington National Capital Region (NCR) and worldwide.

ASET Snapshot:

We are a growing IT consulting and professional services firm that combines large-business experience with small-business efficiency and ingenuity
Founded in 2008
We are a HUBZone certified small business
Profitable from day one, zero debt
Offices in Alexandria, VA and Baltimore, MD
Over 75 employees and growing
Over 15 active contracts supporting 10+ Federal Agencies
Our mission is to drastically change our client's expectations... one program at a time

Top 10 Reasons Our Employees Love Working at ASET:
1. Outstanding benefits! Including: health, dental, and vision care, a 401k retirement plan with up to a 4% company paid contribution, and employee achievement and merit awards to name a few.
2. We have very high hiring standards, so their co-workers are just as smart and talented as they are.
3. We are not profit driven; we are results driven and believe that profits will follow.
4. Unlike Office Space, our employees do not have eight bosses. They have just one or two, and they are not micromanagers. It's not our style and we just don't have time for it!
5. We're different than the rest. Our employees love our unique small-company culture.
6. We have established a trusted relationship with all our clients and refuse to make recommendations that aren't in our clients' best interests.
7. They feel appreciated and recognized for the contributions they make each day.
8. We offer a healthy work-life balance, flexible schedules, and competitive pay.
9. They love that we have been constantly growing. It's exciting to add new folks and it offers many opportunities for career advancement and growth.
10. We work hard, and we play hard, too! We host quarterly company-wide lunches, regular happy hours and an annual New Year's party in January as a way of saying 'thank you' to our hard-working staff.
ASET Partners Corporation is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class."," Must have an active/current TS/SCI and be able to pass a CI Poly. Must have at least five years' experience in technology consulting. Bachelor's degree or equivalent training and experience. Master's degree preferred with advanced training in information technology. Experience and knowledge of tools associated with counterintelligence and HUMINT collectors. Experience in cloud technologies, data layers, microservices. Experience with the following tools  AWS, AWS Cloud and C2S Database experience in SQL/NSQL Experience and knowledge in software coding and unit level testing including  Java, python, Ruby R Knowledge of web services feeds Must be a diverse thinker and be able to work in a large group setting. Write and edit technical documents and reports; Write, edit, and produce contents for contract deliverables  reports, training materials, presentation slides, letters, fact sheets, diagrams, and capability Effectively communicate project expectations to team members and stakeholders in a timely and clear fashion. Communicate formally and informally through existing forums to stakeholders at all levels, including senior leadership.    Develop data flow diagrams depicting data movement through data centric architecture. Provide recommendation to senior leadership to simplify end user processes and create better efficiencies. Translates customer requirements into formal agreements and plans to culminate in customer acceptance or results. Execute a wide range of process activities beginning with the request for proposal through development, test and final delivery. Anticipates future customer, industry and business trends. Challenges the validity of given procedures and processes with a view toward enhancements or improvement. Creates innovative solutions to problems involving finance, scheduling, technology, methodology, tools and solution components. Leads team on large complex projects. Provide capability to ingest and extract data. Create repeatable, reusable procedures. Deliver common services in support of architecture roadmap and Agency direction. Work with a team to design, implement, and maintain new systems. Provide guidance to the customer on best-practices. Perform other duties as assigned.    Travel may be required both inside and outside the Washington National Capital Region  NCR  and worldwide. ","Must have an active/current TS/SCI and be able to pass a CI Poly. at least five years' experience in technology consulting. Bachelor's degree or equivalent training experience. Master's preferred with advanced information technology. Experience knowledge of tools associated counterintelligence HUMINT collectors. cloud technologies, data layers, microservices. the following AWS, AWS Cloud C2S Database SQL/NSQL software coding unit level testing including Java, python, Ruby R Knowledge web services feeds diverse thinker work large group setting. Write edit technical documents reports; Write, edit, produce contents for contract deliverables reports, materials, presentation slides, letters, fact sheets, diagrams, capability Effectively communicate project expectations team members stakeholders timely clear fashion. Communicate formally informally through existing forums all levels, senior leadership. Develop flow diagrams depicting movement centric architecture. Provide recommendation leadership simplify end user processes create better efficiencies. Translates customer requirements into formal agreements plans culminate acceptance results. Execute wide range process activities beginning request proposal development, test final delivery. Anticipates future customer, industry business trends. Challenges validity given procedures view toward enhancements improvement. Creates innovative solutions problems involving finance, scheduling, technology, methodology, solution components. Leads on complex projects. ingest extract data. Create repeatable, reusable procedures. Deliver common support architecture roadmap Agency direction. Work design, implement, maintain new systems. guidance best-practices. Perform other duties as assigned. Travel may required both inside outside Washington National Capital Region NCR worldwide.","Must active/current TS/SCI able pass CI Poly. least five years' experience technology consulting. Bachelor's degree equivalent training experience. Master's preferred advanced information technology. Experience knowledge tools associated counterintelligence HUMINT collectors. cloud technologies, data layers, microservices. following AWS, AWS Cloud C2S Database SQL/NSQL software coding unit level testing including Java, python, Ruby R Knowledge web services feeds diverse thinker work large group setting. Write edit technical documents reports; Write, edit, produce contents contract deliverables reports, materials, presentation slides, letters, fact sheets, diagrams, capability Effectively communicate project expectations team members stakeholders timely clear fashion. Communicate formally informally existing forums levels, senior leadership. Develop flow diagrams depicting movement centric architecture. Provide recommendation leadership simplify end user processes create better efficiencies. Translates customer requirements formal agreements plans culminate acceptance results. Execute wide range process activities beginning request proposal development, test final delivery. Anticipates future customer, industry business trends. Challenges validity given procedures view toward enhancements improvement. Creates innovative solutions problems involving finance, scheduling, technology, methodology, solution components. Leads complex projects. ingest extract data. Create repeatable, reusable procedures. Deliver common support architecture roadmap Agency direction. Work design, implement, maintain new systems. guidance best-practices. Perform duties assigned. Travel may required inside outside Washington National Capital Region NCR worldwide."
64,Data Engineer,Senior Data Engineer,"McLean, VA 22102",McLean,VA,"Do you want to be part of the exciting journey of Cloud Transformation? Are you a seasoned data engineer that thrives in an innovative and collaborative team? An advanced coder who enjoys a rapid, dynamic environment? If you have proven Data Engineering experience with AWS Cloud technologies and would like to help us build an Enterprise Data Lake in the Cloud, apply to join Freddie Macâs Emerging Data Technologies team.

Your work falls into 2 primary categories:

Development & Execution
Demonstrate effective and disciplined software development expertise
Deliver solutions on-time with high bar of quality, and continuously improve software engineering practices
Work with Product Owners to understand the desired capability, to define and prioritize work, determine deliverables, and manage workloads
Involved in application development, prototyping, modeling and technical consulting
Actively facilitates issue resolution and issue tracking. Identifies mitigation steps and ensures risks and issues are mitigated/resolved in a timely manner

Technical Leadership
Leads efforts in data, code, and systems analysis to ensure accuracy and completeness of requirements
Leads the design process and evaluates alternative solutions relating to usability, security, scalability, failover, and performance.
Supports the Development Tech Lead and/or Project Manager in managing projects / Agile Sprints
Performs and leads thorough Unit testing and Integration testing, including test data creation for various test scenarios
Mentor and coach intermediate Developers on both technical and soft skills
Qualifications
4+ years of full-time experience in software development including design, coding, testing, and support
At least 1 year of Cloud infrastructure experience working with one or more of the following Amazon Web Services (AWS) Cloud services: EC2, EMR, ECS, S3, SNS, SQS, Cloud Formation, Cloud watch, Lambda
Hands-on experience with AWS architecture design, Data Management, Big Data, and Data Warehousing
Experience ofwith data application and data product development
2+ years of experience with Agile, Kanban, or Scrum methodologies

Key to success in this role
Strong working knowledge and technical competencies of AWS
Ability to communicate clearly, effectively, persuasively with technology and business stakeholders
Ability to develop mutually beneficial relationships inside and outside of the division, work and collaborate effectively in a team environment
Sense of urgency to delivery and able to apply risk-based approach to prioritize work
Strong work ethic, self-motivated, independent, and works with minimal direction
Motivated to learn new technologies and identify process improvements and efficiencies
Ability to adapt to change while continuing to deliver on assigned objectives
Ability to use data to help inform strategy and direction

Top Personal Competencies to possess
Drive for Execution â Be accountable for strong individual and team performance
Partnership â Build trust and strong partnerships through your own and teamâs actions
Growth and Development â Know or learn what is needed to deliver results and successfully compete
Preferred Skills
Master's Degree in Computer Science or related fields
Proficient with CICD process, Agile and DevOps Software Development Life Cycle including analysis, high level design, coding, testing, and implementation, performance tuning, bug fixing and quality control
Experience building data lakes in AWS Cloud, moving Data applications to the Cloud, and developing cloud native Data applications
Expertise in creating data models, optimizing data, automating & restructuring data reporting system in financial services domain
2+ years of experience in big data technologies (Spark, Hadoop, HDFS, MongoDB, PostGre SQL)
3+ years of experience using Java, Python or Scala
Experience leading complex data applications with large volumes of data
Today, Freddie Mac makes home possible for one in four home borrowers and is one of the largest sources of financing for multifamily housing. Join our smart, creative and dedicated team and youâll do important work for the housing finance system and make a difference in the lives of others. Freddie Mac is an equal opportunity and top diversity employer. EOE, M/F/D/V.","4+ years of full-time experience in software development including design, coding, testing, and support At least 1 year of Cloud infrastructure experience working with one or more of the following Amazon Web Services  AWS  Cloud services  EC2, EMR, ECS, S3, SNS, SQS, Cloud Formation, Cloud watch, Lambda Hands-on experience with AWS architecture design, Data Management, Big Data, and Data Warehousing Experience ofwith data application and data product development 2+ years of experience with Agile, Kanban, or Scrum methodologies Master's Degree in Computer Science or related fields Proficient with CICD process, Agile and DevOps Software Development Life Cycle including analysis, high level design, coding, testing, and implementation, performance tuning, bug fixing and quality control Experience building data lakes in AWS Cloud, moving Data applications to the Cloud, and developing cloud native Data applications Expertise in creating data models, optimizing data, automating & restructuring data reporting system in financial services domain 2+ years of experience in big data technologies  Spark, Hadoop, HDFS, MongoDB, PostGre SQL  3+ years of experience using Java, Python or Scala Experience leading complex data applications with large volumes of data   ","4+ years of full-time experience in software development including design, coding, testing, and support At least 1 year Cloud infrastructure working with one or more the following Amazon Web Services AWS services EC2, EMR, ECS, S3, SNS, SQS, Formation, watch, Lambda Hands-on architecture Data Management, Big Data, Warehousing Experience ofwith data application product 2+ Agile, Kanban, Scrum methodologies Master's Degree Computer Science related fields Proficient CICD process, Agile DevOps Software Development Life Cycle analysis, high level implementation, performance tuning, bug fixing quality control building lakes Cloud, moving applications to developing cloud native Expertise creating models, optimizing data, automating & restructuring reporting system financial domain big technologies Spark, Hadoop, HDFS, MongoDB, PostGre SQL 3+ using Java, Python Scala leading complex large volumes","4+ years full-time experience software development including design, coding, testing, support At least 1 year Cloud infrastructure working one following Amazon Web Services AWS services EC2, EMR, ECS, S3, SNS, SQS, Formation, watch, Lambda Hands-on architecture Data Management, Big Data, Warehousing Experience ofwith data application product 2+ Agile, Kanban, Scrum methodologies Master's Degree Computer Science related fields Proficient CICD process, Agile DevOps Software Development Life Cycle analysis, high level implementation, performance tuning, bug fixing quality control building lakes Cloud, moving applications developing cloud native Expertise creating models, optimizing data, automating & restructuring reporting system financial domain big technologies Spark, Hadoop, HDFS, MongoDB, PostGre SQL 3+ using Java, Python Scala leading complex large volumes"
65,Data Engineer,ISR Data Engineer,"Arlington, VA 22209",Arlington,VA,"ISR Data Engineer
About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

Position Description
Redhorse Corporation is looking for an Intelligence, Surveillance and Reconnaissance (ISR) Data Engineer to support the ISR Operations Division within the Warfighter Support Directorate of the Office of the Under Secretary of Defense for Intelligence (OUSD(I)) on a multi-year contract. The Data Engineering team establishes data capture and storage mechanisms for ISR operations data, develops and implements data mining, advanced data analytics, visualization and other quantitative data science measurement techniques against ISR processes and products. These capabilities directly support government understanding and data-driven decision-making.

Basic Minimum Requirements for Skills, Experience, Education and Credentials include:
Bachelorâs Degree from an accredited college or university is required; Bachelorâs degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics (STEM) degrees or a Masterâs Degree is highly preferred.
Minimum of 5 years of general experience in the military or intelligence community is required.
Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.
Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level (3/4 star officer or SES-3/4) is preferred; at least 3 years of experience with OUSD(I) is highly preferred.
Minimum of 3 years of experience using scripting (e.g. Python, R, VBA) or programming languages (e.g. Java, C++, Ruby) to deliver data engineering / software development services is required.
Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.
Machine Learning: e.g. TensorFlow, PyTorch, Keras
Data Visualization: e.g. Tableau, D3, Kibana
Geospatial Analysis: e.g. ArcGIS, R
Web Services: e.g. SOAP, RESTful
Web Development: e.g. JavaScript, React.js, Node.js, HTML
Database Development: e.g. MongoDB, PostgreSQL, MS-SQL
Active TS/SCI security clearance.
Preferred Duties and Responsibilities for the Analyst include:
Simultaneously support 2-3 ISR Analytic studies with analytic expertise, project-specific web pages, on-demand ETL and data analysis, and the development of web-enabled analytic tools and dashboards.
Continuously supplement, improve, and maintain ISR Data Enrichment and Aggregation (IDEA), a JWICS-based, end-to-end automated ISR data capability, according to OUSD(I) and stakeholder priorities and agile development principles.
Continuously supplement, improve, and maintain a classified studies and analysis website including project repositories, analytic apps, and dynamic decision-support dashboards.
Design, build, and/or insert new technology into extant classified development and production architecture baseline to supplement and improve analytic output, algorithmic performance, and user experience across websites and APIs.
Design and develop custom scripts and tools on SIPRNET and JWICS to solve emergent analytic challenges and/or answer quick-turn or recurring senior executive questions about ISR performance and effectiveness.
Develop custom algorithms to ingest and transform SIPRNET- and JWICS-derived data, artifacts, and information into analytic-ready data.
Leverage advanced analytic techniques, including, but not limited to, geospatial analysis, regression analysis, machine learning, and natural language processing, to derive insight about ISR performance and effectiveness.
Perform DevOps across JWICS and SIPR development architectures containing multiple database clusters, web servers, load balancers, and virtual machine instances, and optimize the deployment and maintenance pipelines for all architectural components in support of IDEA and Data Engineering efforts.
Engage with the ISR community to understand analytic needs, raise awareness of ongoing work, seek feedback on in-progress innovations, and develop one-off analytics aids.
Support technical briefings on analytic methodologies, decision-support dashboards, and capability innovations.
The work environment for this position requires an individual to be able to:
Work sitting or standing at a desk or conference table for extended periods of time with the ability to shift positions while working: sit, stand, pace, adjust positioning in any of those without issue
Walk in the office to collaborate with co-workers, attend meetings or retrieve documents from printer
Must be able to lift and carry up to 10 lbs.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorseâs changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorseâs sole discretion.

EOE/M/F/Vet/Disabled"," Bachelorâs Degree from an accredited college or university is required; Bachelorâs degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics  STEM  degrees or a Masterâs Degree is highly preferred. Minimum of 5 years of general experience in the military or intelligence community is required. Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required. Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level  3/4 star officer or SES-3/4  is preferred; at least 3 years of experience with OUSD I  is highly preferred. Minimum of 3 years of experience using scripting  e.g. Python, R, VBA  or programming languages  e.g. Java, C++, Ruby  to deliver data engineering / software development services is required. Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required. Machine Learning  e.g. TensorFlow, PyTorch, Keras Data Visualization  e.g. Tableau, D3, Kibana Geospatial Analysis  e.g. ArcGIS, R Web Services  e.g. SOAP, RESTful Web Development  e.g. JavaScript, React.js, Node.js, HTML Database Development  e.g. MongoDB, PostgreSQL, MS-SQL Active TS/SCI security clearance.  Simultaneously support 2-3 ISR Analytic studies with analytic expertise, project-specific web pages, on-demand ETL and data analysis, and the development of web-enabled analytic tools and dashboards. Continuously supplement, improve, and maintain ISR Data Enrichment and Aggregation  IDEA , a JWICS-based, end-to-end automated ISR data capability, according to OUSD I  and stakeholder priorities and agile development principles. Continuously supplement, improve, and maintain a classified studies and analysis website including project repositories, analytic apps, and dynamic decision-support dashboards. Design, build, and/or insert new technology into extant classified development and production architecture baseline to supplement and improve analytic output, algorithmic performance, and user experience across websites and APIs. Design and develop custom scripts and tools on SIPRNET and JWICS to solve emergent analytic challenges and/or answer quick-turn or recurring senior executive questions about ISR performance and effectiveness. Develop custom algorithms to ingest and transform SIPRNET- and JWICS-derived data, artifacts, and information into analytic-ready data. Leverage advanced analytic techniques, including, but not limited to, geospatial analysis, regression analysis, machine learning, and natural language processing, to derive insight about ISR performance and effectiveness. Perform DevOps across JWICS and SIPR development architectures containing multiple database clusters, web servers, load balancers, and virtual machine instances, and optimize the deployment and maintenance pipelines for all architectural components in support of IDEA and Data Engineering efforts. Engage with the ISR community to understand analytic needs, raise awareness of ongoing work, seek feedback on in-progress innovations, and develop one-off analytics aids. Support technical briefings on analytic methodologies, decision-support dashboards, and capability innovations. The work environment for this position requires an individual to be able to  Work sitting or standing at a desk or conference table for extended periods of time with the ability to shift positions while working  sit, stand, pace, adjust positioning in any of those without issue Walk in the office to collaborate with co-workers, attend meetings or retrieve documents from printer Must be able to lift and carry up to 10 lbs.  Bachelorâs Degree from an accredited college or university is required; Bachelorâs degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics  STEM  degrees or a Masterâs Degree is highly preferred. Minimum of 5 years of general experience in the military or intelligence community is required. Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required. Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level  3/4 star officer or SES-3/4  is preferred; at least 3 years of experience with OUSD I  is highly preferred. Minimum of 3 years of experience using scripting  e.g. Python, R, VBA  or programming languages  e.g. Java, C++, Ruby  to deliver data engineering / software development services is required. Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required. Machine Learning  e.g. TensorFlow, PyTorch, Keras Data Visualization  e.g. Tableau, D3, Kibana Geospatial Analysis  e.g. ArcGIS, R Web Services  e.g. SOAP, RESTful Web Development  e.g. JavaScript, React.js, Node.js, HTML Database Development  e.g. MongoDB, PostgreSQL, MS-SQL Active TS/SCI security clearance.  Bachelorâs Degree from an accredited college or university is required; Bachelorâs degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics  STEM  degrees or a Masterâs Degree is highly preferred. Minimum of 5 years of general experience in the military or intelligence community is required. Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required. Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level  3/4 star officer or SES-3/4  is preferred; at least 3 years of experience with OUSD I  is highly preferred. Minimum of 3 years of experience using scripting  e.g. Python, R, VBA  or programming languages  e.g. Java, C++, Ruby  to deliver data engineering / software development services is required. Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required. Machine Learning  e.g. TensorFlow, PyTorch, Keras Data Visualization  e.g. Tableau, D3, Kibana Geospatial Analysis  e.g. ArcGIS, R Web Services  e.g. SOAP, RESTful Web Development  e.g. JavaScript, React.js, Node.js, HTML Database Development  e.g. MongoDB, PostgreSQL, MS-SQL Active TS/SCI security clearance. ","Bachelorâs Degree from an accredited college or university is required; degree in Operations Research other applicable Science, Technology, Engineering, and Mathematics STEM degrees a Masterâs highly preferred. Minimum of 5 years general experience the military intelligence community required. 3 applying data engineer software development skills to develop advanced analytic tools, deliver mission support services create visualization capabilities for analysis ISR systems working policy oversight organization led at senior executive-level 3/4 star officer SES-3/4 preferred; least with OUSD I using scripting e.g. Python, R, VBA programming languages Java, C++, Ruby engineering / one following technologies Machine Learning TensorFlow, PyTorch, Keras Data Visualization Tableau, D3, Kibana Geospatial Analysis ArcGIS, R Web Services SOAP, RESTful Development JavaScript, React.js, Node.js, HTML Database MongoDB, PostgreSQL, MS-SQL Active TS/SCI security clearance. Simultaneously 2-3 Analytic studies expertise, project-specific web pages, on-demand ETL analysis, web-enabled tools dashboards. Continuously supplement, improve, maintain Enrichment Aggregation IDEA , JWICS-based, end-to-end automated capability, according stakeholder priorities agile principles. classified website including project repositories, apps, dynamic decision-support Design, build, and/or insert new technology into extant production architecture baseline supplement improve output, algorithmic performance, user across websites APIs. Design custom scripts on SIPRNET JWICS solve emergent challenges answer quick-turn recurring executive questions about performance effectiveness. Develop algorithms ingest transform SIPRNET- JWICS-derived data, artifacts, information analytic-ready data. Leverage techniques, including, but not limited to, geospatial regression machine learning, natural language processing, derive insight Perform DevOps SIPR architectures containing multiple database clusters, servers, load balancers, virtual instances, optimize deployment maintenance pipelines all architectural components Engineering efforts. Engage understand needs, raise awareness ongoing work, seek feedback in-progress innovations, one-off analytics aids. Support technical briefings methodologies, dashboards, capability innovations. The work environment this position requires individual be able Work sitting standing desk conference table extended periods time ability shift positions while sit, stand, pace, adjust positioning any those without issue Walk office collaborate co-workers, attend meetings retrieve documents printer Must lift carry up 10 lbs.","Bachelorâs Degree accredited college university required; degree Operations Research applicable Science, Technology, Engineering, Mathematics STEM degrees Masterâs highly preferred. Minimum 5 years general experience military intelligence community required. 3 applying data engineer software development skills develop advanced analytic tools, deliver mission support services create visualization capabilities analysis ISR systems working policy oversight organization led senior executive-level 3/4 star officer SES-3/4 preferred; least OUSD I using scripting e.g. Python, R, VBA programming languages Java, C++, Ruby engineering / one following technologies Machine Learning TensorFlow, PyTorch, Keras Data Visualization Tableau, D3, Kibana Geospatial Analysis ArcGIS, R Web Services SOAP, RESTful Development JavaScript, React.js, Node.js, HTML Database MongoDB, PostgreSQL, MS-SQL Active TS/SCI security clearance. Simultaneously 2-3 Analytic studies expertise, project-specific web pages, on-demand ETL analysis, web-enabled tools dashboards. Continuously supplement, improve, maintain Enrichment Aggregation IDEA , JWICS-based, end-to-end automated capability, according stakeholder priorities agile principles. classified website including project repositories, apps, dynamic decision-support Design, build, and/or insert new technology extant production architecture baseline supplement improve output, algorithmic performance, user across websites APIs. Design custom scripts SIPRNET JWICS solve emergent challenges answer quick-turn recurring executive questions performance effectiveness. Develop algorithms ingest transform SIPRNET- JWICS-derived data, artifacts, information analytic-ready data. Leverage techniques, including, limited to, geospatial regression machine learning, natural language processing, derive insight Perform DevOps SIPR architectures containing multiple database clusters, servers, load balancers, virtual instances, optimize deployment maintenance pipelines architectural components Engineering efforts. Engage understand needs, raise awareness ongoing work, seek feedback in-progress innovations, one-off analytics aids. Support technical briefings methodologies, dashboards, capability innovations. The work environment position requires individual able Work sitting standing desk conference table extended periods time ability shift positions sit, stand, pace, adjust positioning without issue Walk office collaborate co-workers, attend meetings retrieve documents printer Must lift carry 10 lbs."
66,Data Engineer,Data Engineer,"Rockville, MD",Rockville,MD,"NET ESOLUTIONS CORPORATION (NETE) is a multi-award winning company founded in 1999. NETE is a full service Information Technology (IT) company dedicated to providing value focused services to the Federal Government and the Biomedical Research and Health IT Sector. NETE offers a collaborative working environment where growth is encouraged and nurtured. In addition, we offer competitive salaries that may include performance bonuses and a comprehensive benefits package.

Job Description

At NETE, data is important to us and to our customers. We process large volumes of data and transform it into information that powers decisions for thousands of researchers, scientists, and/or medical professionals. Our work has significant impact on the medical and scientific communities we serve. Your work here matters and has real impact. If you want to learn, grow, and help then this is the job for you.

NETE is seeking a highly motivated Data Engineer to work in a collaborative, intellectually-challenging environment on (i) an exciting research project on research evaluation in collaboration with a global corporation (ii) a second project developing a production database of linkages between biomedical research output such as scientific publications, patents, devices, therapeutics, and grant records at a major federal agency.
This is a 12 month position with no possibility for extension. OPT/CPT candidates are encouraged to apply!
The position incorporates elements of the roles of a data engineer, data wrangler, data scientist, and a junior data architect.
The position is not intended to be a training experience for candidates with weak skills.
The incumbent will, however, gain advanced skills and research experience eventually becoming competitive for elite positions in the national job market.
Responsibilities
Design and implement ETL strategies for diverse sets of structured and unstructured data.
Manage cloud-based Linux servers.
Analyze data to better understand their features and relationships between variables.
Work in a highly collaborative manner with members of a close-knit team while exhibiting independent and creative thought.
Job Requirements
Bachelor's degree in Computer Science or related discipline, e.g. Information Science, Statistics, or Mathematics. Master s degree is preferred.
Experience with data integration, harmonization, and curation is desired.
Must be competent with Python, SQL, and Linux.
Programming experience with Apache Spark, Java, C/C++ in an agile development environment is a plus.
Research experience is a plus.
Must have excellent oral and written communication skills and be able to work collaboratively.
Promising applicants will be tested for programming competence during the interview process.
Benefits
Paid Time Off (PTO)
9 Paid Federal holidays
Various wellness programs
Free parking at corporate offices
Employee Referral Bonus Program (ERBP)
Vision coverage through UHC national network
Dental coverage through UHC national network
401(K) with significant company match & no vesting period
Short and Long-Term Disability coverage (paid by company)
Competitive salaries with opportunity for performance bonuses
Discount plan for pet care, legal services, & identify theft protection
Basic Life and AD&D coverage (paid by company; option to purchase additional coverage)
Medical coverage through UHC national network (option to choose between 3 available plans)
Flexible Spending Accounts:
Healthcare (FSA)
Parking Reimbursement Account (PRK)
Dependent Care Assistant Program (DCAP)
Transportation Reimbursement Account (TRN)
NETE is a multi-award winning company as well as offers a collaborative working environment where growth is encouraged and nurtured. In addition, we offer competitive salaries that may include performance bonuses; and a comprehensive benefits package.

NETE uses E-Verify to validate all new hires' ability to legally work in the United States.

Disclaimer: The above description is intended to describe the general nature of work and level of effort being performed by individual s assigned to this position or job description. This is not to be construed as a complete or exhaustive list of all skills, responsibilities, duties, and/or assignments required. Individuals may be required to perform duties outside of their position, job description, or responsibilities as needed.","  Design and implement ETL strategies for diverse sets of structured and unstructured data. Manage cloud-based Linux servers. Analyze data to better understand their features and relationships between variables. Work in a highly collaborative manner with members of a close-knit team while exhibiting independent and creative thought.   Bachelor's degree in Computer Science or related discipline, e.g. Information Science, Statistics, or Mathematics. Master s degree is preferred. Experience with data integration, harmonization, and curation is desired. Must be competent with Python, SQL, and Linux. Programming experience with Apache Spark, Java, C/C++ in an agile development environment is a plus. Research experience is a plus. Must have excellent oral and written communication skills and be able to work collaboratively. Promising applicants will be tested for programming competence during the interview process. ","Design and implement ETL strategies for diverse sets of structured unstructured data. Manage cloud-based Linux servers. Analyze data to better understand their features relationships between variables. Work in a highly collaborative manner with members close-knit team while exhibiting independent creative thought. Bachelor's degree Computer Science or related discipline, e.g. Information Science, Statistics, Mathematics. Master s is preferred. Experience integration, harmonization, curation desired. Must be competent Python, SQL, Linux. Programming experience Apache Spark, Java, C/C++ an agile development environment plus. Research have excellent oral written communication skills able work collaboratively. Promising applicants will tested programming competence during the interview process.","Design implement ETL strategies diverse sets structured unstructured data. Manage cloud-based Linux servers. Analyze data better understand features relationships variables. Work highly collaborative manner members close-knit team exhibiting independent creative thought. Bachelor's degree Computer Science related discipline, e.g. Information Science, Statistics, Mathematics. Master preferred. Experience integration, harmonization, curation desired. Must competent Python, SQL, Linux. Programming experience Apache Spark, Java, C/C++ agile development environment plus. Research excellent oral written communication skills able work collaboratively. Promising applicants tested programming competence interview process."
67,Data Engineer,Database Engineer,"Dulles, VA 20101",Dulles,VA,"Database Engineer
Residency Status: ALL CANDIDATES MUST BE A U.S. CITIZEN
Clearance: A minimum of Active Top Secret with the ability to obtain SCI and DHS Suitability prior to starting employment
Time Type: Full-Time
Relocation Fees: No
Bonus: Yes
Company Overview:
Novel Applications of Vital Information Inc. (Novel Applications) is a premier technology services company that provides solutions in the areas of Cyber Security, Information Management, Systems Integration. Novel Applications is a business that combines experience, creativity, flexibility, pragmatism, and cost-effective solutions in order to deliver measurable business value to our clients.
Headquartered in Fredericksburg Virginia, Novel Applications employs engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do.
Novel Applications is an AA/EEO Employer - Minorities/Women/Veterans/Disabled.
Job Description:
NAVOI is seeking a Database Engineering Lead to work collaboratively with agile development teams in the design, development, and deployment of advanced cybersecurity capabilities.
Responsibilities Include:
Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system
Working with large structured and unstructured data sets
Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment
Design, setup, administer, and tune NoSQL databases in the AWS cloud
Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on a tradeoff between performance and quality
Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations
Write and refine code to ensure the quality and reliability of data extraction and processing
Analyze and resolve data performance and quality issues
Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.
Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance
Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates
Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to the development team
Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment
Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages
Maintain current industry knowledge of relevant concepts, practices, and procedures
Required Skills:
U.S. Citizenship
Must have an active Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearance
Must be able to obtain DHS Suitability
Demonstrated ability with diverse skill sets (e.g. data architects, data scientists, software developers)
Excellent understanding of big data and data analytics
Experience working with large structured and unstructured data sets
Development experience building ETL pipelines at scale
Solid SQL development skills
Experience with Linux/Unix tools and shell scripts
Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing, and encryption
Good communication skills, both oral and written
Must work well in a team environment as well as independently
Must exhibit good time management skills, independent decision making capability, and a focus on customer service.
Desired Skills:
Experience providing database engineering support to Intelligence, DoD, or DHS Customers
Understanding of Certification and Accreditation (ICD 503/DCID 6/3) processes as they apply to database technologies
Ability to support both SQL and NoSQL data management systems
Expertise in other RDBMS platforms such as Oracle RAC and SQL Server
Familiarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline
Experience with data transformation techniques such as aggregations, joins, and data cleaning
Experience with Red Hat Enterprise Linux (RHEL) operating system, storage configurations, network architecture, VMware, and/or related management tools
Database management experience of SQL databases such as MySQL and PostgreSQL in AWS cloud
Experience creating and managing NoSQL databases such as DynamoDB in the AWS cloud
Object mapping and migration of data from legacy structured and unstructured data sources to Amazon DynamoDB using AWS tools, custom code, or ETL scripts
Programming experience with languages such as R, Python, Java, JavaScript, JSON, etc.
Knowledge of Hadoop ecosystem, Map/Reduce, and data management products including Hbase, Hive, and Pig
DevSecOps and Continuous Integration / Continuous Delivery (CI/CD) knowledge
Experience or training in Six Sigma Methodology
ITIL knowledge and certification
Familiarity with SAFe (Scaled Agile Framework).
Required Education:
BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering. Two years of related work experience may be substituted for each year of degree-level education
Desired Certifications:
IBM Certified Data Engineer Big Data
Google Cloud Certified Professional Data Engineer
Cloudera Certified Professional Data Engineer
CCDH: Cloudera Certified Developer for Apache Hadoop
CCAH: Cloudera Certified Administrator for Apache Hadoop
CCSHB: Cloudera Certified Specialist in Apache HBase
CSSLP Certified Secure Software Lifecycle Professional
Certifications related to Scaled Agile Framework (SAFe) such as SAFe Practitioner (SP) or SAFe Program Consultant (SPC)
DoD 8570.1 IAT Level I"," U.S. Citizenship Must have an active Top Secret  TS  clearance. Must be able to obtain a TS/SCI clearance Must be able to obtain DHS Suitability Demonstrated ability with diverse skill sets  e.g. data architects, data scientists, software developers  Excellent understanding of big data and data analytics Experience working with large structured and unstructured data sets Development experience building ETL pipelines at scale Solid SQL development skills Experience with Linux/Unix tools and shell scripts Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing, and encryption Good communication skills, both oral and written Must work well in a team environment as well as independently Must exhibit good time management skills, independent decision making capability, and a focus on customer service.  Using database expertise to lead teams with diverse skill sets  e.g. data architects, data scientists, software developers  in support of a large, agile-based, cybersecurity system Working with large structured and unstructured data sets Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment Design, setup, administer, and tune NoSQL databases in the AWS cloud Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on a tradeoff between performance and quality Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations Write and refine code to ensure the quality and reliability of data extraction and processing Analyze and resolve data performance and quality issues Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc. Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to the development team Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages Maintain current industry knowledge of relevant concepts, practices, and procedures  BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering. Two years of related work experience may be substituted for each year of degree-level education  ","U.S. Citizenship Must have an active Top Secret TS clearance. be able to obtain a TS/SCI clearance DHS Suitability Demonstrated ability with diverse skill sets e.g. data architects, scientists, software developers Excellent understanding of big and analytics Experience working large structured unstructured Development experience building ETL pipelines at scale Solid SQL development skills Linux/Unix tools shell scripts Expertise in analysis design, modeling, master management, metadata warehousing, performance tuning, quality improvement, security, auditing, encryption Good communication skills, both oral written work well team environment as independently exhibit good time management independent decision making capability, focus on customer service. Using database expertise lead teams support large, agile-based, cybersecurity system Working Implementing systems from conception, through all stages development, deployment CI/CD agile Design, setup, administer, tune NoSQL databases the AWS cloud Design implement technical architecture necessary analytic statistical processing requirements based tradeoff between Performing transformations such aggregations, joins, cleaning applications visualizations Write refine code ensure reliability extraction Analyze resolve issues Make available programmers, other users using programming scripting languages R, Python, Java, JavaScript, etc. Perform business intelligence auditing functions governance Generate estimates performed, compare actuals, continue Work collaboratively teams, attending daily scrums providing related solutions Develop maintain libraries, models, SOPs, documentation collaboration Migrate legacy RDBMSs sources complex processes, tools, Maintain current industry knowledge relevant concepts, practices, procedures BS Computer Science, Engineering, Information Systems, OR Systems Engineering. Two years may substituted for each year degree-level education","U.S. Citizenship Must active Top Secret TS clearance. able obtain TS/SCI clearance DHS Suitability Demonstrated ability diverse skill sets e.g. data architects, scientists, software developers Excellent understanding big analytics Experience working large structured unstructured Development experience building ETL pipelines scale Solid SQL development skills Linux/Unix tools shell scripts Expertise analysis design, modeling, master management, metadata warehousing, performance tuning, quality improvement, security, auditing, encryption Good communication skills, oral written work well team environment independently exhibit good time management independent decision making capability, focus customer service. Using database expertise lead teams support large, agile-based, cybersecurity system Working Implementing systems conception, stages development, deployment CI/CD agile Design, setup, administer, tune NoSQL databases AWS cloud Design implement technical architecture necessary analytic statistical processing requirements based tradeoff Performing transformations aggregations, joins, cleaning applications visualizations Write refine code ensure reliability extraction Analyze resolve issues Make available programmers, users using programming scripting languages R, Python, Java, JavaScript, etc. Perform business intelligence auditing functions governance Generate estimates performed, compare actuals, continue Work collaboratively teams, attending daily scrums providing related solutions Develop maintain libraries, models, SOPs, documentation collaboration Migrate legacy RDBMSs sources complex processes, tools, Maintain current industry knowledge relevant concepts, practices, procedures BS Computer Science, Engineering, Information Systems, OR Systems Engineering. Two years may substituted year degree-level education"
68,Data Engineer,Data Engineer,"Springfield, VA",Springfield,VA,"This position requires a Top Secret security clearance and eligibility for access to Sensitive Compartmented Information (TS/SCI)

Desired Experience:
At least 5 years of experience with Big Data systems, including Hadoop and Cloudera
At least 2 years of experience with the design, implementation, or consulting for applications deployed across multiple organizations or a technical environment
Experience with multiple operating systems, including Linux-, UNIX-, and Windows-based
- Experience running, enhancing, and significantly upgrading and modifying Hadoop- and Cloudera-based environments for support to a wide variety of querying approaches
Experience establishing continuous integration and continuous deployment processes for applications and environments
Experience with extract, transform, and load (ETL) processes
Experience with multiple database technologies


Desired Education: Bachelors of Art (BA) or Bachelors of Science (BS), preferably in a related field.

Required Certifications:Security+ or CISSP Certification required

Work Description:
The Data Engineer is responsible for the configuration and ingestion of structured, unstructured, and semi-structured data repositories. Their work is focused on turning these data repositories into effective resources that satisfy mission requirements and that support a data analytics and rapid development pipeline. They maintain all operational aspects of data transfers, accounting for the security posture of the underlying infrastructure and the systems and applications that are supported, and they monitor the health of the environment through a variety of health tracking capabilities.

The Data Engineer also automates configuration management using NiFi and other tools and they stay current on data extraction, transfer, and loading (ETL) technologies and services. They should be able to work under general guidance, demonstrate the initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.

The Data Engineer applies their specialized knowledge of data and leverages their expertise to structure and retrieve data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically. Lastly, they engage with multiple functional groups to understand client challenges, prototype new ideas and new technologies, help create solutions to drive innovation, and they design, implement, schedule, test, and deploy full features and components of solutions.

Additional Desired Qualifications:
Experience with Amazon Web Services (AWS), Microsoft Azure, or MilCloud 2.0
Experience applying DoD Security Technical Implementation Guides (STIGs) and automating that process
Experience with storage fundamentals, including CIFS and NFS and backup and disaster recovery processes a plus
Experience configuring and aggregating logs for data analysis using Splunk or ELK solutions
Experience maintaining data transfer systems, including NIFI
Experience with two or more programming languages, including C#, Java, .NET, or similar
Knowledge of FedRAMP, RMF, and the implications of C&A and SA&A in a DoD environment a plus
Any industry-recognized Cloud Certifications preferred","Experience with Amazon Web Services  AWS , Microsoft Azure, or MilCloud 2.0   Experience with Amazon Web Services  AWS , Microsoft Azure, or MilCloud 2.0 ","Experience with Amazon Web Services AWS , Microsoft Azure, or MilCloud 2.0","Experience Amazon Web Services AWS , Microsoft Azure, MilCloud 2.0"
69,Data Engineer,Full-Stack Data Engineer-TS/SCI,"Reston, VA 20191",Reston,VA,"Introduction
As a Data Scientist at IBM, you will help transform our clientsâ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether itâs investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.

Your Role and Responsibilities
IBM Global Business Services (GBS) is a team of business, strategy and technology consultants enabling enterprises to make smarter decisions and providing unparalleled client and consumer experiences in cognitive, data analytics, cloud technology and mobile app development. IBM GBS empowers clients to digitally reinvent their business and get the competitive edge in the cognitive era in over 170 countries.

Bottom line? We outthink ordinary. Discover what you can do at IBM.

We are seeking a full-stack data engineer to join IBM in support of a growing Advanced Analytics effort for the US Navy. The Full-Stack Data Engineer will primarily provide React and Node support. There may be additional work in database design, data manipulations, tailored script / algorithm development, and data visualization. The role is in the Washington DC area (Reston and/or Arlington, Virginia).

BENEFITS
Health Insurance. Paid time off. Corporate Holidays. Sick leave. Family planning. Financial Guidance. Competitive 401K. Training and Learning. We continue to expand our benefits and programs, offering some of the best support, guidance and coverage for a diverse employee population.
http://www-01.ibm.com/employment/us/benefits/
https://www-03.ibm.com/press/us/en/pressrelease/50744.wss
CAREER GROWTH
Our goal is to be essential to the world, which starts with our people. Company wide we kicked off an internal talent strategy program called Go Organic. At our core, we are committed to believing and investing in our workforce through:
Skill development: helping our employees grow their foundational skills
Finding the dream job at IBM: navigating our company with the potential for many careers by channeling an employeeâs strengths and career aspirations
Diversity of people: Diversity of thought driving collective innovation
In 2015, Go Organic filled approximately 50% of our open positions with internal talent that were promoted into the role.

CORPORATE CITIZENSHIP
With an employee population of 375,000 in over 170 countries, amazingly we connect, collaborate, and care. IBMers drive a corporate culture of shared responsibility. We love grand challenges and everyday improvements for our company and for the world. We care about each other, our clients, and the communities we live, work, and play in!
http://www.ibm.com/ibm/responsibility/initiatives.html
http://www.ibm.com/ibm/responsibility/corporateservicecorps

Required Professional and Technical Expertise
US Citizen with active TS//SCI clearance or show immediate eligibility according to JPAS security database
5+ years of React and Node development and maintenance.
Demonstrable skills in Node/JavaScript, SQL/NoSQL in a Windows or Linux desktop operating environment
Data visualization skills using JavaScript (such as ChartJS or D3)
Technical skills in MVC frameworks and Web API design
Must possess the ability to communicate clearly, concisely, and with technical accuracy in both oral and written modes.
Must be able to work effectively under time constraints and potentially changing priorities, while maintaining a high level of attention to detail.
Must be able to work in a collaborative, team environment, including on government site where availability of advanced development/database tools may be limited
Bachelors Degree in Engineering, Mathematics, Operations Research, Computer Science, or related technical field

Preferred Professional and Technical Expertise
Masters Degree, preferably in Engineering, Mathematics, Operations Research, Statistics, Computer Science, or related
Experience with React and Node in a military setting
Experience with Python and the ETL process
Experience with identifying and gathering research and analysis data from DOD organizations and systems
Experience with Navy research and analysis databases

About Business Unit
IBM Services is a team of business, strategy and technology consultants that design, build, and run foundational systems and services that is the backbone of the world's economy. IBM Services partners with the world's leading companies in over 170 countries to build smarter businesses by reimagining and reinventing through technology, with its outcome-focused methodologies, industry-leading portfolio and world class research and operations expertise leading to results-driven innovation and enduring excellence.

Your Life @ IBM
What matters to you when youâre looking for your next career challenge?

Maybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities â where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust â where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.

Impact. Inclusion. Infinite Experiences. Do your best work ever.

About IBM
IBMâs greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.

Location Statement
For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBM
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",  http //www-01.ibm.com/employment/us/benefits/ https //www-03.ibm.com/press/us/en/pressrelease/50744.wss  ,http //www-01.ibm.com/employment/us/benefits/ https //www-03.ibm.com/press/us/en/pressrelease/50744.wss,http //www-01.ibm.com/employment/us/benefits/ https //www-03.ibm.com/press/us/en/pressrelease/50744.wss
70,Data Engineer,Data Engineer,"Herndon, VA 20171",Herndon,VA,"Secure our Nation, Ignite your Future
This position will report to the Group CTO and be part of the technology office staff. You will be supporting an internal big data analytics application that will be hosted on a cloud platform. You will have prior experience working in an agile/DevOps environment. You will have the opportunity to work with some of the newest technologies, transformative projects, and play a strong role in shaping the innovation agenda across the company.

Essential Job Duties:
Provide database design, development and implementation support
Participate in daily scrum meetings and support development team
Work with customers and team members in an Agile development environment
Follow Continuous Integration/Continuous Delivery (CI/CD) best practices for code build and deployments
Develop PL/SQL, stored procedures, ETL scrips to support this analytics application
Document database design and develop optimum data ingest techniques from multiple data sources
Minimum Requirements:
Minimum Requirements: Bachelorâs Degree in Computer Science, IT or related field & minimum 5-7 years of QA experience
Minimum experience of 5 years with relational databases such as Oracle, SQL Server, Sybase, RedShift
Minimum 3+ years of experience with ETL technologies
Additional skills:

Experience using Jira, Git, Confluence, Knowledge and understanding of big data technologies like Hadoop, Hive, etc., Ability to handle stress and work well under pressure, Ability to use MS Office, Experience with cloud technologies, Analytical and Critical Thinking Skills, Interpersonal and People Skills, Leadership Skills, Listening Skills, Multitasking Ability, Oral and Written Communication Skills, Organizational Skills
ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.","    Minimum Requirements  Bachelorâs Degree in Computer Science, IT or related field & minimum 5-7 years of QA experience Minimum experience of 5 years with relational databases such as Oracle, SQL Server, Sybase, RedShift Minimum 3+ years of experience with ETL technologies ","Minimum Requirements Bachelorâs Degree in Computer Science, IT or related field & minimum 5-7 years of QA experience 5 with relational databases such as Oracle, SQL Server, Sybase, RedShift 3+ ETL technologies","Minimum Requirements Bachelorâs Degree Computer Science, IT related field & minimum 5-7 years QA experience 5 relational databases Oracle, SQL Server, Sybase, RedShift 3+ ETL technologies"
71,Data Engineer,Data Engineer,"Reston, VA",Reston,VA,"Data Engineer
Reston, VA
We seek an experienced Data Engineer with the skills, energy and business acumen to excel on our aviation Data Engineering Team. Our team performs data acquisition and ingestion, processing, and data delivery across a variety of products.
Your contribution will be critical in many areas such as cleansing, consolidating, and normalizing industry data, and then synthesizing valuable data sets to be served directly to clients or loaded into a data warehouse that supports our online analytics tools. You will support industry-leading Cirium products and help develop the next generation of Cirium Business Intelligence tools for the aviation industry.

Key Accountabilities and Responsibilities:
Maintaining, improving, and executing existing scripts written in SAS, SQL, and Python. Most of the codebase is currently in SAS but the future will contain more Java, and possibly Python or R.
Designing new scripts using the above tools to ingest, cleanse, consolidate, analyze, and summarize the incoming data. You will also be implementing and tuning algorithms and business rules, quality-checking the data results, and working iteratively with evolving requirements.
Meeting delivery deadlines for new products, features, and enhancements.
Implementing and delivering large historical data solutions to new and existing customers.
Coding new applications for ad hoc reporting and for data research inquiries.
Investigating issues with data quality and responding to stakeholdersâ technical questions.
Identifying opportunities to complement, enhance and/or optimize our data processing environment with new tools and techniques.
Provide on-call support on rotation basis, occasionally during afterhours and weekends.
Qualifications:
3+ years of experience with SAS programming in a business environment, particularly with data modeling, or in a âbig dataâ context.
1 year of Java experience
Well qualified with SQL.
BS or MS in any quantitative field (computer science, systems engineering, mathematics, economics, statistics, etc.) or equivalent work experience.
Key Skills Required:
Solid experience with SAS data processing, macro programming and running SQL queries within SAS (PROC SQL and SQL pass-through to Vertica.)
Our planned Data Lake will make use of new tools including AWS Glue, AWS Athena, Spark, etc. Knowledge of these tools or the interest and desire to learn them.
Familiarity with Linux, GIT and AWS Console.
Able to implement complex algorithms which transform, cleanse, impute, and mash up data.
Passion for data processing, data modeling, data mining and tackling complex operations.
Professional experience working in an Agile environment.
Professional experience working with a source code version control system.
Proficiency with Microsoft Excel.
English fluency, both spoken and written. Able to discuss complex technical subjects with clarity and precision.
Optional Skills Preferred:
Knowledge of advanced SAS programming techniques and efficiencies.
Familiar with Linux command line and shell scripting.
Knowledge of passenger aviation data (ticket data, DOT data, schedules), QSI scoring models.
Knowledge of airline terminology and airline business metrics.
Experience with Kanban.
Knowledge of GIT source control, particularly from the Linux command line.
Knowledge of R and / or Python for data processing and machine learning.
Please note: This is a regular, full-time position which requires working out of our office located in downtown Reston, VA.

About Cirium

Cirium provides data services and end-to-end data solutions to customers serving the global travel industry. The company has established a leadership position as a provider of real-time global flight information, serving airlines and airports, travel agencies, developers, consumers, and more. The company is leveraging the platform and domain knowledge it has developed to expand into new data sets and new products that deliver value to the companyâs core markets. We work in an interesting field, we have demonstrated success in what we do, and we are well positioned for future growth.

Our Environment: We hire the best talent in the travel and technology industries. To support our talented team, we offer an extraordinary work environment that places trust and respect at the forefront of our company values. These values enable our employees to do their best work by creating an open, supportive environment that promotes creativity. We think it's a great place to do great work, and if you like the sound of this as well, we encourage you to consider this opening.

Cirium is a branded division of Reed Business Information (RBI). RBI provides information, analytics and data to business professionals worldwide. Our strong global products and services hold market-leading positions across a wide range of industry sectors including banking, petrochemicals and aviation where we help customers make key strategic decisions every day. RBI is part of RELX Group plc, a world-leading provider of information solutions for professional customers across industries.
http://www.reedbusiness.com

RBI is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact 1.877.734.1938 or accommodations@relx.com."," 3+ years of experience with SAS programming in a business environment, particularly with data modeling, or in a âbig dataâ context. 1 year of Java experience Well qualified with SQL. BS or MS in any quantitative field  computer science, systems engineering, mathematics, economics, statistics, etc.  or equivalent work experience. Solid experience with SAS data processing, macro programming and running SQL queries within SAS  PROC SQL and SQL pass-through to Vertica.  Our planned Data Lake will make use of new tools including AWS Glue, AWS Athena, Spark, etc. Knowledge of these tools or the interest and desire to learn them. Familiarity with Linux, GIT and AWS Console. Able to implement complex algorithms which transform, cleanse, impute, and mash up data. Passion for data processing, data modeling, data mining and tackling complex operations. Professional experience working in an Agile environment. Professional experience working with a source code version control system. Proficiency with Microsoft Excel. English fluency, both spoken and written. Able to discuss complex technical subjects with clarity and precision. Maintaining, improving, and executing existing scripts written in SAS, SQL, and Python. Most of the codebase is currently in SAS but the future will contain more Java, and possibly Python or R. Designing new scripts using the above tools to ingest, cleanse, consolidate, analyze, and summarize the incoming data. You will also be implementing and tuning algorithms and business rules, quality-checking the data results, and working iteratively with evolving requirements. Meeting delivery deadlines for new products, features, and enhancements. Implementing and delivering large historical data solutions to new and existing customers. Coding new applications for ad hoc reporting and for data research inquiries. Investigating issues with data quality and responding to stakeholdersâ technical questions. Identifying opportunities to complement, enhance and/or optimize our data processing environment with new tools and techniques. Provide on-call support on rotation basis, occasionally during afterhours and weekends.  ","3+ years of experience with SAS programming in a business environment, particularly data modeling, or âbig dataâ context. 1 year Java Well qualified SQL. BS MS any quantitative field computer science, systems engineering, mathematics, economics, statistics, etc. equivalent work experience. Solid processing, macro and running SQL queries within PROC pass-through to Vertica. Our planned Data Lake will make use new tools including AWS Glue, Athena, Spark, Knowledge these the interest desire learn them. Familiarity Linux, GIT Console. Able implement complex algorithms which transform, cleanse, impute, mash up data. Passion for mining tackling operations. Professional working an Agile environment. source code version control system. Proficiency Microsoft Excel. English fluency, both spoken written. discuss technical subjects clarity precision. Maintaining, improving, executing existing scripts written SAS, SQL, Python. Most codebase is currently but future contain more Java, possibly Python R. Designing using above ingest, consolidate, analyze, summarize incoming You also be implementing tuning rules, quality-checking results, iteratively evolving requirements. Meeting delivery deadlines products, features, enhancements. Implementing delivering large historical solutions customers. Coding applications ad hoc reporting research inquiries. Investigating issues quality responding stakeholdersâ questions. Identifying opportunities complement, enhance and/or optimize our processing environment techniques. Provide on-call support on rotation basis, occasionally during afterhours weekends.","3+ years experience SAS programming business environment, particularly data modeling, âbig dataâ context. 1 year Java Well qualified SQL. BS MS quantitative field computer science, systems engineering, mathematics, economics, statistics, etc. equivalent work experience. Solid processing, macro running SQL queries within PROC pass-through Vertica. Our planned Data Lake make use new tools including AWS Glue, Athena, Spark, Knowledge interest desire learn them. Familiarity Linux, GIT Console. Able implement complex algorithms transform, cleanse, impute, mash data. Passion mining tackling operations. Professional working Agile environment. source code version control system. Proficiency Microsoft Excel. English fluency, spoken written. discuss technical subjects clarity precision. Maintaining, improving, executing existing scripts written SAS, SQL, Python. Most codebase currently future contain Java, possibly Python R. Designing using ingest, consolidate, analyze, summarize incoming You also implementing tuning rules, quality-checking results, iteratively evolving requirements. Meeting delivery deadlines products, features, enhancements. Implementing delivering large historical solutions customers. Coding applications ad hoc reporting research inquiries. Investigating issues quality responding stakeholdersâ questions. Identifying opportunities complement, enhance and/or optimize processing environment techniques. Provide on-call support rotation basis, occasionally afterhours weekends."
72,Data Engineer,Senior Data Engineer,"Washington, DC",Washington,DC,"The Senior Data Engineer responsibilities include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of our client's Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatment, etc .

Required Education/Experience:
Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.

Preferred:
Experience in Healthcare or related industryExperience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plusExperience productizing/automating predictive models that use R, SAS, Python, SPSS, etc.Continuous delivery and deployment automation for analytic solutions using tools like BambooFamiliarity with test driven development methodology for analytic solutionsAGILEAPI developmentData visualization and/or dashboard developmentDemonstrated ability to achieve stretch goals in a highly innovative and fast-paced environment","   Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five  5  years technology industry or related experience, including items such as Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive  5  years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool. ","Bachelor's Degree in computer science or related field, equivalent combination of education and experience/technical training that demonstrates analytical technical competencyMinimum five 5 years technology industry experience, including items such as Build highly scalable, scaled-out architectures on large scale database platformsExperience working a complex data infrastructure environmentFive experience engineering roleExtensive depth pipeline development with standard integration toolsExperience SDLC process requirements gathering, analysis, architecture, design, implementation, testing, deployment support.Experience any tool for Source Control Project ManagementExperience writing test cases scripts quality assuranceExperience creating stored procedures functionsExperience developing dimensional model tool.","Bachelor's Degree computer science related field, equivalent combination education experience/technical training demonstrates analytical technical competencyMinimum five 5 years technology industry experience, including items Build highly scalable, scaled-out architectures large scale database platformsExperience working complex data infrastructure environmentFive experience engineering roleExtensive depth pipeline development standard integration toolsExperience SDLC process requirements gathering, analysis, architecture, design, implementation, testing, deployment support.Experience tool Source Control Project ManagementExperience writing test cases scripts quality assuranceExperience creating stored procedures functionsExperience developing dimensional model tool."
73,Data Engineer,Consultant - Data Engineer,"Rolling Meadows, IL",Rolling Meadows,IL,"Insygnum needs a Consultant - Data Engineer to help our clients for data analysis, data integration and data quality. Our Chicago-based team is small but growing fast and we need to complement our in-house experts who knows how to tame challenging data. This is a unique opportunity to not only work with cool technology, but also to create a new methodologies and techniques. You'll get in on the ground floor of a new company, help shape its future, and benefit directly from your work.
Why work here
Joining insygnum now offers several unique opportunities
You will receive competitive salary, benefits, and stock options
You will be working on hard, interesting problems
You will help shape the culture of the company as we grow
You will have the opportunity to apply your skills in a meaningful way and have a real-world impact
Responsibilities
You'll help design a new system for capturing, storing, analyzing, and acting on performance, security, and network data. The ideal candidate will have a solid grasp of several different database and data warehousing technologies to help architect ETL for. Technologies to be used may include some combination of relational databases (PostgreSQL, Teradata, Aster, HANA), NoSQL, Hadoop, Object-based stores, and OLAP.
Specific responsibilities include:
Help design an architecture for federated data stores and data fusion
Help design methods for storing data in a way that facilitates extremely fast data parsing and management
Implement ""glue code"" that connects middle tier components with backend components
Implement data management and analytics code utilizing data architecture (e.g. map reduce)
Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores
Collaborate with data architects to understand the applications we integrate with and the data they produce
Review requirements for new approaches to big data storage and analytics
Design methods for caching, paging, and integrating real-time data with historical data stores
Desired Skills and Experience
Requirements
Development knowledge for integrating components and contributing to core code base - Java preferred
Solid understanding of database and data warehousing technologies
Knowledge of SQL as well as NoSQL queries, syntax, and technologies
Knowledge of big data requirements, applications, and technologies such as Hadoop
Knowledge of ETL methods and approaches including triggers, named views, temporary tables, etc.
Linux expertise
Bonus Points
Java is strongly preferred (e.g. for working with map reduce) but not ultimately a requirement if you excel in other areas
Strong SQL skills are highly desirable
OLAP experience
Experience with ETL tools like Informatica, Boomi, Pentaho, AbIntio, Datastage, etc.,
Contact: HR Manager
Email: hr@insygnum.com
Phone: (224)-800-1002"," Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Knowledge of SQL as well as NoSQL queries, syntax, and technologies Knowledge of big data requirements, applications, and technologies such as Hadoop Knowledge of ETL methods and approaches including triggers, named views, temporary tables, etc. Linux expertise  Specific responsibilities include  Help design an architecture for federated data stores and data fusion Help design methods for storing data in a way that facilitates extremely fast data parsing and management Implement ""glue code"" that connects middle tier components with backend components Implement data management and analytics code utilizing data architecture  e.g. map reduce  Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores Collaborate with data architects to understand the applications we integrate with and the data they produce Review requirements for new approaches to big data storage and analytics Design methods for caching, paging, and integrating real-time data with historical data stores   Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Knowledge of SQL as well as NoSQL queries, syntax, and technologies Knowledge of big data requirements, applications, and technologies such as Hadoop Knowledge of ETL methods and approaches including triggers, named views, temporary tables, etc. Linux expertise ","Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database data warehousing technologies Knowledge SQL as well NoSQL queries, syntax, big requirements, applications, such Hadoop ETL methods approaches including triggers, named views, temporary tables, etc. Linux expertise Specific responsibilities include Help design an architecture federated stores fusion storing in a way that facilitates extremely fast parsing management Implement ""glue code"" connects middle tier with backend analytics utilizing e.g. map reduce Collaborate machine learning folks determine how analyze various sets set up querying architects understand the applications we integrate they produce Review requirements new storage Design caching, paging, real-time historical","Development knowledge integrating components contributing core code base - Java preferred Solid understanding database data warehousing technologies Knowledge SQL well NoSQL queries, syntax, big requirements, applications, Hadoop ETL methods approaches including triggers, named views, temporary tables, etc. Linux expertise Specific responsibilities include Help design architecture federated stores fusion storing way facilitates extremely fast parsing management Implement ""glue code"" connects middle tier backend analytics utilizing e.g. map reduce Collaborate machine learning folks determine analyze various sets set querying architects understand applications integrate produce Review requirements new storage Design caching, paging, real-time historical"
74,Data Engineer,Google Data Engineer,"Chicago, IL",Chicago,IL,"Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet todayâs high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps â Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform. Multi-cloud experience a plus.   Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP DevOps an platform. Multi-cloud a plus. Proven ability to build, manage and foster team-oriented environment work creatively analytically in problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","Minimum 3 years previous Consulting client service delivery experience Google GCP DevOps platform. Multi-cloud plus. Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
75,Data Engineer,Data Engineer,"Chicago, IL",Chicago,IL,"At Otus, a fast-growing EdTech company based at 1K Fulton in the West Loop of Chicago, you will work with people passionate about improving the lives of teachers and students. We are a group of talented designers, developers, coaches, and leaders. We love our work and strive to do our best each day.

On our team, you will find musicians, beer enthusiasts, designer toy collectors, table tennis fanatics, and more.


Your Role
Otus is a building a next generation Platform for EdTech in order to support our transformative vision for K-12 education across the US
Model and architect our data in a way that will scale with the increasingly complex ways weâre analyzing it
Build robust pipelines that make sure data is where it needs to be, when it needs to be there
Build frameworks and tools to help others design and build their own data pipelines in a self-service manner
Performance testing and engineering to ensure that our systems always scale to meet our needs
Key member of the team focused on pure hands-on contribution to the implementation and operation of our data platform


Qualifications
Breadth - Substantial experience as a high-performance Engineer across multiple environments on high-performance data systems that process data across various sources at a near real-time pace
At least 1 project where you personally designed, implemented, and operated large scale, high throughput data pipelines with a focus on high data quality
At least 1 other project where you built a secure, reliable, scalable system on AWS that saw significant traffic
Depth - You can go up and down the stack from deep in the infrastructure up to the data, application, and client layers
Speed - Experience with small teams that move fast - all members are expected to be able to achieve maximum results with minimal direction
Ownership and accountability - You own the things that you build
Drive - When you see a need, you fill a need. You can step in where you see a need and push us all forward
Modern Infra - Hands-on experience with Kubernetes, Airflow, and Kafka
Modern CI/CD - Hands-on experience with Codefresh, Spinnaker, Jenkins or similar


Benefits and Perks
Competitive salary and stock options
20 PTO days a year, plus a day off for your birthday (or a loved one), an interest day (to do something you love) and paid holidays
Up to 12 weeks of parental leave, and great work/life balance
Flexible hours and work from home policy so you can be at your most productive
Excellent medical, dental, and vision insurance
Life Insurance and disability benefits
401K with an employer match (up to 4%)
Working in arguably the best neighborhood in the city with the excellent food, drinks, coffee and donuts all within walking distance
Working in an amazing office building with gym, rooftop deck, frequent social events, shuttle to and from train stations
Otus is an Equal Opportunity Employer and embraces diversity of every kind. You must be legally authorized to work in the US. Unfortunately, the company is unable to support sponsorships at this time.","Breadth - Substantial experience as a high-performance Engineer across multiple environments on high-performance data systems that process data across various sources at a near real-time pace At least 1 project where you personally designed, implemented, and operated large scale, high throughput data pipelines with a focus on high data quality At least 1 other project where you built a secure, reliable, scalable system on AWS that saw significant traffic Depth - You can go up and down the stack from deep in the infrastructure up to the data, application, and client layers Speed - Experience with small teams that move fast - all members are expected to be able to achieve maximum results with minimal direction Ownership and accountability - You own the things that you build Drive - When you see a need, you fill a need. You can step in where you see a need and push us all forward Modern Infra - Hands-on experience with Kubernetes, Airflow, and Kafka Modern CI/CD - Hands-on experience with Codefresh, Spinnaker, Jenkins or similar     ","Breadth - Substantial experience as a high-performance Engineer across multiple environments on data systems that process various sources at near real-time pace At least 1 project where you personally designed, implemented, and operated large scale, high throughput pipelines with focus quality other built secure, reliable, scalable system AWS saw significant traffic Depth You can go up down the stack from deep in infrastructure to data, application, client layers Speed Experience small teams move fast all members are expected be able achieve maximum results minimal direction Ownership accountability own things build Drive When see need, fill need. step need push us forward Modern Infra Hands-on Kubernetes, Airflow, Kafka CI/CD Codefresh, Spinnaker, Jenkins or similar","Breadth - Substantial experience high-performance Engineer across multiple environments data systems process various sources near real-time pace At least 1 project personally designed, implemented, operated large scale, high throughput pipelines focus quality built secure, reliable, scalable system AWS saw significant traffic Depth You go stack deep infrastructure data, application, client layers Speed Experience small teams move fast members expected able achieve maximum results minimal direction Ownership accountability things build Drive When see need, fill need. step need push us forward Modern Infra Hands-on Kubernetes, Airflow, Kafka CI/CD Codefresh, Spinnaker, Jenkins similar"
76,Data Engineer,Sr. Data Engineer,"Chicago, IL",Chicago,IL,"Job Title: Data Engineer

Location: San Francisco, Chicago, San Jose, Palo Alto, Austin, TX

Terms: Full-time, Contract, Contract-2-Hire

About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.

What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.

As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do

Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:

Cloud
Analytics
Digitization
Infrastructure
Security

Sr. Data Engineer
Job Description
Responsibilities
Ingestion of data from multiple, unstructured sources using multiple analytics tools
Implementing ETL process
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies

Requirements
3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct query performance tuning
Strong skills in one of the scripting language (Python, Ruby, Bash)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)

We are Growing Rapidly: 2019 Highlights

Trianz is growing rapidly. Here are some highlights.

Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.

Won the âCustomer Obsession Awardâ from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.

Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.

Featured by IDC in their Spotlight series under the theme of âOperationalizing Strategies through Execution Excellence: A New Paradigms in Technology Deliveryâ.

Achieved 50%+ revenue and employee growth compared to prior yearâs exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.

Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is whatâs fundamental for everyone at Trianz.
 We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
 Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).","   Ingestion of data from multiple, unstructured sources using multiple analytics tools Implementing ETL process Monitoring performance and advising any necessary infrastructure changes Defining data retention policies   Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.","Ingestion of data from multiple, unstructured sources using multiple analytics tools Implementing ETL process Monitoring performance and advising any necessary infrastructure changes Defining retention policies Voted significantly above other services firms by 90% + clients for business impact, execution predictability organizational commitment in the recent Trianz wide client satisfaction survey.","Ingestion data multiple, unstructured sources using multiple analytics tools Implementing ETL process Monitoring performance advising necessary infrastructure changes Defining retention policies Voted significantly services firms 90% + clients business impact, execution predictability organizational commitment recent Trianz wide client satisfaction survey."
77,Data Engineer,Data Engineer,"Chicago, IL 60654",Chicago,IL,"Chicago, IL

Data Engineering Full-Time
Company Description

rMark Bio helps life science companies solve for the complexities that come with digital transformation by developing end-to-end AI solutions that deliver personalized business intelligence through integrated applications and API accessible services.

Healthcare innovation is best served when individuals with diverse backgrounds come together with a common purpose and clear objectives to improve patient lives.
We are product strategists, engineers, data scientists and designers who are experts in our domain and passionate about our mission to accelerate innovation, collaboration and scientific discovery for life sciences
Job Description

rMark Bio, is searching for an experienced Data Engineer to work within an agile team of data scientists, software architects and developers to implement and maintain secure, scalable, cloud-based software solutions, build customer integration solutions, and streamline the teamâs software delivery tools and processes. Technical aptitude is a must as well as the right team-minded attitude and the ability to work interchangeably with others. The core team is as a small SWAT-style team with everyone pulling their own weight, playing a variety of roles, and covering each othersâ responsibilities when needed. Applicants should be qualified in collecting, cleaning, and maintaining large datasets that are critical to customer and product success.
Job Responsibilities

Build automated ETL pipelines for cloud environments including, AWS, GCP, Azure, and Heroku.
Develop and support data integrity reporting and alerting for ETL pipelines.
Ability to work closely with Engineering, Product and Customer Success Teams.
Build robust and deployable software in Python, C++ or C#.
Build, deploy, and maintain RESTful APIs to access datasets.
Parse and extract data from common formats including, XML, JSON, CSV, and Pipe delimited.
Integrate with customer provided APIs.
Build, organize, and maintain datamarts using any of SQL, JSON, Blob, or other databases as needed.
Write and maintain excellent documentation of all work.
Experience and Qualifications

Minimum 3-5 years of experience is required
B.S. in Computer Science or closely related field preferred, but not required. Real-world experience and proven track records count as much, if not more.
Programming Languages: Python and C# (C++ and R are a plus).
Experience with databricks is a plus.
Established expertise developing ETL pipelines on serverless cloud solutions. (AWS Lambda, Azure App Services, etc..)
Linux/Unix
Docker
Databases: SQL, JSON, object storage, etc. Experience with graph databases like Neo4j and TigerGraph is a plus.
Strong technical writing skills will be heavily stressed
If you are a recruiter or placement agency, please do not submit resumes to any person or email address at rMark Bio prior to having a signed agreement from rMark Bioâs HR department. rMark Bio is not liable for and will not pay placement fees for candidates submitted by any agency other than its prior-approved recruitment partners. Furthermore, any resumes sent to us without a written signed agreement in place will be considered your companyâs gift to rMark Bio. and may be forwarded to our recruiters for their attention. Thank you.

rMark Bio is an equal opportunity employer. All qualified applicants for employment will be considered without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, status as an individual with a disability, veteran status, or any other basis protected by federal, state, or local law."," Minimum 3-5 years of experience is required B.S. in Computer Science or closely related field preferred, but not required. Real-world experience and proven track records count as much, if not more. Programming Languages  Python and C   C++ and R are a plus . Experience with databricks is a plus. Established expertise developing ETL pipelines on serverless cloud solutions.  AWS Lambda, Azure App Services, etc..  Linux/Unix Docker Databases  SQL, JSON, object storage, etc. Experience with graph databases like Neo4j and TigerGraph is a plus. Strong technical writing skills will be heavily stressed    Build automated ETL pipelines for cloud environments including, AWS, GCP, Azure, and Heroku. Develop and support data integrity reporting and alerting for ETL pipelines. Ability to work closely with Engineering, Product and Customer Success Teams. Build robust and deployable software in Python, C++ or C . Build, deploy, and maintain RESTful APIs to access datasets. Parse and extract data from common formats including, XML, JSON, CSV, and Pipe delimited. Integrate with customer provided APIs. Build, organize, and maintain datamarts using any of SQL, JSON, Blob, or other databases as needed. Write and maintain excellent documentation of all work.   ","Minimum 3-5 years of experience is required B.S. in Computer Science or closely related field preferred, but not required. Real-world and proven track records count as much, if more. Programming Languages Python C C++ R are a plus . Experience with databricks plus. Established expertise developing ETL pipelines on serverless cloud solutions. AWS Lambda, Azure App Services, etc.. Linux/Unix Docker Databases SQL, JSON, object storage, etc. graph databases like Neo4j TigerGraph Strong technical writing skills will be heavily stressed Build automated for environments including, AWS, GCP, Azure, Heroku. Develop support data integrity reporting alerting pipelines. Ability to work Engineering, Product Customer Success Teams. robust deployable software Python, Build, deploy, maintain RESTful APIs access datasets. Parse extract from common formats XML, CSV, Pipe delimited. Integrate customer provided APIs. organize, datamarts using any Blob, other needed. Write excellent documentation all work.","Minimum 3-5 years experience required B.S. Computer Science closely related field preferred, required. Real-world proven track records count much, more. Programming Languages Python C C++ R plus . Experience databricks plus. Established expertise developing ETL pipelines serverless cloud solutions. AWS Lambda, Azure App Services, etc.. Linux/Unix Docker Databases SQL, JSON, object storage, etc. graph databases like Neo4j TigerGraph Strong technical writing skills heavily stressed Build automated environments including, AWS, GCP, Azure, Heroku. Develop support data integrity reporting alerting pipelines. Ability work Engineering, Product Customer Success Teams. robust deployable software Python, Build, deploy, maintain RESTful APIs access datasets. Parse extract common formats XML, CSV, Pipe delimited. Integrate customer provided APIs. organize, datamarts using Blob, needed. Write excellent documentation work."
78,Data Engineer,Senior Data Engineer,"Chicago, IL",Chicago,IL,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Hihg
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Mastery in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or customer-facing role     ","Mastery in at least one of the following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing more languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role","Mastery least one following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role"
79,Data Engineer,Sr. Data Engineer,"Chicago, IL 60601",Chicago,IL,"We have a wide variety of career opportunities around the world â come find yours.

Information Technology
United Airlines is seeking talented people to join the Data Engineering team. Data Engineering organization is responsible for driving data driven insights & innovation to support the data needs for commercial and operational projects with a digital focus.
 You will partner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives that create sustainable revenue and share growth
 Design, develop, and implement streaming and near-real time data pipelines that feed systems that are the operational backbone of our business.
 Execute unit tests and validating expected results to ensure accuracy & integrity of data and applications through analysis, coding, writing clear documentation and problem resolution.
 This role will also drive the adoption of data processing and analysis within the Hadoop environment and help cross train other members of the team.
 Leverage strategic and analytical skills to understand and solve customer and business centric questions.
 Coordinate and guide cross-functional projects that involve team members across all areas of the enterprise, vendors, external agencies and partners.
 Leverage data from a variety of sources to develop data marts and insights that provide a comprehensive understanding of the business.
 Develop and implement innovative solutions leading to automation.
 Use of Agile methodologies to manage projects.
 Mentor and train junior engineers.
Required
 BS/BA, in computer science or related STEM field
 We are seeking creative, driven, detail-oriented individuals who enjoy tackling tough problems with data and insights. Individuals who have a natural curiosity and desire to solve problems are encouraged to apply
 10+ years of IT experience in software development
 5+ years of development experience using Java, Python, Scala
 5+ years of experience with Big Data technologies like Spark, Hadoop, Hive, HBASE, Kafka, Nifi
 4+ years of experience with relational database systems like MS SQL Server, Oracle, Teradata
 Must be legally authorized to work in US for any employer without sponsorship - Successful completion of interview required to meet job qualification
 Reliable, punctual attendance is an essential function of the position
Preferred Skills:
 Masterâs in computer science or related STEM field.
 Experience with cloud-based systems like AWS, AZURE or Google Cloud.
 Certified Developer / Architect on AWS.
 Strong experience with continuous integration & delivery using Agile methodologies - Data engineering experience with transportation/airline industry
 Strong problem-solving skills.
 Strong knowledge in Big Data


Equal Opportunity Employer â Minorities/Women/Veterans/Disabled/LGBT","  Experience with cloud-based systems like AWS, AZURE or Google Cloud.   ","Experience with cloud-based systems like AWS, AZURE or Google Cloud.","Experience cloud-based systems like AWS, AZURE Google Cloud."
80,Data Engineer,Data Engineer,"Chicago, IL 60654",Chicago,IL,"The Data Engineer is responsible for applying your expertise in quantitative analysis, database and data warehousing, partnered with operation and product teams, to solve problems and identify trends and opportunities. The Data Engineer role has to work across the following areas:
Database maintenance
Building and analyzing dashboards and reports
Evaluating and defining metrics and perform exploratory analysis
Monitoring key product metrics and understanding root causes of changes in metrics
Empower and assist operation and product teams through building key data sets and data-based recommendations
Automating analyses and authoring pipelines via SQL/python based ETL framework
Key Competencies
Superb SQL programming skill.
Understanding of ETL tools and database architecture.
Advanced knowledge of data warehousing.
Demonstrable familiarity with code and programming concepts. Experience with Python is preferred but not required.
A product mindset - you ask and address the most important analytical questions with a view on enhancing product impact.
Passionate and attentive self-starters, great communicators.
Education and Experience
1-3 years of experience in quantitative analysis experience.
Bachelor's degree in Computer Science, Statistics, Math or other technical field required. Graduate degrees preferred.
Job Classification
This is a full time exempt position
SMS Assist is an Equal Opportunity Employer (EOE) that welcomes and encourages all applicants to apply regardless of age, race, color, religion, sex, sexual orientation, gender identify and/or expression, national origin, disability, veteran status, marital or parental status, ancestry, citizenship status, pregnancy or other reasons prohibited by law.
#ZP
#Indeed","   1-3 years of experience in quantitative analysis experience. Bachelor's degree in Computer Science, Statistics, Math or other technical field required. Graduate degrees preferred.  ","1-3 years of experience in quantitative analysis experience. Bachelor's degree Computer Science, Statistics, Math or other technical field required. Graduate degrees preferred.","1-3 years experience quantitative analysis experience. Bachelor's degree Computer Science, Statistics, Math technical field required. Graduate degrees preferred."
81,Data Engineer,Data Engineer,"Rosemont, IL",Rosemont,IL,"You are known for your development and deployment of innovative big data platforms for advanced analytics and data processing. You lead innovation through exploration, benchmarking, making recommendations, and implementing big data technologies for platforms. You are excited by defining and building data pipelines that will enable faster, better, data-informed decision-making within the business. You have a passion for continued improvement, learning and mentoring, and leverage this enthusiasm when building stakeholder relationships. You are collaborative, insightful and want to work for an organization making a difference in the field of orthopaedics on behalf our members and their patients.
If this sounds like you, read on!
The Data Engineer is primarily responsible for maintaining and enhancing the Registryâs data acquisition, integration, and ETL pipelines in support of both operational and business intelligence data stores. The incumbent is responsible for applying diverse data cleansing and transformation techniques as well as the ongoing management and monitoring of all Registry databases. This includes addressing issues pertaining to the ongoing operations and optimization of the data environment including performance, reliability, logging, scalability, etc. This position will also provide support for the Academyâs database systems, warehouse, marts, and supporting applications.
Lead the effort to develop a unified enterprise data model for the Academy. Design, develop, and maintain high-performance data platforms on premise and in Microsoft Azure cloud-based environments including leading the development of a data warehouse environment to support the Registryâs business intelligence roadmap. Champion efforts that will ensure that the Academyâs business intelligence applications remain relevant for use by internal business groups by actively participating in strategy and project planning discussions. Work collaboratively with Registry participants and internal support teams, identify and implement changes that improve system performance and the user experience.
Design, develop, and maintain the ETL pipelines using SSIS that standardize raw data from multiple data sources and optimize both the operational and dimensional/star schema data model necessary for transactional systems and business intelligence applications. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions aligning to an overall data architecture. Extract, transform, and load data to and from various data sources including relational databases, NoSQL databases, web services, and flat files. Produce various technical documents such as ER diagrams, table schemas, data lineage, API documents, etc.
Provide leadership and oversight on database architectural design for existing Academy systems including database administration, performance monitoring, and troubleshooting. Provide complex analysis, conceptualize, design, implement, and develop solutions for critical data-centric projects. Perform dataflow, system and data analysis, and develop meaningful and useful presentation of data in downstream applications. Plan and implement standards, define/code conformed global and reusable objects, perform complex database design and data repository modelling.
Monitor ETL processes, system audits, dashboard reporting, and presentation layer functioning and performance. Proactively identify and implement procedures that resolve performance and/or data reporting issues. Support the optimal performance of the Academyâs data and BI systems. Monitor database performance, provide optimization recommendations, and implement recommendations. Follow the release cycles and implement on-time delivery of task assignments, defect correction, change requests, and enhancements. Troubleshoot and solve technical problems. Perform other responsibilities as assignment by management.
Required Qualifications:
Bachelorâs degree in Computer Science, Information Systems, Business Administration, or other related field required
Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources
Minimum 2 years of experience with PowerBI
Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications
Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired
Experience working with both structured and unstructured data
Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types (star, snowflake, etc.)
Data modeling experience in building logical and physical data models
Applied knowledge of Microsoft Security/Authentication Concepts (Active Directory, IIS, Windows OS)
Strong technical planning skills with the ability to prioritize and multitask across a number of work streams
Must have a passion for continued improvement, learning, and mentoring
Polished presentation skills; experience creating and presenting findings to executive level staff
Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively
Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables
Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders
Experience with Data Warehouse is a plus
Knowledge of SSRS is a plus
Healthcare industry experience a plus
If this describes YOU, please apply by sharing the following:
Clearly communicate why you are the ideal candidate for this role, providing specific examples and experiences as proof points.
Attach your resume, cover letter and any additional materials that support your application.
Salary expectations must be included to be considered for this opportunity.","Bachelorâs degree in Computer Science, Information Systems, Business Administration, or other related field required Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources Minimum 2 years of experience with PowerBI Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired Experience working with both structured and unstructured data Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types  star, snowflake, etc.  Data modeling experience in building logical and physical data models Applied knowledge of Microsoft Security/Authentication Concepts  Active Directory, IIS, Windows OS  Strong technical planning skills with the ability to prioritize and multitask across a number of work streams Must have a passion for continued improvement, learning, and mentoring Polished presentation skills; experience creating and presenting findings to executive level staff Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders Experience with Data Warehouse is a plus Knowledge of SSRS is a plus Healthcare industry experience a plus    ","Bachelorâs degree in Computer Science, Information Systems, Business Administration, or other related field required Minimum of 3 years relevant work experience a data engineering role leveraging SQL, SSIS; including design and support ETL routines that the import from multiple sources 2 with PowerBI warehousing design, development, ongoing star snowflake schemas to business intelligence applications 5 database administration development SQL MySQL environment; knowledge Microsoft technology stack; background Azure Infrastructure as Service environment desired Experience working both structured unstructured Demonstrated understanding Intelligence solutions cubes, warehouse, marts, supporting schema types star, snowflake, etc. Data modeling building logical physical models Applied Security/Authentication Concepts Active Directory, IIS, Windows OS Strong technical planning skills ability prioritize multitask across number streams Must have passion for continued improvement, learning, mentoring Polished presentation skills; creating presenting findings executive level staff written, verbal interpersonal communication skills, an communicate ideas effectively be highly collaborative manage motivate project teams meet deliverables Ability build strong stakeholder relationships translate complex concepts non-technical stakeholders Warehouse is plus Knowledge SSRS Healthcare industry","Bachelorâs degree Computer Science, Information Systems, Business Administration, related field required Minimum 3 years relevant work experience data engineering role leveraging SQL, SSIS; including design support ETL routines import multiple sources 2 PowerBI warehousing design, development, ongoing star snowflake schemas business intelligence applications 5 database administration development SQL MySQL environment; knowledge Microsoft technology stack; background Azure Infrastructure Service environment desired Experience working structured unstructured Demonstrated understanding Intelligence solutions cubes, warehouse, marts, supporting schema types star, snowflake, etc. Data modeling building logical physical models Applied Security/Authentication Concepts Active Directory, IIS, Windows OS Strong technical planning skills ability prioritize multitask across number streams Must passion continued improvement, learning, mentoring Polished presentation skills; creating presenting findings executive level staff written, verbal interpersonal communication skills, communicate ideas effectively highly collaborative manage motivate project teams meet deliverables Ability build strong stakeholder relationships translate complex concepts non-technical stakeholders Warehouse plus Knowledge SSRS Healthcare industry"
82,Data Engineer,Sr. Consultant - Data Engineer,"Rolling Meadows, IL",Rolling Meadows,IL,"Insygnum needs a Consultant - Data Engineer to help our clients for data analysis, data integration and data quality. Our Chicago-based team is small but growing fast and we need to complement our in-house experts who knows how to tame challenging data. This is a unique opportunity to not only work with cool technology, but also to create a new methodologies and techniques. You'll get in on the ground floor of a new company, help shape its future, and benefit directly from your work.
Why work here
Joining insygnum now offers several unique opportunities
You will receive competitive salary, benefits, and stock options
You will be working on hard, interesting problems
You will help shape the culture of the company as we grow
You will have the opportunity to apply your skills in a meaningful way and have a real-world impact
Responsibilities
Identify data warehouse needs and develop strategy for implementing a warehousing solution including investigating data sources, rationalizing information sources, identifying technology components, building roadmaps and reference architecture stacks
Work with Business Analysts and other information management professionals through all phases of project development, from envisioning to architecture definition and end solution realization
Provide direction and collaborate with Data Engineers to implement enterprise solutions that will support organizational business intelligence and analytics requirements
Work with the business to identify opportunities where technology can be leveraged to solve existing problems and can assist with new market opportunities
Meet with technology vendors, convey technical requirements and business use cases, develop scorecards, install vendor products in a lab environment, and summarize findings and results of testing
Technologies to be used may include some combination of relational databases (PostgreSQL, Teradata, Aster, HANA), NoSQL, Hadoop, Object-based stores, and OLAP.
Specific responsibilities include:
Help design an architecture for federated data stores and data fusion
Help design methods for storing data in a way that facilitates extremely fast data parsing and management
Implement ""glue code"" that connects middle tier components with backend components
Implement data management and analytics code utilizing data architecture (e.g. map reduce)
Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores
Collaborate with enterprise architects to understand the applications we integrate with and the data they produce
Review requirements for new approaches to big data storage and analytics
Design methods for caching, paging, and integrating real-time data with historical data stores
Desired Skills and Experience
Requirements
Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies
Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation
Minimum five years experience using one or more data integration tools including Data Quality and ETL
Development knowledge for integrating components and contributing to core code base - Java preferred
Solid understanding of database and data warehousing technologies
Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies
Experienced in big data requirements, applications, and technologies such as Hadoop
Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc.
Experienced in Linux environments
Bonus Points
Experience working with IT strategy teams, business teams and business analysts to define information systems, services and management
Experience with RDBMS including SQL Server, Oracle 11g, MySQL; Big Data including SQL Data Warehouse Appliance, Oracle Exadata, Netezza, Greenplum, Vertica, Teradata, Aster Data, SAP HANA, Hadoop a plus; Analytics including SAS, SPSS, Spotfire, Tableau, Qlikview, R, Oracle Endeca; BI Tools including Oracle OBIEE, SAP Business Objects, SAS and other Analytics Vendors with BI components; ETL & MDM including Informatica, SAS Dataflux, IBM, Siperion, Rochade, Map/Reduce for ETL is a plus
Java is strongly preferred (e.g. for working with map reduce) but not ultimately a requirement if you excel in other areas
Strong SQL skills are highly desirable
OLAP experience
Contact: HR Manager
Email: hr@insygnum.com
Phone: (630)-799-1556"," Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation Minimum five years experience using one or more data integration tools including Data Quality and ETL Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies Experienced in big data requirements, applications, and technologies such as Hadoop Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc. Experienced in Linux environments  Identify data warehouse needs and develop strategy for implementing a warehousing solution including investigating data sources, rationalizing information sources, identifying technology components, building roadmaps and reference architecture stacks Work with Business Analysts and other information management professionals through all phases of project development, from envisioning to architecture definition and end solution realization Provide direction and collaborate with Data Engineers to implement enterprise solutions that will support organizational business intelligence and analytics requirements Work with the business to identify opportunities where technology can be leveraged to solve existing problems and can assist with new market opportunities Meet with technology vendors, convey technical requirements and business use cases, develop scorecards, install vendor products in a lab environment, and summarize findings and results of testing Technologies to be used may include some combination of relational databases  PostgreSQL, Teradata, Aster, HANA , NoSQL, Hadoop, Object-based stores, and OLAP. Specific responsibilities include  Help design an architecture for federated data stores and data fusion Help design methods for storing data in a way that facilitates extremely fast data parsing and management Implement ""glue code"" that connects middle tier components with backend components Implement data management and analytics code utilizing data architecture  e.g. map reduce  Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores Collaborate with enterprise architects to understand the applications we integrate with and the data they produce Review requirements for new approaches to big data storage and analytics Design methods for caching, paging, and integrating real-time data with historical data stores Desired Skills and Experience Requirements Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation Minimum five years experience using one or more data integration tools including Data Quality and ETL Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies Experienced in big data requirements, applications, and technologies such as Hadoop Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc. Experienced in Linux environments Bonus Points Experience working with IT strategy teams, business teams and business analysts to define information systems, services and management Experience with RDBMS including SQL Server, Oracle 11g, MySQL; Big Data including SQL Data Warehouse Appliance, Oracle Exadata, Netezza, Greenplum, Vertica, Teradata, Aster Data, SAP HANA, Hadoop a plus; Analytics including SAS, SPSS, Spotfire, Tableau, Qlikview, R, Oracle Endeca; BI Tools including Oracle OBIEE, SAP Business Objects, SAS and other Analytics Vendors with BI components; ETL & MDM including Informatica, SAS Dataflux, IBM, Siperion, Rochade, Map/Reduce for ETL is a plus Java is strongly preferred  e.g. for working with map reduce  but not ultimately a requirement if you excel in other areas Strong SQL skills are highly desirable OLAP experience  Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation Minimum five years experience using one or more data integration tools including Data Quality and ETL Development knowledge for integrating components and contributing to core code base - Java preferred Solid understanding of database and data warehousing technologies Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies Experienced in big data requirements, applications, and technologies such as Hadoop Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc. Experienced in Linux environments ","Minimum ten years of experience in an IT environment with at least five designing and working large enterprise database warehouses, robust data models partitioning strategies architecture experience, including modeling project teams, governance strategy, metadata management, system architecture, design, implementation using one or more integration tools Data Quality ETL Development knowledge for integrating components contributing to core code base - Java preferred Solid understanding warehousing technologies Experienced advanced SQL as well NoSQL queries, syntax, big requirements, applications, such Hadoop Proficient methods approaches triggers, named views, temporary tables, etc. Linux environments Identify warehouse needs develop strategy implementing a solution investigating sources, rationalizing information identifying technology components, building roadmaps reference stacks Work Business Analysts other management professionals through all phases development, from envisioning definition end realization Provide direction collaborate Engineers implement solutions that will support organizational business intelligence analytics requirements the identify opportunities where can be leveraged solve existing problems assist new market Meet vendors, convey technical use cases, scorecards, install vendor products lab environment, summarize findings results testing Technologies used may include some combination relational databases PostgreSQL, Teradata, Aster, HANA , NoSQL, Hadoop, Object-based stores, OLAP. Specific responsibilities Help design federated stores fusion storing way facilitates extremely fast parsing Implement ""glue code"" connects middle tier backend utilizing e.g. map reduce Collaborate machine learning folks determine how analyze various sets set up querying architects understand applications we integrate they produce Review storage Design caching, paging, real-time historical Desired Skills Experience Requirements Bonus Points teams analysts define systems, services RDBMS Server, Oracle 11g, MySQL; Big Warehouse Appliance, Exadata, Netezza, Greenplum, Vertica, Aster Data, SAP HANA, plus; Analytics SAS, SPSS, Spotfire, Tableau, Qlikview, R, Endeca; BI Tools OBIEE, Objects, SAS Vendors components; & MDM Informatica, Dataflux, IBM, Siperion, Rochade, Map/Reduce is plus strongly but not ultimately requirement if you excel areas Strong skills are highly desirable OLAP","Minimum ten years experience IT environment least five designing working large enterprise database warehouses, robust data models partitioning strategies architecture experience, including modeling project teams, governance strategy, metadata management, system architecture, design, implementation using one integration tools Data Quality ETL Development knowledge integrating components contributing core code base - Java preferred Solid understanding warehousing technologies Experienced advanced SQL well NoSQL queries, syntax, big requirements, applications, Hadoop Proficient methods approaches triggers, named views, temporary tables, etc. Linux environments Identify warehouse needs develop strategy implementing solution investigating sources, rationalizing information identifying technology components, building roadmaps reference stacks Work Business Analysts management professionals phases development, envisioning definition end realization Provide direction collaborate Engineers implement solutions support organizational business intelligence analytics requirements identify opportunities leveraged solve existing problems assist new market Meet vendors, convey technical use cases, scorecards, install vendor products lab environment, summarize findings results testing Technologies used may include combination relational databases PostgreSQL, Teradata, Aster, HANA , NoSQL, Hadoop, Object-based stores, OLAP. Specific responsibilities Help design federated stores fusion storing way facilitates extremely fast parsing Implement ""glue code"" connects middle tier backend utilizing e.g. map reduce Collaborate machine learning folks determine analyze various sets set querying architects understand applications integrate produce Review storage Design caching, paging, real-time historical Desired Skills Experience Requirements Bonus Points teams analysts define systems, services RDBMS Server, Oracle 11g, MySQL; Big Warehouse Appliance, Exadata, Netezza, Greenplum, Vertica, Aster Data, SAP HANA, plus; Analytics SAS, SPSS, Spotfire, Tableau, Qlikview, R, Endeca; BI Tools OBIEE, Objects, SAS Vendors components; & MDM Informatica, Dataflux, IBM, Siperion, Rochade, Map/Reduce plus strongly ultimately requirement excel areas Strong skills highly desirable OLAP"
83,Data Engineer,Data Engineer,"Chicago, IL 60601",Chicago,IL,"77 West Wacker Dr (35012), United States of America, Chicago, Illinois

At Capital One, weâre building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer

Do you want to work for a tech company that writes its own code, develops its own software, and builds its own products? We experiment and innovate leveraging the latest technologies, engineer breakthrough customer experiences, and bring simplicity and humanity to banking. We make a difference for 65 million customers. We're changing banking for good. At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs. We want you to be curious and ask âwhat if?â Capital One started as an information strategy company that specialized in credit cards, and we have become one of the most impactful and disruptive players in the industry. We have grown to see ourselves as a technology company in consumer finance, with great opportunities for software engineers who want to build innovative applications to give users smarter ways to save, transact, borrow and invest their money, as we seek to disrupt the industry again. As a Capital One Software Engineer, you'll work on everything from customer-facing web and mobile applications using cutting-edge open source frameworks, to highly-available RESTful services, to back-end Java based systems using the hottest techniques in Big Data.

You'll bring solid experience in emerging and traditional technologies such as: node.js, Java, AngularJS, React, Python, REST, JSON, XML, Ruby, HTML / HTML5, CSS, NoSQL databases, relational databases, Hadoop, Chef, Maven, iOS, Android, and AWS/Cloud Infrastructure to name a few.

You will:

Work with product owners to understand desired application capabilities and testing scenarios
Continuously improve software engineering practices
Work within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies
Lead the craftsmanship, availability, resilience, and scalability of your solutions
Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community
Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity

Basic Qualifications:

Bachelorâs Degree
At least 3 years of SDLC experience using Java technologies
At least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper
At least 1 years experience in one of the following Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform

Preferred Qualifications:

Master's Degree
2+ year experience with Spark
3+ years experience developing software solutions to solve complex business problems

At this time, Capital One will NOT sponsor a new applicant for employment authorization for this position."," Bachelorâs Degree At least 3 years of SDLC experience using Java technologies At least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper At least 1 years experience in one of the following Cloud technologies  AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform     ","Bachelorâs Degree At least 3 years of SDLC experience using Java technologies with leading big data like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper 1 in one the following Cloud AWS, Azure, OpenStack, Docker, Ansible, Chef Terraform","Bachelorâs Degree At least 3 years SDLC experience using Java technologies leading big data like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, Zookeeper 1 one following Cloud AWS, Azure, OpenStack, Docker, Ansible, Chef Terraform"
84,Data Engineer,"Big Data Engineer, Senior","Chicago, IL 60654",Chicago,IL,"Summary
The Big Data Engineer is responsible for building scalable data platforms, and large-scale processing systems that enable advanced analytics and support data teams across the enterprise. This hands-on role requires some experience working with AWS cloud platform in addition to expertise in a variety of technologies. The Big Data Engineer will develop and manage the enterprise data warehouse while sourcing data from various databases/applications & web APIs using stream and batch processing architectures.
Responsibilities
Responsible for designing, building & managing the advanced analytics platform to support downstream data science and the business intelligence teams
Work with the product owner, data scientist and various internal data users to understand the business requirements and implement optimal data solutions
Keen eye towards configuration driven approach to automate repeatable processes and tasks
Build and operate stable, scalable and highly performant data pipelines that cleanse, structure and integrate disparate big data sets into a readable and accessible format for end user analyses and targeting.
Develop data quality and governance framework that supports data lineage and ensures delivery of high-quality data to internal and external stakeholders
Using analytical and problem-solving skills to take complex business requests and transform them into clean, simple data solutions.
Implement/improve version control, deployment strategies, notifications to ensure product quality, agility and recoverability.
Understand and work with technology & IT teams to support database procedures, such as upgrade, backup, recovery, migrations, etc.
Role Specific Skills
Designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, S3, EC2, DynamoDB, RedShift, Kinesis, Lambda, Glue, Snowflake etc.
Hands-on experience analyzing, re-architecting and re-platforming on-premise data warehouses to data platforms on AWS cloud using AWS/3rd party services.
Knowledge about other NoSQL databases, such as MongoDB, Cassandra, HBase, etc.
Building and migrating the complex ETL pipelines on Redshift and Elastic Map Reduce to make the system grow elastically
Hands-on knowledge in using advanced SQL queries (analytical functions), experience in writing and optimizing highly efficient SQL queries
Proficiency with Python scripting
Experienced in testing and monitoring data for anomalies and rectifying them.
Advanced communication skills to be able to work with business owners to develop and define key business uses and to build data sets that address them.
Experience in working with Data visualization tools such a Tableau
 Professional Skills
These are the professional skills we would expect from an individual fully established in this role.
Customer Service - Advanced
Verbal Communication - Advanced
Written Communication - Advanced
Teamwork - Advanced
Relationships â proficient
Learning Agility - Expert
Analysis - Expert
Problem Solving - Expert
Process Orientation - Expert
Prioritization - Proficient
Qualifications
Bachelorâs degree or equivalent in an engineering or technical field such as Computer Science, Information Systems, Statistics, Engineering, or similar.
5-10 years of quantitative and qualitative experience in building ETL data flows in Big Data Ecosystem.
___________________________________________________________________________________
Please note, this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities, and schedule may change at any time with or without notice.
SMS Assist is an Equal Opportunity Employer (EOE) that welcomes and encourages all applicants to apply regardless of age, race, color, religion, sex, sexual orientation, gender identify and/or expression, national origin, disability, veteran status, marital or parental status, ancestry, citizenship status, pregnancy or other reasons prohibited by law.
#ZP
#Indeed","Bachelorâs degree or equivalent in an engineering or technical field such as Computer Science, Information Systems, Statistics, Engineering, or similar. 5-10 years of quantitative and qualitative experience in building ETL data flows in Big Data Ecosystem.  Designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, S3, EC2, DynamoDB, RedShift, Kinesis, Lambda, Glue, Snowflake etc. Hands-on experience analyzing, re-architecting and re-platforming on-premise data warehouses to data platforms on AWS cloud using AWS/3rd party services. Knowledge about other NoSQL databases, such as MongoDB, Cassandra, HBase, etc. Building and migrating the complex ETL pipelines on Redshift and Elastic Map Reduce to make the system grow elastically Hands-on knowledge in using advanced SQL queries  analytical functions , experience in writing and optimizing highly efficient SQL queries Proficiency with Python scripting Experienced in testing and monitoring data for anomalies and rectifying them. Advanced communication skills to be able to work with business owners to develop and define key business uses and to build data sets that address them. Experience in working with Data visualization tools such a Tableau   Responsible for designing, building & managing the advanced analytics platform to support downstream data science and the business intelligence teams Work with the product owner, data scientist and various internal data users to understand the business requirements and implement optimal data solutions Keen eye towards configuration driven approach to automate repeatable processes and tasks Build and operate stable, scalable and highly performant data pipelines that cleanse, structure and integrate disparate big data sets into a readable and accessible format for end user analyses and targeting. Develop data quality and governance framework that supports data lineage and ensures delivery of high-quality data to internal and external stakeholders Using analytical and problem-solving skills to take complex business requests and transform them into clean, simple data solutions. Implement/improve version control, deployment strategies, notifications to ensure product quality, agility and recoverability. Understand and work with technology & IT teams to support database procedures, such as upgrade, backup, recovery, migrations, etc.   ","Bachelorâs degree or equivalent in an engineering technical field such as Computer Science, Information Systems, Statistics, Engineering, similar. 5-10 years of quantitative and qualitative experience building ETL data flows Big Data Ecosystem. Designing, operationalizing large-scale enterprise solutions applications using one more AWS analytics services combination with 3rd parties - Spark, EMR, S3, EC2, DynamoDB, RedShift, Kinesis, Lambda, Glue, Snowflake etc. Hands-on analyzing, re-architecting re-platforming on-premise warehouses to platforms on cloud AWS/3rd party services. Knowledge about other NoSQL databases, MongoDB, Cassandra, HBase, Building migrating the complex pipelines Redshift Elastic Map Reduce make system grow elastically knowledge advanced SQL queries analytical functions , writing optimizing highly efficient Proficiency Python scripting Experienced testing monitoring for anomalies rectifying them. Advanced communication skills be able work business owners develop define key uses build sets that address Experience working visualization tools a Tableau Responsible designing, & managing platform support downstream science intelligence teams Work product owner, scientist various internal users understand requirements implement optimal Keen eye towards configuration driven approach automate repeatable processes tasks Build operate stable, scalable performant cleanse, structure integrate disparate big into readable accessible format end user analyses targeting. Develop quality governance framework supports lineage ensures delivery high-quality external stakeholders Using problem-solving take requests transform them clean, simple solutions. Implement/improve version control, deployment strategies, notifications ensure quality, agility recoverability. Understand technology IT database procedures, upgrade, backup, recovery, migrations,","Bachelorâs degree equivalent engineering technical field Computer Science, Information Systems, Statistics, Engineering, similar. 5-10 years quantitative qualitative experience building ETL data flows Big Data Ecosystem. Designing, operationalizing large-scale enterprise solutions applications using one AWS analytics services combination 3rd parties - Spark, EMR, S3, EC2, DynamoDB, RedShift, Kinesis, Lambda, Glue, Snowflake etc. Hands-on analyzing, re-architecting re-platforming on-premise warehouses platforms cloud AWS/3rd party services. Knowledge NoSQL databases, MongoDB, Cassandra, HBase, Building migrating complex pipelines Redshift Elastic Map Reduce make system grow elastically knowledge advanced SQL queries analytical functions , writing optimizing highly efficient Proficiency Python scripting Experienced testing monitoring anomalies rectifying them. Advanced communication skills able work business owners develop define key uses build sets address Experience working visualization tools Tableau Responsible designing, & managing platform support downstream science intelligence teams Work product owner, scientist various internal users understand requirements implement optimal Keen eye towards configuration driven approach automate repeatable processes tasks Build operate stable, scalable performant cleanse, structure integrate disparate big readable accessible format end user analyses targeting. Develop quality governance framework supports lineage ensures delivery high-quality external stakeholders Using problem-solving take requests transform clean, simple solutions. Implement/improve version control, deployment strategies, notifications ensure quality, agility recoverability. Understand technology IT database procedures, upgrade, backup, recovery, migrations,"
85,Data Engineer,Data Engineering Manager,"Chicago, IL",Chicago,IL,"Join SADA as a Data Engineering Manager!

Your Mission

As a Data Engineering Manager at SADA, you will build and lead a growing Data Engineering team as we deliver robust data solutions for our clients on Google Cloud Platform (GCP). You will be responsible for managing a blended team of data engineers and data scientists, so a broad background in Big Data, data warehouse modernization, analytics, disaster recovery, data science, and machine learning is highly advantageous.

The diversity of customers that SADA works with ensures a steady flow of challenging data work. Be prepared to tackle real-world data problems that our customers find too difficult or time-consuming to solve themselves. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of data domain areas. Management here at SADA also means developing people and being a leader.

In this role, you will:

Be comfortable working with customer executives to align business outcomes with technical vision and goals.
Guide the day-to-day activities of a geographically distributed team, including hiring world-class talent, reviewing work and setting goals.
Provide technical and professional leadership and mentorship on a diverse range of subject matter areas, such as Big Data pipelines and data warehouses to statistics and machine learning.
Develop and codify best practices for your team that can be replicated across multiple customer engagements.
Partner with your team to develop services and offerings that scale and are repeatable.
Participate in key technical and design discussions with technical leads as a hands-on manager.
Partner with other practice leads, architects, project managers, executives and sales personnel to develop statements of work, and then oversee execution by your team with high levels of agility and quality.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our employees know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing data practice area with vision and passion. You will be measured by your teamâs performance on customer engagements, how well your team achieves internal organizational goals, how well you collaborate with and support your team and peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the management growth track.

Expectations


Required Travel - 15-25% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Experience leading, managing and hiring a team of talented engineers
Expertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Expertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)
Natural language processing (e.g., conversational chatbots)
Document understanding
Image classification
Marketing analytics
IoT systems
Experience writing software in one or more languages such as Python or Java/Scala
Experience in technical consulting or customer-facing role
Excellent critical thinking, problem-solving and analytical skills

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience in a large scale, high-volume data warehouse environment
Experience operationalizing machine learning models on large datasets
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Experience leading, managing and hiring a team of talented engineers Expertise in at least one of the following engineering domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Expertise in at least one of the following data domains    Predictive analytics  e.g., recommendation systems, predictive maintenance  Natural language processing  e.g., conversational chatbots  Document understanding Image classification Marketing analytics IoT systems Experience writing software in one or more languages such as Python or Java/Scala Experience in technical consulting or customer-facing role Excellent critical thinking, problem-solving and analytical skills     ","Experience leading, managing and hiring a team of talented engineers Expertise in at least one the following engineering domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. domains Predictive analytics e.g., recommendation systems, predictive maintenance Natural language conversational chatbots Document understanding Image classification Marketing IoT systems writing more languages Python Java/Scala consulting customer-facing role Excellent critical thinking, problem-solving analytical skills","Experience leading, managing hiring team talented engineers Expertise least one following engineering domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. domains Predictive analytics e.g., recommendation systems, predictive maintenance Natural language conversational chatbots Document understanding Image classification Marketing IoT systems writing languages Python Java/Scala consulting customer-facing role Excellent critical thinking, problem-solving analytical skills"
86,Data Engineer,Data Engineer,"Chicago, IL",Chicago,IL,"Job Description
Role Summary
At Citadel Securities, a leading global market maker, our team of Data Engineers are tasked with building next generation data analysis platforms. Our data analysis methods evolve on a daily basis and we empower our engineers to make bold decisions that support critical functions across the business.
Opportunities available in Chicago, New York, London
Objectives
Design, develop, test, and deploy elegant software solutions across the firm
Partner with business leaders to define priorities and deliver custom solutions
Skills and Preferred Qualifications
A deep passion for working with data and developing software to address data processing challenges
Bachelorâs, Masterâs or PhD degree in Computer Science or equivalent experience
Proficiency within one or more programming languages including C, C++, Python, R or JavaScript.
Proficiency with multiple data platforms including RDBMS, NoSQL, MongoDB, Spark, Hadoop
Experience with some of the following areas: Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development
Strong written and verbal communications skills
Ability to manage multiple tasks and thrive in a fast-paced team environment
About Citadel Securities
Citadel Securities is a leading global market maker across a broad array of fixed income and equity securities. Our world-class capabilities position us to meet the liquidity demands of our diverse group of institutional clients in all market conditions. In partnering with us, our clients, including asset managers, banks, broker-dealers, hedge funds, government agencies and public pension programs are able to gain a powerful trading advantage and are better positioned to meet their investment goals.

The team makes its mark every day from our offices around the world: Chicago, New York, London, Hong Kong, Toronto, Shanghai, Sydney, Dublin.","A deep passion for working with data and developing software to address data processing challenges Bachelorâs, Masterâs or PhD degree in Computer Science or equivalent experience Proficiency within one or more programming languages including C, C++, Python, R or JavaScript. Proficiency with multiple data platforms including RDBMS, NoSQL, MongoDB, Spark, Hadoop Experience with some of the following areas  Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development Strong written and verbal communications skills Ability to manage multiple tasks and thrive in a fast-paced team environment  A deep passion for working with data and developing software to address data processing challenges Bachelorâs, Masterâs or PhD degree in Computer Science or equivalent experience Proficiency within one or more programming languages including C, C++, Python, R or JavaScript. Proficiency with multiple data platforms including RDBMS, NoSQL, MongoDB, Spark, Hadoop Experience with some of the following areas  Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development Strong written and verbal communications skills Ability to manage multiple tasks and thrive in a fast-paced team environment    ","A deep passion for working with data and developing software to address processing challenges Bachelorâs, Masterâs or PhD degree in Computer Science equivalent experience Proficiency within one more programming languages including C, C++, Python, R JavaScript. multiple platforms RDBMS, NoSQL, MongoDB, Spark, Hadoop Experience some of the following areas Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development Strong written verbal communications skills Ability manage tasks thrive a fast-paced team environment","A deep passion working data developing software address processing challenges Bachelorâs, Masterâs PhD degree Computer Science equivalent experience Proficiency within one programming languages including C, C++, Python, R JavaScript. multiple platforms RDBMS, NoSQL, MongoDB, Spark, Hadoop Experience following areas Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development Strong written verbal communications skills Ability manage tasks thrive fast-paced team environment"
87,Data Engineer,Data Engineer - Associate,"Chicago, IL",Chicago,IL,"Data Engineer (early career)
Austin or Chicago
(Visa sponsorship not currently offered)

Mattersight is a leader in enterprise analytics focused on customer and employee interactions and behaviors. Mattersight's Behavioral Analytics service captures and analyzes customer and employee interactions, employee desktop data, and other contextual information to improve operational performance and predict future customer and employee outcomes. Mattersightâs analytics are based on millions of proprietary algorithms and the application of unique behavioral models. The company's SaaS+ delivery model combines analytics in the cloud with deep customer partnerships to drive significant business value. Mattersight's applications are used by leading companies in Healthcare, Insurance, Financial Services, Telecommunications, Cable, Utilities and Government. See What Mattersâ¢ by visiting www.Mattersight.com.

Data Engineer Role & Responsibilities:
The Data Engineer will be part of the Routing Analytics R&D team, which provides the data sources and analytical tools used to help our clients derive maximum value from our Behavioral Routing solution. This position will support the day to day reporting needs of the Routing clients, including Business Monitoring, Insights, Product Development, Analysis & Testing, and others. Responsibilities include troubleshooting issues that occur with existing report deliverables as well as developing new reports and integrating them into the overall service catalog. This role will require development, testing, and configuration management of all BI deliverables in coordination with Data Engineers, Software Engineers, and Testers within the organization. Additionally, the Data Engineer will be responsible for maintaining any documentation and training materials required to support the various business units the group serves.

This individual will also support the Data Warehouse Specialist to troubleshoot issues relating to the warehouse, especially as they impact the reporting environment.

To summarize, the Data Engineer will be responsible for, but not limited to, the following tasks:

Troubleshoot and resolve issues as they arise related to all BI Tools
Manage iteration and release cycles and deployments
Assist in data modeling and design sessions
Proactively maintain documentation and training materials

Because of the data-centric culture and rapid growth of NICE Mattersight, a rich career path exists for the Data Engineer within Mattersight.

Preferred Skills/Attributes
Experience with RDBMS applications (SQL Server preferred)
Good communication skills and experience working with cross-functional teams
Exposure to the concepts of data warehouse design
SQL programming familiarity in large RDBMS systems (T-SQL preferred)
Exposure to ETL and data integration processes

Required Knowledge, Skills & Abilities

Previous Experience
For this role, Mattersight is not requiring previous work experience in a technical role though some experience in a Business Intelligence environment working with BI visualization tools, relational databases, and/or data warehousing systems is helpful. Experience with ETL applications and data modeling/UML software is also helpful. Required is an interest in data pipelines, data aggregations, and creatively solving data-related problems as well as the motivation to dive deeper into the data engineering world. Weâd also like to see a candidate who displays evidence of strong communication skills and the ability to work under pressure.
Ideal CandidateAnalyticalDetail OrientedStrong CommunicatorEntrepreneurialResults OrientedTask AgilityOperations-MindedAdept Time ManagerProblem SolverTeam PlayerSeeker of ExcellenceHigh Knowledge Bandwidth

Company Culture & Facts
Corporate Culture
Mattersight values diversity amongst its employees. Employees from all levels of experience and backgrounds are mingled together and are encouraged to learn about projects others are working on. Mattersight fosters teamwork as well as self motivation.
Eligibility & Location
Mattersight seeks candidates authorized to work in the United States. The Routing Analytics Data Engineer role will be based out of Mattersightâs Austin location; candidates should anticipate little to no travel.
Compensation
Mattersight is prepared to offer a highly competitive benefits and compensation package for the ideal candidate.
For more information about Mattersight, visit http://www.mattersight.com/. Mattersight is committed to equal opportunity and affirmative action in all employment matters: Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, ancestry, marital or domestic partner status, national origin, disability or medical condition, pregnancy, veteran or military status, sexual orientation, gender identity, or on account of membership or affiliation with anyone in any of the foregoing categories, or any other protected category under federal, state or local law. Although particular legal provisions may differ in various locations in which we do business, our principles are the same worldwide.",  Experience with RDBMS applications  SQL Server preferred  Good communication skills and experience working with cross-functional teams Exposure to the concepts of data warehouse design SQL programming familiarity in large RDBMS systems  T-SQL preferred   Troubleshoot and resolve issues as they arise related to all BI Tools Manage iteration and release cycles and deployments Assist in data modeling and design sessions Proactively maintain documentation and training materials  ,Experience with RDBMS applications SQL Server preferred Good communication skills and experience working cross-functional teams Exposure to the concepts of data warehouse design programming familiarity in large systems T-SQL Troubleshoot resolve issues as they arise related all BI Tools Manage iteration release cycles deployments Assist modeling sessions Proactively maintain documentation training materials,Experience RDBMS applications SQL Server preferred Good communication skills experience working cross-functional teams Exposure concepts data warehouse design programming familiarity large systems T-SQL Troubleshoot resolve issues arise related BI Tools Manage iteration release cycles deployments Assist modeling sessions Proactively maintain documentation training materials
88,Data Engineer,Senior Big Data Engineer,"Chicago, IL 60290",Chicago,IL,"Where good people build rewarding careers.
Think that working in the insurance field canât be exciting, rewarding and challenging? Think again. Youâll help us reinvent protection and retirement to improve customersâ lives. Weâll help you make an impact with our training and mentoring offerings. Here, youâll have the opportunity to expand and apply your skills in ways you never thought possible. And youâll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
About our team
360 Finance Advanced Analytics data engineering team works with multiple internal and external data sources to deliver data that is readily available, easily accessible, accurate and complete. They are responsible for building a centralized data lake/hub using the Hadoop ecosystem that will be used by Reporting & Operational Analytics teams and the Machine learning teams.
Job Description
This Lead Consultant is an experienced professional who is responsible for leveraging data and analytics to help automate and optimize Claims Analytics Data processes enabling our Claims employees to focus on serving our customers and delivering the most advanced claims experience on the planet. They will be responsible for the strategy around how we bring together complex data into clean and useful data structures making our valuable data more approachable.
Key Responsibilities
Responsible for design, prototyping and delivery of software solutions within the big data eco-system
Leading projects and/or serving as analytics SME to provide new or enhanced data to the business
Improving data governance and quality increasing the reliability of our data
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Key Responsibilities (Cont'd)
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e. Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
Job Qualifications (Cont'd)
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
4-5+ years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes, developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow a plus
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work. Good Life. Good HandsÂ®.
As a Fortune 100 company and industry leader, we provide a competitive salary â but thatâs just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, youâll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.
Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.
Allstate generally does not sponsor individuals for employment-based visas for this position.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click âhereâ for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click âhereâ for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the âEEO is the Lawâ poster click âhereâ. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click âhereâ. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.
It is the Companyâs policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employeeâs ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment."," Undergraduate degree in Computer Science, Mathematics, Engineering  or related field  or equivalent experience preferred 5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function Ability to work with broad parameters in complex situations Experience in developing, managing, and manipulating large, complex datasets Expert high-level coding skills such as SQL and Python and/or other scripting languages UNIX  required. Scala is a plus.   Responsible for design, prototyping and delivery of software solutions within the big data eco-system Leading projects and/or serving as analytics SME to provide new or enhanced data to the business Improving data governance and quality increasing the reliability of our data Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise   ","Undergraduate degree in Computer Science, Mathematics, Engineering or related field equivalent experience preferred 5-7 years of a data integration, ETL and/or business intelligence/analytics function Ability to work with broad parameters complex situations Experience developing, managing, and manipulating large, datasets Expert high-level coding skills such as SQL Python other scripting languages UNIX required. Scala is plus. Responsible for design, prototyping delivery software solutions within the big eco-system Leading projects serving analytics SME provide new enhanced Improving governance quality increasing reliability our Influencing creation single, trusted source key Claims that can be shared across Enterprise","Undergraduate degree Computer Science, Mathematics, Engineering related field equivalent experience preferred 5-7 years data integration, ETL and/or business intelligence/analytics function Ability work broad parameters complex situations Experience developing, managing, manipulating large, datasets Expert high-level coding skills SQL Python scripting languages UNIX required. Scala plus. Responsible design, prototyping delivery software solutions within big eco-system Leading projects serving analytics SME provide new enhanced Improving governance quality increasing reliability Influencing creation single, trusted source key Claims shared across Enterprise"
89,Data Engineer,Technical Data Engineer Lead,"Chicago, IL",Chicago,IL,"Job Description:
Role Summary/Purpose:
We are looking for a Technical Data Engineer Lead to lead the development of consumer-centric low latency analytic environment leveraging Big Data technologies and transform the legacy systems.
Essential Responsibilities:
Lead a development team of data engineers
Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases (Hbase) and EMR (Hadoop)
Responsible for design, development, testing oversight and implementation
Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks
Coordinate with data engineers and API developers to drive program delivery.
Drive technical development and application standards across enterprise data lake
Benchmark and debug critical issues with algorithms and software as they arise.
Lead and assist with the technical design and implementation of the Big Data cluster in various environments.
Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.
Perform other duties and/or special projects as assigned
Qualifications/Requirements:
Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.
Understand Hadoop cluster administration concepts.
3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.
Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.
Must have experience with batch and real-time data pipelines.
Must have experience as a Hadoop Technical Lead / Architect
Must have experience with design, development and deployment in the specified technologies.
Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.
Writing complex SQL queries, extracting and importing large amounts of data.
Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams.
Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.
Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.
Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.
Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through
Desired Characteristics:
Extensive experience working with data warehouses and big data platforms
Demonstrated experience building strong relationships with senior leaders
Strong leadership and influencing skills
Outstanding written and verbal skills and the ability to influence and motivate teams
Eligibility Requirements:
You must be 18 years or older
You must have a high school diploma or equivalent
You must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the onboarding process
You must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.
New hires (Level 4-7) must have 9 months of continuous service with the company before they are eligible to post on other roles. Once this new hire time in position requirement is met, the associate will have a minimum 6 monthsâ time in position before they can post for future non-exempt roles. Employees, level 8 or greater, must have at least 24 monthsâ time in position before they can post. All internal employees must have at least a âconsistently meets expectationsâ performance rating and have approval from your manager to post (or the approval of your manager and HR if you donât meet the time in position or performance requirement).
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Reasonable Accommodation Notice:
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
If you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am â 5pm Monday to Friday, Central Standard Time.
The salary range for this position is 85,000.00 - 170,000.00 USD Annual
Salaries are adjusted according to market in CA and Metro NY and some positions are bonus eligible.
Grade/Level: 12
Job Family Group:
Information Technology","Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree  preferred  in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies. Understand Hadoop cluster administration concepts. 3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera. Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies. Must have experience with batch and real-time data pipelines. Must have experience as a Hadoop Technical Lead / Architect Must have experience with design, development and deployment in the specified technologies. Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python. Writing complex SQL queries, extracting and importing large amounts of data. Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams. Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions. Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders. Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment. Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through   Lead a development team of data engineers Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases  Hbase  and EMR  Hadoop  Responsible for design, development, testing oversight and implementation Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks Coordinate with data engineers and API developers to drive program delivery. Drive technical development and application standards across enterprise data lake Benchmark and debug critical issues with algorithms and software as they arise. Lead and assist with the technical design and implementation of the Big Data cluster in various environments. Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts. Perform other duties and/or special projects as assigned   Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree  preferred  in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies. Understand Hadoop cluster administration concepts. 3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera. Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies. Must have experience with batch and real-time data pipelines. Must have experience as a Hadoop Technical Lead / Architect Must have experience with design, development and deployment in the specified technologies. Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python. Writing complex SQL queries, extracting and importing large amounts of data. Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams. Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions. Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders. Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment. Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through ","Must have a Bachelorâs degree in Computer Science, Engineering, or related field, plus 4 years of work experience the IT industry; OR Masterâs preferred 2 industry with Big Data Technologies. Understand Hadoop cluster administration concepts. 3+ hands-on large scale environments such as Hortonworks, Cloudera. Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies. batch and real-time data pipelines. Technical Lead / Architect design, development deployment specified strong OOPS concepts, design principles, patterns java Python. Writing complex SQL queries, extracting importing amounts data. be willing to fast-paced environment an on shore â off distributed Agile teams. technical background building enterprise-wide warehouse solutions. ability develop maintain collaborative relationships at all levels across Business Stakeholders. prioritize multiple tasks deal urgent requests demanding environment. Excellent written oral communication skills. Adept presenting topics, influencing executing timely actionable follow-through team engineers Implement big enterprise lake, BI analytics system using Hive LLAP, Sqoop, NoSQL databases Hbase EMR Responsible for development, testing oversight implementation Works closely program manager, scrum master, architects convey impacts timeline risks Coordinate API developers drive delivery. Drive application standards lake Benchmark debug critical issues algorithms software they arise. assist various environments. Guide/mentor example create custom common utilities/libraries that can reused efforts. Perform other duties and/or special projects assigned","Must Bachelorâs degree Computer Science, Engineering, related field, plus 4 years work experience IT industry; OR Masterâs preferred 2 industry Big Data Technologies. Understand Hadoop cluster administration concepts. 3+ hands-on large scale environments Hortonworks, Cloudera. Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies. batch real-time data pipelines. Technical Lead / Architect design, development deployment specified strong OOPS concepts, design principles, patterns java Python. Writing complex SQL queries, extracting importing amounts data. willing fast-paced environment shore â distributed Agile teams. technical background building enterprise-wide warehouse solutions. ability develop maintain collaborative relationships levels across Business Stakeholders. prioritize multiple tasks deal urgent requests demanding environment. Excellent written oral communication skills. Adept presenting topics, influencing executing timely actionable follow-through team engineers Implement big enterprise lake, BI analytics system using Hive LLAP, Sqoop, NoSQL databases Hbase EMR Responsible development, testing oversight implementation Works closely program manager, scrum master, architects convey impacts timeline risks Coordinate API developers drive delivery. Drive application standards lake Benchmark debug critical issues algorithms software arise. assist various environments. Guide/mentor example create custom common utilities/libraries reused efforts. Perform duties and/or special projects assigned"
90,Data Engineer,Data Engineer,"Rosemont, IL 60018",Rosemont,IL,"You are known for your development and deployment of innovative big data platforms for advanced analytics and data processing. You lead innovation through exploration, benchmarking, making recommendations, and implementing big data technologies for platforms. You are excited by defining and building data pipelines that will enable faster, better, data-informed decision-making within the business. You have a passion for continued improvement, learning and mentoring, and leverage this enthusiasm when building stakeholder relationships. You are collaborative, insightful and want to work for an organization making a difference in the field of orthopaedics on behalf our members and their patients.
If this sounds like you, read on!
The Data Engineer is primarily responsible for maintaining and enhancing the Registryâs data acquisition, integration, and ETL pipelines in support of both operational and business intelligence data stores. The incumbent is responsible for applying diverse data cleansing and transformation techniques as well as the ongoing management and monitoring of all Registry databases. This includes addressing issues pertaining to the ongoing operations and optimization of the data environment including performance, reliability, logging, scalability, etc. This position will also provide support for the Academyâs database systems, warehouse, marts, and supporting applications.
Lead the effort to develop a unified enterprise data model for the Academy. Design, develop, and maintain high-performance data platforms on premise and in Microsoft Azure cloud-based environments including leading the development of a data warehouse environment to support the Registryâs business intelligence roadmap. Champion efforts that will ensure that the Academyâs business intelligence applications remain relevant for use by internal business groups by actively participating in strategy and project planning discussions. Work collaboratively with Registry participants and internal support teams, identify and implement changes that improve system performance and the user experience.
Design, develop, and maintain the ETL pipelines using SSIS that standardize raw data from multiple data sources and optimize both the operational and dimensional/star schema data model necessary for transactional systems and business intelligence applications. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions aligning to an overall data architecture. Extract, transform, and load data to and from various data sources including relational databases, NoSQL databases, web services, and flat files. Produce various technical documents such as ER diagrams, table schemas, data lineage, API documents, etc.
Provide leadership and oversight on database architectural design for existing Academy systems including database administration, performance monitoring, and troubleshooting. Provide complex analysis, conceptualize, design, implement, and develop solutions for critical data-centric projects. Perform dataflow, system and data analysis, and develop meaningful and useful presentation of data in downstream applications. Plan and implement standards, define/code conformed global and reusable objects, perform complex database design and data repository modelling.
Monitor ETL processes, system audits, dashboard reporting, and presentation layer functioning and performance. Proactively identify and implement procedures that resolve performance and/or data reporting issues. Support the optimal performance of the Academyâs data and BI systems. Monitor database performance, provide optimization recommendations, and implement recommendations. Follow the release cycles and implement on-time delivery of task assignments, defect correction, change requests, and enhancements. Troubleshoot and solve technical problems. Perform other responsibilities as assignment by management.
Required Qualifications:
Bachelorâs degree in Computer Science, Information Systems, Business Administration, or other related field required
Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources

Minimum 2 years of experience with PowerBI

Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications

Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired

Experience working with both structured and unstructured data
Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types (star, snowflake, etc.)
Data modeling experience in building logical and physical data models

Applied knowledge of Microsoft Security/Authentication Concepts (Active Directory, IIS, Windows OS)

Strong technical planning skills with the ability to prioritize and multitask across a number of work streams

Must have a passion for continued improvement, learning, and mentoring

Polished presentation skills; experience creating and presenting findings to executive level staff

Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively

Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables

Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders
Experience with Data Warehouse is a plus

Knowledge of SSRS is a plus

Healthcare industry experience a plus

If this describes YOU, please apply by sharing the following:
Clearly communicate why you are the ideal candidate for this role, providing specific examples and experiences as proof points.
Attach your resume, cover letter and any additional materials that support your application.
Salary expectations must be included to be considered for this opportunity.","Bachelorâs degree in Computer Science, Information Systems, Business Administration, or other related field required Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources  Minimum 2 years of experience with PowerBI  Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications  Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired  Experience working with both structured and unstructured data Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types  star, snowflake, etc.  Data modeling experience in building logical and physical data models  Applied knowledge of Microsoft Security/Authentication Concepts  Active Directory, IIS, Windows OS   Strong technical planning skills with the ability to prioritize and multitask across a number of work streams  Must have a passion for continued improvement, learning, and mentoring  Polished presentation skills; experience creating and presenting findings to executive level staff  Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively  Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables  Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders Experience with Data Warehouse is a plus  Knowledge of SSRS is a plus  Healthcare industry experience a plus     ","Bachelorâs degree in Computer Science, Information Systems, Business Administration, or other related field required Minimum of 3 years relevant work experience a data engineering role leveraging SQL, SSIS; including design and support ETL routines that the import from multiple sources 2 with PowerBI warehousing design, development, ongoing star snowflake schemas to business intelligence applications 5 database administration development SQL MySQL environment; knowledge Microsoft technology stack; background Azure Infrastructure as Service environment desired Experience working both structured unstructured Demonstrated understanding Intelligence solutions cubes, warehouse, marts, supporting schema types star, snowflake, etc. Data modeling building logical physical models Applied Security/Authentication Concepts Active Directory, IIS, Windows OS Strong technical planning skills ability prioritize multitask across number streams Must have passion for continued improvement, learning, mentoring Polished presentation skills; creating presenting findings executive level staff written, verbal interpersonal communication skills, an communicate ideas effectively be highly collaborative manage motivate project teams meet deliverables Ability build strong stakeholder relationships translate complex concepts non-technical stakeholders Warehouse is plus Knowledge SSRS Healthcare industry","Bachelorâs degree Computer Science, Information Systems, Business Administration, related field required Minimum 3 years relevant work experience data engineering role leveraging SQL, SSIS; including design support ETL routines import multiple sources 2 PowerBI warehousing design, development, ongoing star snowflake schemas business intelligence applications 5 database administration development SQL MySQL environment; knowledge Microsoft technology stack; background Azure Infrastructure Service environment desired Experience working structured unstructured Demonstrated understanding Intelligence solutions cubes, warehouse, marts, supporting schema types star, snowflake, etc. Data modeling building logical physical models Applied Security/Authentication Concepts Active Directory, IIS, Windows OS Strong technical planning skills ability prioritize multitask across number streams Must passion continued improvement, learning, mentoring Polished presentation skills; creating presenting findings executive level staff written, verbal interpersonal communication skills, communicate ideas effectively highly collaborative manage motivate project teams meet deliverables Ability build strong stakeholder relationships translate complex concepts non-technical stakeholders Warehouse plus Knowledge SSRS Healthcare industry"
91,Data Engineer,"Manager, Data Engineering","Chicago, IL",Chicago,IL,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clientsâ needs. This position will be focused on building out a customer data hubs/profile databases and building data warehouse solutions.

Primary Responsibilities:
Deliver quality work on defined tasks with little oversight and direction.
Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews.
Participate in integrated test sessions of components and subsystems on test and production servers.
Serve as technical resource during software development life cycle to solve business issues through the process of identifying and analyzing detailed requirements that translate into data integration and database system designs.
Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs.
Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers
Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed.
Ability to transform data into actionable information and convert the results of the analysis into a format that is easy to draw insights from and to share with colleagues and peers.
Required Skills and Experience:
Responsible for the maintenance, improvement, cleaning, and manipulation of data in the businessâs customer data platform and analytics databases. Works with the businessâs data analytics teams, data scientists, and software engineers in order to understand and aid in the implementation of database requirements, analyze performance, and troubleshoot any issues. Defines and builds the ETL and data pipelines to enable faster, better, data-informed decision-making within the business.
BS in Computer Science or equivalent education/professional experience is required.
5+ years in a data-engineering role with demonstrable experience with data integration and data warehouse projects.
Experience architecting and building data warehouses, customer profile databases, data marts, etc.
Knowledge of Apache Beam and programming languages including Java and Python.
Experience with MPP systems (Google Big Query, AWS Redshift, Azure Datawarehouse).
Experience with data modeling, warehouse design, and fact/dimension concepts.
Experience working with different query languages (i.e. T-SQL, PostgreSQL, PL-SQL).
Experience in data integration projects and automation via ETL Tools (i.e. Talend, Informatica, SSIS, etc.).
Experience in Hadoop (Hive, Spark, Impala, etc.) ecosystem is a plus.
Experience working with code repositories and continuous integration (i.e. Git, Jenkins, etc.)
Understanding of development and project methodologies.
Ability to work collaboratively in teams with other specialized individuals.
Able to work in a fast-paced, technical environment.
Good verbal and written communication skills.","  BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data-engineering role with demonstrable experience with data integration and data warehouse projects. Experience architecting and building data warehouses, customer profile databases, data marts, etc. Knowledge of Apache Beam and programming languages including Java and Python. Experience with MPP systems  Google Big Query, AWS Redshift, Azure Datawarehouse . Experience with data modeling, warehouse design, and fact/dimension concepts. Experience working with different query languages  i.e. T-SQL, PostgreSQL, PL-SQL . Experience in data integration projects and automation via ETL Tools  i.e. Talend, Informatica, SSIS, etc. . Experience in Hadoop  Hive, Spark, Impala, etc.  ecosystem is a plus. Experience working with code repositories and continuous integration  i.e. Git, Jenkins, etc.  Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills.  Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. Serve as technical resource during software development life cycle to solve business issues through the process of identifying and analyzing detailed requirements that translate into data integration and database system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Ability to transform data into actionable information and convert the results of the analysis into a format that is easy to draw insights from and to share with colleagues and peers.  ","BS in Computer Science or equivalent education/professional experience is required. 5+ years a data-engineering role with demonstrable data integration and warehouse projects. Experience architecting building warehouses, customer profile databases, marts, etc. Knowledge of Apache Beam programming languages including Java Python. MPP systems Google Big Query, AWS Redshift, Azure Datawarehouse . modeling, design, fact/dimension concepts. working different query i.e. T-SQL, PostgreSQL, PL-SQL projects automation via ETL Tools Talend, Informatica, SSIS, Hadoop Hive, Spark, Impala, ecosystem plus. code repositories continuous Git, Jenkins, Understanding development project methodologies. Ability to work collaboratively teams other specialized individuals. Able fast-paced, technical environment. Good verbal written communication skills. Deliver quality on defined tasks little oversight direction. Ensure all deliverables are high throughout the by adhering coding standards best practices participating reviews. Participate integrated test sessions components subsystems production servers. Serve as resource during software life cycle solve business issues through process identifying analyzing detailed requirements that translate into database system designs. Solve Use information gained prior experience, knowledge sharing Technology Associates, education training resolve remove barriers Provide status updates team members regular basis clearly escalate risks management needed. transform actionable convert results analysis format easy draw insights from share colleagues peers.","BS Computer Science equivalent education/professional experience required. 5+ years data-engineering role demonstrable data integration warehouse projects. Experience architecting building warehouses, customer profile databases, marts, etc. Knowledge Apache Beam programming languages including Java Python. MPP systems Google Big Query, AWS Redshift, Azure Datawarehouse . modeling, design, fact/dimension concepts. working different query i.e. T-SQL, PostgreSQL, PL-SQL projects automation via ETL Tools Talend, Informatica, SSIS, Hadoop Hive, Spark, Impala, ecosystem plus. code repositories continuous Git, Jenkins, Understanding development project methodologies. Ability work collaboratively teams specialized individuals. Able fast-paced, technical environment. Good verbal written communication skills. Deliver quality defined tasks little oversight direction. Ensure deliverables high throughout adhering coding standards best practices participating reviews. Participate integrated test sessions components subsystems production servers. Serve resource software life cycle solve business issues process identifying analyzing detailed requirements translate database system designs. Solve Use information gained prior experience, knowledge sharing Technology Associates, education training resolve remove barriers Provide status updates team members regular basis clearly escalate risks management needed. transform actionable convert results analysis format easy draw insights share colleagues peers."
92,Data Engineer,Big Data Engineer,"Chicago, IL 60604",Chicago,IL,"Cars.com is one of Chicago's original tech companies. Our online platform makes it easier for consumers to shop for, sell, and service their cars. With our expert content, mobile app features, millions of new and used vehicle listings, a comprehensive set of research tools and the largest database of consumer reviews in the industry, Cars.com offers innovative products to connect consumers with dealers across the country.

Data is the driver of our future at Cars. We're searching for highly collaborative, analytical, and innovative Engineers to build and scale our big data and ML platform. If you are passionate about using data to solve problems and build game changing products, we'd love to work with you.

About the Role:
Working within a dynamic and fast paced team environment, the Senior Engineer is responsible for the design, construction, and maintenance of mission-critical, highly visible big data and machine learning applications in direct support of Cars.com business objectives. Furthermore, this person is responsible for working with other engineers to develop the technical design by fully understanding the technical details, integration, and functions of multiple applications across their development team. The ideal candidate should have good mentoring and cross-functional skills.

About the Team:
The Big Data and Machine Learning team at Cars.com is responsible for building big data pipelines and deriving insights out of the data using advanced analytic techniques, streaming and machine learning at scale.

Qualifications:

3+ years of experience in programming languages such as Java or Python.
2+ years of experience in big data engineering.
1+ years of experience as Spark Developer.

Required Skills:

Ability to develop spark jobs to cleanse/enrich/process large amounts of data.
Ability to develop spark streaming jobs to read data from Kafka.
Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc.
Good understanding of various file formats and compression techniques.
Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data.
Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level.
Ability to mentor developers and lead projects of medium to high complexity.
Excellent communication and collaboration skills.

Bonus:

Experience developing Big Data applications in the cloud, especially AWS.
Experience tuning HIVE queries.
Experience in deploying ML models into production and integrating them into production applications for use.
Experience with Spark ML.
Experience with machine learning / deep learning using R, Python, Jupyter, Zeppelin, TensorFlow, etc.

"," 3+ years of experience in programming languages such as Java or Python. 2+ years of experience in big data engineering. 1+ years of experience as Spark Developer.   Ability to develop spark jobs to cleanse/enrich/process large amounts of data. Ability to develop spark streaming jobs to read data from Kafka. Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc. Good understanding of various file formats and compression techniques. Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data. Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level. Ability to mentor developers and lead projects of medium to high complexity. Excellent communication and collaboration skills.    ","3+ years of experience in programming languages such as Java or Python. 2+ big data engineering. 1+ Spark Developer. Ability to develop spark jobs cleanse/enrich/process large amounts data. streaming read from Kafka. Experience with tuning for efficient performance including execution time the job, memory, etc. Good understanding various file formats and compression techniques. source code management systems GIT developing CI/CD pipelines tools Jenkins understand deeply entire architecture a major part business be able articulate scaling reliability limits that area; design, debug at an enterprise level design estimate cross-project level. mentor developers lead projects medium high complexity. Excellent communication collaboration skills.","3+ years experience programming languages Java Python. 2+ big data engineering. 1+ Spark Developer. Ability develop spark jobs cleanse/enrich/process large amounts data. streaming read Kafka. Experience tuning efficient performance including execution time job, memory, etc. Good understanding various file formats compression techniques. source code management systems GIT developing CI/CD pipelines tools Jenkins understand deeply entire architecture major part business able articulate scaling reliability limits area; design, debug enterprise level design estimate cross-project level. mentor developers lead projects medium high complexity. Excellent communication collaboration skills."
93,Data Engineer,Data Engineer,"Chicago, IL",Chicago,IL,"Reverb is a leading online marketplace for buying and selling new, used, and vintage musical instruments. Since launching in 2013, Reverb has grown into a vibrant community of buyers and sellers all over the world. By focusing on inspiring content, price transparency, musician-focused eCommerce tools, a music-savvy customer service team, and more, Reverb has created an online destination where the global music community can connect over the perfect piece of music gear.

As part of the Reverb Data Engineering team, youâll help build the platform to enable data-driven decisions and products that scale along with our business. Weâre using Python and Scala in our data pipeline to support our BI users. Weâre a small, eager team so weâre looking for engineers who can take a high degree of initiative and enjoy working across team boundaries.

Everyone at Reverb takes creative initiative, helps set their own priorities, and comes up with new ways to grow the business. Our engineers take pride in building great software but take even more pride in shipping great features for our customers. If you want to learn more, check out
this video on working at Reverb
[https://www.youtube.com/watch?time_continue=45&v=rROUwUytfDU].

Responsibilities :

Design, build and maintain our data pipelines and automate analyses using SQL and Python based ETL framework
Develop ETL ecosystem tools
Analyze query pattern of internal users and adjust analytics schemas based on those patterns
Collaborate with data science and stakeholders across the organization to raise the bar for data best practices and management
Demonstrate and communicate a deep understanding of your chosen languages and frameworks to be able to make tradeoffs. Able to do more with less complexity.
Advocate for internal and external customers to break down problems, set priorities and follow up on performance and functionality
Build and maintain internal data processing and visualization tools to ensure that stakeholders have timely access to data.
Participate in pairing sessions, code reviews and take initiative on research projects/ requirements

Requirements :

Professional experience in ETL systems and database architecture
Production-level Python experience a must, Scala and AWS experience are pluses as they're part of our ecosystem.
Understanding of different types of data storage and their trade-offs with regards to availability, consistency, read/write throughput and maintenance cost.
Ability to communicate effectively with engineering peers, data analytics, and business stakeholders.

What you'll get:
To complement our competitive compensation and equity plans, we offer:

No-bureaucracy environment where ownership and initiative are valued.
Health insurance and a healthy work environment.
401k with company match.
Paid parental leave.
Flexible vacation and sick days.
Pre-tax commuter benefits.
Bi-monthly lunches.
A MacBook, monitor, keyboard, mouse of your choice and standing desk.
Discounts on music gear.

This is a local position in Chicago, please no remote workers or recruiters. Please send us a link to your Github!","   Design, build and maintain our data pipelines and automate analyses using SQL and Python based ETL framework Develop ETL ecosystem tools Analyze query pattern of internal users and adjust analytics schemas based on those patterns Collaborate with data science and stakeholders across the organization to raise the bar for data best practices and management Demonstrate and communicate a deep understanding of your chosen languages and frameworks to be able to make tradeoffs. Able to do more with less complexity. Advocate for internal and external customers to break down problems, set priorities and follow up on performance and functionality Build and maintain internal data processing and visualization tools to ensure that stakeholders have timely access to data. Participate in pairing sessions, code reviews and take initiative on research projects/ requirements    Professional experience in ETL systems and database architecture Production-level Python experience a must, Scala and AWS experience are pluses as they're part of our ecosystem. Understanding of different types of data storage and their trade-offs with regards to availability, consistency, read/write throughput and maintenance cost. Ability to communicate effectively with engineering peers, data analytics, and business stakeholders. ","Design, build and maintain our data pipelines automate analyses using SQL Python based ETL framework Develop ecosystem tools Analyze query pattern of internal users adjust analytics schemas on those patterns Collaborate with science stakeholders across the organization to raise bar for best practices management Demonstrate communicate a deep understanding your chosen languages frameworks be able make tradeoffs. Able do more less complexity. Advocate external customers break down problems, set priorities follow up performance functionality Build processing visualization ensure that have timely access data. Participate in pairing sessions, code reviews take initiative research projects/ requirements Professional experience systems database architecture Production-level must, Scala AWS are pluses as they're part ecosystem. Understanding different types storage their trade-offs regards availability, consistency, read/write throughput maintenance cost. Ability effectively engineering peers, analytics, business stakeholders.","Design, build maintain data pipelines automate analyses using SQL Python based ETL framework Develop ecosystem tools Analyze query pattern internal users adjust analytics schemas patterns Collaborate science stakeholders across organization raise bar best practices management Demonstrate communicate deep understanding chosen languages frameworks able make tradeoffs. Able less complexity. Advocate external customers break problems, set priorities follow performance functionality Build processing visualization ensure timely access data. Participate pairing sessions, code reviews take initiative research projects/ requirements Professional experience systems database architecture Production-level must, Scala AWS pluses they're part ecosystem. Understanding different types storage trade-offs regards availability, consistency, read/write throughput maintenance cost. Ability effectively engineering peers, analytics, business stakeholders."
94,Data Engineer,Market Data Engineer,"Chicago, IL",Chicago,IL,"Are you passionate about market data? Do you think about market microstructure and the trade-offs between low latency and ultra-low latency? Do you love collaborating with others to solve tough problems? Do you crave being involved in mission-critical initiatives that have direct and meaningful impact on an organizationâs daily performance?

We are looking for a software developer with experience in the processing, distribution, and management of financial data. This includes market data, reference data, trade management, and several data-related services. These services are primarily developed in Java and C++ and deployed on Linux. Software development cycles at CTC are short and releases of proprietary software are frequent. You would join our team in a fast-paced, dynamic environment where our team-mates quickly see the results of their effort, and their impact on our business.

Responsibilities
You design and build Market Data server-based components and frameworks
You collaborate with developers on other teams including operations, option pricing, exchange access, inventory and risk management
You develop requirements, propose solutions and deliver software into production environment in a timely and robust manner
You participate in design and code reviews
You partner with QA and support teams to ensure that the software delivered is high quality and easy to manage in a production environment
You keep up to date on the latest technologies that could benefit our system
Qualifications
You have a deep understanding of either Java or C++ and a practical understanding of the other (C++ is preferred)
You have experience writing high-performance feed handlers in a Linux environment
You have experience with multi-threaded programming and distributed application architecture
You have strong analytic and design capabilities
You have the ability to quickly triage issues and drive the resolution effort through completion
You prefer simple, cohesive, and practical solutions that are maintainable and extensible
You demonstrate a high level of interpersonal skills and are skilled in building collaborative relationships","You have a deep understanding of either Java or C++ and a practical understanding of the other  C++ is preferred  You have experience writing high-performance feed handlers in a Linux environment You have experience with multi-threaded programming and distributed application architecture You have strong analytic and design capabilities You have the ability to quickly triage issues and drive the resolution effort through completion You prefer simple, cohesive, and practical solutions that are maintainable and extensible You demonstrate a high level of interpersonal skills and are skilled in building collaborative relationships  You design and build Market Data server-based components and frameworks You collaborate with developers on other teams including operations, option pricing, exchange access, inventory and risk management You develop requirements, propose solutions and deliver software into production environment in a timely and robust manner You participate in design and code reviews You partner with QA and support teams to ensure that the software delivered is high quality and easy to manage in a production environment You keep up to date on the latest technologies that could benefit our system   ","You have a deep understanding of either Java or C++ and practical the other is preferred experience writing high-performance feed handlers in Linux environment with multi-threaded programming distributed application architecture strong analytic design capabilities ability to quickly triage issues drive resolution effort through completion prefer simple, cohesive, solutions that are maintainable extensible demonstrate high level interpersonal skills skilled building collaborative relationships build Market Data server-based components frameworks collaborate developers on teams including operations, option pricing, exchange access, inventory risk management develop requirements, propose deliver software into production timely robust manner participate code reviews partner QA support ensure delivered quality easy manage keep up date latest technologies could benefit our system","You deep understanding either Java C++ practical preferred experience writing high-performance feed handlers Linux environment multi-threaded programming distributed application architecture strong analytic design capabilities ability quickly triage issues drive resolution effort completion prefer simple, cohesive, solutions maintainable extensible demonstrate high level interpersonal skills skilled building collaborative relationships build Market Data server-based components frameworks collaborate developers teams including operations, option pricing, exchange access, inventory risk management develop requirements, propose deliver software production timely robust manner participate code reviews partner QA support ensure delivered quality easy manage keep date latest technologies could benefit system"
95,Data Engineer,Data Engineer,"Chicago, IL",Chicago,IL,"GoHealth is looking for Data Engineers who will be responsible for the design, development, and delivery of data transformation tasks used in transforming data into a format that can be easily analyzed. We are seeking candidates who have experience in data analysis, collection, and optimization for the purpose of informing business decisions. The Data Engineer will work with other team members in owning data pipelines including execution, documentation, maintenance, and metadata management. In this role, you will also support the development of the data infrastructure necessary for full scale data science, predictive analytics and machine learning.

Responsibilities:

Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.
Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.
Perform unit testing, system integration testing and assist with user acceptance testing.
Adapt data components to accommodate changes in source data and new business requirements.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.
Ability to work with the rest of the Data Engineering Team to cross-train and provide support for other BI tasks such as Cube Maintenance, Data Analytics and Requirements Gathering.

Skills and Experience:

Bachelor's Degree in computer science or equivalent experience required.
2+ years of experience in the design and development of data pipelines and tasks.
Strong analytical and problem solving ability with strong attention to detail and accuracy.
Good understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning queries.
Proven experience extracting data from structured data sources (SQL, Excel, CSV files) and unstructured data sources (Couchbase, Splunk, log files) both on-premise and in the cloud
Experience consuming data from web services, SOAP and REST technologies, HTML, XML and JSON.
Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.
Proficient in at least one programming language: Python, Java, Go, C#, Ruby, C, C++.
Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.
Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.

Benefits and Perks:

Open vacation policy
401k program with company match
Medical, dental, vision, and life insurance benefits
Flexible spending accounts
Commuter and transit benefits
Professional growth opportunities
Casual dress code
Generous employee referral bonuses
Happy hours, ping-pong tournaments, and more company-sponsored events
Subsidized gym memberships
GoHealth is an Equal Opportunity Employer

","  Bachelor's Degree in computer science or equivalent experience required. 2+ years of experience in the design and development of data pipelines and tasks. Strong analytical and problem solving ability with strong attention to detail and accuracy. Good understanding of data warehousing concepts and dimensional data modeling. Hands-on experience with troubleshooting performance issues and fine tuning queries. Proven experience extracting data from structured data sources  SQL, Excel, CSV files  and unstructured data sources  Couchbase, Splunk, log files  both on-premise and in the cloud Experience consuming data from web services, SOAP and REST technologies, HTML, XML and JSON. Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation. Proficient in at least one programming language  Python, Java, Go, C , Ruby, C, C++. Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required. Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.   Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources. Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues. Perform unit testing, system integration testing and assist with user acceptance testing. Adapt data components to accommodate changes in source data and new business requirements. Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks. Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline. Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports. Ability to work with the rest of the Data Engineering Team to cross-train and provide support for other BI tasks such as Cube Maintenance, Data Analytics and Requirements Gathering.   ","Bachelor's Degree in computer science or equivalent experience required. 2+ years of the design and development data pipelines tasks. Strong analytical problem solving ability with strong attention to detail accuracy. Good understanding warehousing concepts dimensional modeling. Hands-on troubleshooting performance issues fine tuning queries. Proven extracting from structured sources SQL, Excel, CSV files unstructured Couchbase, Splunk, log both on-premise cloud Experience consuming web services, SOAP REST technologies, HTML, XML JSON. Knowledge version control systems using Git, Bitbucket, SVN, Team Foundation. Proficient at least one programming language Python, Java, Go, C , Ruby, C, C++. Microsoft SQL Server, SSIS, SSRS, Power BI, Azure is preferred but not Familiar other warehouse platforms like AWS Redshift Data Pipeline. Design, develop deploy optimal extraction, transformation, loading various GoHealth external sources. Monitor, execute report on all pipeline tasks while working appropriate teams take corrective action quickly, case issues. Perform unit testing, system integration testing assist user acceptance testing. Adapt components accommodate changes source new business requirements. Create maintain documentation technical design, operational support maintenance procedures for Ensure quality compliance development, architecture, reporting, regulatory standards throughout entire pipeline. Collaborate rest Engineering Team, subject matter experts department leaders understand, analyze, build deliver data-related processes and/or reports. Ability work cross-train provide BI such as Cube Maintenance, Analytics Requirements Gathering.","Bachelor's Degree computer science equivalent experience required. 2+ years design development data pipelines tasks. Strong analytical problem solving ability strong attention detail accuracy. Good understanding warehousing concepts dimensional modeling. Hands-on troubleshooting performance issues fine tuning queries. Proven extracting structured sources SQL, Excel, CSV files unstructured Couchbase, Splunk, log on-premise cloud Experience consuming web services, SOAP REST technologies, HTML, XML JSON. Knowledge version control systems using Git, Bitbucket, SVN, Team Foundation. Proficient least one programming language Python, Java, Go, C , Ruby, C, C++. Microsoft SQL Server, SSIS, SSRS, Power BI, Azure preferred Familiar warehouse platforms like AWS Redshift Data Pipeline. Design, develop deploy optimal extraction, transformation, loading various GoHealth external sources. Monitor, execute report pipeline tasks working appropriate teams take corrective action quickly, case issues. Perform unit testing, system integration testing assist user acceptance testing. Adapt components accommodate changes source new business requirements. Create maintain documentation technical design, operational support maintenance procedures Ensure quality compliance development, architecture, reporting, regulatory standards throughout entire pipeline. Collaborate rest Engineering Team, subject matter experts department leaders understand, analyze, build deliver data-related processes and/or reports. Ability work cross-train provide BI Cube Maintenance, Analytics Requirements Gathering."
96,Data Engineer,"Data Engineer, Zoro","Chicago, IL 60603",Chicago,IL,"Position: Data Engineer
Location: Chicago (primary), Buffalo Grove (secondary)
Reports To: Director â Data Engineering

In the past six years, Zoro has grown from a group of 6 people working out of a 2,000 square foot building, offering fewer than 100,000 products to a group of 450+ working out of a 60,000+ square foot building, offering more than 3,000,000 unique products.

Imagine what you could help us achieve as a Data Engineer!

The Data Engineer will collaborate with various other IT groups, business partners and external service providers and play a key role in the design, development and operations of our new analytics platform, the âZoro Data Platform (ZDP)â.

 Job Responsibilities Include:
Participate in Requirements Gathering: work with key business partner groups (e.g. Product Mgt) and other Data Engineering personnel to understand department-level data requirements for the ZDP
Design Data Pipelines: work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP
Build Data Pipelines: leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP
Support Data Quality Program: work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data
Support Operations: triage alerts channeled to you and remediate as necessary
Technical Documentation: leverage templates provided and create clear, simple and comprehensive documentation for your development
Key contributor to defining, implementing and supporting:
Data Services
Data Dictionary
Tool Standards
Best Practices
Data Lineage
User Training
Skills & Responsibilities:
Strong ELT/ ETL designer/developer
Strong SQL
Strong Python
Structured & unstructured data expertise
Cloud environment development & operations experience (e.g. AWS, GCP)
Preference for candidates experienced with:
Google Cloud Platform (GCP) and associated services; e.g. BigQuery, GCS, Cloud Composer, Dataproc, Dataflow, Dataprep, Cloud Pub/Sub, Metadata DB, Data Studio, Datalab, other
Other important Zoro tools: Apache Airflow (scheduler), Bitbucket and git (version control), Stackdriver (ops monitoring), Opsgenie (alert notification), Docker
Real-time data replication/streaming tools
Data Modeling
Excellent verbal and written communications
Strong team player

Success Criteria
Strong analytical thinking and problem solving skills
Superior communication and business-technical interaction skills
Positive, âget it doneâ attitude
Ability to multi-task and manage multiple activities with varying timelines

To qualify, you must possess the following skills:
Bachelorâs degree in computer science, management information systems, or a related discipline
5+ years hands-on ETL/ELT design/development experience
Key resource on team(s) that have delivered successful enterprise-level analytics platforms

Zoro is an Equal Opportunity Workplace and an Affirmative Action Employer.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status."," Strong ELT/ ETL designer/developer Strong SQL Strong Python Structured & unstructured data expertise Cloud environment development & operations experience  e.g. AWS, GCP  Preference for candidates experienced with  Google Cloud Platform  GCP  and associated services; e.g. BigQuery, GCS, Cloud Composer, Dataproc, Dataflow, Dataprep, Cloud Pub/Sub, Metadata DB, Data Studio, Datalab, other Other important Zoro tools  Apache Airflow  scheduler , Bitbucket and git  version control , Stackdriver  ops monitoring , Opsgenie  alert notification , Docker Real-time data replication/streaming tools Data Modeling Excellent verbal and written communications Strong team player  Participate in Requirements Gathering  work with key business partner groups  e.g. Product Mgt  and other Data Engineering personnel to understand department-level data requirements for the ZDP Design Data Pipelines  work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP Build Data Pipelines  leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP Support Data Quality Program  work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data Support Operations  triage alerts channeled to you and remediate as necessary Technical Documentation  leverage templates provided and create clear, simple and comprehensive documentation for your development Key contributor to defining, implementing and supporting  Data Services Data Dictionary Tool Standards Best Practices Data Lineage User Training   ","Strong ELT/ ETL designer/developer SQL Python Structured & unstructured data expertise Cloud environment development operations experience e.g. AWS, GCP Preference for candidates experienced with Google Platform and associated services; BigQuery, GCS, Composer, Dataproc, Dataflow, Dataprep, Pub/Sub, Metadata DB, Data Studio, Datalab, other Other important Zoro tools Apache Airflow scheduler , Bitbucket git version control Stackdriver ops monitoring Opsgenie alert notification Docker Real-time replication/streaming Modeling Excellent verbal written communications team player Participate in Requirements Gathering work key business partner groups Product Mgt Engineering personnel to understand department-level requirements the ZDP Design Pipelines on an overall design flowing from various internal external sources into Build leverage standard toolset develop ETL/ELT code move Support Quality Program QA Engineer identify automated checks alerting ensure maintains consistently high quality Operations triage alerts channeled you remediate as necessary Technical Documentation templates provided create clear, simple comprehensive documentation your Key contributor defining, implementing supporting Services Dictionary Tool Standards Best Practices Lineage User Training","Strong ELT/ ETL designer/developer SQL Python Structured & unstructured data expertise Cloud environment development operations experience e.g. AWS, GCP Preference candidates experienced Google Platform associated services; BigQuery, GCS, Composer, Dataproc, Dataflow, Dataprep, Pub/Sub, Metadata DB, Data Studio, Datalab, Other important Zoro tools Apache Airflow scheduler , Bitbucket git version control Stackdriver ops monitoring Opsgenie alert notification Docker Real-time replication/streaming Modeling Excellent verbal written communications team player Participate Requirements Gathering work key business partner groups Product Mgt Engineering personnel understand department-level requirements ZDP Design Pipelines overall design flowing various internal external sources Build leverage standard toolset develop ETL/ELT code move Support Quality Program QA Engineer identify automated checks alerting ensure maintains consistently high quality Operations triage alerts channeled remediate necessary Technical Documentation templates provided create clear, simple comprehensive documentation Key contributor defining, implementing supporting Services Dictionary Tool Standards Best Practices Lineage User Training"
97,Data Engineer,"Sr Data Engineer, Zoro","Chicago, IL 60603",Chicago,IL,"Position: Senior Data Engineer
Location: Chicago (primary), Buffalo Grove (secondary)
Reports To: Director â Data Engineering

In the past six years, Zoro has grown from a group of 6 people working out of a 2,000 square foot building, offering fewer than 100,000 products to a group of 450+ working out of a 60,000+ square foot building, offering more than 3,000,000 unique products.

Imagine what you could help us achieve as a Senior Data Engineer!

The Senior Data Engineer will collaborate within Data Engineering and with other IT groups, business partners and external service providers, to play a key role in building, maintaining and supporting our new analytics platform, the âZoro Data Platform (ZDP)â. S/he will also provide direction to, and oversee various service providers and junior engineersâ activities.

Job Responsibilities Include:
Primary responsibility for Zoro Data Platform (ZDP):
Data model ongoing design & development
Conceptual, logical and physical design (database, ODS, aggregates, etc.)
Database administration
Capacity analysis & management
MDM Lead
Identify key domains thatâd benefit from an MDM approach (e.g. Product, Customer), along with best data sources & necessary attributes, and integrate into the ZDP
Define governance strategy with associated roles & responsibilities (e.g. Data Steward, Quality Specialist)
Define & implement Policies & SOPs
Monitor operations, develop and report quality metrics to key stakeholders
Data Pipeline development:
Participate in Requirements Gathering: work with key business partner groups (e.g. Product Mgt) and other Data Engineering personnel to understand department-level data requirements for the ZDP
Design Data Pipelines: work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP
Build Data Pipelines: leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP
Support Data Quality Program: work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data
Support Operations: triage alerts channeled to you and remediate as necessary
Technical Documentation: leverage templates provided and create clear, simple and comprehensive documentation for your development
Key contributor to defining, implementing and supporting:
Data Services
Data Dictionary
Tool Standards
Best Practices
Data Lineage
User Training
Define Best Practices and Guidelines for other Data Engineering team members
Lead the team in developing new technical skills necessary for cloud-native data engineering platform
Explores new tech
Shares and documents learnings
Productionalizes proof of concepts

Skills & Responsibilities:
Expert-level data modeler (back-end and semantic layer)
Expert-level ETL/ELT designer/developer
Strong database administration and operations experience & proficiency
Strong SQL
Structured & unstructured data expertise
Cloud environment development & operations experience (e.g. Google Cloud Platform/GCP experience a plus)
Excellent verbal and written communications
Strong team player
Working knowledge of eCommerce data a plus
Prior experience with Git, Terraform, GCP Deployment Manager, CICD, Docker, Kubernetes, Apache Airflow, Apache Beam, Apache Spark experience is a plus

Success Criteria:
Expert knowledge of data modeling concepts and data relationships
Advanced Analytical Thinking and Problem Solving skills
Solid experience in architecture, advanced reporting and dashboards
Strong SQL skills and experience with performance tuning are required
âGet it doneâ attitude
Superior Communication and Business-Technical Interaction skills

To qualify, you must possess the following skills:
Bachelorâs degree in computer science, management information systems, or a related discipline
15+ years hands-on data warehouse-data modeling experience
15+ years hands-on database admin/ops experience
10+ years hands-on ETL/ELT design/development experience
Key resource on team(s) that have delivered successful enterprise-level analytics platforms

Zoro is an Equal Opportunity Workplace and an Affirmative Action Employer.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status."," Expert-level data modeler  back-end and semantic layer  Expert-level ETL/ELT designer/developer Strong database administration and operations experience & proficiency Strong SQL Structured & unstructured data expertise Cloud environment development & operations experience  e.g. Google Cloud Platform/GCP experience a plus  Excellent verbal and written communications Strong team player Working knowledge of eCommerce data a plus Prior experience with Git, Terraform, GCP Deployment Manager, CICD, Docker, Kubernetes, Apache Airflow, Apache Beam, Apache Spark experience is a plus   Primary responsibility for Zoro Data Platform  ZDP   Data model ongoing design & development Conceptual, logical and physical design  database, ODS, aggregates, etc.  Database administration Capacity analysis & management MDM Lead Identify key domains thatâd benefit from an MDM approach  e.g. Product, Customer , along with best data sources & necessary attributes, and integrate into the ZDP Define governance strategy with associated roles & responsibilities  e.g. Data Steward, Quality Specialist  Define & implement Policies & SOPs Monitor operations, develop and report quality metrics to key stakeholders Data Pipeline development  Participate in Requirements Gathering  work with key business partner groups  e.g. Product Mgt  and other Data Engineering personnel to understand department-level data requirements for the ZDP Design Data Pipelines  work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP Build Data Pipelines  leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP Support Data Quality Program  work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data Support Operations  triage alerts channeled to you and remediate as necessary Technical Documentation  leverage templates provided and create clear, simple and comprehensive documentation for your development Key contributor to defining, implementing and supporting  Data Services Data Dictionary Tool Standards Best Practices Data Lineage User Training Define Best Practices and Guidelines for other Data Engineering team members Lead the team in developing new technical skills necessary for cloud-native data engineering platform Explores new tech Shares and documents learnings Productionalizes proof of concepts    ","Expert-level data modeler back-end and semantic layer ETL/ELT designer/developer Strong database administration operations experience & proficiency SQL Structured unstructured expertise Cloud environment development e.g. Google Platform/GCP a plus Excellent verbal written communications team player Working knowledge of eCommerce Prior with Git, Terraform, GCP Deployment Manager, CICD, Docker, Kubernetes, Apache Airflow, Beam, Spark is Primary responsibility for Zoro Data Platform ZDP model ongoing design Conceptual, logical physical database, ODS, aggregates, etc. Database Capacity analysis management MDM Lead Identify key domains thatâd benefit from an approach Product, Customer , along best sources necessary attributes, integrate into the Define governance strategy associated roles responsibilities Steward, Quality Specialist implement Policies SOPs Monitor operations, develop report quality metrics to stakeholders Pipeline Participate in Requirements Gathering work business partner groups Product Mgt other Engineering personnel understand department-level requirements Design Pipelines on overall flowing various internal external Build leverage standard toolset code move Support Program QA Engineer identify automated checks monitoring alerting ensure maintains consistently high Operations triage alerts channeled you remediate as Technical Documentation templates provided create clear, simple comprehensive documentation your Key contributor defining, implementing supporting Services Dictionary Tool Standards Best Practices Lineage User Training Guidelines members developing new technical skills cloud-native engineering platform Explores tech Shares documents learnings Productionalizes proof concepts","Expert-level data modeler back-end semantic layer ETL/ELT designer/developer Strong database administration operations experience & proficiency SQL Structured unstructured expertise Cloud environment development e.g. Google Platform/GCP plus Excellent verbal written communications team player Working knowledge eCommerce Prior Git, Terraform, GCP Deployment Manager, CICD, Docker, Kubernetes, Apache Airflow, Beam, Spark Primary responsibility Zoro Data Platform ZDP model ongoing design Conceptual, logical physical database, ODS, aggregates, etc. Database Capacity analysis management MDM Lead Identify key domains thatâd benefit approach Product, Customer , along best sources necessary attributes, integrate Define governance strategy associated roles responsibilities Steward, Quality Specialist implement Policies SOPs Monitor operations, develop report quality metrics stakeholders Pipeline Participate Requirements Gathering work business partner groups Product Mgt Engineering personnel understand department-level requirements Design Pipelines overall flowing various internal external Build leverage standard toolset code move Support Program QA Engineer identify automated checks monitoring alerting ensure maintains consistently high Operations triage alerts channeled remediate Technical Documentation templates provided create clear, simple comprehensive documentation Key contributor defining, implementing supporting Services Dictionary Tool Standards Best Practices Lineage User Training Guidelines members developing new technical skills cloud-native engineering platform Explores tech Shares documents learnings Productionalizes proof concepts"
98,Data Engineer,Data Engineer,"Chicago, IL",Chicago,IL,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ","Expertise in at least one of the following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing more languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting other customer-facing role","Expertise least one following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role"
99,Data Engineer,Data Engineer (Operations),"Chicago, IL",Chicago,IL,"Itâs Time For A Changeâ¦
Your Future Evolves Here
Evolent Health has a bold mission to change the health of the nation by changing the way health care is delivered. Our pursuit of this mission is the driving force that brings us to work each day. We believe in embracing new ideas, challenging ourselves and failing forward. We respect and celebrate individual talents and team wins. We have fun while working hard and Evolenteers often make a difference in everything from scrubs to jeans.
Are we growing? Absolutely about 40% in year-over-year revenue growth in 2018 . Are we recognized? Definitely. We have been named one of âBeckerâs 150 Great Places to Work in Healthcareâ in 2016, 2017, 2018 and 2019, and One of the â50 Great Places to Workâ in 2017 by Washingtonian. We recognize employees that live our values, give back to our communities each year, and are champions for bringing our whole selves to work each day. If youâre looking for a place where your work can be personally and professionally rewarding, donât just join a company with a mission. Join a mission with a company behind it.
Our Technical team is looking for a Data Engineer to work on critical, customer impacting events, communication activity, and escalations while the incidents are occurring. Post incident resolution, the Senior Data Engineer assists with technology improvement activities to ensure continuous improvement of all services provided by the technical team. The Senior Data Engineer combines a passion for protecting customers with an ability to think quickly and take decisive action; beyond just putting out fires, working with other teams to ensure they donât happen again.

Required Skills
Strong attention to detail, ensuring processes are followed and root cause remediation is planned and executed for each issue, and that actions are fully documented.
Critical thinker with the vision to work both tactically and strategically.
Exceptional verbal and written communication skills, ability to modify communication style to match the appropriate level of the audience targeted, with strong understanding of the impact of a message on the organization or customer.

Responsibilities
Provide technical and process leadership support to management in driving issue resolution, escalations, SLA adherence, and support process improvements
Collaborate with other team members across IT, Operations, and business functions to troubleshoot and resolve support tickets
Intermediate level T-SQL experience in writing/maintaining stored procedures and complex queries
Reverse engineer application code and configurations and write custom/ad-hoc T-SQL queries to diagnose critical application issues reported by end users
Troubleshoot and provide necessary bug fixes related to client and user reported issues
Provide maintenance support to existing applications
Conduct presentations of the work where requested and participate in knowledge sharing sessions with others on the team

Qualifications
Bachelorâs degree in computer science or another technology/business field of study
4+ years of T-SQL experience
4-5 years of progressive experience within application support/IT operations organizations
Experience in Microsoft .NET Technologies, C#, Windows Forms
Analytical thinker with a problem solving and an entrepreneurial mindset
Ability to communicate and work with both technical and non-technical audiences
Highly Desired Additional Skills
Healthcare industry experience (desirable)
Experience with JIRA Service Desk ticketing system/Confluence
Understanding of Healthcare related EDI ANSI X12, HL7, etc. data formats
Experience in performance tuning databases and SQL statements

Evolent Health is an equal opportunity employer and considers all qualified applicants equally without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin."," Bachelorâs degree in computer science or another technology/business field of study 4+ years of T-SQL experience 4-5 years of progressive experience within application support/IT operations organizations Experience in Microsoft .NET Technologies, C , Windows Forms Analytical thinker with a problem solving and an entrepreneurial mindset Ability to communicate and work with both technical and non-technical audiences  Strong attention to detail, ensuring processes are followed and root cause remediation is planned and executed for each issue, and that actions are fully documented. Critical thinker with the vision to work both tactically and strategically. Exceptional verbal and written communication skills, ability to modify communication style to match the appropriate level of the audience targeted, with strong understanding of the impact of a message on the organization or customer.  Provide technical and process leadership support to management in driving issue resolution, escalations, SLA adherence, and support process improvements Collaborate with other team members across IT, Operations, and business functions to troubleshoot and resolve support tickets Intermediate level T-SQL experience in writing/maintaining stored procedures and complex queries Reverse engineer application code and configurations and write custom/ad-hoc T-SQL queries to diagnose critical application issues reported by end users Troubleshoot and provide necessary bug fixes related to client and user reported issues Provide maintenance support to existing applications Conduct presentations of the work where requested and participate in knowledge sharing sessions with others on the team  ","Bachelorâs degree in computer science or another technology/business field of study 4+ years T-SQL experience 4-5 progressive within application support/IT operations organizations Experience Microsoft .NET Technologies, C , Windows Forms Analytical thinker with a problem solving and an entrepreneurial mindset Ability to communicate work both technical non-technical audiences Strong attention detail, ensuring processes are followed root cause remediation is planned executed for each issue, that actions fully documented. Critical the vision tactically strategically. Exceptional verbal written communication skills, ability modify style match appropriate level audience targeted, strong understanding impact message on organization customer. Provide process leadership support management driving issue resolution, escalations, SLA adherence, improvements Collaborate other team members across IT, Operations, business functions troubleshoot resolve tickets Intermediate writing/maintaining stored procedures complex queries Reverse engineer code configurations write custom/ad-hoc diagnose critical issues reported by end users Troubleshoot provide necessary bug fixes related client user maintenance existing applications Conduct presentations where requested participate knowledge sharing sessions others","Bachelorâs degree computer science another technology/business field study 4+ years T-SQL experience 4-5 progressive within application support/IT operations organizations Experience Microsoft .NET Technologies, C , Windows Forms Analytical thinker problem solving entrepreneurial mindset Ability communicate work technical non-technical audiences Strong attention detail, ensuring processes followed root cause remediation planned executed issue, actions fully documented. Critical vision tactically strategically. Exceptional verbal written communication skills, ability modify style match appropriate level audience targeted, strong understanding impact message organization customer. Provide process leadership support management driving issue resolution, escalations, SLA adherence, improvements Collaborate team members across IT, Operations, business functions troubleshoot resolve tickets Intermediate writing/maintaining stored procedures complex queries Reverse engineer code configurations write custom/ad-hoc diagnose critical issues reported end users Troubleshoot provide necessary bug fixes related client user maintenance existing applications Conduct presentations requested participate knowledge sharing sessions others"
100,Data Engineer,Data Engineer,"Chicago, IL 60601",Chicago,IL,"Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of whatâs possibleâtogether.

Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to over 8,000 employees. We were named one of Fortuneâs 100 Best Companies to Work For in 2018 and are regularly recognized by our employees as a best place to work. You can find us in 32 cities across the U.S., U.K., and Canada.
Slalom is proud to be a Premier AWS Partner, Microsoft Gold Partner, Google Premier Partner, 5x Tableau Partner of the Year, and 2018 Snowflake Partner of the Year.

The Data & Analytics practice at Slalom is a full-service data practice with competencies across information strategy, modern data architecture, data visualization, and data science. We are seeking a Cloud Data Engineer to join our local Chicago consulting team who is passionate about developing innovative solutions to help organizations drive strategic business outcomes and enable data-driven insights. Slalom Cloud Data Engineers are cloud data platform engineering specialists responsible for client delivery, complex solutioning and knowledge management.

Responsibilities
Work as part of a team to design and develop cloud data solutions at local Chicago clients
Deliver on the technical scope of projects & demonstrate thought leadership at clients as well as internally at Slalom
Gather technical requirements, assess client capabilities and analyze findings to provide appropriate cloud solution recommendations and adoption strategy
Research, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems
Assist in designing multi-phased cloud data strategies, including designing multi-phased implementation roadmaps
Analyze, architect, design, and actively develop cloud data warehouse, data lakes, and other cloud-based data solutions
Design and develop scalable data ingestion frameworks to transform variety of datasets
Serve as a subject matter expert in a cloud platform for larger the Slalom practice and contribute back to community

Experience
4+ years of data engineering and/or data warehousing experience
2+ years of deep experience building cloud data solutions (Azure, AWS, GCP, Snowflake)
Experience migrating from an on-prem data to cloud data platform.
Deep experience with designing and deploying end to end solutions with a cloud platformâs analytic services including storage, permissions, private cloud, database services, virtual machines, and parallel processing technologies.
Experience with big data application development and/or with cloud data warehousing (e.g. Hadoop, Spark, Redshift, Snowflake, Azure SQL DW, BigQuery)
Working knowledge of agile development including DevOps concepts
Experience with cloud SDKs and programmatic access services
Proficient in a relevant programming language for cloud platform e.g. Python/Java/C#/Unix
Proficient in SQL
Working experience with version control platforms e.g. Git
Strong communication skills

Preferred
One or more cloud certifications
Consulting experience
Expert programming skills in Python and a software development background
Experience writing âinfrastructure as codeâ deployments e.g. ARM, CloudFormation, Terraform
Understanding of cloud strategies and best practices expanding into cloud networking, cloud security, encryption, private cloud configuration, and overall cloud governance approaches
Strong background in data warehousing concepts, ETL development, data modeling, metadata management, and data quality

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law.","   Work as part of a team to design and develop cloud data solutions at local Chicago clients Deliver on the technical scope of projects & demonstrate thought leadership at clients as well as internally at Slalom Gather technical requirements, assess client capabilities and analyze findings to provide appropriate cloud solution recommendations and adoption strategy Research, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems Assist in designing multi-phased cloud data strategies, including designing multi-phased implementation roadmaps Analyze, architect, design, and actively develop cloud data warehouse, data lakes, and other cloud-based data solutions Design and develop scalable data ingestion frameworks to transform variety of datasets Serve as a subject matter expert in a cloud platform for larger the Slalom practice and contribute back to community  ","Work as part of a team to design and develop cloud data solutions at local Chicago clients Deliver on the technical scope projects & demonstrate thought leadership well internally Slalom Gather requirements, assess client capabilities analyze findings provide appropriate solution recommendations adoption strategy Research, analyze, recommend select approaches for solving challenging complex development integration problems Assist in designing multi-phased strategies, including implementation roadmaps Analyze, architect, design, actively warehouse, lakes, other cloud-based Design scalable ingestion frameworks transform variety datasets Serve subject matter expert platform larger practice contribute back community","Work part team design develop cloud data solutions local Chicago clients Deliver technical scope projects & demonstrate thought leadership well internally Slalom Gather requirements, assess client capabilities analyze findings provide appropriate solution recommendations adoption strategy Research, analyze, recommend select approaches solving challenging complex development integration problems Assist designing multi-phased strategies, including implementation roadmaps Analyze, architect, design, actively warehouse, lakes, cloud-based Design scalable ingestion frameworks transform variety datasets Serve subject matter expert platform larger practice contribute back community"
101,Data Engineer,Data Engineer Intern,"Chicago, IL 60606",Chicago,IL,"Only accepting applicants local to the state of the posting.

Are you looking to gain real world experience in cutting-edge technology firm with an innovative culture?
Consider an internship and start building your career at Concurrency. Our competitive intern program is an exciting experience that offers interns like yourself exposure to the real, business world and a meaningful work experience. Interns are treated as valuable team members while learning from experienced professionals in a collaborative, team environment.
Concurrency is an award-winning Microsoft Gold Partner with over 30 years of helping customers achieve digital transformation. Concurrency helps clients find better ways to leverage technology to fulfill their strategies and ultimately improve their businesses.
Responsibilities
Helping transform data into usable information based on business needs and requirements
Working closely on a cross functional team including other engineers, analysts, and architects
Identifying and implementing automation opportunities that streamline and enhance the process
Interacting with customers and creating technology solutions to help their business succeed
Learning new technologies with an eagerness to learn innovative and unfamiliar languages quickly
Participating in information gathering sessions with the clients to understand data requirements and objectives
Designing reports, dashboards, and metrics to summarize information and leverage solutions
Qualifications
Actively pursuing a degree in Information Science & Technology, Computer Science, or related field.
Relevant Computer Science coursework, development bootcamp, or experience on any platform as well as a passion for coding
Preferred skills
Excellent analytical and problem-solving skills
Familiarity in data analysis code (SQL, SAS, Oracle, Tableau, PowerBI, Python)
Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Concurrency a great place to work. Concurrency full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and a comprehensive benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs, excellent training program and bonus opportunities."," Actively pursuing a degree in Information Science & Technology, Computer Science, or related field. Relevant Computer Science coursework, development bootcamp, or experience on any platform as well as a passion for coding   Helping transform data into usable information based on business needs and requirements Working closely on a cross functional team including other engineers, analysts, and architects Identifying and implementing automation opportunities that streamline and enhance the process Interacting with customers and creating technology solutions to help their business succeed Learning new technologies with an eagerness to learn innovative and unfamiliar languages quickly Participating in information gathering sessions with the clients to understand data requirements and objectives Designing reports, dashboards, and metrics to summarize information and leverage solutions  ","Actively pursuing a degree in Information Science & Technology, Computer Science, or related field. Relevant coursework, development bootcamp, experience on any platform as well passion for coding Helping transform data into usable information based business needs and requirements Working closely cross functional team including other engineers, analysts, architects Identifying implementing automation opportunities that streamline enhance the process Interacting with customers creating technology solutions to help their succeed Learning new technologies an eagerness learn innovative unfamiliar languages quickly Participating gathering sessions clients understand objectives Designing reports, dashboards, metrics summarize leverage","Actively pursuing degree Information Science & Technology, Computer Science, related field. Relevant coursework, development bootcamp, experience platform well passion coding Helping transform data usable information based business needs requirements Working closely cross functional team including engineers, analysts, architects Identifying implementing automation opportunities streamline enhance process Interacting customers creating technology solutions help succeed Learning new technologies eagerness learn innovative unfamiliar languages quickly Participating gathering sessions clients understand objectives Designing reports, dashboards, metrics summarize leverage"
102,Data Engineer,Data Engineer,"Chicago, IL 60616",Chicago,IL,"Description:
U.S. Soccer Overview
We are U.S. Soccer and we are the future of sport in the United States. Our mission is to make soccer the preeminent sport in the United States. We embrace diversity, technology and global connections to drive the growth of our sport and by supporting members, impacting athletes, and serving fans. We seek motivated, passionate, skilled people who can think, create and work on a team.

U.S. Soccer is a growing company that looks for team members to grow with it. U.S. Soccer offers a comprehensive compensation package, casual work environment, an inclusive culture and an atmosphere for professional development.

Position Description
The Data Engineer will be responsible for assisting in managing the interchange of data between transactional systems and analytical applications.
The position will initially focus on unifying data from discrete sources into a data management platform but will also be responsible for long term strategic design and implementation of a suite of highly robust applications. The position focuses on programmatic solution building but also requires a strong understanding of sporting business, technology, and a passion for soccer.

Primary Responsibilities

Help manage U.S. Soccerâs data pipeline
Collaborate with the data engineering team to design, integrate and maintain schemas to meet the needs of various platforms
Focus on security and data protection
Employ cloud-based data storage best practices
Review new technologies and create best practices in enterprise data architecture and their suitability for the organizationâs mission and operations
Collaborate with all U.S. Soccer departments and partner organizations to evolve datasets and create scalable solutions
Collaborate with the Technology team to facilitate long-term application and systems integration through centralized cloud-based services
Additional duties as assigned.

Requirements:
Minimum Qualifications

Bachelor's degree
Developing applications in a linux environment in at least one of the following languages: Python, Scala, Java
Experience with file formats JSON, CSV, XML
Experience maintaining shared code bases
Previous experience using APIs and web services: SOAP, REST
Deep understanding of relational and dimensional database designs
Experience designing and maintaining relational databases such as MySQL and columnar databases such as Redshift
Experience with integrating multiple data sources and managing large data structures (data warehouse, data lake) in cloud architectures (Azure, AWS)
Proven ability to communicate effectively with technical and non-technical stakeholders
Excellent written and verbal communication skills
Exceptional time management skills and ability to prioritize workflow
Full-time position, 40+ hour work week
Some weekend and holiday work may be required (depending on the project)

Desired Qualifications

Bachelor's degree in computer science, information science, engineering or a similar field
Graduate degree is a plus
Knowledge of BI, Analytics, AI, and machine learning
Experience creating data pipelines using Apache Spark
Passion for soccer and a solid understanding of the soccer landscape in the United States

Working at U.S. Soccer is a unique opportunity. Employees who work at U.S. Soccer have the following attributes:

Adopt a company centric approachâServe the Athlete and the Fan
Embrace and see learning as a lifelong pursuit
Possess a growth mindsetâkeeps an open mind and seeks new challenges
Practice self-assessment and self-reflection
Open to criticism and does not make excuses
Possess a tireless work ethic
Wants to be part of a team that wins
Has the ability to be firm but fair
Communicate in a direct, open and honest manner
Build relationships through genuine interpersonal skills

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","     Bachelor's degree Developing applications in a linux environment in at least one of the following languages  Python, Scala, Java Experience with file formats JSON, CSV, XML Experience maintaining shared code bases Previous experience using APIs and web services  SOAP, REST Deep understanding of relational and dimensional database designs Experience designing and maintaining relational databases such as MySQL and columnar databases such as Redshift Experience with integrating multiple data sources and managing large data structures  data warehouse, data lake  in cloud architectures  Azure, AWS  Proven ability to communicate effectively with technical and non-technical stakeholders Excellent written and verbal communication skills Exceptional time management skills and ability to prioritize workflow Full-time position, 40+ hour work week Some weekend and holiday work may be required  depending on the project  ","Bachelor's degree Developing applications in a linux environment at least one of the following languages Python, Scala, Java Experience with file formats JSON, CSV, XML maintaining shared code bases Previous experience using APIs and web services SOAP, REST Deep understanding relational dimensional database designs designing databases such as MySQL columnar Redshift integrating multiple data sources managing large structures warehouse, lake cloud architectures Azure, AWS Proven ability to communicate effectively technical non-technical stakeholders Excellent written verbal communication skills Exceptional time management prioritize workflow Full-time position, 40+ hour work week Some weekend holiday may be required depending on project","Bachelor's degree Developing applications linux environment least one following languages Python, Scala, Java Experience file formats JSON, CSV, XML maintaining shared code bases Previous experience using APIs web services SOAP, REST Deep understanding relational dimensional database designs designing databases MySQL columnar Redshift integrating multiple data sources managing large structures warehouse, lake cloud architectures Azure, AWS Proven ability communicate effectively technical non-technical stakeholders Excellent written verbal communication skills Exceptional time management prioritize workflow Full-time position, 40+ hour work week Some weekend holiday may required depending project"
103,Data Engineer,Senior Big Data Engineer,"Chicago, IL 60604",Chicago,IL,"Cars.com is one of Chicago's original tech companies. Our online platform makes it easier for consumers to shop for, sell, and service their cars. With our expert content, mobile app features, millions of new and used vehicle listings, a comprehensive set of research tools and the largest database of consumer reviews in the industry, Cars.com offers innovative products to connect consumers with dealers across the country.

Data is the driver of our future at Cars. We're searching for highly collaborative, analytical, and innovative Engineers to build and scale our big data and ML platform. If you are passionate about using data to solve problems and build game changing products, we'd love to work with you.

About the Role:
Working within a dynamic and fast paced team environment, the Senior Engineer is responsible for the design, construction, and maintenance of mission-critical, highly visible big data and machine learning applications in direct support of Cars.com business objectives. Furthermore, this person is responsible for working with other engineers to develop the technical design by fully understanding the technical details, integration, and functions of multiple applications across their development team. The ideal candidate should have good mentoring and cross-functional skills.

About the Team:
The Big Data and Machine Learning team at Cars.com is responsible for building big data pipelines and deriving insights out of the data using advanced analytic techniques, streaming and machine learning at scale.

Qualifications:

5+ years of experience in programming languages such as Java or Python.
3+ years of experience in big data engineering.
2+ years of experience as Spark Developer.

Required Skills:

Ability to develop spark jobs to cleanse/enrich/process large amounts of data.
Ability to develop spark streaming jobs to read data from Kafka.
Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc.
Good understanding of various file formats and compression techniques.
Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data.
Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level.
Ability to mentor developers and lead projects of medium to high complexity.
Excellent communication and collaboration skills.

Bonus:

Experience developing Big Data applications in the cloud, especially AWS.
Experience tuning HIVE queries.
Experience in deploying ML models into production and integrating them into production applications for use.
Experience with Spark ML.
Experience with machine learning / deep learning using R, Python, Jupyter, Zeppelin, TensorFlow, etc.

"," 5+ years of experience in programming languages such as Java or Python. 3+ years of experience in big data engineering. 2+ years of experience as Spark Developer.   Ability to develop spark jobs to cleanse/enrich/process large amounts of data. Ability to develop spark streaming jobs to read data from Kafka. Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc. Good understanding of various file formats and compression techniques. Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data. Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level. Ability to mentor developers and lead projects of medium to high complexity. Excellent communication and collaboration skills.    ","5+ years of experience in programming languages such as Java or Python. 3+ big data engineering. 2+ Spark Developer. Ability to develop spark jobs cleanse/enrich/process large amounts data. streaming read from Kafka. Experience with tuning for efficient performance including execution time the job, memory, etc. Good understanding various file formats and compression techniques. source code management systems GIT developing CI/CD pipelines tools Jenkins understand deeply entire architecture a major part business be able articulate scaling reliability limits that area; design, debug at an enterprise level design estimate cross-project level. mentor developers lead projects medium high complexity. Excellent communication collaboration skills.","5+ years experience programming languages Java Python. 3+ big data engineering. 2+ Spark Developer. Ability develop spark jobs cleanse/enrich/process large amounts data. streaming read Kafka. Experience tuning efficient performance including execution time job, memory, etc. Good understanding various file formats compression techniques. source code management systems GIT developing CI/CD pipelines tools Jenkins understand deeply entire architecture major part business able articulate scaling reliability limits area; design, debug enterprise level design estimate cross-project level. mentor developers lead projects medium high complexity. Excellent communication collaboration skills."
104,Data Engineer,Data Engineer,"Chicago, IL 60654",Chicago,IL,"Job Title: Data Engineer Reports To:
Deputy Director/Chief Informatics
Officer
Location: Chicago Travel Required:
Level/Salary Range: DOE Position Type: Exempt
HR Contact: Claudria Hurt Date Posted: 07/31/2019
External Posting URL:
Applications Accepted By:
Email: careers@alliancechicago.org or Fax: 312.274.0069
Subject Line: Data Engineer
Job Description:
Position Overview:
Responsible for building out and migrating the AllianceChicago data warehouse to the Health Catalyst platform,
and then expanding and maintaining the warehouse. This position has primary responsibility for the extraction of
health information from diverse clinical and business sources and its transformation into objects in a standardized
data model. These objects will be the primary source for quality and performance reporting activities for
AllianceChicago, its members, stakeholders and user community or primary care delivery sites. The platform also
supports research and grant reporting needs, queries for public health surveillance, and other deliverables. The
data extraction work involves design, construction, and maintenance of modules using the Health Catalyst toolset.

The work provides the data for analysists and presentation specialists working in Power BI, SQL, SSRS, Excel and
Health Catalyst presentation tools, and benefits from familiarity with these technologies. Overall the position
contributes to the design, build, testing, and maintenance of the AllianceChicagoâs data warehouse.
Essential Duties:
Overseeing the initial load to Health Catalyst data lake, a process that will be staffed by Health Catalyst
Working with AllianceChicago subject matter experts to design the rules to transform data lake data into
standardized models of health data (Health Catalysts DOS environment) including encounters, problems,
screenings, interventions, tests, and medications
Acquiring from Health Catalyst the knowledge to implement the design of the data model
Manage the transition and go-live from the current data warehouse environment based on Microsoft
APS/PDW and SSIS extraction to the Health Catalyst environment
Maintenance of the extract and transformation process
Supports Alliance SQL query analysts, report writers and Alliance partners who perform data analysis
from the DOS model or the data lake
Works with query writers and report designers to resolves data issues with current reports.
Is a member of the Alliance Data Warehouse team, and works collaboratively with its members and other
stakeholders
Oversees bringing new source systems into the Catalystâs Data Operating System
Ability to dig into the data and understand business logic within the source system data
Perform data validation tests to ensure extractions and transformations are true to the source
Required Skills:
Intermediate to advanced level in Structured Query Language (SQL)
Experience working with EMR\EHR systems and an understanding of the healthcare clinical domain
Exposure to Extract, Transform and Load (ETL) concepts and processes
Excellent analytical and troubleshooting skills
Working knowledge of database principles, processes, technologies and tools
Other Requirements:
Flexibility and ability to work with individuals of diverse backgrounds and educational levels
Ability to function in a collaborative and collegial environment as a team player.
Ability to coordinate and communicate effectively with other team members
Ability to generate trust and build alliances with coworkers
Self-motivated; comfortable working under general direction
Strong sense of customer service to consistently and effectively address client needs
Ability to work in variety of settings
Detail oriented; highly organized; ability to prioritize and set expectations
Strong technical writing skills and written communications skills
Strong Analytical skills
Ability to work independently to organize work in a manner that ensures accuracy and efficiency
Familiarity with agile development process


Education/Training/Expertise:

Bachelor Level preparation in computer, mathematical, information sciences or equivalent training â
Master Level Preferred
Experience/Years:

At least three years of experience with Microsoft SQL Server 2008 or higher including SSIS and SSAS,
and familiarity with the MS BI stack. Familiarity with other data, analytic and reporting tools
5-8 years of experience in computer science or information science related field
Experience with Relational Databases and data mining

Working Conditions:
General office setting, extensive telephone and desk work at computer terminal
May be required to lift, carry, bend, reach and stand with parcels up to 25 lbs.
Will work in a close multidisciplinary team environment
May interface with clients in various settings and may be working during on-site visits in clinical
environments where medical equipment, chemicals and where communicable diseases and certain
pathogens are present.

ORGANIZATIONAL OVERVIEW:
Founded by four partner Community Health Centers in 1997, AllianceChicagoâs three core areas of focus are
Health Care Collaboration, Health Information Technology, and Health Research & Education. AllianceChicago
supports the use of HIT to improve quality, efficiency, and access to services in a national network of community
Safety Net health care organizations. The mission of AllianceChicago is to improve personal, community, and
public health through innovative collaboration.

ADA Statement: The Americans with Disabilities Act prohibits discrimination and ensures equal opportunity for
persons with disabilities in employment, state and local government services, public accommodations, commercial
facilities, and transportation. It also mandates the establishment of TDD/telephone relay services.

EEO Statement: AllianceChicago believes that all applicants and employees are entitled to equal employment
opportunities and maintains a policy of nonâdiscrimination with respect to religion, color, sex, sexual orientation,
national origin, age, veteran status, marital status, physical or mental disability, or any other legally protected class
in accordance with applicable law, except where a bona fide occupational qualification exists. AllianceChicago will
comply with all phases of employment including, but not limited to, hiring practices, transfers, promotions, benefits,
discipline, and discharge.


Disclaimer: The above statements are intended to describe the general nature and level of work being performed
by employees assigned to this position. They are not intended to be construed as an exhaustive list of all
responsibilities, duties, and skills required of personnel as qualified."," Intermediate to advanced level in Structured Query Language  SQL    Bachelor Level preparation in computer, mathematical, information sciences or equivalent training â Flexibility and ability to work with individuals of diverse backgrounds and educational levels ","Intermediate to advanced level in Structured Query Language SQL Bachelor Level preparation computer, mathematical, information sciences or equivalent training â Flexibility and ability work with individuals of diverse backgrounds educational levels","Intermediate advanced level Structured Query Language SQL Bachelor Level preparation computer, mathematical, information sciences equivalent training â Flexibility ability work individuals diverse backgrounds educational levels"
105,Data Engineer,Data Engineer,"Chicago, IL 60654",Chicago,IL,"Riskonnect is the leading integrated risk management software solution provider that empowers organizations to anticipate, manage and respond in real-time to strategic and operational risks across the extended enterprise. Riskonnect is the only provider ranked in the leadership and visionary quadrants by world renowned industry analysts - Gartner, Forrester and Advisen RMIS Review. We employ more than 500 risk professionals in the Americas, EMEA and Asia Pacific and serve over 900 customers across 6 continents. The combination of innovative risk technology, a customer success mindset, and employee-first belief makes Riskonnect a sought after place to work.
Responsibilities:
Develop strategy for new multi-platform data integration and analytics.
Develop strategy for new multi-platform-sourced data lake.
Contribute to API strategy to facilitate application connectivity and analytics.
Contribute to the maintenance and evolution of best practices.
Contribute to process documentation.
Perform multiple proofs of concept (POCs).
Contribute to implementation plan for decided-upon solution(s).

Required Qualifications:
Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.
Experience with Data Analytics.
Experience with Web Services and APIs.
Experience in the development of batch and real-time data integration and data consolidation processes.
Experience with machine learning, AI, and data lakes.
Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.
Strong analytical skills with ability for problem-solving.
Understands the importance of data provenance and the ability to demonstrate it to clients.
Detail oriented, organized, self-motivated.

Preferred Qualifications:
Experience with Salesforce.
Experience in the Risk Management, Healthcare, Financial, and/or Insurance industries is recommended.
Experience with Financial data sets, involving financial validation.","Experience with JavaScript/Java/ Python or Jitterbit and other developer languages. Experience with Data Analytics. Experience with Web Services and APIs. Experience in the development of batch and real-time data integration and data consolidation processes. Experience with machine learning, AI, and data lakes. Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views. Strong analytical skills with ability for problem-solving. Understands the importance of data provenance and the ability to demonstrate it to clients. Detail oriented, organized, self-motivated.    Develop strategy for new multi-platform data integration and analytics. Develop strategy for new multi-platform-sourced data lake. Contribute to API strategy to facilitate application connectivity and analytics. Contribute to the maintenance and evolution of best practices. Contribute to process documentation. Perform multiple proofs of concept  POCs . Contribute to implementation plan for decided-upon solution s .    ","Experience with JavaScript/Java/ Python or Jitterbit and other developer languages. Data Analytics. Web Services APIs. in the development of batch real-time data integration consolidation processes. machine learning, AI, lakes. Proficiency TSQL/PLSQL query-writing, stored procedure development, views. Strong analytical skills ability for problem-solving. Understands importance provenance to demonstrate it clients. Detail oriented, organized, self-motivated. Develop strategy new multi-platform analytics. multi-platform-sourced lake. Contribute API facilitate application connectivity maintenance evolution best practices. process documentation. Perform multiple proofs concept POCs . implementation plan decided-upon solution s","Experience JavaScript/Java/ Python Jitterbit developer languages. Data Analytics. Web Services APIs. development batch real-time data integration consolidation processes. machine learning, AI, lakes. Proficiency TSQL/PLSQL query-writing, stored procedure development, views. Strong analytical skills ability problem-solving. Understands importance provenance demonstrate clients. Detail oriented, organized, self-motivated. Develop strategy new multi-platform analytics. multi-platform-sourced lake. Contribute API facilitate application connectivity maintenance evolution best practices. process documentation. Perform multiple proofs concept POCs . implementation plan decided-upon solution"
106,Data Engineer,Data Engineer - Hux,"Chicago, IL 60606",Chicago,IL,"Hux Data Engineer
Locations: New York, NY â Greensboro, NC - Chicago, IL â Raleigh, Durham, Chapel-Hill, NC - Denver, CO
What is Hux? Hux is the Human Experience Platform by Deloitte Digital.
In todayâs world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.
Customers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.
Hux by Deloitte Digital gives companies the ability to build and leverage the connections â between people, systems, data and technologies â so they can deliver personalized, contextual experiences to customers at scale.

Work youâll do
As a Hux Data Engineer, youâll design, implement, and maintain a full suite of real-time and batch jobs that fuels our cutting edge AI to provide real-time marketing intelligence to our existing clients.
Youâll develop, test and deliver production grade code to help our clients solve their marketing challenges using cutting-edge big-data tools. Youâll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall Platform.
As you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, youâll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.
Your responsibilities will include:
Design, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.
Bring together large, complex and sparse data sets to meet functional and non-functional business requirements.
Design and implements data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.
Integrate new data management technologies and software engineering tools into existing structures.
Help build high-performance algorithms, prototypes, predictive models and proof of concepts.
Use a variety of languages, tools and frameworks to marry data and systems together.
Recommend ways to improve data reliability, efficiency and quality.
Collaborate with Data Scientists, DevOps and Project Managers on meeting project goals.
Tackle challenges and solve complex problems on a daily basis.
Qualifications
Required:
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.
Preferred:
Producing high-quality code in Python.
Passionate about testing, and with extensive experience in Agile teams using SCRUM you consider automated build and test to be the norm.
Proven ability to communicate in both verbal and writing in a high performance, collaborative environment.
Follows data development best practices, and enjoy helping others learn to do the same.
An independent thinker who considers the operating context of what he/she is developing.
Believes that the best data pipelines run unattended for weeks and months on end.
Familiar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.
Helpful, but not required:
Knowledge in:
Experience with large consumer data sets used in performance marketing is a major advantage.
Familiarity with machine learning libraries is a plus.
Well-versed in (or contributes to) data-centric open source projects.
Reads Hacker News, blogs, or stays on top of emerging tools in some other way
Data visualization
Industry-specific marketing data
Technologies of Interest:
Languages/Libraries â Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.
Cloud â AWS, Azure, Google
The team
Advertising, Marketing & Commerce
Our Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clientsâ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clientsâ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.

We serve our clients through the following types of work:Cross-channel customer engagement strategy, design and development(web, mobile, social, physical)eCommerce strategy, implementation and operationsMarketing Content and digital asset management solutionsMarketing Technology and Advertising Technology solutionsMarketing analytics implementation and operationsAdvertising campaign ideation, development and executionAcquisition and engagement campaign ideation, development and executionAgile based, design-thinking, user-centric, empirical projects that accelerate results

How youâll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe thereâs always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitteâs culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitteâs impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youâre applying to. Check out recruiting tips from Deloitte professionals.
kwhux"," 4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment. 2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores. 1+ years of experience on distributed, high throughput and low latency architecture. 1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale. A successful track-record of manipulating, processing and extracting value from large disconnected datasets.    ","4+ years of experience in software development, a substantial part which was gained high-throughput, decision-automation related environment. 2+ working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores. 1+ on distributed, high throughput low latency architecture. deploying or managing pipelines for supporting data-science-driven decisioning at scale. A successful track-record manipulating, processing extracting value from large disconnected datasets.","4+ years experience software development, substantial part gained high-throughput, decision-automation related environment. 2+ working big data using technologies like Spark, Kafka, Flink, Hadoop, NoSQL datastores. 1+ distributed, high throughput low latency architecture. deploying managing pipelines supporting data-science-driven decisioning scale. A successful track-record manipulating, processing extracting value large disconnected datasets."
107,Data Engineer,Data Engineer,"Chicago, IL",Chicago,IL,"Overview
UL is seeking a Data Engineer to join the iON Compliance team in Chicago, IL.

iON Compliance is a big data platform combining data ingest, analytics, machine learning and elements of artificial intelligence within a data lake. iON is now looking to add a Data Engineer to our team to accelerate the growth of the most innovative UL digital technology investment in its history. Our goal is to enable the UL client base to effectively apply the recent developments in the digital revolution, by giving them the ability to truly leverage their own corpus of data with internal core competences and win in a rapidly evolving digital eco-system.

iON aims to disseminate Artificial Intelligence & Machine Learning based cutting edge techniques and solutions to solve challenging and complex industrial problems. Joining the iON UL team gives you the opportunity to:
Work on disruptive products that are still in its early stages
Solve challenging problems that will revolutionize text processing and text mining
Work for a company thatâs a recognized leader in data engineering and processing
Be involved in the fast growing, big data space
We are looking for data engineers with expertise and passion for building large analytical platforms systems.
Responsibilities
Profile and analyze data in designing scalable solutions
Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data
Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale.
Analyze, develop, and maintain data pipelines from internal and external sources
Ability to adapt to new technologies and learn quickly
Qualifications
Skills & Qualifications:
C# programming and .NET platform experience
Azure Data Architecture
Experience with data lakes, MS SQL, Data Bricks
NoSQL environments such as CosmosDB
Engineering and/or software development experience and demonstrable architecture experience at enterprise scale
History of working successfully with cross-functional engineering teams
Structured, Unstructured, Semi-Structured Data techniques and processes
Working knowledge of open source Apache products, such as Hadoop
Optional Additional Experience:
Big Data platforms e.g. Azure DW, SQL PDW, Cloudera, Hortonworks
Visualization: Qlikview, Tableau, D3.js
ETL tools such as Microsoft SSIS, Azure Data Factory, Pentaho PDI or Talend or Informatica
Continuous delivery and deployment using Agile Methodologies.
Data Warehouse and DataMart design and implementation
NoSQL environments such as MongoDB, Cassandra
Data modeling of relational and dimensional databases
Architect/design data management processes and capabilities (e.g governance, archiving, metadata, master data mgmt., data modeling, data quality)
A nice to have would be knowledge of ML / AI environments and challenges â particularly as it relates to data management
Experience & Education Requirements:
Bachelorâs Degree in Computer Science, Engineering, Mathematics, Statistics or related field
3+ yearsâ experience with data and cloud technologies"," C  programming and .NET platform experience Azure Data Architecture Experience with data lakes, MS SQL, Data Bricks NoSQL environments such as CosmosDB Engineering and/or software development experience and demonstrable architecture experience at enterprise scale History of working successfully with cross-functional engineering teams Structured, Unstructured, Semi-Structured Data techniques and processes Working knowledge of open source Apache products, such as Hadoop  Profile and analyze data in designing scalable solutions Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale. Analyze, develop, and maintain data pipelines from internal and external sources Ability to adapt to new technologies and learn quickly  ","C programming and .NET platform experience Azure Data Architecture Experience with data lakes, MS SQL, Bricks NoSQL environments such as CosmosDB Engineering and/or software development demonstrable architecture at enterprise scale History of working successfully cross-functional engineering teams Structured, Unstructured, Semi-Structured techniques processes Working knowledge open source Apache products, Hadoop Profile analyze in designing scalable solutions Develop highly extensible Big platforms which enable collection, storage, modeling, analysis massive sets including those from streaming Leverage technologies Gremlin API / Graph Model to host high speed applications global scale. Analyze, develop, maintain pipelines internal external sources Ability adapt new learn quickly","C programming .NET platform experience Azure Data Architecture Experience data lakes, MS SQL, Bricks NoSQL environments CosmosDB Engineering and/or software development demonstrable architecture enterprise scale History working successfully cross-functional engineering teams Structured, Unstructured, Semi-Structured techniques processes Working knowledge open source Apache products, Hadoop Profile analyze designing scalable solutions Develop highly extensible Big platforms enable collection, storage, modeling, analysis massive sets including streaming Leverage technologies Gremlin API / Graph Model host high speed applications global scale. Analyze, develop, maintain pipelines internal external sources Ability adapt new learn quickly"
108,Data Engineer,Data Engineer,"Chicago, IL",Chicago,IL,"Overview
UL is seeking a Data Engineer to join the iON Compliance team in Chicago, IL.

iON Compliance is a big data platform combining data ingest, analytics, machine learning and elements of artificial intelligence within a data lake. iON is now looking to add a Data Engineer to our team to accelerate the growth of the most innovative UL digital technology investment in its history. Our goal is to enable the UL client base to effectively apply the recent developments in the digital revolution, by giving them the ability to truly leverage their own corpus of data with internal core competences and win in a rapidly evolving digital eco-system.

iON aims to disseminate Artificial Intelligence & Machine Learning based cutting edge techniques and solutions to solve challenging and complex industrial problems. Joining the iON UL team gives you the opportunity to:
Work on disruptive products that are still in its early stages
Solve challenging problems that will revolutionize text processing and text mining
Work for a company thatâs a recognized leader in data engineering and processing
Be involved in the fast growing, big data space
We are looking for data engineers with expertise and passion for building large analytical platforms systems.
Responsibilities
Profile and analyze data in designing scalable solutions
Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data
Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale.
Analyze, develop, and maintain data pipelines from internal and external sources
Ability to adapt to new technologies and learn quickly
Qualifications
Skills & Qualifications:
C# programming and .NET platform experience
Azure Data Architecture
Experience with data lakes, MS SQL, Data Bricks
NoSQL environments such as CosmosDB
Engineering and/or software development experience and demonstrable architecture experience at enterprise scale
History of working successfully with cross-functional engineering teams
Structured, Unstructured, Semi-Structured Data techniques and processes
Working knowledge of open source Apache products, such as Hadoop
Optional Additional Experience:
Big Data platforms e.g. Azure DW, SQL PDW, Cloudera, Hortonworks
Visualization: Qlikview, Tableau, D3.js
ETL tools such as Microsoft SSIS, Azure Data Factory, Pentaho PDI or Talend or Informatica
Continuous delivery and deployment using Agile Methodologies.
Data Warehouse and DataMart design and implementation
NoSQL environments such as MongoDB, Cassandra
Data modeling of relational and dimensional databases
Architect/design data management processes and capabilities (e.g governance, archiving, metadata, master data mgmt., data modeling, data quality)
A nice to have would be knowledge of ML / AI environments and challenges â particularly as it relates to data management
Experience & Education Requirements:
Bachelorâs Degree in Computer Science, Engineering, Mathematics, Statistics or related field
3+ yearsâ experience with data and cloud technologies"," C  programming and .NET platform experience Azure Data Architecture Experience with data lakes, MS SQL, Data Bricks NoSQL environments such as CosmosDB Engineering and/or software development experience and demonstrable architecture experience at enterprise scale History of working successfully with cross-functional engineering teams Structured, Unstructured, Semi-Structured Data techniques and processes Working knowledge of open source Apache products, such as Hadoop  Profile and analyze data in designing scalable solutions Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale. Analyze, develop, and maintain data pipelines from internal and external sources Ability to adapt to new technologies and learn quickly  ","C programming and .NET platform experience Azure Data Architecture Experience with data lakes, MS SQL, Bricks NoSQL environments such as CosmosDB Engineering and/or software development demonstrable architecture at enterprise scale History of working successfully cross-functional engineering teams Structured, Unstructured, Semi-Structured techniques processes Working knowledge open source Apache products, Hadoop Profile analyze in designing scalable solutions Develop highly extensible Big platforms which enable collection, storage, modeling, analysis massive sets including those from streaming Leverage technologies Gremlin API / Graph Model to host high speed applications global scale. Analyze, develop, maintain pipelines internal external sources Ability adapt new learn quickly","C programming .NET platform experience Azure Data Architecture Experience data lakes, MS SQL, Bricks NoSQL environments CosmosDB Engineering and/or software development demonstrable architecture enterprise scale History working successfully cross-functional engineering teams Structured, Unstructured, Semi-Structured techniques processes Working knowledge open source Apache products, Hadoop Profile analyze designing scalable solutions Develop highly extensible Big platforms enable collection, storage, modeling, analysis massive sets including streaming Leverage technologies Gremlin API / Graph Model host high speed applications global scale. Analyze, develop, maintain pipelines internal external sources Ability adapt new learn quickly"
109,Data Engineer,Senior Data Engineer - Hux,"Chicago, IL 60606",Chicago,IL,"Hux Senior Data Engineer
Locations: New York, NY â Greensboro, NC - Chicago, IL â Raleigh, Durham, Chapel-Hill, NC
What is Hux? Hux is the Human Experience Platform by Deloitte Digital.
In todayâs world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.
Customers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.
Hux by Deloitte Digital gives companies the ability to build and leverage the connections â between people, systems, data and technologies â so they can deliver personalized, contextual experiences to customers at scale.
Work youâll do

As a Senior Data Engineer-Hux, youâll design, implement and maintain a full suite of real-time and batch jobs that fuels our cutting-edge AI to provide real-time marketing intelligence to our existing clients.
Youâll develop, test and deliver production-grade code to help our clients solve their most critical marketing challenges using cutting-edge big-data tools. Youâll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall platform.
As you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, youâll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.
Your responsibilities will include:
Design, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.
Bring together large, complex and sparse data sets to meet functional and non-functional business requirements.
Design and implement data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.
Integrate new data management technologies and software engineering tools into existing structures.
Help in building high-performance algorithms, prototypes, predictive models and proof of concepts.
Use a variety of languages, tools and frameworks to marry data and systems together.
Recommend ways to improve data reliability, efficiency and quality.
Collaborate with Data Scientists, DevOps and Project Managers on meeting project goals.
Tackle challenges and solve complex problems on a daily basis.
Qualifications
Required:
8+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
4+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
3+ years of experience on distributed, high-throughput and low-latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
Limited immigration sponsorship may be available.
Preferred:
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.
Producing high-quality code in Python.
Passionate about testing, and with extensive experience in Agile teams using SCRUM; you consider automated build-and-test to be the norm.
Proven ability to communicate in both verbal and writing in a high performance, collaborative environment.
Follows data development best practices, and enjoy helping others learn to do the same.
An independent thinker who considers the operating context of what he/she is developing.
Believes that the best data pipelines run unattended for weeks and months on end.
Familiar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.
Ability to travel 5-10% of the time
Helpful, but not required:
Knowledge in:
Experience with large consumer data sets used in performance marketing is a major advantage.
Familiarity with machine learning libraries is a plus.
Well-versed in (or contributes to) data-centric open source projects.
Reads Hacker News, blogs, or stays on top of emerging tools in some other way
Data visualization
Industry-specific marketing data
Technologies of Interest:
Languages/Libraries â Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.
Cloud â AWS, Azure, Google

The team
Advertising, Marketing & Commerce
Our Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clientsâ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clientsâ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.

We serve our clients through the following types of work:Cross-channel customer engagement strategy, design and development(web, mobile, social, physical)eCommerce strategy, implementation and operationsMarketing Content and digital asset management solutionsMarketing Technology and Advertising Technology solutionsMarketing analytics implementation and operationsAdvertising campaign ideation, development and executionAcquisition and engagement campaign ideation, development and executionAgile based, design-thinking, user-centric, empirical projects that accelerate results
How youâll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe thereâs always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitteâs culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitteâs impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youâre applying to. Check out recruiting tips from Deloitte professionals.
#LI:PTY
#IND:PTY","8+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment. 4+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores. 3+ years of experience on distributed, high-throughput and low-latency architecture. 1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale. Limited immigration sponsorship may be available.    ","8+ years of experience in software development, a substantial part which was gained high-throughput, decision-automation related environment. 4+ working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores. 3+ on distributed, high-throughput low-latency architecture. 1+ deploying or managing pipelines for supporting data-science-driven decisioning at scale. Limited immigration sponsorship may be available.","8+ years experience software development, substantial part gained high-throughput, decision-automation related environment. 4+ working big data using technologies like Spark, Kafka, Flink, Hadoop, NoSQL datastores. 3+ distributed, high-throughput low-latency architecture. 1+ deploying managing pipelines supporting data-science-driven decisioning scale. Limited immigration sponsorship may available."
110,Data Engineer,Data Engineer - SQL / Big Data / Java,"Chicago, IL",Chicago,IL,"Job Description

The Opportunity
Your consulting projects will include integrating data in a virtual manner for operational and/or informational purposes - Integration of 100+ data sources for a Customer Service Multichannel IT Infrastructure; implementation of Logical Data Warehouses and Virtual Datamarts to enable modern Business Intelligence solutions, Integration Layers for Hadoop-based Data Lakes, and support for Agile Operational Reporting on a diverse Big Data infrastructure are just a few flavours of your future projects.
Be part of an elite team in a rapidly growing international software product company. Your career with us will combine cutting edge technology, exposure to worldwide clients across all industries (Financial Services, Automotive, Insurance, Pharma, etc.), exciting growth path for technical, product and customer-facing roles, direct mentorship, and access to senior management as part of a global team. Your mission is to help our clients and prospects in the region to realize their full potential through accelerated adoption and productive use of Denodo's data virtualization capability in many solutions.

Duties & Responsibilities
As a Data Engineer Virtualization (f/m) you will successfully employ a combination of high technical expertise, client communication and coordination skills between clients and internal Denodo teams to achieve your mission.
Conception, implementation, and execution of customer-specific integration projects based on the Denodo Platform.
Education, coaching and support during the introduction as well as ongoing projects of the Denodo Platform to achieve high level of client satisfaction.
Diagnose and resolve clients inquiries related to operating Denodo software products in their environment.
Participate in problem escalation and call prevention projects to help clients and other technical specialists increase their efficiency when using Denodo products.
Contribute to knowledge management activities and promote best practices for project execution.
Implement product demos and pilots to showcase Data Virtualization in enterprise scenarios, cloud deployments and Big Data projects.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding clientâs business cases, requirements and issues.

Qualifications

Desired Skills & Experience
Experience Range: 2-5 years (Fresh Graduates must be top-ranked and exceptionally qualified)
Bachelor or higher Degree in information systems or computer science
Understanding of Data Integration flavors
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice.
Good knowledge of software development and architectural patterns.
Technical skills include Java development, JDBC, XML, Web Service related APIs, experience with version control systems (e.g. SVN, git).
Basic experience in Big Data, NoSQL, and InMemory environments is welcome.
Experience in Windows & Linux (and UNIX) operating systems in server environments.
Personal and Relationship qualities: Professional curiosity and the ability to enable yourself in new technologies and tasks. Active listener. Curiosity and continuous learning. Creativity. Team worker.
Communications: Good written/verbal communication skills in English (other international languages a plus) are essential for interaction with clients, making presentations, attending meetings and writing technical documentation.
Willingness to travel.
Additional Information

Employment Practices
We are committed to equal employment opportunity.
We respect, value and welcome diversity in our workforce.
We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee.","  Experience Range  2-5 years  Fresh Graduates must be top-ranked and exceptionally qualified  Bachelor or higher Degree in information systems or computer science Understanding of Data Integration flavors Solid understanding of SQL and good grasp of relational and analytical database management theory and practice. Good knowledge of software development and architectural patterns. Technical skills include Java development, JDBC, XML, Web Service related APIs, experience with version control systems  e.g. SVN, git . Basic experience in Big Data, NoSQL, and InMemory environments is welcome. Experience in Windows & Linux  and UNIX  operating systems in server environments. Personal and Relationship qualities  Professional curiosity and the ability to enable yourself in new technologies and tasks. Active listener. Curiosity and continuous learning. Creativity. Team worker. Communications  Good written/verbal communication skills in English  other international languages a plus  are essential for interaction with clients, making presentations, attending meetings and writing technical documentation. Willingness to travel.  Conception, implementation, and execution of customer-specific integration projects based on the Denodo Platform. Education, coaching and support during the introduction as well as ongoing projects of the Denodo Platform to achieve high level of client satisfaction. Diagnose and resolve clients inquiries related to operating Denodo software products in their environment. Participate in problem escalation and call prevention projects to help clients and other technical specialists increase their efficiency when using Denodo products. Contribute to knowledge management activities and promote best practices for project execution. Implement product demos and pilots to showcase Data Virtualization in enterprise scenarios, cloud deployments and Big Data projects. Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding clientâs business cases, requirements and issues.  ","Experience Range 2-5 years Fresh Graduates must be top-ranked and exceptionally qualified Bachelor or higher Degree in information systems computer science Understanding of Data Integration flavors Solid understanding SQL good grasp relational analytical database management theory practice. Good knowledge software development architectural patterns. Technical skills include Java development, JDBC, XML, Web Service related APIs, experience with version control e.g. SVN, git . Basic Big Data, NoSQL, InMemory environments is welcome. Windows & Linux UNIX operating server environments. Personal Relationship qualities Professional curiosity the ability to enable yourself new technologies tasks. Active listener. Curiosity continuous learning. Creativity. Team worker. Communications written/verbal communication English other international languages a plus are essential for interaction clients, making presentations, attending meetings writing technical documentation. Willingness travel. Conception, implementation, execution customer-specific integration projects based on Denodo Platform. Education, coaching support during introduction as well ongoing Platform achieve high level client satisfaction. Diagnose resolve clients inquiries products their environment. Participate problem escalation call prevention help specialists increase efficiency when using products. Contribute activities promote best practices project execution. Implement product demos pilots showcase Virtualization enterprise scenarios, cloud deployments projects. Provide timely, prioritized complete customer-based feedback Product Management, Sales, Support and/or Development regarding clientâs business cases, requirements issues.","Experience Range 2-5 years Fresh Graduates must top-ranked exceptionally qualified Bachelor higher Degree information systems computer science Understanding Data Integration flavors Solid understanding SQL good grasp relational analytical database management theory practice. Good knowledge software development architectural patterns. Technical skills include Java development, JDBC, XML, Web Service related APIs, experience version control e.g. SVN, git . Basic Big Data, NoSQL, InMemory environments welcome. Windows & Linux UNIX operating server environments. Personal Relationship qualities Professional curiosity ability enable new technologies tasks. Active listener. Curiosity continuous learning. Creativity. Team worker. Communications written/verbal communication English international languages plus essential interaction clients, making presentations, attending meetings writing technical documentation. Willingness travel. Conception, implementation, execution customer-specific integration projects based Denodo Platform. Education, coaching support introduction well ongoing Platform achieve high level client satisfaction. Diagnose resolve clients inquiries products environment. Participate problem escalation call prevention help specialists increase efficiency using products. Contribute activities promote best practices project execution. Implement product demos pilots showcase Virtualization enterprise scenarios, cloud deployments projects. Provide timely, prioritized complete customer-based feedback Product Management, Sales, Support and/or Development regarding clientâs business cases, requirements issues."
111,Data Engineer,Sr. Data Engineer,"Chicago, IL 60654",Chicago,IL,"Position Purpose

As a Senior Data Engineer on the Echo Global Logistics team, you will contribute to database management of large scale web-based applications through the use of SQL, C#, and .NET tools. These technologies enable Echo's business while supporting architectural vision of quality, scalability, performance and function. Our proprietary software is created with the goal to simplify transportation for our customers and carriers, and is one of our largest competitive advantages in an ever growing market.

Echo Global Logistics has recently been ranked the third largest digital company by employee size in Chicago and we are continuing to see increased growth in virtually all of our technical teams. Additionally, we have been ranked as the #1 Inbound Logistics 3PL for 2017 and look forward to speaking with you further about our team!

Essential Position Functions

You...


See technology as a passion, not something you just do between 9-5
Possess the ability to create new solutions; we operate on a web based platform and constantly facing unchartered waters
Possess strong fundamentals within coding technologies and a willingness to wear several hats when called upon
Do not wait for something to break; find a problem before it becomes one and constantly aiming to improve
Having a willingness to vocalize these ideas and pick yourself up if you get knocked down

Weâ¦


Value passionate technologists, go-getters, and people who never stop seeking ways to improve existing technology
Have a high focus on career development and the runway to get you there
Work hard, period
Offer competitive compensation, benefits, 401k, challenging projects, company wide events, coworkers and leaders who will push you to get better, a sense of community not found anywhere else

Responsibilities:
Data Engineer's work in conjunction with Software Engineers, DBA's, Business Analysts, Quality Assurance and business owners


Serve as a member of a data team that solves complex challenges and builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, and table functions
Develop solutions and contributing to development, leveraging Object-Oriented programming techniques (.Net), Software Development Lifecycles, Unit Test Techniques, and Debugging/Analytical Techniques.
Collaborate with the team to develop database structures that fit into the overall architecture of the systems under development.
Code, install, optimize, and debug database queries and stored procedures using appropriate tools and editors.
Perform code reviews and provide feedback in a timely manner.
Promote collective code ownership for everyone to have visibility into the feature codebase.
Present technical ideas and concepts in business-friendly language.
Provide recommendations, analysis, and evaluation of systems improvements, optimization, development, and maintenance efforts, including capacity planning.
Identify and correct performance bottlenecks related to SQL code.
Support timely production releases and adherence to release activities.
Contribute to data retention strategy.

Position Requirements


1-3 years in commercial-grade business applications environment leveraging the following:
SQL Server, T-SQL, SSIS, stored procedures, user-defined functions and table functions
Managing design risk
1-3 years leveraging OO programming techniques
Software development lifecycles, Unit test techniques, debugging/analytical techniques

What's in it for you?


Help career growth by joining industry leader and continuing to advance Echo web based technologies
Working with an organization with defined market goals, products, customers, revenue, and development teams
Experienced mentors to learn and adopt new practices
Ability to introduce your own views and takes on our product offerings
Work in wide variety of data management
Ability to constantly enhance and improve applications
Have a clearly defined career growth track with enough flexibility to pave your own way

All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, status as a qualified individual with a disability, or Vietnam era or other protected veteran.","   Serve as a member of a data team that solves complex challenges and builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, and table functions Develop solutions and contributing to development, leveraging Object-Oriented programming techniques  .Net , Software Development Lifecycles, Unit Test Techniques, and Debugging/Analytical Techniques. Collaborate with the team to develop database structures that fit into the overall architecture of the systems under development. Code, install, optimize, and debug database queries and stored procedures using appropriate tools and editors. Perform code reviews and provide feedback in a timely manner. Promote collective code ownership for everyone to have visibility into the feature codebase. Present technical ideas and concepts in business-friendly language. Provide recommendations, analysis, and evaluation of systems improvements, optimization, development, and maintenance efforts, including capacity planning. Identify and correct performance bottlenecks related to SQL code. Support timely production releases and adherence to release activities. Contribute to data retention strategy.   ","Serve as a member of data team that solves complex challenges and builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, table functions Develop contributing to development, leveraging Object-Oriented programming techniques .Net , Software Development Lifecycles, Unit Test Techniques, Debugging/Analytical Techniques. Collaborate with the develop structures fit into overall architecture systems under development. Code, install, optimize, debug queries procedures appropriate tools editors. Perform code reviews provide feedback in timely manner. Promote collective ownership for everyone have visibility feature codebase. Present technical ideas concepts business-friendly language. Provide recommendations, analysis, evaluation improvements, optimization, maintenance efforts, including capacity planning. Identify correct performance bottlenecks related code. Support production releases adherence release activities. Contribute retention strategy.","Serve member data team solves complex challenges builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, table functions Develop contributing development, leveraging Object-Oriented programming techniques .Net , Software Development Lifecycles, Unit Test Techniques, Debugging/Analytical Techniques. Collaborate develop structures fit overall architecture systems development. Code, install, optimize, debug queries procedures appropriate tools editors. Perform code reviews provide feedback timely manner. Promote collective ownership everyone visibility feature codebase. Present technical ideas concepts business-friendly language. Provide recommendations, analysis, evaluation improvements, optimization, maintenance efforts, including capacity planning. Identify correct performance bottlenecks related code. Support production releases adherence release activities. Contribute retention strategy."
112,Data Engineer,AWS Data Engineer,"Chicago, IL",Chicago,IL,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
113,Data Engineer,Data Engineer,"Chicago, IL 60654",Chicago,IL,"Job Summary:

The Data Engineer is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The Data Engineer will create processes and data models to enable performance management reporting as well as data science, advanced analytics, and personalization efforts.

 The Data Engineer will work closely with Technology, Finance, and Commercial Analytics to drive value through best in class data architecture and data model design.

Key Responsibilities

Act as data strategist responsible for the short and long term vision of data management, warehouse performance, and data architecture supporting both business intelligence and prescriptive analytics
Partner with both internal stakeholders and external vendors involved in project definition, design and planning, and mapping the data journey from source through consumer (data visualization, application, or predictive model)
Gather, document, and analyze business requirements; establish and prioritize efforts to deliver data models that support business needs
Design, develop, test and deploy data models, data collection, and transformation components. Determine best point for transformations, calculations, and joins (e.g. data lake, data warehouse, or Tableau data source)
Troubleshoot and support existing data workflow processes; deliver fixes and optimizations where appropriate
Work closely with CIO, Chief Architect, and Chief Analytics Officer on data governance and planning - ensure scalability and sustainability of business intelligence data architecture
Required Qualifications:
2+ years' experience building data solutions including AWS S3, EMR, and Redshift, Tableau and Tableau server
Expert SQL scripting skills; R, Python, or SAS preferred but not required
Ability to effectively build relationships across the business at all levels
Self-starter, entrepreneurial, high-energy who can take initiative in a fast-moving environment
Strong technical understanding of current and emerging business intelligence and analytics technologies
 Education:

Bachelor's Degree in Technology, Computer Science, or Data Science preferred","2+ years' experience building data solutions including AWS S3, EMR, and Redshift, Tableau and Tableau server Expert SQL scripting skills; R, Python, or SAS preferred but not required Ability to effectively build relationships across the business at all levels Self-starter, entrepreneurial, high-energy who can take initiative in a fast-moving environment Strong technical understanding of current and emerging business intelligence and analytics technologies    Act as data strategist responsible for the short and long term vision of data management, warehouse performance, and data architecture supporting both business intelligence and prescriptive analytics Partner with both internal stakeholders and external vendors involved in project definition, design and planning, and mapping the data journey from source through consumer  data visualization, application, or predictive model  Gather, document, and analyze business requirements; establish and prioritize efforts to deliver data models that support business needs Design, develop, test and deploy data models, data collection, and transformation components. Determine best point for transformations, calculations, and joins  e.g. data lake, data warehouse, or Tableau data source  Troubleshoot and support existing data workflow processes; deliver fixes and optimizations where appropriate Work closely with CIO, Chief Architect, and Chief Analytics Officer on data governance and planning - ensure scalability and sustainability of business intelligence data architecture  ","2+ years' experience building data solutions including AWS S3, EMR, and Redshift, Tableau server Expert SQL scripting skills; R, Python, or SAS preferred but not required Ability to effectively build relationships across the business at all levels Self-starter, entrepreneurial, high-energy who can take initiative in a fast-moving environment Strong technical understanding of current emerging intelligence analytics technologies Act as strategist responsible for short long term vision management, warehouse performance, architecture supporting both prescriptive Partner with internal stakeholders external vendors involved project definition, design planning, mapping journey from source through consumer visualization, application, predictive model Gather, document, analyze requirements; establish prioritize efforts deliver models that support needs Design, develop, test deploy models, collection, transformation components. Determine best point transformations, calculations, joins e.g. lake, warehouse, Troubleshoot existing workflow processes; fixes optimizations where appropriate Work closely CIO, Chief Architect, Analytics Officer on governance planning - ensure scalability sustainability","2+ years' experience building data solutions including AWS S3, EMR, Redshift, Tableau server Expert SQL scripting skills; R, Python, SAS preferred required Ability effectively build relationships across business levels Self-starter, entrepreneurial, high-energy take initiative fast-moving environment Strong technical understanding current emerging intelligence analytics technologies Act strategist responsible short long term vision management, warehouse performance, architecture supporting prescriptive Partner internal stakeholders external vendors involved project definition, design planning, mapping journey source consumer visualization, application, predictive model Gather, document, analyze requirements; establish prioritize efforts deliver models support needs Design, develop, test deploy models, collection, transformation components. Determine best point transformations, calculations, joins e.g. lake, warehouse, Troubleshoot existing workflow processes; fixes optimizations appropriate Work closely CIO, Chief Architect, Analytics Officer governance planning - ensure scalability sustainability"
114,Data Engineer,Azure Data Engineer,"Chicago, IL",Chicago,IL,"Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, âas isâ and âto beâ scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ","At least 5 years of consulting or client service delivery experience on Azure DevOps an platform Proven ability to build, manage and foster a team-oriented environment","At least 5 years consulting client service delivery experience Azure DevOps platform Proven ability build, manage foster team-oriented environment"
115,Data Engineer,Senior Data Engineer,"Chicago, IL",Chicago,IL,"Location:
Chicago, Illinois
We're looking for a hands-on, collaborative Senior Data Engineer to join our Data Engineering team. When you get out of bed in the morning, you look forward to building solutions, you love working with data, and you enjoy working as a team.
In this role, you'll have the opportunity to make code decisions and build cloud infrastructure and pipelines that will deliver data solutions to our Food and Beverage and Sports industry clients. You will report to our Director of Data Engineering and help bring best practices to the team while working with a team of Data Engineers.
If you have a git repository, we'd be excited to see it!
The Company is not able to sponsor employment visas for this position.
Core Responsibilities
Collaborate with our reporting, analytics, and data science teams to understand data sources and business requirements
Work within a collaborative team, adhering to Agile best practices, documentation, and knowledge sharing
Gather, clean, enrich, and transform data to feed internal and external client needs
Define, build, test, and implement data pipelines, batch and streaming
Monitor pipeline performance and document infrastructure changes
Make code decisions and adhere to best practices for ETL and programming
Contribute to our overall architecture and pipeline design and make contributions to the product road map
Minimum Qualifications
3+ years experience programming with Python is required
3+ years in an ETL or Data Engineering role, building and implementing data pipelines
Strong skills with PySpark and SQL with the ability to write efficient queries
Familiarity with AWS big data services: S3, Redshift, Glue, EC2, Lambda, SageMaker, Dynamo
Experience working in a highly collaborative environment - we do Agile using sprints, planning, retro, etc.
Experience with Airflow and other open source technologies
Excited and willing to learn new things
Background in computer science, engineering, mathematics, related field, or equivalent work experience
Preferred Qualifications
Interest or experience in DevOps and CI/CD
Experience with building data lake solutions
Experience with JavaScript
Experience with Netezza and Data Stage

About E15
At E15, we are the spark that ignites. Our team delivers next-generation insights based on data, not hunches, to drive business in MLB, NHL, NBA, NFL, College Sports, and beyond. E15 brings unmatched industry intelligence and cutting edge analytics to sports, entertainment, hospitality, and retail industries to help companies make forward-looking decisions to benefit their business, fans and customers. www.e15group.com

About Levy
Levy is the leader in Sports and Entertainment dining, catering such renowned sports venues at Wrigley Field in Chicago, STAPLES Center and Dodger Stadium in Los Angeles, Ford Field in Detroit and Churchill Downs in Louisville. Levy also caters events including Super Bowls, World Series, NASCAR Racing, the Kentucky Derby, the U.S. Open Tennis Tournament and the Grammy Awards. www.levyrestaurants.com

E15 is an equal opportunity employer. At E15 we are committed to treating all Applicants and Team Members fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law.","3+ years experience programming with Python is required 3+ years in an ETL or Data Engineering role, building and implementing data pipelines Strong skills with PySpark and SQL with the ability to write efficient queries Familiarity with AWS big data services  S3, Redshift, Glue, EC2, Lambda, SageMaker, Dynamo Experience working in a highly collaborative environment - we do Agile using sprints, planning, retro, etc. Experience with Airflow and other open source technologies Excited and willing to learn new things Background in computer science, engineering, mathematics, related field, or equivalent work experience  Collaborate with our reporting, analytics, and data science teams to understand data sources and business requirements Work within a collaborative team, adhering to Agile best practices, documentation, and knowledge sharing Gather, clean, enrich, and transform data to feed internal and external client needs Define, build, test, and implement data pipelines, batch and streaming Monitor pipeline performance and document infrastructure changes Make code decisions and adhere to best practices for ETL and programming Contribute to our overall architecture and pipeline design and make contributions to the product road map  ","3+ years experience programming with Python is required in an ETL or Data Engineering role, building and implementing data pipelines Strong skills PySpark SQL the ability to write efficient queries Familiarity AWS big services S3, Redshift, Glue, EC2, Lambda, SageMaker, Dynamo Experience working a highly collaborative environment - we do Agile using sprints, planning, retro, etc. Airflow other open source technologies Excited willing learn new things Background computer science, engineering, mathematics, related field, equivalent work Collaborate our reporting, analytics, science teams understand sources business requirements Work within team, adhering best practices, documentation, knowledge sharing Gather, clean, enrich, transform feed internal external client needs Define, build, test, implement pipelines, batch streaming Monitor pipeline performance document infrastructure changes Make code decisions adhere practices for Contribute overall architecture design make contributions product road map","3+ years experience programming Python required ETL Data Engineering role, building implementing data pipelines Strong skills PySpark SQL ability write efficient queries Familiarity AWS big services S3, Redshift, Glue, EC2, Lambda, SageMaker, Dynamo Experience working highly collaborative environment - Agile using sprints, planning, retro, etc. Airflow open source technologies Excited willing learn new things Background computer science, engineering, mathematics, related field, equivalent work Collaborate reporting, analytics, science teams understand sources business requirements Work within team, adhering best practices, documentation, knowledge sharing Gather, clean, enrich, transform feed internal external client needs Define, build, test, implement pipelines, batch streaming Monitor pipeline performance document infrastructure changes Make code decisions adhere practices Contribute overall architecture design make contributions product road map"
116,Data Engineer,Data Engineer Intern,"Naperville, IL 60563",Naperville,IL,"Overview
KeHE-a natural, organic, specialty and fresh food distributor-is all about ""good"" and is growing, so there's never been a more exciting time to join our team. If you're enthusiastic about working in an environment with a people-first culture and an organization committed to good living, good food and good service, we'd love to talk to you!

Where KeHE goes, goodness follows. KeHE has grown tremendously over the years by exceeding expectations and bringing goodness in all we do. Weâre now bringing that same goodness to our workforce by creating friction-free processes.
Weâre focused on simple and intuitive ways to work. Weâre building systems where our employees come together each day and do what truly makes us human; collaborate, share, and connect.
Youâll have the opportunity to expand and apply your skills in new and interesting ways. And youâll have fun doing it. Join a team where our collective intelligence, collective effort, and collective passion is brought together to do something worth doing!
Primary Responsibilities
KeHE Distributors is looking for ambitious and energetic people to join our Summer 2020 Technology Internship Program. A technology internship at KeHE will help you develop the skills you need to help you thrive throughout your career. Working alongside the best technologists in the industry, you will perform relevant work with direct impact on our business and customers. The Internship Program also provides opportunities for interns to tour a warehouse, participate in a trade show, and opportunities for networking.

Opportunities available in the corporate office in Naperville, IL, lasting between 10-12 weeks long. Our technology organization focuses on new, visionary ideas. You will learn about KeHE's continuous research, effective deployment, and rapid adoption of groundbreaking technology platforms. KeHE prides itself on its culture and values. During your time as KeHE technical intern, you will begin to build a foundation around understanding our culture and values as well as receive an introduction to the food distribution industry and the technologies that support it.
Essential Functions
Conceptual, analytical thinker, with a real passion for data exploration and design
Exposure to business intelligence which includes Databases, ETL, Machine Learning, and Data Mining
Participate in hands-on technical projects and enthusiastic to tackle problems supporting our team.
Shows great attention to detail
Minimum Requirements, Qualifications, Additional Skills, Aptitude
Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.
Interest in Machine Learning algorithms, techniques and methodologies and growing in that space
Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies
Familiarity with one or more general purpose programming language including but not limited to Python and C++.
Demonstrated ability to learn quickly and has a passion for emerging digital technologies
Comfortable with communicating clearly and concisely across a variety of audiences including technology and business.
Ability to think strategically as well as tactically in order to drive ideas into action.
Ability to collaborate in a team-oriented, dynamic environment
Requisition ID
2019-5992","Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science. Interest in Machine Learning algorithms, techniques and methodologies and growing in that space Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies Familiarity with one or more general purpose programming language including but not limited to Python and C++. Demonstrated ability to learn quickly and has a passion for emerging digital technologies Comfortable with communicating clearly and concisely across a variety of audiences including technology and business. Ability to think strategically as well as tactically in order to drive ideas into action. Ability to collaborate in a team-oriented, dynamic environment Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science. Interest in Machine Learning algorithms, techniques and methodologies and growing in that space Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies Familiarity with one or more general purpose programming language including but not limited to Python and C++. Demonstrated ability to learn quickly and has a passion for emerging digital technologies Comfortable with communicating clearly and concisely across a variety of audiences including technology and business. Ability to think strategically as well as tactically in order to drive ideas into action. Ability to collaborate in a team-oriented, dynamic environment Conceptual, analytical thinker, with a real passion for data exploration and design Exposure to business intelligence which includes Databases, ETL, Machine Learning, and Data Mining Participate in hands-on technical projects and enthusiastic to tackle problems supporting our team. Shows great attention to detail  Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science. Interest in Machine Learning algorithms, techniques and methodologies and growing in that space Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies Familiarity with one or more general purpose programming language including but not limited to Python and C++. Demonstrated ability to learn quickly and has a passion for emerging digital technologies Comfortable with communicating clearly and concisely across a variety of audiences including technology and business. Ability to think strategically as well as tactically in order to drive ideas into action. Ability to collaborate in a team-oriented, dynamic environment","Pursuing a Bachelor's degree in Technology, with focus on Machine Learning and / or Data Science. Interest algorithms, techniques methodologies growing that space Familiarity Semi-Structured, Unstructured databases, one more general purpose programming language including but not limited to Python C++. Demonstrated ability learn quickly has passion for emerging digital technologies Comfortable communicating clearly concisely across variety of audiences technology business. Ability think strategically as well tactically order drive ideas into action. collaborate team-oriented, dynamic environment Conceptual, analytical thinker, real data exploration design Exposure business intelligence which includes Databases, ETL, Learning, Mining Participate hands-on technical projects enthusiastic tackle problems supporting our team. Shows great attention detail","Pursuing Bachelor's degree Technology, focus Machine Learning / Data Science. Interest algorithms, techniques methodologies growing space Familiarity Semi-Structured, Unstructured databases, one general purpose programming language including limited Python C++. Demonstrated ability learn quickly passion emerging digital technologies Comfortable communicating clearly concisely across variety audiences technology business. Ability think strategically well tactically order drive ideas action. collaborate team-oriented, dynamic environment Conceptual, analytical thinker, real data exploration design Exposure business intelligence includes Databases, ETL, Learning, Mining Participate hands-on technical projects enthusiastic tackle problems supporting team. Shows great attention detail"
117,Data Engineer,Data Engineer,"Chicago, IL 60604",Chicago,IL,"Location: Chicago, IL Job Code: HCR# 2534
Description
Position Purpose:
The Data Engineer is responsible for empowering the Data team to achieve its primary objectives: ingesting, mastering and exposing real-time, event-driven data streams pertaining to the firmâs data assets. The ideal candidate will exhibit passion for continuous improvement and a dedicated focus on enabling consumers to achieve their goals by making data driven decisions.
Primary Accountabilities/Responsibilities:
Prioritizes and executes rapid raw data collection from source systems, targets and implements efficient storage of, employs fast and reliable access patterns.
Understands system protocols, how systems operate and data flows. Aware of current and emerging technology tools and their benefits. Expected to independently develop a full software stack. Understands the building blocks, interactions, dependencies, and tools required to complete software and automation work. Independent study of evolving technology is expected.
Drives engineering projects by developing software solutions; conducting tests and inspections; building reports and calculations.
Strong focus on innovation and enablement, contributes to designs to implement new ideas which improve an existing and new system/process/service. Understands and can apply new industry perspectives to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.
Maintains knowledge of existing technology documents. Writes basic documentation on how technology works using collaboration tools like Confluence. Creates clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption and consideration at the manager level.
Collaborates with technical teams and utilizes system expertise to deliver technical solutions. Continuously learns and teaches others existing and new technologies. Contributes to the development of others through mentoring or in-house workshops and learning sessions.
Drives team practices and procedures to achieve repeatable success and defined expectation of services
Provides a significant collaborative role in long-term department planning, with focus on initiatives achieving data empowerment, operational efficiency and sustainability
Monitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities.

Job Requirements:
Bachelorâs degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.
Prior experience in Capital Markets strongly preferred.
5+ years of experience developing software in a professional environment (preferably financial services but not required)
3 years of hands on Data Driven Enterprise Application development, preferable in financial industry
Strong understanding of Enterprise architecture patterns, Object Oriented & Service Oriented principles, design patterns, industry best practices
Foundational knowledge of data structures, algorithms, and designing for performance.
Proficiency in programming in Java, C# or Python and willingness to learn and adopt new languages as necessary
Experience in database technology like MSSQL and one of key value and document databases like MongoDb, Dynamo Db, Casandra.
Exposure to containers, microservices, distributed systems architecture, orchestrators and cloud computing.
Comfortable with core programming concepts and techniques (e.g. concurrency, memory management)
Enjoys working with algorithms and data structures (e.g. trees, hash maps, queues)
Data Analytics and Data Science experience will be a plus.
Good sense of user interaction and usability design to provide an intuitive, seamless end user experience.
Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts.
Ability to work and potentially lead in an Agile methodology environment.

Physical requirements/Working conditions:
Climate controlled office environment
Minimal physical requirements other than occasional light lifting of boxed materials â¢ Dynamic, time-sensitive, trade room environment
Travel as needed

We encourage applicants of all ages and experience, as we do not discriminate on the basis of the applicant's age.","  Prioritizes and executes rapid raw data collection from source systems, targets and implements efficient storage of, employs fast and reliable access patterns. Understands system protocols, how systems operate and data flows. Aware of current and emerging technology tools and their benefits. Expected to independently develop a full software stack. Understands the building blocks, interactions, dependencies, and tools required to complete software and automation work. Independent study of evolving technology is expected. Drives engineering projects by developing software solutions; conducting tests and inspections; building reports and calculations. Strong focus on innovation and enablement, contributes to designs to implement new ideas which improve an existing and new system/process/service. Understands and can apply new industry perspectives to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives. Maintains knowledge of existing technology documents. Writes basic documentation on how technology works using collaboration tools like Confluence. Creates clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption and consideration at the manager level. Collaborates with technical teams and utilizes system expertise to deliver technical solutions. Continuously learns and teaches others existing and new technologies. Contributes to the development of others through mentoring or in-house workshops and learning sessions. Drives team practices and procedures to achieve repeatable success and defined expectation of services Provides a significant collaborative role in long-term department planning, with focus on initiatives achieving data empowerment, operational efficiency and sustainability Monitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities.   Bachelorâs degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline. Prior experience in Capital Markets strongly preferred. 5+ years of experience developing software in a professional environment  preferably financial services but not required  3 years of hands on Data Driven Enterprise Application development, preferable in financial industry Strong understanding of Enterprise architecture patterns, Object Oriented & Service Oriented principles, design patterns, industry best practices Foundational knowledge of data structures, algorithms, and designing for performance. Proficiency in programming in Java, C  or Python and willingness to learn and adopt new languages as necessary Experience in database technology like MSSQL and one of key value and document databases like MongoDb, Dynamo Db, Casandra. Exposure to containers, microservices, distributed systems architecture, orchestrators and cloud computing. Comfortable with core programming concepts and techniques  e.g. concurrency, memory management  Enjoys working with algorithms and data structures  e.g. trees, hash maps, queues  Data Analytics and Data Science experience will be a plus. Good sense of user interaction and usability design to provide an intuitive, seamless end user experience. Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts. Ability to work and potentially lead in an Agile methodology environment. ","Prioritizes and executes rapid raw data collection from source systems, targets implements efficient storage of, employs fast reliable access patterns. Understands system protocols, how systems operate flows. Aware of current emerging technology tools their benefits. Expected to independently develop a full software stack. the building blocks, interactions, dependencies, required complete automation work. Independent study evolving is expected. Drives engineering projects by developing solutions; conducting tests inspections; reports calculations. Strong focus on innovation enablement, contributes designs implement new ideas which improve an existing system/process/service. can apply industry perspectives our business models. Reviews processes highlight more ways workload effectively through perspectives. Maintains knowledge documents. Writes basic documentation works using collaboration like Confluence. Creates clear for code used. Documenting designs, presentations, requirements consumption consideration at manager level. Collaborates with technical teams utilizes expertise deliver solutions. Continuously learns teaches others technologies. Contributes development mentoring or in-house workshops learning sessions. team practices procedures achieve repeatable success defined expectation services Provides significant collaborative role in long-term department planning, initiatives achieving empowerment, operational efficiency sustainability Monitors evaluates overall strategic infrastructure; tracks reliability; identifies recommends improvements mitigates vulnerabilities. Bachelorâs degree relevant work experience Computer Science, Mathematics, Electrical Engineering related discipline. Prior Capital Markets strongly preferred. 5+ years professional environment preferably financial but not 3 hands Data Driven Enterprise Application development, preferable understanding architecture patterns, Object Oriented & Service principles, design best Foundational structures, algorithms, designing performance. Proficiency programming Java, C Python willingness learn adopt languages as necessary Experience database MSSQL one key value document databases MongoDb, Dynamo Db, Casandra. Exposure containers, microservices, distributed architecture, orchestrators cloud computing. Comfortable core concepts techniques e.g. concurrency, memory management Enjoys working algorithms structures trees, hash maps, queues Analytics Science will be plus. Good sense user interaction usability provide intuitive, seamless end experience. Excellent communications skills ability subject matter expert extract critical concepts. Ability potentially lead Agile methodology environment.","Prioritizes executes rapid raw data collection source systems, targets implements efficient storage of, employs fast reliable access patterns. Understands system protocols, systems operate flows. Aware current emerging technology tools benefits. Expected independently develop full software stack. building blocks, interactions, dependencies, required complete automation work. Independent study evolving expected. Drives engineering projects developing solutions; conducting tests inspections; reports calculations. Strong focus innovation enablement, contributes designs implement new ideas improve existing system/process/service. apply industry perspectives business models. Reviews processes highlight ways workload effectively perspectives. Maintains knowledge documents. Writes basic documentation works using collaboration like Confluence. Creates clear code used. Documenting designs, presentations, requirements consumption consideration manager level. Collaborates technical teams utilizes expertise deliver solutions. Continuously learns teaches others technologies. Contributes development mentoring in-house workshops learning sessions. team practices procedures achieve repeatable success defined expectation services Provides significant collaborative role long-term department planning, initiatives achieving empowerment, operational efficiency sustainability Monitors evaluates overall strategic infrastructure; tracks reliability; identifies recommends improvements mitigates vulnerabilities. Bachelorâs degree relevant work experience Computer Science, Mathematics, Electrical Engineering related discipline. Prior Capital Markets strongly preferred. 5+ years professional environment preferably financial 3 hands Data Driven Enterprise Application development, preferable understanding architecture patterns, Object Oriented & Service principles, design best Foundational structures, algorithms, designing performance. Proficiency programming Java, C Python willingness learn adopt languages necessary Experience database MSSQL one key value document databases MongoDb, Dynamo Db, Casandra. Exposure containers, microservices, distributed architecture, orchestrators cloud computing. Comfortable core concepts techniques e.g. concurrency, memory management Enjoys working algorithms structures trees, hash maps, queues Analytics Science plus. Good sense user interaction usability provide intuitive, seamless end experience. Excellent communications skills ability subject matter expert extract critical concepts. Ability potentially lead Agile methodology environment."
118,Data Engineer,Technical Data Engineer Lead,"Chicago, IL 60606",Chicago,IL,"Job Description:
Role Summary/Purpose:
We are looking for a Technical Data Engineer Lead to lead the development of consumer-centric low latency analytic environment leveraging Big Data technologies and transform the legacy systems.
Essential Responsibilities:
Lead a development team of data engineers
Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases (Hbase) and EMR (Hadoop)
Responsible for design, development, testing oversight and implementation
Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks
Coordinate with data engineers and API developers to drive program delivery.
Drive technical development and application standards across enterprise data lake
Benchmark and debug critical issues with algorithms and software as they arise.
Lead and assist with the technical design and implementation of the Big Data cluster in various environments.
Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.
Perform other duties and/or special projects as assigned
Qualifications/Requirements:
Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.
Understand Hadoop cluster administration concepts.
3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.
Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.
Must have experience with batch and real-time data pipelines.
Must have experience as a Hadoop Technical Lead / Architect
Must have experience with design, development and deployment in the specified technologies.
Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.
Writing complex SQL queries, extracting and importing large amounts of data.
Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams.
Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.
Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.
Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.
Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through
Desired Characteristics:
Extensive experience working with data warehouses and big data platforms
Demonstrated experience building strong relationships with senior leaders
Strong leadership and influencing skills
Outstanding written and verbal skills and the ability to influence and motivate teams
Eligibility Requirements:
You must be 18 years or older
You must have a high school diploma or equivalent
You must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the onboarding process
You must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.
New hires (Level 4-7) must have 9 months of continuous service with the company before they are eligible to post on other roles. Once this new hire time in position requirement is met, the associate will have a minimum 6 monthsâ time in position before they can post for future non-exempt roles. Employees, level 8 or greater, must have at least 24 monthsâ time in position before they can post. All internal employees must have at least a âconsistently meets expectationsâ performance rating and have approval from your manager to post (or the approval of your manager and HR if you donât meet the time in position or performance requirement).
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Reasonable Accommodation Notice:
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
If you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am â 5pm Monday to Friday, Central Standard Time.
The salary range for this position is 85,000.00 - 170,000.00 USD Annual
Salaries are adjusted according to market in CA and Metro NY and some positions are bonus eligible.
Grade/Level: 12
Job Family Group:
Information Technology"," Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree  preferred  in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies. Understand Hadoop cluster administration concepts. 3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera. Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies. Must have experience with batch and real-time data pipelines. Must have experience as a Hadoop Technical Lead / Architect Must have experience with design, development and deployment in the specified technologies. Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python. Writing complex SQL queries, extracting and importing large amounts of data. Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams. Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions. Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders. Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment. Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through   Lead a development team of data engineers Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases  Hbase  and EMR  Hadoop  Responsible for design, development, testing oversight and implementation Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks Coordinate with data engineers and API developers to drive program delivery. Drive technical development and application standards across enterprise data lake Benchmark and debug critical issues with algorithms and software as they arise. Lead and assist with the technical design and implementation of the Big Data cluster in various environments. Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts. Perform other duties and/or special projects as assigned   Must have a Bachelorâs degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Masterâs degree  preferred  in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies. Understand Hadoop cluster administration concepts. 3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera. Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies. Must have experience with batch and real-time data pipelines. Must have experience as a Hadoop Technical Lead / Architect Must have experience with design, development and deployment in the specified technologies. Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python. Writing complex SQL queries, extracting and importing large amounts of data. Must be willing to work in a fast-paced environment with an on shore â off shore distributed Agile teams. Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions. Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders. Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment. Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through","Must have a Bachelorâs degree in Computer Science, Engineering, or related field, plus 4 years of work experience the IT industry; OR Masterâs preferred 2 industry with Big Data Technologies. Understand Hadoop cluster administration concepts. 3+ hands-on large scale environments such as Hortonworks, Cloudera. Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies. batch and real-time data pipelines. Technical Lead / Architect design, development deployment specified strong OOPS concepts, design principles, patterns java Python. Writing complex SQL queries, extracting importing amounts data. be willing to fast-paced environment an on shore â off distributed Agile teams. technical background building enterprise-wide warehouse solutions. ability develop maintain collaborative relationships at all levels across Business Stakeholders. prioritize multiple tasks deal urgent requests demanding environment. Excellent written oral communication skills. Adept presenting topics, influencing executing timely actionable follow-through team engineers Implement big enterprise lake, BI analytics system using Hive LLAP, Sqoop, NoSQL databases Hbase EMR Responsible for development, testing oversight implementation Works closely program manager, scrum master, architects convey impacts timeline risks Coordinate API developers drive delivery. Drive application standards lake Benchmark debug critical issues algorithms software they arise. assist various environments. Guide/mentor example create custom common utilities/libraries that can reused efforts. Perform other duties and/or special projects assigned","Must Bachelorâs degree Computer Science, Engineering, related field, plus 4 years work experience IT industry; OR Masterâs preferred 2 industry Big Data Technologies. Understand Hadoop cluster administration concepts. 3+ hands-on large scale environments Hortonworks, Cloudera. Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies. batch real-time data pipelines. Technical Lead / Architect design, development deployment specified strong OOPS concepts, design principles, patterns java Python. Writing complex SQL queries, extracting importing amounts data. willing fast-paced environment shore â distributed Agile teams. technical background building enterprise-wide warehouse solutions. ability develop maintain collaborative relationships levels across Business Stakeholders. prioritize multiple tasks deal urgent requests demanding environment. Excellent written oral communication skills. Adept presenting topics, influencing executing timely actionable follow-through team engineers Implement big enterprise lake, BI analytics system using Hive LLAP, Sqoop, NoSQL databases Hbase EMR Responsible development, testing oversight implementation Works closely program manager, scrum master, architects convey impacts timeline risks Coordinate API developers drive delivery. Drive application standards lake Benchmark debug critical issues algorithms software arise. assist various environments. Guide/mentor example create custom common utilities/libraries reused efforts. Perform duties and/or special projects assigned"
119,Data Engineer,"Cloud Data Engineer, Google Professional Services","Chicago, IL 60607",Chicago,IL,"Note: By applying to this position your application is automatically submitted to the following locations: Chicago, IL, USA; Atlanta, GA, USA
Minimum qualifications:

BA/BS degree in Computer Science, Mathematics or related technical field, or equivalent practical experience.
Experience with data processing software (such as Hadoop, Spark, Pig, Hive) and with data processing algorithms (MapReduce, Flume).
Experience in writing software in one or more languages such as Java, C++, Python, Go and/or JavaScript.
Experience managing internal or client-facing projects to completion; experience troubleshooting clients' technical issues; experience working with engineering teams, sales, services, and customers.

Preferred qualifications:
Experience working data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments.
Experience in technical consulting.
Experience working with big data, information retrieval, data mining or machine learning as well as experience in building multi-tier high availability applications with modern web technologies (such as NoSQL, MongoDB, SparkML, Tensorflow).
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments.
About the job
As a Cloud Data Engineer, you'll guide customers on how to ingest, store, process, analyze and explore/visualize data on the Google Cloud Platform. You will work on data migrations and transformational projects, and with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform challenges.
In this role you are the Google Engineer working with Google's most strategic Cloud customers. Together with the team you will support customer implementation of Google Cloud products through: architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more.
The Google Cloud Platform team helps customers transform and evolve their business through the use of Googleâs global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of businesses of all sizes use technology to connect with customers, employees and partners.
Responsibilities
Act as a trusted technical advisor to customers and solve complex Big Data challenges.
Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.
Travel up to 30% of the time.
Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.
At Google, we donât just accept differenceâwe celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.","   Act as a trusted technical advisor to customers and solve complex Big Data challenges. Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders. Travel up to 30% of the time. Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.  ","Act as a trusted technical advisor to customers and solve complex Big Data challenges. Create deliver best practices recommendations, tutorials, blog articles, sample code, presentations adapting different levels of key business stakeholders. Travel up 30% the time. Communicate effectively via video conferencing for meetings, reviews onsite delivery activities.","Act trusted technical advisor customers solve complex Big Data challenges. Create deliver best practices recommendations, tutorials, blog articles, sample code, presentations adapting different levels key business stakeholders. Travel 30% time. Communicate effectively via video conferencing meetings, reviews onsite delivery activities."
120,Data Engineer,Data Engineer,"Chicago, IL 60626",Chicago,IL,"Gallagher is a global leader in insurance, risk management and consulting services. We help businesses grow, communities thrive and people prosper. We live a culture defined by The Gallagher Way, our set of shared values and guiding tenets. A culture driven by our people, over 30,000 strong, serving our clients with customized solutions that will protect them and fuel their futures.

Position Summary:
Consider joining our growing team of data and analytics experts responsible for expanding our data and pipeline architecture, as well as optimizing data flow and collection for multiple functional teams. We support our data analysts and data scientists on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. You will engage in supporting the data needs of multiple teams, systems and products. Do you find the prospect of optimizing or even re-designing our companyâs data architecture to support our next generation of products and data initiatives most exciting? We really should explore together.

Essential Duties and Responsibilities:
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Craft analytics tools that utilize the data pipeline to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Orchestrate large, complex data sets that meet functional / non-functional business requirements.
Seek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Partner with data and analytics talent to strive for greater functionality in our data systems.


Required:
Bachelor's degree in Computer Science, Engineering or Information Systems Management or related field.
At least 5 years of data engineering experience/ETL experience leveraging tools such as Integration Services (SSIS), Azure Data Factory, Informatica and Ab Initio.
U.S. Eligibility Requirements:
Interested candidates must submit an application and resume/CV online to be considered
Must be 18 years of age or older
Must be willing to submit to a background investigation; any offer of employment is conditioned upon the successful completion of a background investigation
Must have unrestricted work authorization to work in the United States. For U.S. employment opportunities, Gallagher hires U.S. citizens, permanent residents, asylees, refugees, and temporary residents. Temporary residence does not include those with non-immigrant work authorization (F, J, H or L visas), such as students in practical training status. Exceptions to these requirements will be determined based on shortage of qualified candidates with a particular skill. Gallagher will require proof of work authorization
Must be willing to execute Gallagher's Employee Agreement or Confidentiality and Non-Disclosure Agreement, which require, among other things, post-employment obligations relating to non-solicitation, confidentiality and non-disclosure
Gallagher offers competitive salaries and benefits, including: medical/dental/vision plans, life and accident insurance, 401(K), employee stock purchase plan, educational expense reimbursement, employee assistance program, flexible work hours (availability varies by office and job function) training programs, matching gift program, and more.


Gallagher believes that all persons are entitled to equal employment opportunity and does not discriminate against nor favor any applicant because of race, sex, color, disability, national origin, religion, creed, age, marital status, citizenship, veteran status, gender, gender identity / expression, actual or perceived sexual orientation, or any other protected characteristic. Equal employment opportunity will be extended in all aspects of the employer-employee relationship, including, but not limited to, recruitment, hiring, training, promotion, transfer, demotion, compensation, benefits, layoff, and termination. In addition, Gallagher will make reasonable accommodations to known physical or mental limitations of an otherwise qualified applicant with a disability, unless the accommodation would impose an undue hardship on the operation of our business.","  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Craft analytics tools that utilize the data pipeline to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Orchestrate large, complex data sets that meet functional / non-functional business requirements. Seek out, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. Partner with data and analytics talent to strive for greater functionality in our data systems.  Interested candidates must submit an application and resume/CV online to be considered Must be 18 years of age or older Must be willing to submit to a background investigation; any offer of employment is conditioned upon the successful completion of a background investigation Must have unrestricted work authorization to work in the United States. For U.S. employment opportunities, Gallagher hires U.S. citizens, permanent residents, asylees, refugees, and temporary residents. Temporary residence does not include those with non-immigrant work authorization  F, J, H or L visas , such as students in practical training status. Exceptions to these requirements will be determined based on shortage of qualified candidates with a particular skill. Gallagher will require proof of work authorization Must be willing to execute Gallagher's Employee Agreement or Confidentiality and Non-Disclosure Agreement, which require, among other things, post-employment obligations relating to non-solicitation, confidentiality and non-disclosure","Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety sources using Microsoft Azure technologies such as Data Factory Databricks. Create tools analytics scientist team members that assist them in building optimizing our product into an innovative industry leader. Craft utilize pipeline to deliver actionable insights customer acquisition, operational efficiency other key business performance metrics. Work with partners including Executive, Product, Design teams data-related technical issues support their needs. Orchestrate large, complex sets meet functional / non-functional requirements. Seek out, design, implement internal process improvements automating manual processes, delivery, re-designing greater scalability. Partner talent strive functionality systems. Interested candidates must submit application resume/CV online be considered Must 18 years age or older willing background investigation; any offer employment is conditioned upon successful completion investigation have unrestricted work authorization United States. For U.S. opportunities, Gallagher hires citizens, permanent residents, asylees, refugees, temporary residents. Temporary residence does not include those non-immigrant F, J, H L visas , students practical training status. Exceptions these requirements will determined based on shortage qualified particular skill. require proof execute Gallagher's Employee Agreement Confidentiality Non-Disclosure Agreement, which require, among things, post-employment obligations relating non-solicitation, confidentiality non-disclosure","Build infrastructure required optimal extraction, transformation, loading data wide variety sources using Microsoft Azure technologies Data Factory Databricks. Create tools analytics scientist team members assist building optimizing product innovative industry leader. Craft utilize pipeline deliver actionable insights customer acquisition, operational efficiency key business performance metrics. Work partners including Executive, Product, Design teams data-related technical issues support needs. Orchestrate large, complex sets meet functional / non-functional requirements. Seek out, design, implement internal process improvements automating manual processes, delivery, re-designing greater scalability. Partner talent strive functionality systems. Interested candidates must submit application resume/CV online considered Must 18 years age older willing background investigation; offer employment conditioned upon successful completion investigation unrestricted work authorization United States. For U.S. opportunities, Gallagher hires citizens, permanent residents, asylees, refugees, temporary residents. Temporary residence include non-immigrant F, J, H L visas , students practical training status. Exceptions requirements determined based shortage qualified particular skill. require proof execute Gallagher's Employee Agreement Confidentiality Non-Disclosure Agreement, require, among things, post-employment obligations relating non-solicitation, confidentiality non-disclosure"
121,Data Engineer,"Data Engineer, Python","Chicago, IL",Chicago,IL,"GoHealth is looking for Data Engineers who will be responsible for the design, development, and delivery of data transformation tasks used in transforming data into a format that can be easily analyzed. We are seeking candidates who have experience in data analysis, collection, and optimization for the purpose of informing business decisions. The Data Engineer will work with other team members in owning data pipelines including execution, documentation, maintenance, and metadata management. In this role, you will also support the development of the data infrastructure necessary for full scale data science, predictive analytics and machine learning.

Responsibilities:

Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.
Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.
Perform unit testing, system integration testing and assist with user acceptance testing.
Adapt data components to accommodate changes in source data and new business requirements.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.

Skills and Experience:

Bachelor's Degree in computer science or equivalent experience required.
2+ years of experience in the design and development of data pipelines and tasks.
Good understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning SQL queries.
Experience in Python including in modules/libraries such as pandas, numpy, Flask, scikit-learn, and sci-py.
Proven experience extracting data from structured data sources (SQL, Excel, CSV files, Couchbase) and unstructured data sources
(Splunk, log files) both on-premise and in the cloud.
Experience consuming data from web services, REST and SOAP, HTML, XML and JSON.
Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.
Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.
Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.

Benefits and Perks:

Open vacation policy
401k program with company match
Medical, dental, vision, and life insurance benefits
Flexible spending accounts
Commuter and transit benefits
Professional growth opportunities
Casual dress code
Generous employee referral bonuses
Happy hours, ping-pong tournaments, and more company-sponsored events
Subsidized gym memberships
GoHealth is an Equal Opportunity Employer

","  Bachelor's Degree in computer science or equivalent experience required. 2+ years of experience in the design and development of data pipelines and tasks. Good understanding of data warehousing concepts and dimensional data modeling. Hands-on experience with troubleshooting performance issues and fine tuning SQL queries. Experience in Python including in modules/libraries such as pandas, numpy, Flask, scikit-learn, and sci-py. Proven experience extracting data from structured data sources  SQL, Excel, CSV files, Couchbase  and unstructured data sources  Splunk, log files  both on-premise and in the cloud. Experience consuming data from web services, REST and SOAP, HTML, XML and JSON. Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation. Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required. Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.   Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources. Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues. Perform unit testing, system integration testing and assist with user acceptance testing. Adapt data components to accommodate changes in source data and new business requirements. Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks. Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline. Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.   ","Bachelor's Degree in computer science or equivalent experience required. 2+ years of the design and development data pipelines tasks. Good understanding warehousing concepts dimensional modeling. Hands-on with troubleshooting performance issues fine tuning SQL queries. Experience Python including modules/libraries such as pandas, numpy, Flask, scikit-learn, sci-py. Proven extracting from structured sources SQL, Excel, CSV files, Couchbase unstructured Splunk, log files both on-premise cloud. consuming web services, REST SOAP, HTML, XML JSON. Knowledge version control systems using Git, Bitbucket, SVN, Team Foundation. Microsoft Server, SSIS, SSRS, Power BI, Azure is preferred but not Familiar other warehouse platforms like AWS Redshift Data Pipeline. Design, develop deploy optimal extraction, transformation, loading various GoHealth external sources. Monitor, execute report on all pipeline tasks while working appropriate teams to take corrective action quickly, case issues. Perform unit testing, system integration testing assist user acceptance testing. Adapt components accommodate changes source new business requirements. Create maintain documentation technical detail design, operational support maintenance procedures for Ensure quality compliance development, architecture, reporting, regulatory standards throughout entire pipeline. Collaborate rest Engineering Team, subject matter experts department leaders understand, analyze, build deliver data-related processes and/or reports.","Bachelor's Degree computer science equivalent experience required. 2+ years design development data pipelines tasks. Good understanding warehousing concepts dimensional modeling. Hands-on troubleshooting performance issues fine tuning SQL queries. Experience Python including modules/libraries pandas, numpy, Flask, scikit-learn, sci-py. Proven extracting structured sources SQL, Excel, CSV files, Couchbase unstructured Splunk, log files on-premise cloud. consuming web services, REST SOAP, HTML, XML JSON. Knowledge version control systems using Git, Bitbucket, SVN, Team Foundation. Microsoft Server, SSIS, SSRS, Power BI, Azure preferred Familiar warehouse platforms like AWS Redshift Data Pipeline. Design, develop deploy optimal extraction, transformation, loading various GoHealth external sources. Monitor, execute report pipeline tasks working appropriate teams take corrective action quickly, case issues. Perform unit testing, system integration testing assist user acceptance testing. Adapt components accommodate changes source new business requirements. Create maintain documentation technical detail design, operational support maintenance procedures Ensure quality compliance development, architecture, reporting, regulatory standards throughout entire pipeline. Collaborate rest Engineering Team, subject matter experts department leaders understand, analyze, build deliver data-related processes and/or reports."
122,Data Engineer,Senior Data Engineer â DBA (Data Base Admin),"Chicago, IL",Chicago,IL,"Manage data users to enable appropriate data distribution to the user in a timely manner
Manage transaction recovery and backup data
Minimize database downtime and manage parameters to provide fast query responses
Determine, enforce, and document database policies, procedures, and standards
Perform regular tests and evaluations to ensure data security, privacy and integrity
Monitor database performance, implement changes and apply new patches and versions when required
Automate SOX security compliance group checks
Required Education and Experience
BS degree in a computer discipline or relevant certification
3+ years of database management with expertise in backup, recovery and replication
1+ years of managing an AWS Ecosystem with knowledge in areas of RDS, Dynamo DB, MySQL, Maria DB, and/or AWS Aurora
Strong practical knowledge of SQL query performance, resolution and overall tuning
Proven knowledge of identifying and setting up system and performance monitoring
Understanding of the development lifecycle and service management processes including Code Promotion, Change Control and Incident/Problem Management
Hands on Security and Compliance (PCI / SOX) on data infrastructure
Backup/recovery, replication, cluster failover and disaster recovery
Willingness to participate in On-call rotation and off hour releases
Worked on DB Migrations with zero downtime
Excellent oral and written communication
Nice to have
Demonstration of use of Source Code Management Tools E.g. Git
Experience in cross-region replication with RDS
Worked in an agile environment and participated in Daily Scrum activities
Knowledge of programming such as Python
Gogo is the inflight internet company. Our worldwide inflight Wi-Fi services have made internet and video entertainment a regular part of flying. We are a diverse and mission-minded group of professionals all working together in extraordinary harmony. And thatâs just the beginning. We connect the aviation industry and air travelers with innovative technology and applications, and we do it all in a high-energy environment that welcomes the next challenge. Be prepared to join a performance-obsessed team that is passionate about bringing the internet to every device, every flight, everywhere.
Equal Opportunity Employer/Vets/Disabled
Gogo participates in E-Verify. Details in English and Spanish. Right to Work Statement in English and Spanish","    BS degree in a computer discipline or relevant certification 3+ years of database management with expertise in backup, recovery and replication 1+ years of managing an AWS Ecosystem with knowledge in areas of RDS, Dynamo DB, MySQL, Maria DB, and/or AWS Aurora Strong practical knowledge of SQL query performance, resolution and overall tuning Proven knowledge of identifying and setting up system and performance monitoring Understanding of the development lifecycle and service management processes including Code Promotion, Change Control and Incident/Problem Management Hands on Security and Compliance  PCI / SOX  on data infrastructure Backup/recovery, replication, cluster failover and disaster recovery Willingness to participate in On-call rotation and off hour releases Worked on DB Migrations with zero downtime Excellent oral and written communication ","BS degree in a computer discipline or relevant certification 3+ years of database management with expertise backup, recovery and replication 1+ managing an AWS Ecosystem knowledge areas RDS, Dynamo DB, MySQL, Maria and/or Aurora Strong practical SQL query performance, resolution overall tuning Proven identifying setting up system performance monitoring Understanding the development lifecycle service processes including Code Promotion, Change Control Incident/Problem Management Hands on Security Compliance PCI / SOX data infrastructure Backup/recovery, replication, cluster failover disaster Willingness to participate On-call rotation off hour releases Worked DB Migrations zero downtime Excellent oral written communication","BS degree computer discipline relevant certification 3+ years database management expertise backup, recovery replication 1+ managing AWS Ecosystem knowledge areas RDS, Dynamo DB, MySQL, Maria and/or Aurora Strong practical SQL query performance, resolution overall tuning Proven identifying setting system performance monitoring Understanding development lifecycle service processes including Code Promotion, Change Control Incident/Problem Management Hands Security Compliance PCI / SOX data infrastructure Backup/recovery, replication, cluster failover disaster Willingness participate On-call rotation hour releases Worked DB Migrations zero downtime Excellent oral written communication"
123,Data Engineer,Data Engineer,"Allen, TX",Allen,TX,"The engineering team consists of talented, team-oriented individuals who are empowered to take advantage of the latest cloud and distributed technologies to deliver reliable, high-throughput applications.

As a Data Engineer, youâll employ your skills on a daily basis to design and build data processing and storage applications to handle millions of transactions per day. You will analyze business requirements and consult with the broader team to ensure successful processing, storage and reporting of our Big Data. Youâll have a wide variety of languages and technologies at your disposal that you can use to solve problems. Your work will directly shape and create our data architecture to ultimately deliver systems that stand up to unpredictable environments at massive scale.

Technical Skills Needed:
5 years of working with following technology stack: Core languages are Java and C#; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.
Growing track record of success or the groundwork to be an impactful member of the team. Weâre looking for candidates that exhibit many of the following skills/attributes:
Strong Educational Background
Hands-on Engineering experience in
Problem solving and debugging skillsWriting and deploying code the Linux, Windows, or cloud environmentsFamiliarity with algorithms and performance analysisWillingness to contribute to the operational responsibility of the teamâs applications
Some experience with one or more of the following:
Relational Databases & SQL NoSQL databases (Cassandra, Redis, DynamoDB, MongoDB)Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase"," 5 years of working with following technology stack  Core languages are Java and C ; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS. Growing track record of success or the groundwork to be an impactful member of the team. Weâre looking for candidates that exhibit many of the following skills/attributes  Strong Educational Background Hands-on Engineering experience in Problem solving and debugging skillsWriting and deploying code the Linux, Windows, or cloud environmentsFamiliarity with algorithms and performance analysisWillingness to contribute to the operational responsibility of the teamâs applications Some experience with one or more of the following  Relational Databases & SQL NoSQL databases  Cassandra, Redis, DynamoDB, MongoDB Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase   ","5 years of working with following technology stack Core languages are Java and C ; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, SQS. Growing track record success or the groundwork to be an impactful member team. Weâre looking for candidates that exhibit many skills/attributes Strong Educational Background Hands-on Engineering experience in Problem solving debugging skillsWriting deploying code Linux, Windows, cloud environmentsFamiliarity algorithms performance analysisWillingness contribute operational responsibility teamâs applications Some one more Relational Databases & NoSQL databases Cassandra, Redis, MongoDB Big Data tools EMR, HBase","5 years working following technology stack Core languages Java C ; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, AWS Services Kinesis, DynamoDB, Redshift, Lamda, SQS. Growing track record success groundwork impactful member team. Weâre looking candidates exhibit many skills/attributes Strong Educational Background Hands-on Engineering experience Problem solving debugging skillsWriting deploying code Linux, Windows, cloud environmentsFamiliarity algorithms performance analysisWillingness contribute operational responsibility teamâs applications Some one Relational Databases & NoSQL databases Cassandra, Redis, MongoDB Big Data tools EMR, HBase"
124,Data Engineer,Data Engineer-Senior Advisor,"Richardson, TX",Richardson,TX,"The Data Analyst, Senior Advisor will work with data stewards, data owners, master data management analysts, operations teams and IT partners across the organization to drive the enterprise data conformance program. Data research and analysis, cross-functional requirements gathering and documentation and solution development are key aspects of the role. The Data Analyst, Senior Advisor supports reactive data quality by researching and fixing known data issues and proactive data quality by defining the requirements for data controls and enhancements to data capture, monitoring and maintenance processes, procedures and standards.

Primary responsibilities include performing data operations such as conversion, address hygiene, and postal presort, as well as variable document composition and file creation.

Essential Functions:

Data Analysis
Idependently design, develop and implement data integration solutions that support our platforms resiliency, stability, and supportability using a variety of ETL and database technologies.Rapidly develop and refine data integration solutions using Infosphere Datastage, SQL, FastTrack, or other technologies.Experience integrating large structured and unstructured data in multiple format, character sets and delivery methodsWorks with business sponsors, SMES and application teams; to understand the business requirements; analyze and assess availability, quality, and lineage of source system data.Design, map data from source to target and develop data integration solutions that meet business needs.Develop and socialize data integration standards.Partners with other engineers through design reviews, providing feedback on feasibility, scalability, performance and adherence to standards.Partners with business, analysts, BI team, application teams and other stakeholders to design, map data from source to target, develop, test, and implement production data integration solutions that are fully integrated into the Enterprise Data Warehouse.Ensuring model design solves the end users need.Contribute information to the data governance software to improve knowledge downstream.
Data Conformance
Performs analysis on known data quality issues and develops and/or recommends operational or technical solutions for remediation, including development and implementation of automated data quality controls that proactively trigger notifications to process owners when data is out of range. Keeps stakeholders informed of progress and solutions in a timely manner.Autonomously, and proactively performs data profiling to explore data, identify issues and summarize findings.Defines data quality metrics to assess completeness, accuracy, consistency, and conformance to business rules. Designs dashboards to support continuous monitoring and measurement of data quality.Partners with IT to cleanse data to achieve the desired level of data qualityPartners with data stakeholders, process and product owners across the organization to define data standards and communicate changes to data capture procedures, processes, standards, and controls.Cleanses and prepares datasets to be consumed by data scientists and other analystsCollaborates with external teams working on data integration and engineeringAdvocates data governance and hygiene best practicesAssists with scoping and integrating orphan datasets
You will gain hands on experience implementing through embedding standardized data elements in the database or system, employing standardized data elements in an exchange mechanism (usually XML schema), or mapping the application data elements to the standardized elements for purposes of exchange.

Big Data tools:
ï¶ Big Data: Hadoop, PIG, Sqoop, Hive and Hcatalog & NoSQL (HBase, Cassandra) , SQL.
ï¶ Programming: Scala, Java, Python, Spark

Required Qualifications
7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.7+ years of SQL development skills within a multi-tier environment are required.

Preferred Qualifications
In depth understanding of data integration best practices, leading industry applications and features such as master data management, entity resolution, data quality assessment, metadata management, etc.Expertise in flat file formats, XML within PL/SQL and file format conversion.Exposure to application security technologies and approaches is preferred.Experience processing and parsing CSV, JSON and XML file formatsDatastage, SQL, FastTrack, or other technologiesStrong analytical, debugging and testing skillsSoftware development experience using scripting languages such as JavaScript, Python or RubyProficient using Infosphere/DataStage or equivalent ETL software.Proficient with relational databases and using SQL to query, create tables, views, indexes, joins.Proficient using Unix and applicable scripting/scheduling tools.Experience with Python for data analysis
â¢ Knowledge of clinical and financial Healthcare data â¢ â¢ Knowledge of all data formats (HL7, EDI, CSV, XML, etc)

Education
Bachelor's Degree in Computer Science, Computer Engineering, Computer Information Systems, Information Systems, Management Information Systems or related engineering discipline.

Equivalent work experience will be considered in lieu of degree.

Business Overview
Itâs a new day in health care.

Combining CVS Health and Aetna was a transformative moment for our company and our industry, establishing CVS Health as the nationâs premier health innovation company. Through our health services, insurance plans and community pharmacists, weâre pioneering a bold new approach to total health. As a CVS Health colleague, youâll be at the center of it all.

We offer a diverse work experience that empowers colleagues for career success. In addition to skill and experience, we also seek to attract and retain colleagues whose beliefs and behaviors are in alignment with our core values of collaboration, innovation, caring, integrity and accountability.

CVS Health is an equal opportunity/affirmative action employer. Gender/Ethnicity/Disability/Protected Veteran â we highly value and are committed to all forms of diversity in the workplace. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities. We comply with the laws and regulations set forth in the following EEO is the Law Poster: EEO IS THE LAW and EEO IS THE LAW SUPPLEMENT. We provide reasonable accommodations to qualified individuals with disabilities. If you require assistance to apply for this job, please contact our Advice and Counsel Reasonable Accommodations team. Please note that we only accept applications for employment via this site.

If technical issues are preventing you from applying to a position, contact Kenexa Helpdesk at 1-855-338-5609 or cvshealthsupport@us.ibm.com. For technical issues with the Virtual Job Tryout assessment, contact the Shaker Help Desk at 1-877-987-5352.",7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.7+ years of SQL development skills within a multi-tier environment are required.    ,7+ years of experience in data analysis.7+ integrating large multiple formats7+ working with high volume exchange and transaction processing systems. Preferably a custom software development environment.7+ SQL skills within multi-tier environment are required.,7+ years experience data analysis.7+ integrating large multiple formats7+ working high volume exchange transaction processing systems. Preferably custom software development environment.7+ SQL skills within multi-tier environment required.
125,Data Engineer,Sr. Data Engineer,"Dallas, TX",Dallas,TX,"Beyondsoft Consulting, Inc., is a leading, technical solutions and consulting partner. We combine emerging technologies and proven methodologies to tailor elegant solutions that solve complex challenges and empower our customers to accelerate their business goals. Our services include end-to-end support for cloud, digital, data analytics, multi-language translation, and testing.

Our client is growing their Data Engineering team within a demanding and well recognized enterprise and information technology company. This role will be the core solution of the Strategic Analytics organization, ensuring both the reliability and applicability of the teamâs data products to the organization. This individual will have extensive experience with ETL design, coding, and testing patterns as well as engineering software platforms and large-scale data infrastructures. The Data Engineers will have the capability to architect highly scalable end-to-end pipeline using different open source tools, including building and operationalizing high-performance algorithms. Proven experience with technologies to solve big data problems with expert knowledge in programming languages like Java, Python, Linux, PHP, Hive, Impala, and Spark.
Responsibilities
Responsibilities:

Translate complex functional and technical requirements into detailed design
Hadoop technical development and implementation
Loading from disparate data sets by leveraging various big data technology e.g. Kafka
Pre-processing using Hive, Impala, Spark, and Pig
Design and implement data modeling
Maintain security and data privacy in an environment secured using Kerberos and LDAP
High-speed querying using in-memory technologies such as Spark
Following and contributing best engineering practice for source control, release management, deployment etc
Production support, job scheduling/monitoring, ETL data quality, data freshness reporting
Qualifications
Qualifications:

5-8 years of Python or Java/J2EE development experience
3+ years of demonstrated technical proficiency with Hadoop and big data projects
5-8 years of demonstrated experience and success in data modeling
Fluent in writing shell scripts [bash, korn]
Writing high-performance, reliable and maintainable code
Ability to write MapReduce jobs
Knowledge of database structures, theories, principles, and practices
Understand how to develop code in an environment secured using a local KDC and OpenLDAP
Familiarity with and implementation knowledge of loading data using Sqoop
Knowledge and ability to implement workflow/schedulers within Oozie
Experience working with AWS components [EC2, S3, SNS, SQS]
Analytical and problem-solving skills, applied to Big Data domain
Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark
Aptitude in multi-threading and concurrency concepts
M.S. in Computer Science or Engineering"," 5-8 years of Python or Java/J2EE development experience 3+ years of demonstrated technical proficiency with Hadoop and big data projects 5-8 years of demonstrated experience and success in data modeling Fluent in writing shell scripts [bash, korn] Writing high-performance, reliable and maintainable code Ability to write MapReduce jobs Knowledge of database structures, theories, principles, and practices Understand how to develop code in an environment secured using a local KDC and OpenLDAP Familiarity with and implementation knowledge of loading data using Sqoop Knowledge and ability to implement workflow/schedulers within Oozie Experience working with AWS components [EC2, S3, SNS, SQS] Analytical and problem-solving skills, applied to Big Data domain Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark Aptitude in multi-threading and concurrency concepts M.S. in Computer Science or Engineering   Translate complex functional and technical requirements into detailed design Hadoop technical development and implementation Loading from disparate data sets by leveraging various big data technology e.g. Kafka Pre-processing using Hive, Impala, Spark, and Pig Design and implement data modeling Maintain security and data privacy in an environment secured using Kerberos and LDAP High-speed querying using in-memory technologies such as Spark Following and contributing best engineering practice for source control, release management, deployment etc Production support, job scheduling/monitoring, ETL data quality, data freshness reporting  ","5-8 years of Python or Java/J2EE development experience 3+ demonstrated technical proficiency with Hadoop and big data projects success in modeling Fluent writing shell scripts [bash, korn] Writing high-performance, reliable maintainable code Ability to write MapReduce jobs Knowledge database structures, theories, principles, practices Understand how develop an environment secured using a local KDC OpenLDAP Familiarity implementation knowledge loading Sqoop ability implement workflow/schedulers within Oozie Experience working AWS components [EC2, S3, SNS, SQS] Analytical problem-solving skills, applied Big Data domain Proven understanding hands on Hadoop, Hive, Pig, Impala, Spark Aptitude multi-threading concurrency concepts M.S. Computer Science Engineering Translate complex functional requirements into detailed design Loading from disparate sets by leveraging various technology e.g. Kafka Pre-processing Spark, Pig Design Maintain security privacy Kerberos LDAP High-speed querying in-memory technologies such as Following contributing best engineering practice for source control, release management, deployment etc Production support, job scheduling/monitoring, ETL quality, freshness reporting","5-8 years Python Java/J2EE development experience 3+ demonstrated technical proficiency Hadoop big data projects success modeling Fluent writing shell scripts [bash, korn] Writing high-performance, reliable maintainable code Ability write MapReduce jobs Knowledge database structures, theories, principles, practices Understand develop environment secured using local KDC OpenLDAP Familiarity implementation knowledge loading Sqoop ability implement workflow/schedulers within Oozie Experience working AWS components [EC2, S3, SNS, SQS] Analytical problem-solving skills, applied Big Data domain Proven understanding hands Hadoop, Hive, Pig, Impala, Spark Aptitude multi-threading concurrency concepts M.S. Computer Science Engineering Translate complex functional requirements detailed design Loading disparate sets leveraging various technology e.g. Kafka Pre-processing Spark, Pig Design Maintain security privacy Kerberos LDAP High-speed querying in-memory technologies Following contributing best engineering practice source control, release management, deployment etc Production support, job scheduling/monitoring, ETL quality, freshness reporting"
126,Data Engineer,Data Engineer,"Dallas, TX",Dallas,TX,"Summary:

You have experience with client projects and in handling vast amounts of data â working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Analytics team. Play a key role on projects from a data engineering perspective, working with our Architects and clients to model the data landscape, obtain data extracts and define secure data exchange approaches.
Plan and execute secure, good practice data integration strategies and approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Create and manage data environments in the Cloud
Collaborate with our business analysts and data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models
Have a strong understanding of Information Security principles to ensure compliant handling and management of client data
This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science
Qualifications:
Experience on client-facing projects, including working in close-knit teams
Experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)
Experience or familiarity with real-time ingestion and streaming frameworks is a plus
Experience and desire to work with open source and branded open source frameworks
Experience working on projects within the cloud ideally AWS or Azure
Experience with NLP, Machine Learning, etc. is a plus
Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time
Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R
Data Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models
Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.
A deep personal motivation to always produce outstanding work for your clients and colleagues
Excel in team collaboration and working with others from diverse skill-sets and backgrounds
Cervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.

hG1oHsQXvK"," Experience on client-facing projects, including working in close-knit teams Experience and interest in Big Data technologies  Hadoop / Spark / NoSQL DBs  Experience or familiarity with real-time ingestion and streaming frameworks is a plus Experience and desire to work with open source and branded open source frameworks Experience working on projects within the cloud ideally AWS or Azure Experience with NLP, Machine Learning, etc. is a plus Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C , R Data Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner. A deep personal motivation to always produce outstanding work for your clients and colleagues Excel in team collaboration and working with others from diverse skill-sets and backgrounds    ","Experience on client-facing projects, including working in close-knit teams and interest Big Data technologies Hadoop / Spark NoSQL DBs or familiarity with real-time ingestion streaming frameworks is a plus desire to work open source branded projects within the cloud ideally AWS Azure NLP, Machine Learning, etc. lively consulting setting, often different multiple at same time Strong development background experience least two scripting, object oriented functional programming language, SQL, Python, Java, Scala, C , R Warehousing experience, building operational ETL data pipelines across number of sources, constructing relational dimensional models Excellent interpersonal skills when interacting clients clear, timely, professional manner. A deep personal motivation always produce outstanding for your colleagues Excel team collaboration others from diverse skill-sets backgrounds","Experience client-facing projects, including working close-knit teams interest Big Data technologies Hadoop / Spark NoSQL DBs familiarity real-time ingestion streaming frameworks plus desire work open source branded projects within cloud ideally AWS Azure NLP, Machine Learning, etc. lively consulting setting, often different multiple time Strong development background experience least two scripting, object oriented functional programming language, SQL, Python, Java, Scala, C , R Warehousing experience, building operational ETL data pipelines across number sources, constructing relational dimensional models Excellent interpersonal skills interacting clients clear, timely, professional manner. A deep personal motivation always produce outstanding colleagues Excel team collaboration others diverse skill-sets backgrounds"
127,Data Engineer,Technical Advisor - Data Engineer,"Plano, TX",Plano,TX,"Company: FedEx Services
Job Title: Technical Advisor - Data Engineer
Job Requisition Number: RC196455
Category: Information Technology
Locations:
Plano, Texas 75024
United States

Job Summary:
The Technical Advisor - Data Engineer will play a critical role in modernizing the architectures and processes enabling operationalization of big data acquisition, processing and quality monitoring for our retail domain (Omni-channel, manufacturing network, back office). You will work closely with technical solution architects, data scientists and business stakeholders to develop capabilities for enabling timely connections to high-quality data sourced from many disparate systems. While this role will primarily be technical in nature, we are looking for an individual who can effectively collaborate with our business partners and other IT members to identify and define scalable architectures and capabilities. This is an exciting opportunity to work with a team of passionate practitioners using current data technologies, and with significant potential for growth (machine learning, etc.).

Key Responsibilities
Develop, construct, test and maintain architectures for scalable, timely and efficient big data processingBuild data pipelines in lambda architectures for structured/unstructured, streaming/batch dataDevelop solutions for data modeling, mining and quality monitoringDevelop solutions for deploying models to productionIdentify and make recommendations on ways to improve data quality, reliability and efficiencyDiscover opportunities for data acquisitionMaintain awareness of ongoing developments in big data and data engineering technologies both in the industry in general and within the enterprise

Basic Qualifications
Bachelorâs degree; STEM background preferred5+ yearsâ experience developing scalable big data solutions in cloud and on premise environmentsExperience with big data technologies (Spark, Hadoop); data lakesExperience developing data processing/ETL/ELT solutions using e.g. Scala, Python, Java, SQLExperience deploying statistical/machine learning models to productionExperience building cloud-based data solutions; Azure experience preferred

Preferred Qualifications
Experience with notebook-based development (Databricks, Jupyter)Experience with data governance/master data managementExperience developing statistical and/or machine learning modelsExperience developing serverless solutionsExperience with graph databasesIoT knowledge; edge computingRetail domain knowledgeSAFe/agile experience
Domicile: Plano, TX
Relocation assistance may be available for this position, but is based on business decision.

Minimum Qualifications:
Bachelor's Degree, in computer science, engineering, information system or related field and/or equivalent formal training or work experience. Requires five (5) or more years qualifying work experience in information technology or engineering environment. A related advanced degree may offset the related experience requirements.

To apply for this position, please upload a current copy of your resume and complete the required screening questionnaire by close of business (5:00 PM CST) on _10/14/19, to be considered.


Want a career where you are empowered to make a difference? Want to work for a company that is environmentally responsible? Want to grow and develop on the job? If so, FedEx is the place for you! Every day FedEx delivers for its customers with transportation and business solutions. FedEx serves more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx employees. FedEx has over 400,000 talented employees who are tasked with making every FedEx experience outstanding. FedEx has been recognized on many different lists both for business success and for being a great employer.
Here are some of the recognitions FedEx has received from the past couple of years:
Fortune ""Worldâs Most Admired Companies"" â 2019
Forbes ""Best Employers for Diversity"" - 2019
Reputation Institute ""World\âs Most Reputable Companies"" â 2019
National Business Inclusion Consortium ""Best-of-the-Best Corporations for Inclusion"" - 2019
Women\âs Business Enterprise National Council ""Americaâs Top Corporations for Womenâs Business Enterprises"" - 2018
Corporate Responsibility Magazine ""100 Best Corporate Citizens"" â 2018
Black Enterprise ""50 Best Companies For Diversity"" â 2018
When 400,000 employees around the globe are all working together it is amazing what we can achieve! FedEx connects people and ideas. If you would like to make a difference on a global scale while receiving top notch benefits, competitive pay, and plenty of opportunities to develop, click âApplyâ and tell us more about yourself.
EEO Statement - FedEx is an equal opportunity/affirmative action employer (minorities/females/disability/veterans) that is committed to diversifying its workforce.","Bachelorâs degree; STEM background preferred5+ yearsâ experience developing scalable big data solutions in cloud and on premise environmentsExperience with big data technologies  Spark, Hadoop ; data lakesExperience developing data processing/ETL/ELT solutions using e.g. Scala, Python, Java, SQLExperience deploying statistical/machine learning models to productionExperience building cloud-based data solutions; Azure experience preferred  Develop, construct, test and maintain architectures for scalable, timely and efficient big data processingBuild data pipelines in lambda architectures for structured/unstructured, streaming/batch dataDevelop solutions for data modeling, mining and quality monitoringDevelop solutions for deploying models to productionIdentify and make recommendations on ways to improve data quality, reliability and efficiencyDiscover opportunities for data acquisitionMaintain awareness of ongoing developments in big data and data engineering technologies both in the industry in general and within the enterprise  ","Bachelorâs degree; STEM background preferred5+ yearsâ experience developing scalable big data solutions in cloud and on premise environmentsExperience with technologies Spark, Hadoop ; lakesExperience processing/ETL/ELT using e.g. Scala, Python, Java, SQLExperience deploying statistical/machine learning models to productionExperience building cloud-based solutions; Azure preferred Develop, construct, test maintain architectures for scalable, timely efficient processingBuild pipelines lambda structured/unstructured, streaming/batch dataDevelop modeling, mining quality monitoringDevelop productionIdentify make recommendations ways improve quality, reliability efficiencyDiscover opportunities acquisitionMaintain awareness of ongoing developments engineering both the industry general within enterprise","Bachelorâs degree; STEM background preferred5+ yearsâ experience developing scalable big data solutions cloud premise environmentsExperience technologies Spark, Hadoop ; lakesExperience processing/ETL/ELT using e.g. Scala, Python, Java, SQLExperience deploying statistical/machine learning models productionExperience building cloud-based solutions; Azure preferred Develop, construct, test maintain architectures scalable, timely efficient processingBuild pipelines lambda structured/unstructured, streaming/batch dataDevelop modeling, mining quality monitoringDevelop productionIdentify make recommendations ways improve quality, reliability efficiencyDiscover opportunities acquisitionMaintain awareness ongoing developments engineering industry general within enterprise"
128,Data Engineer,Data Engineer Cloud,"Dallas, TX",Dallas,TX,"COMPANY OVERVIEW
For over a century, Neiman Marcus Group has served the unique needs of our discerning customers by staying true to the principles of our founders: to be the premier omni-channel retailer of luxury and fashion merchandise dedicated to providing superior service and a distinctive shopping experience in our stores and on our websites. Neiman Marcus Group is comprised of the Specialty Retail Stores division, which includes Neiman Marcus and Bergdorf Goodman, and our international brand, mytheresa.com. Our portfolio of brands offers the finest luxury and fashion apparel, accessories, jewelry, beauty, and home dÃ©cor. The Company operates more than 40 Neiman Marcus full-line stores in the most affluent markets across the United States, including U.S. gateway cities that draw an international clientele. In addition, we operate 2 Bergdorf Goodman stores in landmark locations on Fifth Avenue in New York City. We also operate more than 40 Last Call by Neiman Marcus off-price stores that cater to a value oriented, yet fashion minded customer. Our upscale eCommerce and direct-to-consumer division includes NeimanMarcus.com, BergdorfGoodman.com Horchow.com, LastCall.com, and CUSP.com. Every day each of our 15,000 NMG associates works towards the goal of enabling our customer to shop any of our brands ""anytime, anywhere, and on any device."" Whether the merchandise we sell, the customer service we offer, or our investments in technology, everything we do is to enhance the customer experience across all channels and brands.
DESCRIPTION
Neiman Marcus Group has an immediate opening for a Lead Data Platform Engineer.
The Senior Data Platform Engineer will have the unique combination of business acumen needed to interface directly with key stakeholders to understand business challenges, along with the skills and vision required to translate the need into a world-class technical solution using the latest technologies.
This person will be in a hands-on role as part of a development team responsible for building data engineering solutions for the NMG Enterprise using cloud based data platforms. They will work closely with solution architects and support teams and take a lead on day-to-day development and support for data engineering workloads. In this role, you need to be equally skilled with the whiteboard and the keyboard.
SUMMARY
Primary focus of this position is to provide design and development support to the Data Platform team. Day to day activities will include, but not be limited toâ¦ingesting various data sources into the data platform, designing and deploying consolidation and summary tables, design and deployment of data marts, data/process modeling, systematic auditing of load processes, creating batch process automation, and analyzing source data from various applications for purposes of reporting, business intelligence and data science initiatives.
This position is a hands-on, intense role of working side by side with our end user partners to drive better analytics, reporting and most importantlyâ¦competitive advantage. This person will be an integral part of the Customer Data platform team, working with architects, developers and data platform engineers to support the overall Neiman Marcus enterprise.

ESSENTIAL DUTIES AND RESPONSIBILITIES (include the following, other duties not listed may be assigned)â¦
Work primarily with architects and at times with business partners and data science teams to understand business context and craft best-in-class solutions to their toughest problems
Provide data modeling, process modeling, and data mart design support.
Create Python Scripts/SQL scripts in support of data platform load and batch processes.
Design and deploy consolidation and summary tables as required within the data warehousing environment.
Perform periodic performance assessments of the automated load processes.
Be proactive in identifying and working with issues.
Provide specialized support for our Legacy platforms, as well as the new EDW.
Documentation of deliverables.
Standardization of deliverables.
Create robust and automated pipelines to ingest and process structured and unstructured data from source systems into analytical platforms using batch and streaming mechanisms leveraging a cloud native toolset
Implements automation to optimize data platform compute and storage resources
Develops and enhances end to end monitoring capability of cloud Data platforms
Responsible for implementing custom data applications as required for delivering actionable insights
Provides regular status updates to all relevant stakeholders
Participates in daily scrum calls and provides clear visibility to work products
Participates in developing projects plan, timelines and providing estimates
Provide hands-on technical assistance in all aspects of data engineering design and implementations including data ingestion, data models, data structures, data storage, data processing, and data monitoring at scale
Develop data engineering best practices with considerations for high data availability, fault tolerance, computational efficiency, cost, and quality

Qualifications

REQUIREMENTS
4 year College degree in Computer Science, Information Technology or equivalent demonstrated experience. Masters degree preferred.
Strong SQL development skills using databases like oracle and Vertica.
Experience in cloud databases like Snowflake or Redshift is a plus
Experience with AWS technologies such as EC2, S3 and other basic AWS technologies
Certification âpreferably AWS Certified Big Data or any other cloud data platforms, big data platforms
4+ years of experience in the data and analytics space
Solid Programing experience in Python - needs to be an expert in this 4/5 level.
Experience with workload automation tools such as Airflow, Autosys.
4+ years experience developing and implementing enterprise-level data solutions utilizing Python , Java, Spark, and Scala, Airflow , Hive
3+ years in key aspects of software engineering such as parallel data processing, data flows, REST APIs, JSON, XML, and micro service architectures.
6+ years of RDBMS concepts with Strong Data analysis and SQL experience
3+ years of Linux OS command line tools and bash scripting proficiency
Cloud data warehouse experience - Snowflake is a plus
Must be an EXPERT with ETL Development on Unix Servers.
Must have demonstratable working knowledge of modern information and delivery practices for on-premises and cloud environments.
Must have demonstratable experience delivering robust information delivery and management solutions as part of a fast paced data platform program.
MUST BE AN EXPERT applying business rules/logic using SQL scripts.
Must have working knowledge of various data modeling techniques (3NF, denormalized, STAR Schema).
Position requires a self-starter, capable of quickly turning around vaguely defined projects with minimal supervision or assistance.
Ability to conduct analysis of source data sets to achieve target data set objectives.
STRONG VERBAL/WRITTEN COMMUNICATION is a MUST. Interacting with business community/users is a core requirement of the role.
Must be able to provide specialized support for our Legacy platforms, as well as the new Cloud Based Data Platform.
ADDITIONAL REQUIREMENTS
Candidate MUST possess a STRONG INITIATIVE.
Candidate MUST be able to run with a project on their own, with as little as a few sentences to begin the project with. Candidates requiring design specs will not be successful.
Retail experience is a plus.
Experience in the migration of data from an on premise database to a cloud based data warehouse platform is a strong plus.
Experience with Qlik, Business Objects, or Tableau is a plus.
A candidate with experience working with terabyte sized data warehouses and complex ETL mappings that process 50+ million records per day is strongly preferred.
NICE TO HAVE
Kubernetes and Docker experience a plus
Prior working experience on data science work bench
Cloud data warehouse experience - Snowflake is a plus
Data Modeling experience a plus
LANGUAGE/WRITING SKILLS
Strong, concise and grammatical oral and written communication skills.

MATHEMATICAL SKILLS

Basic math skills
REASONING/ ANALYTICAL ABILITY

Complex problem solving skills
Extensive data analytical skills
Initiative to develop efficiencies and process improvements
Documentation

Primary Location: United States of America-Texas-DALLAS-Irving-Information Services
Work Locations: Information Services Neiman Marcus 111 Customer Way Irving 75039
Job: Information Technology
Organization: Corporate
Schedule: Full-time
Shift: Day
Employee Status: Regular
Job Type: Standard
Job Level: Individual Contributor
Travel: No
Job Posting: Jun 3, 2019, 1:35:50 PM","4 year College degree in Computer Science, Information Technology or equivalent demonstrated experience. Masters degree preferred. Strong SQL development skills using databases like oracle and Vertica. Experience in cloud databases like Snowflake or Redshift is a plus Experience with AWS technologies such as EC2, S3 and other basic AWS technologies Certification âpreferably AWS Certified Big Data or any other cloud data platforms, big data platforms 4+ years of experience in the data and analytics space Solid Programing experience in Python - needs to be an expert in this 4/5 level. Experience with workload automation tools such as Airflow, Autosys. 4+ years experience developing and implementing enterprise-level data solutions utilizing Python , Java, Spark, and Scala, Airflow , Hive 3+ years in key aspects of software engineering such as parallel data processing, data flows, REST APIs, JSON, XML, and micro service architectures. 6+ years of RDBMS concepts with Strong Data analysis and SQL experience 3+ years of Linux OS command line tools and bash scripting proficiency Cloud data warehouse experience - Snowflake is a plus Must be an EXPERT with ETL Development on Unix Servers. Must have demonstratable working knowledge of modern information and delivery practices for on-premises and cloud environments. Must have demonstratable experience delivering robust information delivery and management solutions as part of a fast paced data platform program. MUST BE AN EXPERT applying business rules/logic using SQL scripts. Must have working knowledge of various data modeling techniques  3NF, denormalized, STAR Schema . Position requires a self-starter, capable of quickly turning around vaguely defined projects with minimal supervision or assistance. Ability to conduct analysis of source data sets to achieve target data set objectives. STRONG VERBAL/WRITTEN COMMUNICATION is a MUST. Interacting with business community/users is a core requirement of the role. Must be able to provide specialized support for our Legacy platforms, as well as the new Cloud Based Data Platform.    ","4 year College degree in Computer Science, Information Technology or equivalent demonstrated experience. Masters preferred. Strong SQL development skills using databases like oracle and Vertica. Experience cloud Snowflake Redshift is a plus with AWS technologies such as EC2, S3 other basic Certification âpreferably Certified Big Data any data platforms, big platforms 4+ years of experience the analytics space Solid Programing Python - needs to be an expert this 4/5 level. workload automation tools Airflow, Autosys. developing implementing enterprise-level solutions utilizing , Java, Spark, Scala, Airflow Hive 3+ key aspects software engineering parallel processing, flows, REST APIs, JSON, XML, micro service architectures. 6+ RDBMS concepts analysis Linux OS command line bash scripting proficiency Cloud warehouse Must EXPERT ETL Development on Unix Servers. have demonstratable working knowledge modern information delivery practices for on-premises environments. delivering robust management part fast paced platform program. MUST BE AN applying business rules/logic scripts. various modeling techniques 3NF, denormalized, STAR Schema . Position requires self-starter, capable quickly turning around vaguely defined projects minimal supervision assistance. Ability conduct source sets achieve target set objectives. STRONG VERBAL/WRITTEN COMMUNICATION MUST. Interacting community/users core requirement role. able provide specialized support our Legacy well new Based Platform.","4 year College degree Computer Science, Information Technology equivalent demonstrated experience. Masters preferred. Strong SQL development skills using databases like oracle Vertica. Experience cloud Snowflake Redshift plus AWS technologies EC2, S3 basic Certification âpreferably Certified Big Data data platforms, big platforms 4+ years experience analytics space Solid Programing Python - needs expert 4/5 level. workload automation tools Airflow, Autosys. developing implementing enterprise-level solutions utilizing , Java, Spark, Scala, Airflow Hive 3+ key aspects software engineering parallel processing, flows, REST APIs, JSON, XML, micro service architectures. 6+ RDBMS concepts analysis Linux OS command line bash scripting proficiency Cloud warehouse Must EXPERT ETL Development Unix Servers. demonstratable working knowledge modern information delivery practices on-premises environments. delivering robust management part fast paced platform program. MUST BE AN applying business rules/logic scripts. various modeling techniques 3NF, denormalized, STAR Schema . Position requires self-starter, capable quickly turning around vaguely defined projects minimal supervision assistance. Ability conduct source sets achieve target set objectives. STRONG VERBAL/WRITTEN COMMUNICATION MUST. Interacting community/users core requirement role. able provide specialized support Legacy well new Based Platform."
129,Data Engineer,Data Engineer,"Plano, TX",Plano,TX,"Technical Lead

Qualification: Bachelors in science , engineering or equivalent

Responsibility:Project Planning and Setup:

Understand the project scope, identify activities/ tasks, task level estimates, schedule, dependencies, risks and provide inputs to Module Lead for review.
Provide inputs to testing strategy, configuration, deployment, hardware/software requirement etc.
Review plan and provide feedback on gaps, timeline and execution feasibility etc as required in the project.
Participate in KT sessions conducted by customer/ other business teams and provide feedback on requirements.

Requirement Understanding and Analysis: â¢ Analyze functional/non functional requirements and seek clarifications for better understanding of requirements.

Based on understanding of system upstream & downstream, provide feedback and inputs on gaps in requirements and technical feasibility of requirements.

Design: â¢ Prepare the LLD/ detailed design documents based on HLD and briefing from Module Lead.

Seek inputs from the developers on specific modules as applicable.
Consolidate all modules and provide to Module Lead/ Architects/ Designers for review.
Suggest changes in design on technical grounds.
Develop components inventory for the code to be developed tying it to the nonfunctional requirements.
Perform sampling of data to understand the character/ quality of the data (project dependent in the absence of data analyst or designer).
Identify tools and technologies to be used in the project as well as reusable objects that could be customized for the project.

Coding: â¢ Follow coding standards and best practices to develop code and check code quality.

Share developed code with supervisor for review.
Rework on the code based on inputs if required.
Perform complex integration.
Guide the developers in identifying, preparing and conducting unit test cases and fixing defects based on results.
Consolidate the test results and share with supervisor.
Provide periodic status update to supervisor and highlight / recommend any changes in design based on challenges faced.
Anticipate unreported defects and raise the same to supervisor.
Conduct technical troubleshooting.
Conduct reviews for codes created by team.

Testing Management: â¢ Develop unit test case for each module.

Conduct/ guide conducting of unit and integration testing and fix defects.
Review/ approve code to be moved to testing environment.
Provide support to the QA team and coordinate for various phases of testing.
Address queries raised by QA within defined timelines.
Investigate critical defects and establish need for fixing.
Raise issues to leads/QA.
Report defect status as per project standard process within agreed timelines.
Share revised code with supervisor for review.
Assist team lead and project manager on estimates around defect fixes.

Configuration Management: â¢ Maintain versions of the code or consolidate version maintained by the Developers.

Provide support as required to the Administrators during configuration, code backups, deployment etc.

Deployment: â¢ Assess and create deployment/ roll back plan.

Validate if all the components have been migrated and the right version is checked in.
Maintain deployment tracker.
Perform sanity checks post deployment to ensure smooth production.
Share activity status with supervisor and highlight concerns if any.

Project Execution Monitoring & closure (Support to Project Management activities): â¢ Monitor work of the developers and share work achieved with them.

Provide guidance through SDLC.
Provide status of progress to leads.
In case of change requests, provide inputs on the plan.

Service Support and Maintenance:
Specific to production and maintenance support: â¢ Provide support for 1 week and hand over to production team.

Identify if the incoming request is a service request/ defect during the warranty or an incident.
If it is code defect in the warranty, highlight to Lead and initiate defect fix process.
Post warranty, support in transition to maintenance team.

Knowledge Management: â¢ Post release participate in project review call and discuss points on what went well and what didn't.

Create and update knowledge articles (case studies, lessons learnt) in the knowledge management repository.
Guide developers in creating such documents.
Publish white papers/ blogs/ articles (if required).

People Management: â¢ Conduct training through academy or internally within the team.

Conduct technical, face to face interviews for internal transfer or external hiring.
Provide feedback on Developers form technical /domain standpoint to the module lead.

Technical Skills SNo Primary Skill Proficiency Level * Rqrd./Dsrd. 1 Snowflake PL1 Desired 2 Apache Spark PL3 Required 3 Amazon Web Services PL1 Required 4 Python PL1 Required 5 Core Java PL1 Desired


Proficiency Legends Proficiency Level Generic Reference PL1 The associate has basic awareness and comprehension of the skill and is in the process of acquiring this skill through various channels. PL2 The associate possesses working knowledge of the skill, and can actively and independently apply this skill in engagements and projects. PL3 The associate has comprehensive, in-depth and specialized knowledge of the skill. She / he has extensively demonstrated successful application of the skill in engagements or projects. PL4 The associate can function as a subject matter expert for this skill. The associate is capable of analyzing, evaluating and synthesizing solutions using the skill.

Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Aug 28 2019

About Cognizant Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 193 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @Cognizant.
Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.","     Based on understanding of system upstream & downstream, provide feedback and inputs on gaps in requirements and technical feasibility of requirements. ","Based on understanding of system upstream & downstream, provide feedback and inputs gaps in requirements technical feasibility requirements.","Based understanding system upstream & downstream, provide feedback inputs gaps requirements technical feasibility requirements."
130,Data Engineer,Senior Big Data Engineer / ML Engineer,"Dallas, TX",Dallas,TX,"Responsibilities:
Participate in the design and development of a big data analytics application
Design, support and continuously enhance the project code base, continuous integration pipeline, etc.
Write complex ETL processes and frameworks for analytics and data management
Developing new processes and models
Implement large-scale near real-time streaming data processing pipelines
Work with a team of industry experts on cutting-edge big data technologies to develop solutions for deployment at massive scale
Requirements:
5+ years of experience in Hadoop ecosystem
3+ years of hands-on experience in architecting, designing, and implementing data ingestion pipes for batch, real-time, and streams.
3+ years of hands-on experience with a proven track record in building data lakes on Public Clouds (preferably GCP and Azure (HDInsight))
3+ years of hands-on experience in Bigdata tools such as Sqoop, Hive, Spark, Scala, HBase, MapReduce etc.
2+ years of experience in Python and Scala.
Ready to work onsite I the USA up to 3 months.
Experience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred.
BA/BS required; preferably in Computer Science, Data Analytics, Data Science or Operations Research.
Highly analytical, motivated, decisive thought leader with solid critical thinking able to quickly connect technical and business âdotsâ.
Has strong communication and organizational skills and has the ability to deal with ambiguity while juggling multiple priorities and projects at the same time.
Able to understand statistical solutions and execute similar activities.
Will be a plus:
Having hands-on experience in using Infoworks / Nifi will be an added advantage.
Having experience in leading, guiding, and coaching data engineers is a plus.
Having exposure to R and ML technologies is a plus.
We offer:
Opportunity to work on bleeding-edge projects
Work with a highly motivated and dedicated team
Competitive salary
Flexible schedule
Benefits program
Social package - medical insurance, sports
Corporate social events
Professional development opportunities
About us:
Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.","   Participate in the design and development of a big data analytics application Design, support and continuously enhance the project code base, continuous integration pipeline, etc. Write complex ETL processes and frameworks for analytics and data management Developing new processes and models Implement large-scale near real-time streaming data processing pipelines Work with a team of industry experts on cutting-edge big data technologies to develop solutions for deployment at massive scale   5+ years of experience in Hadoop ecosystem 3+ years of hands-on experience in architecting, designing, and implementing data ingestion pipes for batch, real-time, and streams. 3+ years of hands-on experience with a proven track record in building data lakes on Public Clouds  preferably GCP and Azure  HDInsight   3+ years of hands-on experience in Bigdata tools such as Sqoop, Hive, Spark, Scala, HBase, MapReduce etc. 2+ years of experience in Python and Scala. Ready to work onsite I the USA up to 3 months. Experience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred. BA/BS required; preferably in Computer Science, Data Analytics, Data Science or Operations Research. Highly analytical, motivated, decisive thought leader with solid critical thinking able to quickly connect technical and business âdotsâ. Has strong communication and organizational skills and has the ability to deal with ambiguity while juggling multiple priorities and projects at the same time. Able to understand statistical solutions and execute similar activities.","Participate in the design and development of a big data analytics application Design, support continuously enhance project code base, continuous integration pipeline, etc. Write complex ETL processes frameworks for management Developing new models Implement large-scale near real-time streaming processing pipelines Work with team industry experts on cutting-edge technologies to develop solutions deployment at massive scale 5+ years experience Hadoop ecosystem 3+ hands-on architecting, designing, implementing ingestion pipes batch, real-time, streams. proven track record building lakes Public Clouds preferably GCP Azure HDInsight Bigdata tools such as Sqoop, Hive, Spark, Scala, HBase, MapReduce 2+ Python Scala. Ready work onsite I USA up 3 months. Experience wrangling, advanced analytic modeling, AI/ML capabilities is preferred. BA/BS required; Computer Science, Data Analytics, Science or Operations Research. Highly analytical, motivated, decisive thought leader solid critical thinking able quickly connect technical business âdotsâ. Has strong communication organizational skills has ability deal ambiguity while juggling multiple priorities projects same time. Able understand statistical execute similar activities.","Participate design development big data analytics application Design, support continuously enhance project code base, continuous integration pipeline, etc. Write complex ETL processes frameworks management Developing new models Implement large-scale near real-time streaming processing pipelines Work team industry experts cutting-edge technologies develop solutions deployment massive scale 5+ years experience Hadoop ecosystem 3+ hands-on architecting, designing, implementing ingestion pipes batch, real-time, streams. proven track record building lakes Public Clouds preferably GCP Azure HDInsight Bigdata tools Sqoop, Hive, Spark, Scala, HBase, MapReduce 2+ Python Scala. Ready work onsite I USA 3 months. Experience wrangling, advanced analytic modeling, AI/ML capabilities preferred. BA/BS required; Computer Science, Data Analytics, Science Operations Research. Highly analytical, motivated, decisive thought leader solid critical thinking able quickly connect technical business âdotsâ. Has strong communication organizational skills ability deal ambiguity juggling multiple priorities projects time. Able understand statistical execute similar activities."
131,Data Engineer,Big Data Engineer II,"Irving, TX",Irving,TX,"We are seeking a passionate and intellectually curious Big Data Engineer II for our Data Engineering team. Data Engineering team is responsible for creating data pipelines in big data space including data lake and data warehouse in AWS (Amazon Web Services), Azure cloud and on premise environments.

Essential Responsibilities:
Design, implement, and test major subsystems of AWS or AZURE cloud platform and core service offerings using the Scrum agile framework
Develop and follow best practices relative to design, implementation, and testing
Prototype new ideas or technologies to prove efficacy and usefulness in production
Build a service structure on AWS or AZURE capable of being deployed and scaled to run a variety of platform components dynamically
Build a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS or AZURE cloud
Construct a state-of-the-art data lake using Hadoop or Cassandra, Apache Spark, NiFi, and Kafka
Design & develop data pipelines for batch & streaming data sets using ETL and Data Integration tools, open source, AWS & Azure tech stack
Mentor junior team members
Non-Essential Responsibilities:
Other duties as assigned
Qualifications : Knowledge, Skills and Abilities:
Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming
Experience in Big Data technologies (Hadoop, Spark, NiFi, Kafka)
Ability to demonstrate experience in distributed UNIX environments.
Experience in writing shell scripts
Ability to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio.
Ability to multi-task and work under pressure.
Ability to be careful and thorough with detail.
Ability to work both independently and in a collaborative environment.
Ability to analyze information and use logic to address work related issues and problems.
Experience in the Healthcare Industry is a plus.
Work Conditions and Physical Demands:
Primarily sedentary work in a general office environment
Ability to communicate and exchange information
Ability to comprehend and interpret documents and data
Requires occasional standing, walking, lifting, and moving objects (up to 10 lbs.)
Requires manual dexterity to use computer, telephone and peripherals
May be required to work extended hours for special business needs
May be required to travel at least 10% of time based on business needs
Minimum Education:
The knowledge typically acquired during the course of attaining a Bachelorâs degree in Computer Science, Mathematics, or related discipline is required. A combination of education and experience may be used in lieu of a diploma.
Minimum Related Work Experience:
2-4 yearsâ experience designing and delivering production software
2 yearsâ experience designing and implementing big data high performance operational systems
Proven experience using the Microsoft development tools and stack, e.g., TFS, Github, Eclipse, JVM, etc
Nothing in this job description restricts managementâs right to assign or reassign duties and responsibilities to this job at any time.

EOE including disability/veteran.
Job Posting : Aug 13, 2019, 4:09:21 PM
Work Locations : USA-Texas-Irving","Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming Experience in Big Data technologies  Hadoop, Spark, NiFi, Kafka  Ability to demonstrate experience in distributed UNIX environments. Experience in writing shell scripts Ability to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio. Ability to multi-task and work under pressure. Ability to be careful and thorough with detail. Ability to work both independently and in a collaborative environment. Ability to analyze information and use logic to address work related issues and problems. Experience in the Healthcare Industry is a plus. Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming Experience in Big Data technologies  Hadoop, Spark, NiFi, Kafka  Ability to demonstrate experience in distributed UNIX environments. Experience in writing shell scripts Ability to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio. Ability to multi-task and work under pressure. Ability to be careful and thorough with detail. Ability to work both independently and in a collaborative environment. Ability to analyze information and use logic to address work related issues and problems. Experience in the Healthcare Industry is a plus. Design, implement, and test major subsystems of AWS or AZURE cloud platform and core service offerings using the Scrum agile framework Develop and follow best practices relative to design, implementation, and testing Prototype new ideas or technologies to prove efficacy and usefulness in production Build a service structure on AWS or AZURE capable of being deployed and scaled to run a variety of platform components dynamically Build a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS or AZURE cloud Construct a state-of-the-art data lake using Hadoop or Cassandra, Apache Spark, NiFi, and Kafka Design & develop data pipelines for batch & streaming data sets using ETL and Data Integration tools, open source, AWS & Azure tech stack Mentor junior team members The knowledge typically acquired during the course of attaining a Bachelorâs degree in Computer Science, Mathematics, or related discipline is required. A combination of education and experience may be used in lieu of a diploma. ","Command-level knowledge of Java and Python programming, the fundamentals computer science, data structures programming Experience in Big Data technologies Hadoop, Spark, NiFi, Kafka Ability to demonstrate experience distributed UNIX environments. writing shell scripts proficiency Microsoft Access, Excel, Word, PowerPoint Visio. multi-task work under pressure. be careful thorough with detail. both independently a collaborative environment. analyze information use logic address related issues problems. Healthcare Industry is plus. Design, implement, test major subsystems AWS or AZURE cloud platform core service offerings using Scrum agile framework Develop follow best practices relative design, implementation, testing Prototype new ideas prove efficacy usefulness production Build structure on capable being deployed scaled run variety components dynamically next-generation tools for creating, managing deploying multi-channel outreach campaigns Construct state-of-the-art lake Hadoop Cassandra, Apache Design & develop pipelines batch streaming sets ETL Integration tools, open source, Azure tech stack Mentor junior team members The typically acquired during course attaining Bachelorâs degree Computer Science, Mathematics, discipline required. A combination education may used lieu diploma.","Command-level knowledge Java Python programming, fundamentals computer science, data structures programming Experience Big Data technologies Hadoop, Spark, NiFi, Kafka Ability demonstrate experience distributed UNIX environments. writing shell scripts proficiency Microsoft Access, Excel, Word, PowerPoint Visio. multi-task work pressure. careful thorough detail. independently collaborative environment. analyze information use logic address related issues problems. Healthcare Industry plus. Design, implement, test major subsystems AWS AZURE cloud platform core service offerings using Scrum agile framework Develop follow best practices relative design, implementation, testing Prototype new ideas prove efficacy usefulness production Build structure capable deployed scaled run variety components dynamically next-generation tools creating, managing deploying multi-channel outreach campaigns Construct state-of-the-art lake Hadoop Cassandra, Apache Design & develop pipelines batch streaming sets ETL Integration tools, open source, Azure tech stack Mentor junior team members The typically acquired course attaining Bachelorâs degree Computer Science, Mathematics, discipline required. A combination education may used lieu diploma."
132,Data Engineer,Sr Security Data Engineer - Security Incident Response Team,"Dallas, TX 75201",Dallas,TX,"MORE ABOUT THIS JOB
Goldman Sachs Technology Risk is leading threat, risk analysis and data science initiatives
that are helping to protect the firm and our clients from information and cyber security risks. Our team equips the firm with the knowledge and tools to measure risk, identify and mitigate threats and protect against unauthorized disclosure of confidential information for our clients, internal business functions, and our extended supply chain.
SECURITY INCIDENT RESPONSE TEAM (SIRT) supports and enables a comprehensive technical Cyber Defense program for the firm while increasing awareness of current and potential Cyber Threats. Works across the organization to operate efficiently, provide technical
investigative support and mitigate threats to the firm.
Do you enjoy solving challenging puzzles? Protecting critical networks from cyber-attacks? Designing and integrating state-of-the-art technical solutions? A position as a Security Data Engineer on Goldman Sachsâ Threat Management Center lets you do all this and more:
RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
Design and develop data ingest and transform processesEngineer streaming data processing pipelinesDrive adoption of Cloud technology for data processing and warehousingEngage with data consumers and producers in order to design appropriate models to suit all needsApply latest technologies in machine learning, data mining, and predictive analytics to correlate the big datasets and events, and derive dynamic cybersecurity rules.Collaborate with a global team to continually operate and improve a world-class cyber program by driving the uplift of sensory tools, detection tuning, and access to data sources to increase detection effectiveness by applying data analytics.Participate in a 24x7 coverage model to prevent and remediate security threats against Goldman Sachsâ global business network.

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
5+ years of relevant work experience in a team-focused environmentBachelorâs degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)Working knowledge one or more programming languages (Python, Java, C++, C#, etc.)Extensive knowledge and proven experience applying domain driven design to build complex business applicationsDeep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processesIn-depth knowledge of relational and columnar SQL databases, including database designExcellent communications skills and the ability to work with subject matter experts to extract critical conceptsAbility to multi-task and prioritize work effectivelyIndependent thinker, willing to engage, challenge or learnHighly motivated self-starter who can provide thought leadership in big data analyticsStrong work ethic, a sense of ownership and urgencyStrong analytical and problem solving skillsResponsive to challenging tasking.Ability to document and explain technical details in a concise and understandable manner.Strong sense of ownership and driven to manage tasks to completion.

Preferred Qualifications
Financial Services industry experiencePrevious work experience in Cyber Security field is a plus.Experience with the Hadoop eco-system (HDFS, Spark)Experience with cloud based big data platforms such as AWS or Google a plus.
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

ÃÂ© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.","Financial Services industry experiencePrevious work experience in Cyber Security field is a plus.Experience with the Hadoop eco-system  HDFS, Spark Experience with cloud based big data platforms such as AWS or Google a plus.    ","Financial Services industry experiencePrevious work experience in Cyber Security field is a plus.Experience with the Hadoop eco-system HDFS, Spark Experience cloud based big data platforms such as AWS or Google plus.","Financial Services industry experiencePrevious work experience Cyber Security field plus.Experience Hadoop eco-system HDFS, Spark Experience cloud based big data platforms AWS Google plus."
133,Data Engineer,Senior Big Data Engineer,"Irving, TX",Irving,TX,"Where good people build rewarding careers.
Think that working in the insurance field canât be exciting, rewarding and challenging? Think again. Youâll help us reinvent protection and retirement to improve customersâ lives. Weâll help you make an impact with our training and mentoring offerings. Here, youâll have the opportunity to expand and apply your skills in ways you never thought possible. And youâll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
About our team
360 Finance Advanced Analytics data engineering team works with multiple internal and external data sources to deliver data that is readily available, easily accessible, accurate and complete. They are responsible for building a centralized data lake/hub using the Hadoop ecosystem that will be used by Reporting & Operational Analytics teams and the Machine learning teams.
Job Description
This Lead Consultant is an experienced professional who is responsible for leveraging data and analytics to help automate and optimize Claims Analytics Data processes enabling our Claims employees to focus on serving our customers and delivering the most advanced claims experience on the planet. They will be responsible for the strategy around how we bring together complex data into clean and useful data structures making our valuable data more approachable.
Key Responsibilities
Responsible for design, prototyping and delivery of software solutions within the big data eco-system
Leading projects and/or serving as analytics SME to provide new or enhanced data to the business
Improving data governance and quality increasing the reliability of our data
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Key Responsibilities (Cont'd)
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e. Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
Job Qualifications (Cont'd)
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
4-5+ years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes, developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow a plus
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work. Good Life. Good HandsÂ®.
As a Fortune 100 company and industry leader, we provide a competitive salary â but thatâs just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, youâll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.
Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.
Allstate generally does not sponsor individuals for employment-based visas for this position.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click âhereâ for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click âhereâ for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the âEEO is the Lawâ poster click âhereâ. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click âhereâ. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.
It is the Companyâs policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employeeâs ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment."," Undergraduate degree in Computer Science, Mathematics, Engineering  or related field  or equivalent experience preferred 5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function Ability to work with broad parameters in complex situations Experience in developing, managing, and manipulating large, complex datasets Expert high-level coding skills such as SQL and Python and/or other scripting languages UNIX  required. Scala is a plus.   Responsible for design, prototyping and delivery of software solutions within the big data eco-system Leading projects and/or serving as analytics SME to provide new or enhanced data to the business Improving data governance and quality increasing the reliability of our data Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise   ","Undergraduate degree in Computer Science, Mathematics, Engineering or related field equivalent experience preferred 5-7 years of a data integration, ETL and/or business intelligence/analytics function Ability to work with broad parameters complex situations Experience developing, managing, and manipulating large, datasets Expert high-level coding skills such as SQL Python other scripting languages UNIX required. Scala is plus. Responsible for design, prototyping delivery software solutions within the big eco-system Leading projects serving analytics SME provide new enhanced Improving governance quality increasing reliability our Influencing creation single, trusted source key Claims that can be shared across Enterprise","Undergraduate degree Computer Science, Mathematics, Engineering related field equivalent experience preferred 5-7 years data integration, ETL and/or business intelligence/analytics function Ability work broad parameters complex situations Experience developing, managing, manipulating large, datasets Expert high-level coding skills SQL Python scripting languages UNIX required. Scala plus. Responsible design, prototyping delivery software solutions within big eco-system Leading projects serving analytics SME provide new enhanced Improving governance quality increasing reliability Influencing creation single, trusted source key Claims shared across Enterprise"
134,Data Engineer,Google Data Engineer,"Dallas, TX",Dallas,TX,"Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet todayâs high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps â Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform. Multi-cloud experience a plus.   Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP DevOps an platform. Multi-cloud a plus. Proven ability to build, manage and foster team-oriented environment work creatively analytically in problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","Minimum 3 years previous Consulting client service delivery experience Google GCP DevOps platform. Multi-cloud plus. Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
135,Data Engineer,Senior Big Data Engineer*,"Irving, TX",Irving,TX,"Overview
Who we are
Imagine working in a place where continuous improvement and innovation is celebrated and rewarded; where fast-paced, high-impact teams come together to positively drive results for one of the largest & most iconic brands in the world.

As the only rapidly growing retailer, you may know us as your friendly neighborhood store. You probably know our familiar name, have seen our pervasive logo, and have tried our highly sought-after products, such as SlurpeeÂ® and Big BiteÂ®. âBrain Freezeâ is a 7-Eleven registered trademark for our 53-year old SlurpeeÂ® and with over 67,000 stores globally (more than any other retailer or food service provider), we sell over 14 million a month.

But thereâs a lot more to our story and much more left to be written. We are transforming our business, ensuring we are customer obsessed and digitally enabled to seamlessly link our brick and mortar stores with digital products and services.
At 7-Eleven the entrepreneurial spirit is in our DNA and has been ever since our inception 90+ years ago. Itâs what drove us to invent the convenience industry in 1927 by envisioning how a simple ice dock could provide household staples such as milk and eggs to better serve the needs of our customers.
Today we are redefining convenience and the customer experience in big ways...we are fundamentally changing our culture and we want talented, innovative, customer obsessed, and entrepreneurial people like you to come make history with us.
How we lead
At 7-Eleven we are guided by our Leadership Principles.
Be Customer Obsessed
Be Courageous with Your Point of View
Challenge the Status Quo
Act Like an Entrepreneur
Have an âIt Can Be Doneâ Attitude
Do the Right Thing
Be Accountable
Each principle has a defined set of behaviors which help guide the 7-Eleven team to Serve Customers and Support Stores.
About This Opportunity
Responsibilities
Work in the digital delivery organization to partner with vendors to support delivery of data integration solutions utilizing tools such as Databricks, Python custom development, Azure containers, and other Azure tool kits. Must have a strong and continuously evolving technical mastery of RESTful API development focused on Python.
Development of custom APIs for inbound/outbound data integrations, Spark jobs for ETL development and data integrations to internal and external systems.
Participating in designing new data applications.
Code deployments, Dev Ops, and Reviews.
Qualifications
Bachelors/4 Year Degree.
7-10 years of experience.
Expert knowledge of Big Data technologies including but not limited to Python and/or Databricks.
Strong Analytical and problem-solving skills.
Knowledgeable in cloud platforms (preferable AWS: both traditional EC2 and serverless ambda), micro-services architecture, CI/CD solutions (including Docker), DevOps principles, message queue system.
Proficiency in API security frameworks, token management and user access control including OAuth, JWT, etc.
Solid foundation and understanding of relational and NoSQL database principles.","Bachelors/4 Year Degree. 7-10 years of experience. Expert knowledge of Big Data technologies including but not limited to Python and/or Databricks. Strong Analytical and problem-solving skills. Knowledgeable in cloud platforms  preferable AWS  both traditional EC2 and serverless ambda , micro-services architecture, CI/CD solutions  including Docker , DevOps principles, message queue system. Proficiency in API security frameworks, token management and user access control including OAuth, JWT, etc. Solid foundation and understanding of relational and NoSQL database principles.   Development of custom APIs for inbound/outbound data integrations, Spark jobs for ETL development and data integrations to internal and external systems. Participating in designing new data applications. Code deployments, Dev Ops, and Reviews.  ","Bachelors/4 Year Degree. 7-10 years of experience. Expert knowledge Big Data technologies including but not limited to Python and/or Databricks. Strong Analytical and problem-solving skills. Knowledgeable in cloud platforms preferable AWS both traditional EC2 serverless ambda , micro-services architecture, CI/CD solutions Docker DevOps principles, message queue system. Proficiency API security frameworks, token management user access control OAuth, JWT, etc. Solid foundation understanding relational NoSQL database principles. Development custom APIs for inbound/outbound data integrations, Spark jobs ETL development integrations internal external systems. Participating designing new applications. Code deployments, Dev Ops, Reviews.","Bachelors/4 Year Degree. 7-10 years experience. Expert knowledge Big Data technologies including limited Python and/or Databricks. Strong Analytical problem-solving skills. Knowledgeable cloud platforms preferable AWS traditional EC2 serverless ambda , micro-services architecture, CI/CD solutions Docker DevOps principles, message queue system. Proficiency API security frameworks, token management user access control OAuth, JWT, etc. Solid foundation understanding relational NoSQL database principles. Development custom APIs inbound/outbound data integrations, Spark jobs ETL development integrations internal external systems. Participating designing new applications. Code deployments, Dev Ops, Reviews."
136,Data Engineer,Data Engineering/ETL/BI - Senior/Mid-Level Roles,"Dallas, TX 75207",Dallas,TX,"Job Category
Products and Technology
Job Details
Senior Data Engineer - Dallas
Software Engineer - ETL - Informatica
Tableau Developer/Admin

Locations: Dallas


Location Guidance - While we are always looking for great talent for this role in all locations , we are currently hiring for opportunities in Dallas . Please don't let that stop you from applying. While this/these are the current openings, your information will be saved for new opportunities as they open in all locations.


In school, or graduated within the last 12 months? Please visit FutureForce for opportunities.
Senior Data Engineer - Dallas

The IT Enterprise Data Warehouse (EDW) function is responsible for the delivery of operational reporting and performance metrics to various business domains including Sales and Sales Operations, Marketing, Finance, and Employee Success. Team also manages all aspects of rolling out Salesforceâs analytics tool Wave (Einstein Analytics) to internal business partners. The Salesforce IT EDW team is looking for an experienced BI developer who will work on designing, developing and implementing new functionality and increasing test coverage of systems that support various internal business processes at Salesforce.com . This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel.


Responsibilities
Design and develop data models, ETL mappings and associated database objects for analytical solutions using ERWin, ETL tools like Informatica, Oracle DB.
Create/review technical documentation for all new and modified data model.
Review solution design and ensure that the defined EDW standards and framework are followed.
Review and validate logical and physical design to ensure alignment with the defined solution architecture.
Ensure quality assurance plan and cases are comprehensive to validate the solution thoroughly.
Experience developing Packages, Procedures and Functions with PL/SQL to support ETL processes.
Experience in performance analysis and optimization of reports, dashboards, ETL and other components.
Tableau or equivalent reporting platformâs Infrastructure experience, including tuning performance issues, monitoring applications, analyzing logs and performing system operations
Required Skills
Bachelor's Degree in Computer Science, MIS, or a related discipline, with 7+ years of related Information systems experience in Data Engineering, Data warehousing, Business Intelligence and delivery of BI solutions.
Deep understanding of Data extraction mechanisms, data warehousing concepts, relational star-schema database designs, big data platforms and associated tools.
4+ years of experience in implementing solutions using Informatica and Oracle database or equivalent BI/reporting platforms.
Deep and strong knowledge of SQL and relational database models.
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Experience working in an agile environment is required, must have a clear understanding of the role and responsibility of a Scrum Master.
Excellent team player able to work with virtual and global across functional teams at all levels.
Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines.
Excellent spoken and written communication as well as receptive listening skills, with the ability to present complex ideas in a clear, concise fashion to technical and nontechnical audiences.
Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for the success of this role.
Nice to Have
Deep understanding and hands-on experience in unix or Python scripting
Understanding of Cloud Infrastructure and ecosystem: AWS or GCP
Hands on experience in Tableau tool implementations.
Knowledge around salesforce products
Software Engineer - ETL - Informatica - Dallas, TX


The IT Enterprise Data Services (EDS) function has historically been responsible for the delivery of operational reporting and performance metrics to various business domains including Sales and Sales Operations, Marketing, Finance, and Employee Success. This platform also includes management of a Big Data Hadoop platform to store unstructured application log data that is generated by the Salesforce product offerings, this data is predominately used by the data science teams. Team also manages all aspects of rolling out Salesforceâs analytics tool Wave to internal business partners.


The Salesforce IT EDS team is looking for an experienced ETL Engineer who will work on designing, developing and implementing new functionality and increasing test coverage of systems that support various internal business processes at Salesforce.com. This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel.


Responsibilities:
Design and development of analytical solutions including Informatica,Oracle, Cloud, Hadoop technologies.
Identify opportunities to refine the code release and deployment processes
Actively Participate in performance testing of various BI tiers and tools, monitor resource utilization and provide improvement recommendations.
Maintain the tools and framework defined for automating unit and system testing of ETL processes.
Develop and implement ETL frameworks using Shell and Python languages.
Design and Develop Pig/hive as well support existing jobs in Hadoop
Evaluate, determine root cause and resolve production issues.
Evaluate and actively participate in migration of BI services to cloud.
Required Skills:
Bachelor's Degree in Computer Science, MIS, or related discipline, with 5+ years related ETL experience.
Deep understanding of Informatica 9.x, and Oracle 11g system components, Big data/ Hadoop internal processes and architecture
Excellent knowledge with Python scripting and hand-on experience with scheduler tools like Control-M, Tidal, DAC etc.
Hands on knowledge in Git repository branching and code versioning
Good knowledge of SQL and relational database models.
Deep hands-on experience with Linux is a must
Hands-on experience in process automation, test automation, technology efficiency, and best practices
Good understanding of data warehousing concepts and relational star-schema database designs
Experience working with IT systems in a global business environment
Excellent team player able to work with virtual and global crosses functional teams.
Ability to working an Agile/Scrum environment and manage multiple projects/tasks in a fast paced environment.

Desired Skills:
Salesforce, Data Warehousing, ERP solutions and Data Analysis
Data visualization tool experience like Tableau, Qlikview or Spotfire is a plus.
Strong understanding of Hadoop ecosystem (HDFS, YARN, Zookeeper, HCatalog, HBase, Pig, Hive)
Java, HTML,CSS knowledge is preferable
Tableau Developer/Admin
Enterprise Data Services - Member of Technical Staff


Salesforce, the Customer Success Platform and world's #1 CRM, empowers companies to connect with their customers in a whole new way. We are the fastest growing of the top 10 enterprise software companies, the Worldâs Most Innovative Company according to Forbes, and one of Fortuneâs 100 Best Companies to Work For six years running. The growth, innovation, and Aloha spirit of Salesforce are driven by our incredible employees who thrive on delivering success for our customers while also finding time to give back through our 1/1/1 model, which leverages 1% of our time, equity, and product to improve communities around the world. Salesforce is a team sport, and we play to win. Join us!The IT Enterprise Data Warehouse (EDW) function is responsible for the delivery of operational reporting and performance metrics to various business domains including Sales and Sales Operations, Marketing, Finance, and Employee Success.Team also manages all aspects of rolling out Salesforceâs analytics tool Wave to internal business partners.The Salesforce IT EDW team is looking for an experienced BI developer who will work on designing, developing and implementing new functionality and increasing test coverage of systems that support various internal business processes at Salesforce.com . This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel.


Responsibilities
Install, Configure and manage Tableau platform or equivalent reporting platforms, Configuring the metadata repository (RPD) at the physical, logical and presentation layers to meet business requirements
Install , Configure and Maintain our Informatica, Business objects & Tidal scheduler .
Perform performance analysis and optimization of BI (Tableau or Business objects) reports, dashboards, ETL and other components.
Tableau or equivalent reporting platformâs Infrastructure experience, including tuning performance issues, monitoring applications, analyzing logs and performing system operations
Reporting layer security configuration experience, including Single Sign-On with EBS Financials R12, definitions of groups and roles, restricting dashboard and Analysis access to specific groups, performing data filtering based on user group, and configuring LDAP integration
Build Frameworks and automation across our stack for efficiency.
Required Skills
Bachelor's Degree in Computer Science, MIS, or a related discipline, with 5+ years of related Information systems experience in Data Engineering, Datawarehousing, Business Intelligence and delivery of BI solutions.
Basic understanding of Data extraction mechanisms, data warehousing concepts, relational star-schema database designs, big data platforms and associated tools.
Deep understanding and hands-on experience in unix shell scripting
System Administration experience with Tableau, Informatica , Enterprise schedulers or equivalent BI / reporting platforms
Cloud infrastructure knowledge of Aws or GCP is preferred.
Deep and strong knowledge of SQL and relational database models.
Experience working in an agile environment is required, must have a clear understanding of the role and responsibility of a Scrum Master.
Excellent team player able to work with virtual and global across functional teams at all levels.
Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines.
Excellent spoken and written communication as well as receptive listening skills, with the ability to present complex ideas in a clear, concise fashion to technical and nontechnical audiences.
Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for the success of this role.
Excellent analytical, problem solving and debugging skills, with strong ability to quickly learn and solve problems in order to effectively develop technical solutions to their requirements.
Posting Statement
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay fees to any third-party agency or company that does not have a signed agreement with Salesforce.com or Salesforce.org.","  Design and develop data models, ETL mappings and associated database objects for analytical solutions using ERWin, ETL tools like Informatica, Oracle DB. Create/review technical documentation for all new and modified data model. Review solution design and ensure that the defined EDW standards and framework are followed. Review and validate logical and physical design to ensure alignment with the defined solution architecture. Ensure quality assurance plan and cases are comprehensive to validate the solution thoroughly. Experience developing Packages, Procedures and Functions with PL/SQL to support ETL processes. Experience in performance analysis and optimization of reports, dashboards, ETL and other components. Tableau or equivalent reporting platformâs Infrastructure experience, including tuning performance issues, monitoring applications, analyzing logs and performing system operations   ","Design and develop data models, ETL mappings associated database objects for analytical solutions using ERWin, tools like Informatica, Oracle DB. Create/review technical documentation all new modified model. Review solution design ensure that the defined EDW standards framework are followed. validate logical physical to alignment with architecture. Ensure quality assurance plan cases comprehensive thoroughly. Experience developing Packages, Procedures Functions PL/SQL support processes. in performance analysis optimization of reports, dashboards, other components. Tableau or equivalent reporting platformâs Infrastructure experience, including tuning issues, monitoring applications, analyzing logs performing system operations","Design develop data models, ETL mappings associated database objects analytical solutions using ERWin, tools like Informatica, Oracle DB. Create/review technical documentation new modified model. Review solution design ensure defined EDW standards framework followed. validate logical physical alignment architecture. Ensure quality assurance plan cases comprehensive thoroughly. Experience developing Packages, Procedures Functions PL/SQL support processes. performance analysis optimization reports, dashboards, components. Tableau equivalent reporting platformâs Infrastructure experience, including tuning issues, monitoring applications, analyzing logs performing system operations"
137,Data Engineer,Data Engineer,"Dallas, TX",Dallas,TX,"Weâre looking for an experienced Data Engineer to help deliver critical business intelligence through our data warehouse. Our Data Engineering team handles all aspects of managing our data lake, data warehouse, and real-time data pipelines from four ecommerce products. This position is responsible for understanding stakeholders requirements and building out application database to data warehouse ETL.
Requirements
3+ years of data engineering experience
Robust experience with Python, Spark, and Jenkins
Experience writing and executing complex SQL queries
Experience building data pipelines and ETL design (implementation and maintenance)
Experience with AWS or other cloud provider
Scrum/Agile software development process.
Nice to have
Experience with data technologies, Hadoop, Hive, Postgres, Kafka, Solr
Deep understanding of AWS services: EMR, S3, Redshift
Operational experience with Jenkins or Airflow
Proficiency in using BI dashboard tools
Analytical experience debugging slow queries and scripts
About Us

Scalable Press is using technology to reinvent the mass customization industry.

Scalable Press was founded with an initial investment of just $2,000. Today, weâre on track to produce more than 12 million shipments across 185 countries. Weâre bootstrapped, profitable, and rapidly expanding.

We operate four B2C and B2B websites that are accessed by over 100 million visitors per year. Whether itâs embroidered dress shirts, all-over printed socks, or digitally printed t-shirts, we make it easier, cheaper, and faster to order customized goods than ever before.

Scalable Press is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.","     3+ years of data engineering experience Robust experience with Python, Spark, and Jenkins Experience writing and executing complex SQL queries Experience building data pipelines and ETL design  implementation and maintenance  Experience with AWS or other cloud provider Scrum/Agile software development process.","3+ years of data engineering experience Robust with Python, Spark, and Jenkins Experience writing executing complex SQL queries building pipelines ETL design implementation maintenance AWS or other cloud provider Scrum/Agile software development process.","3+ years data engineering experience Robust Python, Spark, Jenkins Experience writing executing complex SQL queries building pipelines ETL design implementation maintenance AWS cloud provider Scrum/Agile software development process."
138,Data Engineer,Big Data Engineer,"Plano, TX",Plano,TX,"Req ID: 58054

At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our companyâs growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here.

NTT DATA Services currently seeks a Big Data Engineer to join our team in Plano, Texas (US-TX), United States (US).

Responsibilities
Partner with data analyst, product owners and data scientists, to better understand requirements, solution designs, finding bottlenecks, resolutions, etc.
Support/Enhance data pipelines and ETL using heterogeneous sources
Transform data using data mapping and data processing capabilities like Spark, Spark SQL, HiveQL etc.
Expands and grows data platform capabilities to solve new data problems and challenges
Ability to dynamically adapt to conventional big-data frameworks and tools with the use-cases required by the project

Basic Skill Requirements
10+ years Enterprise Development
5+ years of experience with the Hadoop and Big Data technologies
2+ Years Design with Big Data Strategies.
Other Skills
Hands-on experience with the Hadoop eco-system - HDFS, MapReduce, HBase, Hive, Impala, Spark, Kafka
Experience in implementing Hadoop Data Lakes - Data storage, partitioning, splitting, file types (Parquet, Avro, ORC) for specific use cases etc.
Experience with Query languages â SQL, Hive, Impala, Drill etc.
Experience with NoSQL databases â MapR-DB, HBase, MongoDB, Cassandra etc.
Experience in agile(scrum) development methodology
Exposure to Data ingestion frameworks such as Kafka, Sqoop, Storm, Nifi, Spring Cloud, etc.
Experience with building stream-processing systems using solutions such as Kafka, MapR-Streams, Spark-Streaming etc.
Experience with development/automation skills. Must be very comfortable with reading and writing Scala, Python or Java code
Experience with one of the Hadoop open source distributions - MapR and Cloudera
Bachelor or masterâs Degree in engineering in Computer Science or Information Technology

This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment.
About NTT DATA Services

NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services.

NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more.

NTT DATA, Inc. (the âCompanyâ) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.

INDAPPS","  Hands-on experience with the Hadoop eco-system - HDFS, MapReduce, HBase, Hive, Impala, Spark, Kafka Experience in implementing Hadoop Data Lakes - Data storage, partitioning, splitting, file types  Parquet, Avro, ORC  for specific use cases etc. Experience with Query languages â SQL, Hive, Impala, Drill etc. Experience with NoSQL databases â MapR-DB, HBase, MongoDB, Cassandra etc. Experience in agile scrum  development methodology Exposure to Data ingestion frameworks such as Kafka, Sqoop, Storm, Nifi, Spring Cloud, etc. Experience with building stream-processing systems using solutions such as Kafka, MapR-Streams, Spark-Streaming etc. Experience with development/automation skills. Must be very comfortable with reading and writing Scala, Python or Java code Experience with one of the Hadoop open source distributions - MapR and Cloudera Bachelor or masterâs Degree in engineering in Computer Science or Information Technology  Partner with data analyst, product owners and data scientists, to better understand requirements, solution designs, finding bottlenecks, resolutions, etc. Support/Enhance data pipelines and ETL using heterogeneous sources Transform data using data mapping and data processing capabilities like Spark, Spark SQL, HiveQL etc. Expands and grows data platform capabilities to solve new data problems and challenges Ability to dynamically adapt to conventional big-data frameworks and tools with the use-cases required by the project  10+ years Enterprise Development 5+ years of experience with the Hadoop and Big Data technologies 2+ Years Design with Big Data Strategies.","Hands-on experience with the Hadoop eco-system - HDFS, MapReduce, HBase, Hive, Impala, Spark, Kafka Experience in implementing Data Lakes storage, partitioning, splitting, file types Parquet, Avro, ORC for specific use cases etc. Query languages â SQL, Drill NoSQL databases MapR-DB, MongoDB, Cassandra agile scrum development methodology Exposure to ingestion frameworks such as Kafka, Sqoop, Storm, Nifi, Spring Cloud, building stream-processing systems using solutions MapR-Streams, Spark-Streaming development/automation skills. Must be very comfortable reading and writing Scala, Python or Java code one of open source distributions MapR Cloudera Bachelor masterâs Degree engineering Computer Science Information Technology Partner data analyst, product owners scientists, better understand requirements, solution designs, finding bottlenecks, resolutions, Support/Enhance pipelines ETL heterogeneous sources Transform mapping processing capabilities like Spark HiveQL Expands grows platform solve new problems challenges Ability dynamically adapt conventional big-data tools use-cases required by project 10+ years Enterprise Development 5+ Big technologies 2+ Years Design Strategies.","Hands-on experience Hadoop eco-system - HDFS, MapReduce, HBase, Hive, Impala, Spark, Kafka Experience implementing Data Lakes storage, partitioning, splitting, file types Parquet, Avro, ORC specific use cases etc. Query languages â SQL, Drill NoSQL databases MapR-DB, MongoDB, Cassandra agile scrum development methodology Exposure ingestion frameworks Kafka, Sqoop, Storm, Nifi, Spring Cloud, building stream-processing systems using solutions MapR-Streams, Spark-Streaming development/automation skills. Must comfortable reading writing Scala, Python Java code one open source distributions MapR Cloudera Bachelor masterâs Degree engineering Computer Science Information Technology Partner data analyst, product owners scientists, better understand requirements, solution designs, finding bottlenecks, resolutions, Support/Enhance pipelines ETL heterogeneous sources Transform mapping processing capabilities like Spark HiveQL Expands grows platform solve new problems challenges Ability dynamically adapt conventional big-data tools use-cases required project 10+ years Enterprise Development 5+ Big technologies 2+ Years Design Strategies."
139,Data Engineer,Big Data Engineer with HL7 Healthcare/Python,"Dallas, TX",Dallas,TX,"Job Details
Job Code
JPSC-6455
Posted Date
10/25/17
Experience
8 Years
Primary Skills
Automation,integration,Performance,KAFKA,Sqoop,Flume,1) Big Data Solutions in Cloudera Environment (Act as a technical expert addressing problems related to system and application design,etc.) 2) Enterprise Data Lake App. Development(Hive
Required Documents
Resume
Overview
Role: Big Data Engineer with HL7 Healthcare/Python
Location: Dallas, TX
Duration: 9 Months
Top Three Skills:
1) Big Data Solutions in Cloudera Environment (Act as a technical expert addressing problems related to system and application design, performance, integration, Automation, etc.)
2) Enterprise Data Lake App. Development(Hive, Kafka, Flume, Sqoop, oozie, Spark, Shell Scripting, Python, SQL.all a must)
3) Data Warehousing (experience with relational database management systems like Oracle, Microsoft SQL Server, etc. )
4) Healthcare Domain Knowledge (HL 7, Health care exchange data, population health, claims data etc)
Job Description:
The Big Data Engineer is responsible for designing and building big data platforms, tools, and solutions that help manage, secure, and generate value from data. The person in this role creates scalable and reusable solutions for gathering, collecting, storing, processing, and serving data at scales.

As a member of the Big Data team at Ascension, this person will specialize in Big Data technologies and solutions, helping to support teams in response to deliverables, ownership and pro-activeness to drive positive customer experience.

Ascension Health (Ascension Information Systems) is in the process of constructing a large enterprise data warehouse (data lake, etc. ). Construction of structured data segments within the enterprise data warehouse is imperative to segmenting the provider healthcare data being pulled from the different Ministries (hospital networks) within the Ascension Health Network. The developers within this task will have expert understanding of design patterns and have strong analytical skills. The organization craves forward thinking, resourceful developers with experience in Big Data solutions for enterprise businesses. Proficiency in gathering requirements, analysis and validation of business requirements, specifications; along with functional specifications for schema creations and table creations.

Extensive experience in all phases of Software development life cycle (SDLC) is a must.Excellent skills in analyzing system architecture usage, defining and implementing procedures.Hands on experience in programming and implementation of Java, Scala and Python codes with strong knowledge in Object Oriented Concepts."," Extensive experience in all phases of Software development life cycle  SDLC  is a must.Excellent skills in analyzing system architecture usage, defining and implementing procedures.Hands on experience in programming and implementation of Java, Scala and Python codes with strong knowledge in Object Oriented Concepts.   ","Extensive experience in all phases of Software development life cycle SDLC is a must.Excellent skills analyzing system architecture usage, defining and implementing procedures.Hands on programming implementation Java, Scala Python codes with strong knowledge Object Oriented Concepts.","Extensive experience phases Software development life cycle SDLC must.Excellent skills analyzing system architecture usage, defining implementing procedures.Hands programming implementation Java, Scala Python codes strong knowledge Object Oriented Concepts."
140,Data Engineer,Senior Data Engineer,"Dallas, TX",Dallas,TX,"Lucid is a market research platform that provides access to authentic, first-party data in over 90 countries. Our products and services enable anyone, in any industry, to ask questions of targeted audiences and find the answers they need â fast. These answers can be used to uncover consumer motivations, increase revenue, and measure the impact of digital advertising. Founded in 2010, Lucid is headquartered in New Orleans, LA with offices in Dallas, New York, London, Sydney, Singapore, Gurgaon, Prague, and Hamburg.


The Opportunity

We are looking for a talented Data Engineer to join our expanding team of engineers. This candidate will be responsible for expanding and optimizing our growing data needs for cross-functional teams. The ideal candidate is experienced in traditional and cloud databases and one who enjoys optimizing data systems and building them from the ground up. The Data Engineer will collaborate with cross-functional teams on data initiatives and will ensure optimal data architecture. The ideal candidate must be self-motivated and comfortable supporting the data needs of multiple product and company initiatives.
Responsibilities
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of structured and unstructured data sources using âbig dataâ technologies preferably using AWS services
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure through multiple AWS regions
Create data tools for analytics and data science team members and assist them in building and optimizing our product into an innovative industry leader
Assemble large, complex data sets that meet functional / non-functional business requirements
Create and maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with data and analytics experts to strive for greater functionality in our data systems

Qualifications
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and CDC processes
Experience building and optimizing âbig dataâ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytical skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable âbig dataâ data stores
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
5+ years of experience in a Data Engineer role
Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Experience using the following software/tools:
Spark, Kafka, Kinesis, etc.
Relational SQL and NoSQL databases, including Postgres, MSSQL, Redis
Data pipeline and workflow management tools: Airflow, AWS Glue, Step functions etc.
AWS cloud services: EC2, EMR, RDS, Redshift, Athena, Lambda
Stream-processing systems: Storm, Spark-Streaming, etc.
Languages: Python, Java, Golang


At Lucid we foster a collaborative and inspiring workplace. We pride ourselves in doing this by recruiting, hiring and retaining diverse, passionate, and forward-thinking talent. Lucid is committed to and encourages an inclusive environment and we are dedicated to providing equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know."," Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases and CDC processes Experience building and optimizing âbig dataâ data pipelines, architectures and data sets Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Strong analytical skills related to working with unstructured datasets Build processes supporting data transformation, data structures, metadata, dependency and workload management A successful history of manipulating, processing and extracting value from large disconnected datasets Working knowledge of message queuing, stream processing, and highly scalable âbig dataâ data stores Strong project management and organizational skills Experience supporting and working with cross-functional teams in a dynamic environment 5+ years of experience in a Data Engineer role Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field Experience using the following software/tools  Spark, Kafka, Kinesis, etc. Relational SQL and NoSQL databases, including Postgres, MSSQL, Redis Data pipeline and workflow management tools  Airflow, AWS Glue, Step functions etc. AWS cloud services  EC2, EMR, RDS, Redshift, Athena, Lambda Stream-processing systems  Storm, Spark-Streaming, etc. Languages  Python, Java, Golang   Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of structured and unstructured data sources using âbig dataâ technologies preferably using AWS services Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs Keep our data separated and secure through multiple AWS regions Create data tools for analytics and data science team members and assist them in building and optimizing our product into an innovative industry leader Assemble large, complex data sets that meet functional / non-functional business requirements Create and maintain optimal data pipeline architecture Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Work with data and analytics experts to strive for greater functionality in our data systems   ","Advanced working SQL knowledge and experience with relational databases, query authoring as well familiarity a variety of databases CDC processes Experience building optimizing âbig dataâ data pipelines, architectures sets performing root cause analysis on internal external to answer specific business questions identify opportunities for improvement Strong analytical skills related unstructured datasets Build supporting transformation, structures, metadata, dependency workload management A successful history manipulating, processing extracting value from large disconnected Working message queuing, stream processing, highly scalable stores project organizational cross-functional teams in dynamic environment 5+ years Data Engineer role Graduate degree Computer Science, Statistics, Informatics, Information Systems or another quantitative field using the following software/tools Spark, Kafka, Kinesis, etc. Relational NoSQL including Postgres, MSSQL, Redis pipeline workflow tools Airflow, AWS Glue, Step functions cloud services EC2, EMR, RDS, Redshift, Athena, Lambda Stream-processing systems Storm, Spark-Streaming, Languages Python, Java, Golang infrastructure required optimal extraction, loading wide structured sources technologies preferably analytics that utilize provide actionable insights into customer acquisition, operational efficiency, other key performance metrics Work stakeholders Product, Design assist data-related technical issues support their needs Keep our separated secure through multiple regions Create science team members them product an innovative industry leader Assemble large, complex meet functional / non-functional requirements maintain architecture Identify, design, implement process improvements automating manual processes, delivery, re-designing greater scalability, experts strive functionality","Advanced working SQL knowledge experience relational databases, query authoring well familiarity variety databases CDC processes Experience building optimizing âbig dataâ data pipelines, architectures sets performing root cause analysis internal external answer specific business questions identify opportunities improvement Strong analytical skills related unstructured datasets Build supporting transformation, structures, metadata, dependency workload management A successful history manipulating, processing extracting value large disconnected Working message queuing, stream processing, highly scalable stores project organizational cross-functional teams dynamic environment 5+ years Data Engineer role Graduate degree Computer Science, Statistics, Informatics, Information Systems another quantitative field using following software/tools Spark, Kafka, Kinesis, etc. Relational NoSQL including Postgres, MSSQL, Redis pipeline workflow tools Airflow, AWS Glue, Step functions cloud services EC2, EMR, RDS, Redshift, Athena, Lambda Stream-processing systems Storm, Spark-Streaming, Languages Python, Java, Golang infrastructure required optimal extraction, loading wide structured sources technologies preferably analytics utilize provide actionable insights customer acquisition, operational efficiency, key performance metrics Work stakeholders Product, Design assist data-related technical issues support needs Keep separated secure multiple regions Create science team members product innovative industry leader Assemble large, complex meet functional / non-functional requirements maintain architecture Identify, design, implement process improvements automating manual processes, delivery, re-designing greater scalability, experts strive functionality"
141,Data Engineer,Data Engineer,"Dallas, TX 75244",Dallas,TX,"Job Description:
Basic Function
Komen is seeking a Data Engineer who lives and breathes data, sweats the details, deeply cares about data quality, data flows, integration, ETL, storage & performance. The Data Engineer will be creating data pipelines and process data sets that are available within Komen covering constituents, campaigns, research and patient domains.
Primary Responsibilities
Building and maintaining data processing workflows feeding our analytics, CRM and various other internal applications

Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organizationResponsible for design, development and implementation of optimal solutions to integrate, store, process and analyze huge data setsRecommend and implement strategies for bi-directional synchronization between sourcing data repositories and the central normalized data repositoryBuild data pipeline frameworks to automate high-volume and real-time data deliveryBuild data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partnersWork on multiple projects/tasks simultaneously to meet project deadlines for self and others as required.Establish and maintain positive working relationships with other employeesAll other duties as assigned.
Job Qualifications:
Minimum Experience Required
The ideal candidate will have a Bachelor's Degree in Computer Science or Math and 7-10 years of directly related experience that includes:

3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.
At least 2 years of experience with Big Data technologies.
At least 2 years of coding experience in data environments.
3+ years design & implementation experience with distributed applications.
2+ years of experience in database architectures and data pipeline development.
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience in manipulating multiple, complex and large data sources.
Experienced in Data Science, statistical models as a plus.
Experience in Designing, implementing and maintaining SQL Server databases.
Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.
Experience in SQL query tuning and optimization.
Experience working in SaaS, IaaS, and PaaS.
In addition to the minimum qualifications above, the successful candidate should have:

Strong working and conceptual knowledge of building and maintaining physical and logical data models.
Strong project management, business writing, communication and presentation skills.
Ability to work cross-functionally within the organization.
Familiarity with or experience working in Agile Scrum software development teams.
Ability to multi-task and maintain flexibility and creativity in a variety of situations.
Ability to analyze and resolve problems.
Ability to set and meet goals and consistently meet deadlines.
Preferred Experience

Specialized Knowledge Requirements

Department:
Information Technology/Gift Management System"," 3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions. At least 2 years of experience with Big Data technologies. At least 2 years of coding experience in data environments. 3+ years design & implementation experience with distributed applications. 2+ years of experience in database architectures and data pipeline development. Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools. Experience in manipulating multiple, complex and large data sources. Experienced in Data Science, statistical models as a plus. Experience in Designing, implementing and maintaining SQL Server databases. Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS. Experience in SQL query tuning and optimization. Experience working in SaaS, IaaS, and PaaS.   Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organizationResponsible for design, development and implementation of optimal solutions to integrate, store, process and analyze huge data setsRecommend and implement strategies for bi-directional synchronization between sourcing data repositories and the central normalized data repositoryBuild data pipeline frameworks to automate high-volume and real-time data deliveryBuild data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partnersWork on multiple projects/tasks simultaneously to meet project deadlines for self and others as required.Establish and maintain positive working relationships with other employeesAll other duties as assigned.  ","3+ years of experience in SQL and developing server objects e.g., store procedures, tables, triggers, views functions. At least 2 with Big Data technologies. coding data environments. design & implementation distributed applications. 2+ database architectures pipeline development. Strong working conceptual knowledge reporting visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools. Experience manipulating multiple, complex large sources. Experienced Science, statistical models a plus. Designing, implementing maintaining Server databases. ETL processes using SSIS. query tuning optimization. SaaS, IaaS, PaaS. Develop sustainable driven solutions current new gen technologies to meet the needs our organizationResponsible for design, development optimal integrate, store, process analyze huge setsRecommend implement strategies bi-directional synchronization between sourcing repositories central normalized repositoryBuild frameworks automate high-volume real-time deliveryBuild APIs delivery services that support critical operational analytical applications internal operations, customers partnersWork on multiple projects/tasks simultaneously project deadlines self others required.Establish maintain positive relationships employeesAll duties assigned.","3+ years experience SQL developing server objects e.g., store procedures, tables, triggers, views functions. At least 2 Big Data technologies. coding data environments. design & implementation distributed applications. 2+ database architectures pipeline development. Strong working conceptual knowledge reporting visualization tools SSRS, PowerBI, Tableau, business intelligence tools. Experience manipulating multiple, complex large sources. Experienced Science, statistical models plus. Designing, implementing maintaining Server databases. ETL processes using SSIS. query tuning optimization. SaaS, IaaS, PaaS. Develop sustainable driven solutions current new gen technologies meet needs organizationResponsible design, development optimal integrate, store, process analyze huge setsRecommend implement strategies bi-directional synchronization sourcing repositories central normalized repositoryBuild frameworks automate high-volume real-time deliveryBuild APIs delivery services support critical operational analytical applications internal operations, customers partnersWork multiple projects/tasks simultaneously project deadlines self others required.Establish maintain positive relationships employeesAll duties assigned."
142,Data Engineer,Data Engineer - Senior Level,"Plano, TX",Plano,TX,"Purpose of Job
We are currently seeking a talented Data Engineer - Senior Level for our Plano facility. Data Engineers are engaged in all phases of the software development lifecycle which include; gathering and analyzing user/business system requirements, responding to outages and creating application system models. Data Engineers primary functions are to design, develop, document, test and debug new and existing software systems and/or applications for internal use, perform defect corrections (analysis, design, code). In addition, Data Engineers participate in design meetings and consult with business clients to refine, test, and debug programs to meet business needs, and interact and sometimes direct third-party partners in the achievement of business and technology initiatives. This is an advanced-level role occupied by those demonstrating in-depth technical and/or business knowledge within their respective areas of specialization. Incumbents work independently on complex work assignments and may determine methods and procedural approaches on new assignments. In addition, responsibilities include serving as resources to internal and third-party team members on escalated technical issues and analyzing and designing specifications for less experienced team members to execute.
Job Requirements
ABOUT USAA
USAA knows what it means to serve. We facilitate the financial security of millions of U.S. military members and their families. This singular mission requires a dedication to innovative thinking at every level.
In each of the past five years, we've been a top-40 Fortune 100 Best Companies to Work ForÂ®, and we've ranked among Victory Media's Top 10 Military FriendlyÂ® Employers primar13 years straight. We embrace a robust veteran workforce and encourage veterans and veteran spouses to apply.
ABOUT USAA IT
Our most important qualification isn't technical, it's human. Here, we don't just sit in front of a screen. We stand behind our 11 million members who rely on us every day.

We are over 3,000 employees strong, a passionately supportive and collaborative team built on Agile principles. We've been a top-two Computerworld 100 Best Places to Work in IT five years in a row and were recently named a Top 50 Employer for Minority Engineers & IT by Workforce Diversity Magazine.

See what it's like to work for a company where your passion meets our purpose:

USAA Information Technology: A Realistic Preview

Utilizing technical expertise; Creates system requirements, performs design and analysis, and coding and unit testing of complex to highly complex system functionality and/or defect correction across multiple platforms.
Identifies ideas to improve system performance and impact availability.
Resolves complex technical design issues.
Provides functional and/or technical guidance in evaluating applications systems or evaluating requests for proposals.
Coordinates changes and influences and prioritizes tasks with business or technical departments.
Analyzes and influences technical, system, and/or user requirements.
Identifies and creates solutions to improve system performance and availability.
Facilitates root cause analysis of system issues to minimize impact and future occurrences.
Creates system documentation/play book(s) and serves as a lead technical reviewer and contributor in requirements, design and code reviews.
Typically serves as a resource to the business.
Develops accurate estimates on work packages and ensures the accuracy of estimates developed by less experienced internal and third-party team members.
Analyzes and designs specifications for less experienced internal and third-party team members to execute.
Acts as a technical resource throughout the development life cycle.
May also actively contribute to the technical and soft skills development of team members.
Assists team leads and management with delegation of technical work packages to cross functional and third-party team members for execution through the full development life cycle.
Keeps management appropriately informed of progress and issues.
Coordinates system application transition from development teams to maintenance and production teams, and/or constructs and implements necessary controls to assure system/application traceability.
MINIMUM REQUIREMENTS
Bachelor's degree or 4 additional years of related experience beyond the minimum required may be substituted in lieu of a degree
6 or more years of software development experience demonstrating depth of technical understanding within Business Intelligence, ETL, and/or Data Engineering
PREFERRED REQUIREMENTS
2 or more years of ETL development in tools like DataStage or Informatica
2 or more years of Java or Python programming experience.
1 or more years of experience building streaming data pipelines (Kafka, NiFi, Spark, and Hadoop)
Advanced in SQL
Advanced with Linux or Unix systems / scripting
Experience predictive analytics implementations
Experience with NoSQL data stores such as Couchbase, Cassandra, HBase
Experience with agile development methodologies
DESIRED CHARACTERISTICS
USAA Data Engineers create innovative solutions that impact our members. Collectively, we are:
Curious and excited by new ideas
Energized by a fast-paced environment
Able to understand and translate business needs into leading-edge technology
Comfortable working as part of a connected team, but self-motivated
Community-focused, dependable and committee
Exceptionally detail-oriented
The above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job.
At USAA our employees enjoy one of the best benefits package in the business, including a flexible business casual or casual dress environment, comprehensive medical, dental and vision plans, along with wellness and wealth building programs. Additionally, our career path planning and continuing education will assist you with your professional goals.
Relocation assistance is available for this position.","     Utilizing technical expertise; Creates system requirements, performs design and analysis, and coding and unit testing of complex to highly complex system functionality and/or defect correction across multiple platforms. Identifies ideas to improve system performance and impact availability. Resolves complex technical design issues. Provides functional and/or technical guidance in evaluating applications systems or evaluating requests for proposals. Coordinates changes and influences and prioritizes tasks with business or technical departments. Analyzes and influences technical, system, and/or user requirements. Identifies and creates solutions to improve system performance and availability. Facilitates root cause analysis of system issues to minimize impact and future occurrences. Creates system documentation/play book s  and serves as a lead technical reviewer and contributor in requirements, design and code reviews. Typically serves as a resource to the business. Develops accurate estimates on work packages and ensures the accuracy of estimates developed by less experienced internal and third-party team members. Analyzes and designs specifications for less experienced internal and third-party team members to execute. Acts as a technical resource throughout the development life cycle. May also actively contribute to the technical and soft skills development of team members. Assists team leads and management with delegation of technical work packages to cross functional and third-party team members for execution through the full development life cycle. Keeps management appropriately informed of progress and issues. Coordinates system application transition from development teams to maintenance and production teams, and/or constructs and implements necessary controls to assure system/application traceability.","Utilizing technical expertise; Creates system requirements, performs design and analysis, coding unit testing of complex to highly functionality and/or defect correction across multiple platforms. Identifies ideas improve performance impact availability. Resolves issues. Provides functional guidance in evaluating applications systems or requests for proposals. Coordinates changes influences prioritizes tasks with business departments. Analyzes technical, system, user requirements. creates solutions Facilitates root cause analysis issues minimize future occurrences. documentation/play book s serves as a lead reviewer contributor code reviews. Typically resource the business. Develops accurate estimates on work packages ensures accuracy developed by less experienced internal third-party team members. designs specifications members execute. Acts throughout development life cycle. May also actively contribute soft skills Assists leads management delegation cross execution through full Keeps appropriately informed progress application transition from teams maintenance production teams, constructs implements necessary controls assure system/application traceability.","Utilizing technical expertise; Creates system requirements, performs design analysis, coding unit testing complex highly functionality and/or defect correction across multiple platforms. Identifies ideas improve performance impact availability. Resolves issues. Provides functional guidance evaluating applications systems requests proposals. Coordinates changes influences prioritizes tasks business departments. Analyzes technical, system, user requirements. creates solutions Facilitates root cause analysis issues minimize future occurrences. documentation/play book serves lead reviewer contributor code reviews. Typically resource business. Develops accurate estimates work packages ensures accuracy developed less experienced internal third-party team members. designs specifications members execute. Acts throughout development life cycle. May also actively contribute soft skills Assists leads management delegation cross execution full Keeps appropriately informed progress application transition teams maintenance production teams, constructs implements necessary controls assure system/application traceability."
143,Data Engineer,Big Data Engineer,"Dallas, TX",Dallas,TX,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.
This role will be responsible for Architecture, Designing and implementing Advanced Analytics capabilities . These capabilities include Batch and Streaming Analytics, Machine learning models, Natural Language processing and Natural language generation and other emerging technologies in the field of Advanced Analytics.

Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale
Experience working in Cloud-based Big Data Infrastructure eg: Azure
Good working experience on Cloud, Delta Lake, ETL processing.
Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc.
Working knowledge on Python and PySpark Programming.
Working with a wide range of data sources like (DB2, SAP HANA etc) and intermediate expertise in SQL and PL/SQL(optional)
Ability to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners.
Work in a highly agile environment
Excellent communication and teamwork skills.
Knowledge on Data Governance & Security Principles
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.","     Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure eg  Azure Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming. Working with a wide range of data sources like  DB2, SAP HANA etc  and intermediate expertise in SQL and PL/SQL optional  Ability to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners. Work in a highly agile environment Excellent communication and teamwork skills. Knowledge on Data Governance & Security Principles ","Experience in developing, deploying and operating on large scale distributed systems a commercial working Cloud-based Big Data Infrastructure eg Azure Good experience Cloud, Delta Lake, ETL processing. technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge Python PySpark Programming. with wide range of data sources DB2, SAP HANA etc intermediate expertise SQL PL/SQL optional Ability to work global team, playing key role communicating problem context the remote teams, stake holders product owners. Work highly agile environment Excellent communication teamwork skills. Knowledge Governance & Security Principles","Experience developing, deploying operating large scale distributed systems commercial working Cloud-based Big Data Infrastructure eg Azure Good experience Cloud, Delta Lake, ETL processing. technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge Python PySpark Programming. wide range data sources DB2, SAP HANA etc intermediate expertise SQL PL/SQL optional Ability work global team, playing key role communicating problem context remote teams, stake holders product owners. Work highly agile environment Excellent communication teamwork skills. Knowledge Governance & Security Principles"
144,Data Engineer,AWS Data Engineer,"Dallas, TX",Dallas,TX,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
145,Data Engineer,Senior Data Engineer,"Dallas, TX",Dallas,TX,"Summary:

You have experience with client projects and in handling vast amounts of data â working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Data Science team. Lead on projects from a data engineering perspective, working with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches
Plan and execute secure, good practice data integration strategies and approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Create and manage data environments in the Cloud
Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models
Have a strong understanding of Information Security principles to ensure compliant handling and management of client data
This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science
Qualifications:
Commercial experience leading on client-facing projects, including working in close-knit teams
5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)
5+ years of experience working on projects within the cloud ideally AWS or Azure
5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent
Experience with open source tools like Apache Airflow and Griffin
Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform
Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift
Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models
Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma
Experience building automated data quality and testing into data pipelines
Experience with AI, NLP, Machine Learning, etc. is a plus
Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R
Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time
Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.
A deep personal motivation to always produce outstanding work for your clients and colleagues
Excel in team collaboration and working with others from diverse skill-sets and backgrounds
Cervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.

7E9OZvBK9m"," Commercial experience leading on client-facing projects, including working in close-knit teams 5+ years of experience and interest in Big Data technologies  Hadoop / Spark / NoSQL DBs  5+ years of experience working on projects within the cloud ideally AWS or Azure 5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent Experience with open source tools like Apache Airflow and Griffin Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma Experience building automated data quality and testing into data pipelines Experience with AI, NLP, Machine Learning, etc. is a plus Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C , R Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner. A deep personal motivation to always produce outstanding work for your clients and colleagues Excel in team collaboration and working with others from diverse skill-sets and backgrounds    ","Commercial experience leading on client-facing projects, including working in close-knit teams 5+ years of and interest Big Data technologies Hadoop / Spark NoSQL DBs projects within the cloud ideally AWS or Azure with streaming architectures patterns like Kafka, Kinesis, Flink, Confluent Experience open source tools Apache Airflow Griffin DevOps DataOps Jenkins, Kubernetes, Docker, Terraform Warehousing products Snowflake, DW, Redshift building operational ETL data pipelines across a number sources, constructing relational dimensional models one more ETL/ELT Talend, Matillion, FiveTran, Alooma automated quality testing into AI, NLP, Machine Learning, etc. is plus Strong development background at least two scripting, object oriented functional programming language, SQL, Python, Java, Scala, C , R lively consulting setting, often different multiple same time Excellent interpersonal skills when interacting clients clear, timely, professional manner. A deep personal motivation to always produce outstanding work for your colleagues Excel team collaboration others from diverse skill-sets backgrounds","Commercial experience leading client-facing projects, including working close-knit teams 5+ years interest Big Data technologies Hadoop / Spark NoSQL DBs projects within cloud ideally AWS Azure streaming architectures patterns like Kafka, Kinesis, Flink, Confluent Experience open source tools Apache Airflow Griffin DevOps DataOps Jenkins, Kubernetes, Docker, Terraform Warehousing products Snowflake, DW, Redshift building operational ETL data pipelines across number sources, constructing relational dimensional models one ETL/ELT Talend, Matillion, FiveTran, Alooma automated quality testing AI, NLP, Machine Learning, etc. plus Strong development background least two scripting, object oriented functional programming language, SQL, Python, Java, Scala, C , R lively consulting setting, often different multiple time Excellent interpersonal skills interacting clients clear, timely, professional manner. A deep personal motivation always produce outstanding work colleagues Excel team collaboration others diverse skill-sets backgrounds"
146,Data Engineer,Data Engineer 1,"Dallas, TX 75201",Dallas,TX,"Cvent is an exciting, fast-growing tech company that provides industry-leading software to event professionals around the world. Our suite of services â online event registration, venue selection, mobile apps, email marketing, web surveys, and targeted hotel advertising opportunities â have positioned us a major player in the estimated $565 billion global meetings and events industry.

Essential Duties and Responsibilities

Design and implement database structures for OLAP and OLTP systems
Design and implement ETL and ELT process to consolidate data across multiple systems
Design and implement APIs, services, data transfers to internal and external systems
Identify and resolve performance and security issues relating to data access and maintenance
Define and enforce data design, security and performance standards
Understand and contribute to a corporate data model and overall data governance
Communicate with application, back-office and external customer teams regarding data requirements, standards, performance and access
Define and follow best practices for a full software development lifecycle involving data and database code.
Define and perform unit testing of database code
Contribute to the analysis and remediation of system behavior using tools like New Relic to understand application and process bottlenecks
Perform code reviews and audits of application teamâs database code to ensure compliance with established best practices.
Contribute to new technology evaluations and recommended usage
Provide on call support for database related issues affecting system or process availability.


Job Requirements

Bachelors degree in Computer Science, CIS or related field
4+ yearsâ experience with multiple databases (SQL Server, Oracle, Postgres, NoSQL)
Experience with ETL, ELT, Replication (SSIS, Informatica, GoldenGate)
Experience with Data Marts, Data Warehouses, Data Lakes
Experience with one or more reporting tools (Birst, Cognos, Business Objects)
Experience with Amazon, RedShift, Cloud


Key Skills & Competencies

Strong Database Development skills across multiple platforms
Superior Interpersonal Skills: Ability to interface with a wide range of personalities and levels within Cvent and client organizations; Professional communication style
Data Collection and Analysis: Proactive listening; resourceful in collecting sufficient data; Analysis of data to develop and implement best solution
Initiative: Self-starter with strong sense of ownership; Tenacity in problem solving with positive outcomes; Motivated to increase capacity and responsibility
Detailed Oriented: Detailed administrative skills for tracking and reporting","  Strong Database Development skills across multiple platforms Superior Interpersonal Skills  Ability to interface with a wide range of personalities and levels within Cvent and client organizations; Professional communication style Data Collection and Analysis  Proactive listening; resourceful in collecting sufficient data; Analysis of data to develop and implement best solution Initiative  Self-starter with strong sense of ownership; Tenacity in problem solving with positive outcomes; Motivated to increase capacity and responsibility Detailed Oriented  Detailed administrative skills for tracking and reporting  Design and implement database structures for OLAP and OLTP systems Design and implement ETL and ELT process to consolidate data across multiple systems Design and implement APIs, services, data transfers to internal and external systems Identify and resolve performance and security issues relating to data access and maintenance Define and enforce data design, security and performance standards Understand and contribute to a corporate data model and overall data governance Communicate with application, back-office and external customer teams regarding data requirements, standards, performance and access Define and follow best practices for a full software development lifecycle involving data and database code. Define and perform unit testing of database code Contribute to the analysis and remediation of system behavior using tools like New Relic to understand application and process bottlenecks Perform code reviews and audits of application teamâs database code to ensure compliance with established best practices. Contribute to new technology evaluations and recommended usage Provide on call support for database related issues affecting system or process availability.    Bachelors degree in Computer Science, CIS or related field 4+ yearsâ experience with multiple databases  SQL Server, Oracle, Postgres, NoSQL  Experience with ETL, ELT, Replication  SSIS, Informatica, GoldenGate  Experience with Data Marts, Data Warehouses, Data Lakes Experience with one or more reporting tools  Birst, Cognos, Business Objects  Experience with Amazon, RedShift, Cloud ","Strong Database Development skills across multiple platforms Superior Interpersonal Skills Ability to interface with a wide range of personalities and levels within Cvent client organizations; Professional communication style Data Collection Analysis Proactive listening; resourceful in collecting sufficient data; data develop implement best solution Initiative Self-starter strong sense ownership; Tenacity problem solving positive outcomes; Motivated increase capacity responsibility Detailed Oriented administrative for tracking reporting Design database structures OLAP OLTP systems ETL ELT process consolidate APIs, services, transfers internal external Identify resolve performance security issues relating access maintenance Define enforce design, standards Understand contribute corporate model overall governance Communicate application, back-office customer teams regarding requirements, standards, follow practices full software development lifecycle involving code. perform unit testing code Contribute the analysis remediation system behavior using tools like New Relic understand application bottlenecks Perform reviews audits teamâs ensure compliance established practices. new technology evaluations recommended usage Provide on call support related affecting or availability. Bachelors degree Computer Science, CIS field 4+ yearsâ experience databases SQL Server, Oracle, Postgres, NoSQL Experience ETL, ELT, Replication SSIS, Informatica, GoldenGate Marts, Warehouses, Lakes one more Birst, Cognos, Business Objects Amazon, RedShift, Cloud","Strong Database Development skills across multiple platforms Superior Interpersonal Skills Ability interface wide range personalities levels within Cvent client organizations; Professional communication style Data Collection Analysis Proactive listening; resourceful collecting sufficient data; data develop implement best solution Initiative Self-starter strong sense ownership; Tenacity problem solving positive outcomes; Motivated increase capacity responsibility Detailed Oriented administrative tracking reporting Design database structures OLAP OLTP systems ETL ELT process consolidate APIs, services, transfers internal external Identify resolve performance security issues relating access maintenance Define enforce design, standards Understand contribute corporate model overall governance Communicate application, back-office customer teams regarding requirements, standards, follow practices full software development lifecycle involving code. perform unit testing code Contribute analysis remediation system behavior using tools like New Relic understand application bottlenecks Perform reviews audits teamâs ensure compliance established practices. new technology evaluations recommended usage Provide call support related affecting availability. Bachelors degree Computer Science, CIS field 4+ yearsâ experience databases SQL Server, Oracle, Postgres, NoSQL Experience ETL, ELT, Replication SSIS, Informatica, GoldenGate Marts, Warehouses, Lakes one Birst, Cognos, Business Objects Amazon, RedShift, Cloud"
147,Data Engineer,Data Engineer,"Dallas, TX",Dallas,TX,"Pieces Tech is currently in need of a Data Engineer to be responsible for building and scaling the next generation of the Pieces data engineering solutions to support our fast-growing artificial intelligent (AI) and analytics business in healthcare.

More specifically, this role will leverage the state-of-the-art data engineering technologies to develop and evolve the Pieces AI engine that delivers machine learning, natural language processing (NLP), and other AI and analytics based solutions in a highly scalable fashion. It involves the development of the following infrastructure and systems:

AI data engineering infrastructure to efficiently support predictive modeling and analytics tasks on healthcare big data
Analytics and Clinician-in-the-loop systems for AI-assisted chart review, rapid model development, explanatory model development, and model quality assurance (QA)
This role is also responsible for exploratory work, improving and innovating, while focusing on delivering the required applications of relevance to Pieces Techâs goals.

Additional roles and responsibilities include the following:

Develop and maintain data solution architecture (e.g. data ingestion modules, AI pipelines, ETL, data models, etc) in and around the Pieces AI engine (including NLP engine)
Develop and maintain distributed computing infrastructures within and around Pieces AI engine that are horizontally scalable (e.g. Docker and Kubernetes)
Develop and maintain relational databases (e.g. MySQL), distributed databases (e.g. Elasticsearch), and their data models that support both analytics and production pipelines
Develop and maintain an efficient and reliable deployment process across all clients for both analytical model deployment and functional module deployment
Implement monitoring and QA layers for all existing and new components within and around Pieces AI engine
Collaborate with front-end developers, back-end developers, AI architect, and data scientists to build end-to-end AI systems
Support general data analytics to answer business questions for product ideation, product development, and client engagement
 Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.


Some of the benefits that the successful candidate can expect are below:


Opportunity to work remotely
A knowledgeable, high-achieving, experienced and fun team
The chance to be part of a rapidly growing startup and next success story
A competitive base salary
Structured career development



Required Skills
Ability to self-manage, work independently and meet deadlines
Critical thinking skills
Willingness to learn
Teamwork mentality and skills
Strong communication, interpersonal and analytical skills
Team-based communication and project management tools (Google Doc, SmartSheet, JIRA, Confluence, Bitbuckets, etc)
Proficient with Linux and MacOS operating system
Proficient with programming languages including Python, Java, and SQL


Required Experience
Required Education

Bachelor in Computer Science, Data Science, Informatics, Engineering, or a related field.


Preferred Education

Master, or PhD degree in Computer Science, Data Science, Informatics, Engineering or equivalent.


Required Experiences


At least 3-year experience in full-stack software engineering
Proven experience in data modeling in relational databases (e.g. MySQL, PostgreSQL, Snowflake, etc) and/or NoSQL databases (e.g. Elasticsearch, MongoDB, Columnar databases, etc)
Proven experience with web service development using Spring (Spring Boot, Spring Data, Spring security, etc), REST, and JSON.
Proven experience in developing container-based system (e.g. Docker and Kubernetes)


Preferred Experiences

Experience in building distributed systems (e.g. MapReduce, Hadoop, or Spark)
Experience in front-end development (e.g. AngularJS, JavaScript, HTML5, etc)
Experience in developing systems on healthcare data (e.g. Epic, Meditech, Cerner, Allscripts, etc)
Experience in machine learning, deep learning, active learning, and/or transfer learning to solve practical AI problems in a primary or supporting role
Experience in developing NLP system","    Bachelor in Computer Science, Data Science, Informatics, Engineering, or a related field.  ","Bachelor in Computer Science, Data Informatics, Engineering, or a related field.","Bachelor Computer Science, Data Informatics, Engineering, related field."
148,Data Engineer,Data Integration Engineer,"Arlington, TX 76016",Arlington,TX,"Currently seeking Data Integration Engineers located in or near Arlington, TX. As a data engineer, you'll be handling the design and construction of scalable systems, ensure that all data systems meet company requirements, and also research new uses for data acquisition. You should also know the ins and outs of the industry such as data mining practices, algorithms, and how data can be used.

Required Skills
Proficient with interviewing and gathering business requirements, definition and design of data source and data flow, data quality analysis, and working with the data architect on the development of logical data models. Proficient using Infosphere/DataStage or equivalent ETL software.
Proficient with relational databases and using SQL to query, create tables, views, indexes, joins
Proficient using Unix and applicable scripting/scheduling tools
Experience with the SDLC, ITSM and privacy and security concepts.

Qualifications
You can consume and utilize new languages, design patterns, APIs and toolsets.
You can work in a fast-paced and collaborative environment.
Effective communication skills and a willingness to learn are a must.
Experience with Test-Driven Development and writing unit and integration tests.
Experience using a Behavior-Driven Development suite like Cucumber.
Competent writing software with JavaScript ecosystems like React.
Comfortable working in a cloud environment like AWS.
Must have experience basic Linux/Unix CLI and using Git and GitHub for source code control.
Knowledge of Continuous Integration/Continuous Delivery systems like Jenkins.
Knowledge of Docker and Kubernetes is a plus.
Exhibits enthusiasm and well-rounded knowledge of backend systems and software architecture.
Applies best practices including design patterns and linting to all software development.
Approaches engineering requests from a user's vantage point to form architectural and technical requirements.
Stays well-informed of emerging technologies and software trends.
Capable of debugging problems related to HTTP, XHR, JSON, CORS, SSL, S3, etc.
Able to investigate performance and memory issues.
Able to reduce complex requirements and user interaction flows into long-term API designs.
Good understanding of architectural messaging patterns and pitfalls using Kafka, Rabbit MQ, etc.
Endeavors to establish positive relationships, both inter-departmentally, and cross-functionally.

Requirements
3 Years Working as a Data Integration Engineer or Data Integration Production Support Engineer with a Bachelor's degree is required.
Experience working in a healthcare or related field
Teradata Database Experience
Experience upgrading IBM Infosphere Tools

Education
Bachelor's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology required upon hire
Master's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology preferred
3xW99DwBQZ"," You can consume and utilize new languages, design patterns, APIs and toolsets. You can work in a fast-paced and collaborative environment. Effective communication skills and a willingness to learn are a must. Experience with Test-Driven Development and writing unit and integration tests. Experience using a Behavior-Driven Development suite like Cucumber. Competent writing software with JavaScript ecosystems like React. Comfortable working in a cloud environment like AWS. Must have experience basic Linux/Unix CLI and using Git and GitHub for source code control. Knowledge of Continuous Integration/Continuous Delivery systems like Jenkins. Knowledge of Docker and Kubernetes is a plus. Exhibits enthusiasm and well-rounded knowledge of backend systems and software architecture. Applies best practices including design patterns and linting to all software development. Approaches engineering requests from a user's vantage point to form architectural and technical requirements. Stays well-informed of emerging technologies and software trends. Capable of debugging problems related to HTTP, XHR, JSON, CORS, SSL, S3, etc. Able to investigate performance and memory issues. Able to reduce complex requirements and user interaction flows into long-term API designs. Good understanding of architectural messaging patterns and pitfalls using Kafka, Rabbit MQ, etc. Endeavors to establish positive relationships, both inter-departmentally, and cross-functionally.  Proficient with interviewing and gathering business requirements, definition and design of data source and data flow, data quality analysis, and working with the data architect on the development of logical data models. Proficient using Infosphere/DataStage or equivalent ETL software. Proficient with relational databases and using SQL to query, create tables, views, indexes, joins Proficient using Unix and applicable scripting/scheduling tools Experience with the SDLC, ITSM and privacy and security concepts.    Bachelor's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology required upon hire Master's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology preferred  3 Years Working as a Data Integration Engineer or Data Integration Production Support Engineer with a Bachelor's degree is required. Experience working in a healthcare or related field Teradata Database Experience Experience upgrading IBM Infosphere Tools","You can consume and utilize new languages, design patterns, APIs toolsets. work in a fast-paced collaborative environment. Effective communication skills willingness to learn are must. Experience with Test-Driven Development writing unit integration tests. using Behavior-Driven suite like Cucumber. Competent software JavaScript ecosystems React. Comfortable working cloud environment AWS. Must have experience basic Linux/Unix CLI Git GitHub for source code control. Knowledge of Continuous Integration/Continuous Delivery systems Jenkins. Docker Kubernetes is plus. Exhibits enthusiasm well-rounded knowledge backend architecture. Applies best practices including patterns linting all development. Approaches engineering requests from user's vantage point form architectural technical requirements. Stays well-informed emerging technologies trends. Capable debugging problems related HTTP, XHR, JSON, CORS, SSL, S3, etc. Able investigate performance memory issues. reduce complex requirements user interaction flows into long-term API designs. Good understanding messaging pitfalls Kafka, Rabbit MQ, Endeavors establish positive relationships, both inter-departmentally, cross-functionally. Proficient interviewing gathering business requirements, definition data flow, quality analysis, the architect on development logical models. Infosphere/DataStage or equivalent ETL software. relational databases SQL query, create tables, views, indexes, joins Unix applicable scripting/scheduling tools SDLC, ITSM privacy security concepts. Bachelor's Degree Business Administration, Information Science, Computer Engineering, Technology required upon hire Master's preferred 3 Years Working as Data Integration Engineer Production Support degree required. healthcare field Teradata Database upgrading IBM Infosphere Tools","You consume utilize new languages, design patterns, APIs toolsets. work fast-paced collaborative environment. Effective communication skills willingness learn must. Experience Test-Driven Development writing unit integration tests. using Behavior-Driven suite like Cucumber. Competent software JavaScript ecosystems React. Comfortable working cloud environment AWS. Must experience basic Linux/Unix CLI Git GitHub source code control. Knowledge Continuous Integration/Continuous Delivery systems Jenkins. Docker Kubernetes plus. Exhibits enthusiasm well-rounded knowledge backend architecture. Applies best practices including patterns linting development. Approaches engineering requests user's vantage point form architectural technical requirements. Stays well-informed emerging technologies trends. Capable debugging problems related HTTP, XHR, JSON, CORS, SSL, S3, etc. Able investigate performance memory issues. reduce complex requirements user interaction flows long-term API designs. Good understanding messaging pitfalls Kafka, Rabbit MQ, Endeavors establish positive relationships, inter-departmentally, cross-functionally. Proficient interviewing gathering business requirements, definition data flow, quality analysis, architect development logical models. Infosphere/DataStage equivalent ETL software. relational databases SQL query, create tables, views, indexes, joins Unix applicable scripting/scheduling tools SDLC, ITSM privacy security concepts. Bachelor's Degree Business Administration, Information Science, Computer Engineering, Technology required upon hire Master's preferred 3 Years Working Data Integration Engineer Production Support degree required. healthcare field Teradata Database upgrading IBM Infosphere Tools"
149,Data Engineer,"Senior Data Engineer, Products","Farmers Branch, TX 75244",Farmers Branch,TX,"Job Description

We are seeking a highly motivated, and technically proficient Data Engineer to work on architecting, designing, building and managing data products on ""big data"" infrastructure.
You will expand upon your current skill set through cross-disciplinary collaboration with some of the smartest and nicest people in the industry, while learning the inner workings of a fast-paced global performance marketing agency. You will work closely with a team of engineers in an agile development environment to expand our proprietary marketing products.
Additionally, as a Data Engineer, youâll play an integral role in the growth of our team. You will assist in reviewing complicated and mission-critical designs, code, and tests; and documenting ways to improve our current codebase and system processes using the latest technologies.
Key ResponsibilitiesLead a technical team to rapidly architect, design, prototype, and implement and optimize architectures to tackle the Big Data and Data Science problems.Design, maintain and oversee the operational process to develop modular code base to solve ârealâ world problems.Conduct regular peer code reviews to ensure code quality and compliance following best practices in the industry.Work in cross-disciplinary teams to understand client needs and ingest rich data sources.Research, experiment, and utilize leading Big Data methodologies (Hadoop, Spark, Kafka, Netezza, Snowflake, and AWS) with cloud/on premise/hybrid hosting solutions.Oversee a technical team to provide proficient documentation and operating guidance for users of all levels.Lead a technical team to architect, implement, and test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings (AWS, Azure, client technology stacks, and on-prem clusters)Translate advanced business analytics problems into technical approaches that yield actionable recommendations across multiple, diverse domains; Communicate results and educate others through design and build of insightful visualizations, reports, and presentations.Develop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management and client relationship developmentHelp drive the process for pursuing innovations, target solutions, and extendable platforms for Merkleâs products.Participate in developing and presenting thought leadership, and assist in ensuring that Merkleâs âdata sourceâ technology stack incorporates and is optimized for using specific technologies.Promote the Merkle brand in the broader âdata sourceâ community.

Qualifications

Qualified individuals possess the Merkle attributes of being smart, curious, committed to vision, passionate, fun/pleasant, an achiever and having a sense of urgencyMinimum of ten years of big data experience with multiple programming languages and technologies, three years as a lead and three years at a management level with minimum five years of big data experience.Bachelor's degree or Master's degree from an accredited college/university in Computer Science, Computer Engineering, or related field(i.e. math and physics);Ability to manage complex engagements and interface with senior level management internally as well as with clients.Ability to communicate complex technical concepts succinctly to non-technical colleagues, understand & manage interdependencies between all facets of a project.Ability to lead client presentations; Must have demonstrated advanced proficiency in complex, mature and sophisticated Design & Analysis technologies and solutions.Ability to mentor others and publish whitepapers or articles on complex D&A technologies or solutions.Market-leading proficiency with multiple large scale and/or distributed processing methodologies (Hadoop, Storm, Spark).Skilled ability to rapidly ingest, transform, engineer, and visualize data, both for ad hoc and product-level (e.g., automated) data & analytics solutions.Market-leading fluency in several programming languages (Python, Scala, or Java), with the ability to pick up new languages and technologies quickly.Understanding of cloud and distributed systems principles (such as load balancing, networks, scaling, in-memory vs. disk).Experience with large-scale, big data methods (MapReduce, Hadoop, Spark, Hive, Impala, or Storm, SnowFlake) and AWS solutions (EC2, S3, RDS, EMR, Kinesis, DynamoDB, Redshift).Experience storing, managing, and processing massive data sets using tools such as Hadoop, Apache Spark, AWS EMR, Hive, SnowFlake or the like.Ability to work efficiently under Unix/Linux environment and .NET, having experience with source code management systems like GIT and SVN.Strong knowledge with programming methodologies (version control, testing, QA) and development methodologies (Waterfall and Agile).Experience with object-oriented design, coding, and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures.Familiarity with different architecture patterns of development such as Event Driven, SOA, micro services, functional programming, Lambda.Capability to architect highly scalable distributed systems, using different open source tools.Knowledge of traditional and digital data-driven marketing.
Additional Information

All your information will be kept confidential according to EEO guidelines. At Merkle, we believe that a diverse environment improves us as a community and as a business. We want to foster an environment of growth, where all ideas and contributions are encouraged. We need this culture of courage to continue to thrive in our fast-paced industry. We embrace differences of opinion. We value diversity of experience and thought, which help us to challenge and define industry-leading solutions, and support our goal of being a great place to work.","  Lead a technical team to rapidly architect, design, prototype, and implement and optimize architectures to tackle the Big Data and Data Science problems.Design, maintain and oversee the operational process to develop modular code base to solve ârealâ world problems.Conduct regular peer code reviews to ensure code quality and compliance following best practices in the industry.Work in cross-disciplinary teams to understand client needs and ingest rich data sources.Research, experiment, and utilize leading Big Data methodologies  Hadoop, Spark, Kafka, Netezza, Snowflake, and AWS  with cloud/on premise/hybrid hosting solutions.Oversee a technical team to provide proficient documentation and operating guidance for users of all levels.Lead a technical team to architect, implement, and test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings  AWS, Azure, client technology stacks, and on-prem clusters Translate advanced business analytics problems into technical approaches that yield actionable recommendations across multiple, diverse domains; Communicate results and educate others through design and build of insightful visualizations, reports, and presentations.Develop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management and client relationship developmentHelp drive the process for pursuing innovations, target solutions, and extendable platforms for Merkleâs products.Participate in developing and presenting thought leadership, and assist in ensuring that Merkleâs âdata sourceâ technology stack incorporates and is optimized for using specific technologies.Promote the Merkle brand in the broader âdata sourceâ community.  ","Lead a technical team to rapidly architect, design, prototype, and implement optimize architectures tackle the Big Data Science problems.Design, maintain oversee operational process develop modular code base solve ârealâ world problems.Conduct regular peer reviews ensure quality compliance following best practices in industry.Work cross-disciplinary teams understand client needs ingest rich data sources.Research, experiment, utilize leading methodologies Hadoop, Spark, Kafka, Netezza, Snowflake, AWS with cloud/on premise/hybrid hosting solutions.Oversee provide proficient documentation operating guidance for users of all levels.Lead implement, test processing pipelines, mining/data science algorithms on variety hosted settings AWS, Azure, technology stacks, on-prem clusters Translate advanced business analytics problems into approaches that yield actionable recommendations across multiple, diverse domains; Communicate results educate others through design build insightful visualizations, reports, presentations.Develop skills requirement capture translation, hypothesis-driven consulting, work stream project management relationship developmentHelp drive pursuing innovations, target solutions, extendable platforms Merkleâs products.Participate developing presenting thought leadership, assist ensuring âdata sourceâ stack incorporates is optimized using specific technologies.Promote Merkle brand broader community.","Lead technical team rapidly architect, design, prototype, implement optimize architectures tackle Big Data Science problems.Design, maintain oversee operational process develop modular code base solve ârealâ world problems.Conduct regular peer reviews ensure quality compliance following best practices industry.Work cross-disciplinary teams understand client needs ingest rich data sources.Research, experiment, utilize leading methodologies Hadoop, Spark, Kafka, Netezza, Snowflake, AWS cloud/on premise/hybrid hosting solutions.Oversee provide proficient documentation operating guidance users levels.Lead implement, test processing pipelines, mining/data science algorithms variety hosted settings AWS, Azure, technology stacks, on-prem clusters Translate advanced business analytics problems approaches yield actionable recommendations across multiple, diverse domains; Communicate results educate others design build insightful visualizations, reports, presentations.Develop skills requirement capture translation, hypothesis-driven consulting, work stream project management relationship developmentHelp drive pursuing innovations, target solutions, extendable platforms Merkleâs products.Participate developing presenting thought leadership, assist ensuring âdata sourceâ stack incorporates optimized using specific technologies.Promote Merkle brand broader community."
150,Data Engineer,Data Engineer,"Dallas, TX 75244",Dallas,TX,"Basic Function:

Komen is seeking a Data Engineer who lives and breathes data, sweats the details, deeply cares about data quality, data flows, integration, ETL, storage & performance. The Data Engineer will be creating data pipelines and process data sets that are available within Komen covering constituents, campaigns, research and patient domains.

Primary Responsibilities:

Building and maintaining data processing workflows feeding our analytics, CRM and various other internal applications

Ã¢â¬Â¢ Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organization
Ã¢â¬Â¢ Responsible for design, development and implementation of optimal solutions to integrate, store, process and analyze huge data sets
Ã¢â¬Â¢ Recommend and implement strategies for bi-directional synchronization between sourcing data repositories and the central normalized data repository
Ã¢â¬Â¢ Build data pipeline frameworks to automate high-volume and real-time data delivery
Ã¢â¬Â¢ Build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners
Ã¢â¬Â¢ Work on multiple projects/tasks simultaneously to meet project deadlines for self and others as required.
Ã¢â¬Â¢ Establish and maintain positive working relationships with other employees
Ã¢â¬Â¢ All other duties as assigned.

Position Qualifications:

The ideal candidate will have a Bachelor's Degree in Computer Science or Math and 7-10 years of directly related experience that includes:
3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.
At least 2 years of experience with Big Data technologies.
At least 2 years of coding experience in data environments.
3+ years design & implementation experience with distributed applications.
2+ years of experience in database architectures and data pipeline development.
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience in manipulating multiple, complex and large data sources.
Experienced in Data Science, statistical models as a plus.
Experience in Designing, implementing and maintaining SQL Server databases.
Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.
Experience in SQL query tuning and optimization.
Experience working in SaaS, IaaS, and PaaS.

In addition to the minimum qualifications above, the successful candidate should have:
Strong working and conceptual knowledge of building and maintaining physical and logical data models.
Strong project management, business writing, communication and presentation skills.
Ability to work cross-functionally within the organization.
Familiarity with or experience working in Agile Scrum software development teams.
Ability to multi-task and maintain flexibility and creativity in a variety of situations.
Ability to analyze and resolve problems.
Ability to set and meet goals and consistently meet deadlines.","3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions. At least 2 years of experience with Big Data technologies. At least 2 years of coding experience in data environments. 3+ years design & implementation experience with distributed applications. 2+ years of experience in database architectures and data pipeline development. Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools. Experience in manipulating multiple, complex and large data sources. Experienced in Data Science, statistical models as a plus. Experience in Designing, implementing and maintaining SQL Server databases. Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS. Experience in SQL query tuning and optimization. Experience working in SaaS, IaaS, and PaaS.    3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions. At least 2 years of experience with Big Data technologies. At least 2 years of coding experience in data environments. 3+ years design & implementation experience with distributed applications. 2+ years of experience in database architectures and data pipeline development. Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools. Experience in manipulating multiple, complex and large data sources. Experienced in Data Science, statistical models as a plus. Experience in Designing, implementing and maintaining SQL Server databases. Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS. Experience in SQL query tuning and optimization. Experience working in SaaS, IaaS, and PaaS.    ","3+ years of experience in SQL and developing server objects e.g., store procedures, tables, triggers, views functions. At least 2 with Big Data technologies. coding data environments. design & implementation distributed applications. 2+ database architectures pipeline development. Strong working conceptual knowledge reporting visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools. Experience manipulating multiple, complex large sources. Experienced Science, statistical models a plus. Designing, implementing maintaining Server databases. ETL processes using SSIS. query tuning optimization. SaaS, IaaS, PaaS.","3+ years experience SQL developing server objects e.g., store procedures, tables, triggers, views functions. At least 2 Big Data technologies. coding data environments. design & implementation distributed applications. 2+ database architectures pipeline development. Strong working conceptual knowledge reporting visualization tools SSRS, PowerBI, Tableau, business intelligence tools. Experience manipulating multiple, complex large sources. Experienced Science, statistical models plus. Designing, implementing maintaining Server databases. ETL processes using SSIS. query tuning optimization. SaaS, IaaS, PaaS."
151,Data Engineer,Azure Data Engineer,"Dallas, TX",Dallas,TX,"Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, âas isâ and âto beâ scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ","At least 5 years of consulting or client service delivery experience on Azure DevOps an platform Proven ability to build, manage and foster a team-oriented environment","At least 5 years consulting client service delivery experience Azure DevOps platform Proven ability build, manage foster team-oriented environment"
152,Data Engineer,"Sr Professional, Data Engineer","Irving, TX",Irving,TX,"Job Summary
Join the team that powers the global real estate economy - CoreLogic is an innovative, future focused company whose vision is to deliver unique property-level insights that power the global real estate economy.
We are a $1.95 billion in sales company with more than 6,000 employees globally serving the financial services and insurance industries. We are evolving at a rapid pace and the clients we serve are challenged from every direction, which means we are growing and innovating to help drive their success. Working together, and differentiated by our superior data, analytics and data-enabled solutions, we empower our clients to make smarter business decisions through data-driven insights. We take initiative, are fully accountable, build respect and trust, make transparency a mustâand engage, include and collaborate at every turn.
We take pride in our work and believe in cultivating a work environment that supports and values our greatest asset: our talented employees.
Job Description:
Job Duties
Devises/modifies procedures to solve problems considering computer equipment capacity and limitations, operating time, and desired results for a specific data process. Designs, codes, tests, debugs, and documents those programs.
Prepare detailed specifications from which programs will be written, designed, coded, tested and debugged.
Consult with users and develop business relationships and integrate activities with other departments to ensure successful implementation.
May lead small projects or regularly coach other team members to ensure data processes and systems are developed in a way that complies with data architecture, standards and internally established methodologies and practices applicable to Data Operations.
Monitor and report to management on the status of project efforts, anticipating/identifying issues that inhibit the attainment of project goals and implementing corrective actions.
Develop business relationships and integrate activities with other departments to ensure successful implementation and support project efforts. Foster and maintain good relationships with customers and colleagues to meet expected customer service levels.
Job Qualifications:
Education, Experience, Knowledge and Skills
BS Degree or equivalent work experience
Formal training and /or certifications in programming languages a plus
Typically have 4-8 years of directly related experience.
Experience with data used in CoreLogic products and processes (public record, tax, census, appraisal, etc.).
Fluent in multiple program languages at an expert level
Strong experience with various computer platforms and application environments
Expertise with developing multiple tiers of multi-tiered software applications
Expertise designing programs and data systems
Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team
Project management skills
Experience with ETL methodologies
Strong understanding of data structures and design
Experience with data flow, data enrichment, transformations
Ability to optimize database queries and performance
Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation
Experience in developing data service processes and system infrastructure to be used Enterprise Wide
Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.
CoreLogic offers an empowered work environment that encourages creativity, initiative and professional growth and provides a competitive salary and benefits package. CoreLogic is an Equal Opportunity/Affirmative Action employer committed to attracting and retaining the best-qualified people available, without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, disability or status as a veteran of the Armed Forces, or any other basis protected by federal, state or local law. CoreLogic maintains a Drug-Free Workplace. We are fully committed to employing a diverse workforce and creating an inclusive work environment that embraces everyoneâs unique contributions, experiences and values. Please apply on our website for consideration.","BS Degree or equivalent work experience Formal training and /or certifications in programming languages a plus Typically have 4-8 years of directly related experience. Experience with data used in CoreLogic products and processes  public record, tax, census, appraisal, etc. . Fluent in multiple program languages at an expert level Strong experience with various computer platforms and application environments Expertise with developing multiple tiers of multi-tiered software applications Expertise designing programs and data systems Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team Project management skills Experience with ETL methodologies Strong understanding of data structures and design Experience with data flow, data enrichment, transformations Ability to optimize database queries and performance Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation Experience in developing data service processes and system infrastructure to be used Enterprise Wide Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.  BS Degree or equivalent work experience Formal training and /or certifications in programming languages a plus Typically have 4-8 years of directly related experience. Experience with data used in CoreLogic products and processes  public record, tax, census, appraisal, etc. . Fluent in multiple program languages at an expert level Strong experience with various computer platforms and application environments Expertise with developing multiple tiers of multi-tiered software applications Expertise designing programs and data systems Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team Project management skills Experience with ETL methodologies Strong understanding of data structures and design Experience with data flow, data enrichment, transformations Ability to optimize database queries and performance Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation Experience in developing data service processes and system infrastructure to be used Enterprise Wide Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.   BS Degree or equivalent work experience Formal training and /or certifications in programming languages a plus Typically have 4-8 years of directly related experience. Experience with data used in CoreLogic products and processes  public record, tax, census, appraisal, etc. . Fluent in multiple program languages at an expert level Strong experience with various computer platforms and application environments Expertise with developing multiple tiers of multi-tiered software applications Expertise designing programs and data systems Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team Project management skills Experience with ETL methodologies Strong understanding of data structures and design Experience with data flow, data enrichment, transformations Ability to optimize database queries and performance Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation Experience in developing data service processes and system infrastructure to be used Enterprise Wide Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.  ","BS Degree or equivalent work experience Formal training and /or certifications in programming languages a plus Typically have 4-8 years of directly related experience. Experience with data used CoreLogic products processes public record, tax, census, appraisal, etc. . Fluent multiple program at an expert level Strong various computer platforms application environments Expertise developing tiers multi-tiered software applications designing programs systems Constantly updating personal technical business knowledge skills mentoring others to increase the team Project management ETL methodologies understanding structures design flow, enrichment, transformations Ability optimize database queries performance Demonstrates expertise variety concepts, practices, procedures implementation service system infrastructure be Enterprise Wide communication order participate meetings provide clear written documentation on complex issues wide audience.","BS Degree equivalent work experience Formal training /or certifications programming languages plus Typically 4-8 years directly related experience. Experience data used CoreLogic products processes public record, tax, census, appraisal, etc. . Fluent multiple program expert level Strong various computer platforms application environments Expertise developing tiers multi-tiered software applications designing programs systems Constantly updating personal technical business knowledge skills mentoring others increase team Project management ETL methodologies understanding structures design flow, enrichment, transformations Ability optimize database queries performance Demonstrates expertise variety concepts, practices, procedures implementation service system infrastructure Enterprise Wide communication order participate meetings provide clear written documentation complex issues wide audience."
153,Data Engineer,Data Engineer - Wealth Management,"Plano, TX 75024",Plano,TX,"As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. Youâll work in a collaborative, trusting, thought-provoking environmentâone that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

This role requires a wide variety of strengths and capabilities, including:
Strong Data Engineer with strong command over PL/SQL, data integration/transformation tools like Pentaho, Informatica and exposure to Big Data technology stack (Hadoop, Spark, Hive, Impala, etc.).
Utilize Agile methodology and adhere to coding standards, procedures and techniques while contributing to the technical code documentation.
Provide high quality technology solutions that address business needs developing applications within mature technology environments.
Converting data to stories using advanced analytical and visualization techniques to help with data-driven decision making and management reporting.
Devops model - Design, develop, code, test, debug, document and support.
Collaborate with team and come up with solutions for any identified problem by team.
Deployment of newly build modules in QA and Production environment.
Manage code quality for total build effort.
Coordinate with end users during User Acceptance Testing.
BS/BA degree or equivalent experience
Advanced knowledge of application, data and infrastructure architecture disciplines
Qualifications/Skills Required:
Bachelorâs degree in Computer Science or any other relevant field.
Strong Database/SQL skills
5+ years hands on ETL skills â Informatica, Pentaho
Unix/Perl scripting
Strong knowledge of Data modeling/Data Warehousing concepts
Experience of a scheduling tool - CA/Autosys
Experience working within an Agile environment
Demonstrated analytical and problem solving skills.
Desirable Skills
Hadoop / Spark / Hive, big data exposure is a big plus
Experience with any Reporting/Business Intelligence tools (Qlikview/Qliksense/Tableau/OBIEE)
Coding skills in Java/JavaScript/Python
Financial industry experience
Scrum/Agile knowledge
Strong verbal communication skills
Our Asset and Wealth Management division is driven by innovators like you who are driven to create technology solutions that make us work more efficiently and help our businesses grow. Itâs our mission to efficiently take care of our clientsâ wealth, helping them get, and remain properly invested. Across 27 cities, our team of 4,600 agile technologists thrive in a cloud-native environment that values continuous learning using a data-centric approach in developing innovative technology solutions.

When you work at JPMorgan Chase & Co., youâre not just working at a global financial institution. Youâre an integral part of one of the worldâs biggest tech companies. In 20 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $10B+ annual investment in technology enables us to hire people to create innovative solutions that will are transforming the financial services industry.


At JPMorgan Chase & Co. we value the unique skills of every employee, and weâre building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If youâre looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you.","Bachelorâs degree in Computer Science or any other relevant field. Strong Database/SQL skills 5+ years hands on ETL skills â Informatica, Pentaho Unix/Perl scripting Strong knowledge of Data modeling/Data Warehousing concepts Experience of a scheduling tool - CA/Autosys Experience working within an Agile environment Demonstrated analytical and problem solving skills.  Bachelorâs degree in Computer Science or any other relevant field. Strong Database/SQL skills 5+ years hands on ETL skills â Informatica, Pentaho Unix/Perl scripting Strong knowledge of Data modeling/Data Warehousing concepts Experience of a scheduling tool - CA/Autosys Experience working within an Agile environment Demonstrated analytical and problem solving skills.    ","Bachelorâs degree in Computer Science or any other relevant field. Strong Database/SQL skills 5+ years hands on ETL â Informatica, Pentaho Unix/Perl scripting knowledge of Data modeling/Data Warehousing concepts Experience a scheduling tool - CA/Autosys working within an Agile environment Demonstrated analytical and problem solving skills.","Bachelorâs degree Computer Science relevant field. Strong Database/SQL skills 5+ years hands ETL â Informatica, Pentaho Unix/Perl scripting knowledge Data modeling/Data Warehousing concepts Experience scheduling tool - CA/Autosys working within Agile environment Demonstrated analytical problem solving skills."
154,Data Engineer,Lean Big Data Engineer,"Irving, TX",Irving,TX,"Where good people build rewarding careers.
Think that working in the insurance field canât be exciting, rewarding and challenging? Think again. Youâll help us reinvent protection and retirement to improve customersâ lives. Weâll help you make an impact with our training and mentoring offerings. Here, youâll have the opportunity to expand and apply your skills in ways you never thought possible. And youâll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
About our team
360 Finance Advanced Analytics data engineering team works with multiple internal and external data sources to deliver data that is readily available, easily accessible, accurate and complete. They are responsible for building a centralized data lake/hub using the Hadoop ecosystem that will be used by Reporting & Operational Analytics teams and the Machine learning teams.
Job Description
This Lead Consultant is an experienced professional who is responsible for leveraging data and analytics to help automate and optimize Claims Analytics Data processes enabling our Claims employees to focus on serving our customers and delivering the most advanced claims experience on the planet. They will be responsible for the strategy around how we bring together complex data into clean and useful data structures making our valuable data more approachable.
Key Responsibilities
Responsible for design, prototyping and delivery of software solutions within the big data eco-system
Leading projects and/or serving as analytics SME to provide new or enhanced data to the business
Improving data governance and quality increasing the reliability of our data
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Key Responsibilities (Cont'd)
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Develop solutions and iterates quickly to continuously improve
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e. Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Roles and Responsibilities
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
5-6+ years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes,
Roles and Responsibilities (Cont'd)
developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow a plus
Working knowledge of Tableau â a plus
Advanced oral and written communication skills
Strong quantitative and analytical abilities
Good organizational and time management skills
Ability to manage and coach others
Decision making capabilities including problem solving approaches, decision frameworks; ability to design and lead complex analysis
Strong interpersonal skills
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work. Good Life. Good HandsÂ®.
As a Fortune 100 company and industry leader, we provide a competitive salary â but thatâs just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, youâll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.
Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.
Allstate generally does not sponsor individuals for employment-based visas for this position.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click âhereâ for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click âhereâ for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the âEEO is the Lawâ poster click âhereâ. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click âhereâ. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.
It is the Companyâs policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employeeâs ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment."," Undergraduate degree in Computer Science, Mathematics, Engineering  or related field  or equivalent experience preferred 5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function Ability to work with broad parameters in complex situations Experience in developing, managing, and manipulating large, complex datasets   Responsible for design, prototyping and delivery of software solutions within the big data eco-system Leading projects and/or serving as analytics SME to provide new or enhanced data to the business Improving data governance and quality increasing the reliability of our data Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise   ","Undergraduate degree in Computer Science, Mathematics, Engineering or related field equivalent experience preferred 5-7 years of a data integration, ETL and/or business intelligence/analytics function Ability to work with broad parameters complex situations Experience developing, managing, and manipulating large, datasets Responsible for design, prototyping delivery software solutions within the big eco-system Leading projects serving as analytics SME provide new enhanced Improving governance quality increasing reliability our Influencing creation single, trusted source key Claims that can be shared across Enterprise","Undergraduate degree Computer Science, Mathematics, Engineering related field equivalent experience preferred 5-7 years data integration, ETL and/or business intelligence/analytics function Ability work broad parameters complex situations Experience developing, managing, manipulating large, datasets Responsible design, prototyping delivery software solutions within big eco-system Leading projects serving analytics SME provide new enhanced Improving governance quality increasing reliability Influencing creation single, trusted source key Claims shared across Enterprise"
155,Data Engineer,Data Engineer,"Irving, TX",Irving,TX,"Overview:
Our client is looking for a Data Engineer to join the R&D Team. Youâre joining a multidisciplinary team of product directors, product managers, and digital analysts to create product listings across the top ecommerce marketplaces.

We are preferred supplier to 7-Eleven. This position may be available for conversion to full time employment upon successful evaluation of performance. Youâll be working in a fast-paced, eclectic environment of talented professionals who are leading the industry in digital capabilities.

Responsibilities:
The ideal candidate will be driven and passionate in creating the next generation of data products and capabilities. You will work directly with Product Owners and customers to deliver data products in a collaborative and agile environment. The ideal candidate will build data pipeline frameworks to automate high-volume and real-time data delivery and streaming data hub.

Basic Qualifications:

Skilled in Python and preferably in one or more programming languages like C++, Java, Go, etc
Experience with Docker and Kubernetes
Experience working with SQL and NoSQL based database solutions
Public cloud technology experience in production (Azure, AWS, or Equivalent)
3+ years of collective experience in data engineering and data analysis
2+ years of experience architecting, building and administering large-scale distributed applications

Preferred Qualifications:

Experience in engineering data pipelines using Big Data technologies (Hadoop, Spark, Storm, Kafka, etc) on large scale unstructured data sets is a plus
Familiarity with distributed data stores like Elasticsearch is a plus
Familiarity with Machine Learning concepts is a plus

GOIN Technology is a technology consulting and managed services business headquartered in Dallas/Ft. Worth. The company is focused on CIO/CTO Advisory Consulting, Information Security, Digital Transformation, Software/Data Engineering, Innovation as a Service and Managed Services."," Skilled in Python and preferably in one or more programming languages like C++, Java, Go, etc Experience with Docker and Kubernetes Experience working with SQL and NoSQL based database solutions Public cloud technology experience in production  Azure, AWS, or Equivalent  3+ years of collective experience in data engineering and data analysis 2+ years of experience architecting, building and administering large-scale distributed applications    Skilled in Python and preferably in one or more programming languages like C++, Java, Go, etc Experience with Docker and Kubernetes Experience working with SQL and NoSQL based database solutions Public cloud technology experience in production  Azure, AWS, or Equivalent  3+ years of collective experience in data engineering and data analysis 2+ years of experience architecting, building and administering large-scale distributed applications   ","Skilled in Python and preferably one or more programming languages like C++, Java, Go, etc Experience with Docker Kubernetes working SQL NoSQL based database solutions Public cloud technology experience production Azure, AWS, Equivalent 3+ years of collective data engineering analysis 2+ architecting, building administering large-scale distributed applications","Skilled Python preferably one programming languages like C++, Java, Go, etc Experience Docker Kubernetes working SQL NoSQL based database solutions Public cloud technology experience production Azure, AWS, Equivalent 3+ years collective data engineering analysis 2+ architecting, building administering large-scale distributed applications"
156,Data Engineer,Data Engineer - Wealth Management,"Plano, TX 75024",Plano,TX,"As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. Youâll work in a collaborative, trusting, thought-provoking environmentâone that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

This role requires a wide variety of strengths and capabilities, including:
Strong Data Engineer with strong command over PL/SQL, data integration/transformation tools like Pentaho, Informatica and exposure to Big Data technology stack (Hadoop, Spark, Hive, Impala, etc.).
Utilize Agile methodology and adhere to coding standards, procedures and techniques while contributing to the technical code documentation.
Provide high quality technology solutions that address business needs developing applications within mature technology environments.
Converting data to stories using advanced analytical and visualization techniques to help with data-driven decision making and management reporting.
Devops model - Design, develop, code, test, debug, document and support.
Collaborate with team and come up with solutions for any identified problem by team.
Deployment of newly build modules in QA and Production environment.
Manage code quality for total build effort.
Coordinate with end users during User Acceptance Testing.
BS/BA degree or equivalent experience
Advanced knowledge of application, data and infrastructure architecture disciplines
Qualifications/Skills Required:
Bachelorâs degree in Computer Science or any other relevant field.
Strong Database/SQL skills
5+ years hands on ETL skills â Informatica, Pentaho
Unix/Perl scripting
Strong knowledge of Data modeling/Data Warehousing concepts
Experience of a scheduling tool - CA/Autosys
Experience working within an Agile environment
Demonstrated analytical and problem solving skills.
Desirable Skills
Hadoop / Spark / Hive, big data exposure is a big plus
Experience with any Reporting/Business Intelligence tools (Qlikview/Qliksense/Tableau/OBIEE)
Coding skills in Java/JavaScript/Python
Financial industry experience
Scrum/Agile knowledge
Strong verbal communication skills
Our Asset and Wealth Management division is driven by innovators like you who are driven to create technology solutions that make us work more efficiently and help our businesses grow. Itâs our mission to efficiently take care of our clientsâ wealth, helping them get, and remain properly invested. Across 27 cities, our team of 4,600 agile technologists thrive in a cloud-native environment that values continuous learning using a data-centric approach in developing innovative technology solutions.

When you work at JPMorgan Chase & Co., youâre not just working at a global financial institution. Youâre an integral part of one of the worldâs biggest tech companies. In 20 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $10B+ annual investment in technology enables us to hire people to create innovative solutions that will are transforming the financial services industry.


At JPMorgan Chase & Co. we value the unique skills of every employee, and weâre building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If youâre looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you.","Bachelorâs degree in Computer Science or any other relevant field. Strong Database/SQL skills 5+ years hands on ETL skills â Informatica, Pentaho Unix/Perl scripting Strong knowledge of Data modeling/Data Warehousing concepts Experience of a scheduling tool - CA/Autosys Experience working within an Agile environment Demonstrated analytical and problem solving skills.  Bachelorâs degree in Computer Science or any other relevant field. Strong Database/SQL skills 5+ years hands on ETL skills â Informatica, Pentaho Unix/Perl scripting Strong knowledge of Data modeling/Data Warehousing concepts Experience of a scheduling tool - CA/Autosys Experience working within an Agile environment Demonstrated analytical and problem solving skills.    ","Bachelorâs degree in Computer Science or any other relevant field. Strong Database/SQL skills 5+ years hands on ETL â Informatica, Pentaho Unix/Perl scripting knowledge of Data modeling/Data Warehousing concepts Experience a scheduling tool - CA/Autosys working within an Agile environment Demonstrated analytical and problem solving skills.","Bachelorâs degree Computer Science relevant field. Strong Database/SQL skills 5+ years hands ETL â Informatica, Pentaho Unix/Perl scripting knowledge Data modeling/Data Warehousing concepts Experience scheduling tool - CA/Autosys working within Agile environment Demonstrated analytical problem solving skills."
157,Data Engineer,Data Engineer,"Austin, TX 78746",Austin,TX,"Data Engineer â Austin, TX
Amherst is revolutionizing the way U.S. real estate is priced, managed and financed in order to unlock opportunities for all market participants. Driven by data, analytics, and technology, Amherst has a 20-year history of anticipating where the next risks and opportunities are likely to emerge and designing actionable strategies for investors to capitalize on opportunities across residential real estate, commercial real estate and public securities. Amherst, along with its affiliates and subsidiaries, has more than 900 employees, $5 billion under management and approximately $15 billion under advisement and oversight. www.amherst.com.
We are hiring for a Data Engineer to utilize their industry knowledge, technical skills and passion for data to work closely with executive stakeholders and our financial engineering team developing solutions that support and optimize business operations. Weâre solving a variety of Big Data challenges and modernizing legacy data loaders as well as exploring the benefits and tradeoffs of other cutting edge tools. In this role, you will be responsible for a variety of duties including; understanding our data, application design and development, SQL query optimization and ensuring accuracy and consistency of data.
Ideal Candidate:
Analytical mindset with the ability to structure and process qualitative data and draw insightful conclusions.
Problem solver able to take a complex business request and transform it into a clean, simple data solution.
Quick learner open to new ideas and technologies, and willing to offer creative solutions.
Ownership and prideful in work and brings new ways and ideas to the table.
Responsibilities:
Develop logical data models and processes to transform, cleanse, and normalize raw data into high-quality datasets aligned with our analytical requirements.
Develop and maintain comprehensive controls to ensure data quality and completeness.
Manage data movement through our infrastructure. Streamline existing data workflows to create a flexible, reliable, and faster process.
Develop real-time data transformations and validations.
Identify and onboard new data sources. Collaborate with data vendors and internal stakeholders to define requirements and build interfaces. Troubleshoot and resolve issues with data feeds.
Work with our team of researchers to identify and analyze investment opportunities in real estate and fixed income securities markets.
Requirements:
3-5 years of experience in data analysis and/or management in an enterprise environment within finance, operations or analytics.
Passion for data organization, quality, and reliability.
MS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. Experience developing complex efficient queries, designing and building logical data models, and working with large datasets on a relational database system
Experience with at least one language (e.g. Python, C#, Scala, Java).
Strong problem-solving skills.
Must be an intellectually curious self-starter and motivated to continually learn.
Proactive, hardworking team player with excellent communication skills
Bonus Skills:
Strong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI.
Experience working with large datasets (1B+ Records)
Knowledge of Big Data or Cloud technologies
Experience with version control (e.g. TFS, SVN or Git) and build tools.
Tableau/BI Tools
What We Offer:
Competitive salaries
Choice of Mac or Windows hardware
Flexible vacation days, paid holidays, and work from home options
Medical, Dental, Vision, LTD, Life, EAP, and 401K with matching benefits
Stellar colleagues with proven track records
Free sodas, kombucha, cold brew, beer and healthy and unhealthy snacks at our Barton Springs WeWork location
Free lunch every day and breakfast tacos on Thursdays!
Amherst is proud to be an Equal Opportunity Employer and committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, color, religion, national origin, gender, pregnancy, sexual orientation, gender identity, age, physical or mental disability, genetic information or veteran status, and encourage all applicants to apply."," Strong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI. Experience working with large datasets  1B+ Records  Knowledge of Big Data or Cloud technologies Experience with version control  e.g. TFS, SVN or Git  and build tools. Tableau/BI Tools  Develop logical data models and processes to transform, cleanse, and normalize raw data into high-quality datasets aligned with our analytical requirements. Develop and maintain comprehensive controls to ensure data quality and completeness. Manage data movement through our infrastructure. Streamline existing data workflows to create a flexible, reliable, and faster process. Develop real-time data transformations and validations. Identify and onboard new data sources. Collaborate with data vendors and internal stakeholders to define requirements and build interfaces. Troubleshoot and resolve issues with data feeds. Work with our team of researchers to identify and analyze investment opportunities in real estate and fixed income securities markets.   3-5 years of experience in data analysis and/or management in an enterprise environment within finance, operations or analytics. Passion for data organization, quality, and reliability. MS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. Experience developing complex efficient queries, designing and building logical data models, and working with large datasets on a relational database system Experience with at least one language  e.g. Python, C , Scala, Java . Strong problem-solving skills. Must be an intellectually curious self-starter and motivated to continually learn. Proactive, hardworking team player with excellent communication skills ","Strong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI. Experience working large datasets 1B+ Records Knowledge Big Data or Cloud technologies version control e.g. TFS, SVN Git and build tools. Tableau/BI Tools Develop logical data models processes to transform, cleanse, normalize raw into high-quality aligned our analytical requirements. maintain comprehensive controls ensure quality completeness. Manage movement through infrastructure. Streamline existing workflows create a flexible, reliable, faster process. real-time transformations validations. Identify onboard new sources. Collaborate vendors internal stakeholders define requirements interfaces. Troubleshoot resolve issues feeds. Work team researchers identify analyze investment opportunities in real estate fixed income securities markets. 3-5 years analysis and/or management an enterprise environment within finance, operations analytics. Passion for organization, quality, reliability. MS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. developing complex efficient queries, designing building models, on relational database system at least one language Python, C , Scala, Java . problem-solving skills. Must be intellectually curious self-starter motivated continually learn. Proactive, hardworking player excellent communication skills","Strong knowledge statistics, including hands-on experience SAS, R, Matlab, Machine Learning, AI. Experience working large datasets 1B+ Records Knowledge Big Data Cloud technologies version control e.g. TFS, SVN Git build tools. Tableau/BI Tools Develop logical data models processes transform, cleanse, normalize raw high-quality aligned analytical requirements. maintain comprehensive controls ensure quality completeness. Manage movement infrastructure. Streamline existing workflows create flexible, reliable, faster process. real-time transformations validations. Identify onboard new sources. Collaborate vendors internal stakeholders define requirements interfaces. Troubleshoot resolve issues feeds. Work team researchers identify analyze investment opportunities real estate fixed income securities markets. 3-5 years analysis and/or management enterprise environment within finance, operations analytics. Passion organization, quality, reliability. MS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. developing complex efficient queries, designing building models, relational database system least one language Python, C , Scala, Java . problem-solving skills. Must intellectually curious self-starter motivated continually learn. Proactive, hardworking player excellent communication skills"
158,Data Engineer,Data Engineering Manager,"Austin, TX",Austin,TX,"Join SADA as a Data Engineering Manager!

Your Mission

As a Data Engineering Manager at SADA, you will build and lead a growing Data Engineering team as we deliver robust data solutions for our clients on Google Cloud Platform (GCP). You will be responsible for managing a blended team of data engineers and data scientists, so a broad background in Big Data, data warehouse modernization, analytics, disaster recovery, data science, and machine learning is highly advantageous.

The diversity of customers that SADA works with ensures a steady flow of challenging data work. Be prepared to tackle real-world data problems that our customers find too difficult or time-consuming to solve themselves. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of data domain areas. Management here at SADA also means developing people and being a leader.

In this role, you will:

Be comfortable working with customer executives to align business outcomes with technical vision and goals.
Guide the day-to-day activities of a geographically distributed team, including hiring world-class talent, reviewing work and setting goals.
Provide technical and professional leadership and mentorship on a diverse range of subject matter areas, such as Big Data pipelines and data warehouses to statistics and machine learning.
Develop and codify best practices for your team that can be replicated across multiple customer engagements.
Partner with your team to develop services and offerings that scale and are repeatable.
Participate in key technical and design discussions with technical leads as a hands-on manager.
Partner with other practice leads, architects, project managers, executives and sales personnel to develop statements of work, and then oversee execution by your team with high levels of agility and quality.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our employees know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing data practice area with vision and passion. You will be measured by your teamâs performance on customer engagements, how well your team achieves internal organizational goals, how well you collaborate with and support your team and peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the management growth track.

Expectations


Required Travel - 15-25% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Experience leading, managing and hiring a team of talented engineers
Expertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Expertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)
Natural language processing (e.g., conversational chatbots)
Document understanding
Image classification
Marketing analytics
IoT systems
Experience writing software in one or more languages such as Python or Java/Scala
Experience in technical consulting or customer-facing role
Excellent critical thinking, problem-solving and analytical skills

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience in a large scale, high-volume data warehouse environment
Experience operationalizing machine learning models on large datasets
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Experience leading, managing and hiring a team of talented engineers Expertise in at least one of the following engineering domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Expertise in at least one of the following data domains    Predictive analytics  e.g., recommendation systems, predictive maintenance  Natural language processing  e.g., conversational chatbots  Document understanding Image classification Marketing analytics IoT systems Experience writing software in one or more languages such as Python or Java/Scala Experience in technical consulting or customer-facing role Excellent critical thinking, problem-solving and analytical skills     ","Experience leading, managing and hiring a team of talented engineers Expertise in at least one the following engineering domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. domains Predictive analytics e.g., recommendation systems, predictive maintenance Natural language conversational chatbots Document understanding Image classification Marketing IoT systems writing more languages Python Java/Scala consulting customer-facing role Excellent critical thinking, problem-solving analytical skills","Experience leading, managing hiring team talented engineers Expertise least one following engineering domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. domains Predictive analytics e.g., recommendation systems, predictive maintenance Natural language conversational chatbots Document understanding Image classification Marketing IoT systems writing languages Python Java/Scala consulting customer-facing role Excellent critical thinking, problem-solving analytical skills"
159,Data Engineer,Reporting and Data Engineer-Signify Community,"Austin, TX 78730",Austin,TX,"Position Overview:
Data and human connection come together in our mission to bring communities together to collaborate and solve Social Determinants of Health â a career at Signify Community (a subsidiary of Signify Health) is a career with purpose. Our employees are empathic, passionate, confident, innovative change agents who arenât afraid to take risks and also have fun.
Reporting to the Director of Data Intelligence, the Reporting and Data Engineer is responsible for building the tools and reports that enable us to discover and share insights about our customers, as well as identifying system and workflow enhancements to Signify Communityâs platform. The Reporting and Data Engineer will uncover novel ways of connecting, comparing and using information from across communities, populations and contacts to provide key input for product development and management.
The ideal candidate thrives on the opportunity to collect and assimilate data and has a passion to collaborate closely with other analysts, engineers, and customer facing teams to drive decision-making and build innovative products that help people live better lives.
If this sounds like an exciting opportunity, and you are interested in realizing our mission to bring communities together to collaborate, address unmet social needs, and improve outcomes, contact us to learn more!
Qualifications:
Education/Licensing Requirements:
Bachelorâs degree in Engineering, Mathematics, Computer Science or related field
Clinical and healthcare relevance; a Clinical degree isnât required, but a deep understanding of the healthcare market is.

Experience Requirements:
2+ years of experience with healthcare data.
2+ years of data analysis

Essential Skills/Experience:
Solid knowledge of SQL queries and relational databases
Good understanding of math and statistics
Deep experience with Excel and/or other data manipulation tools
Familiarity with data visualization tools (e.g. Looker, Tableau)
Programming experience with R/Python for data analysis preferred

Essential Characteristics:
Accuracy/Attention to Detail - Diligently attends to details and pursues quality in accomplishing tasks.
Analysis/Reasoning - Examines data to grasp issues, draw conclusions, and solve complex business problems.
Critical Listening - Makes a concerted effort to listen to what people are saying and what they are trying to say; thinks carefully before responding and avoids jumping to conclusions; asks thoughtful questions to elicit more information; maintains positive and supportive demeanor to allow meaningful conversation.
Adaptability - Maintains job performance and focus under pressure; adapts to changing needs; able to respond appropriately to new information and schedule changes; handles stress in an appropriate manner; uses appropriate skills, tools, and techniques to manage time when accomplishing specific tasks.
Organization/Time Management - Establishes course of action for self and others to ensure that the work is completed efficiently, without jeopardizing excellent service; prioritizes tasks appropriately; adjusts priorities when appropriate; able to stay focused on the task at hand; allocates appropriate amount of time for completing own work; quickly able to determine roadblocks to accomplishing a goal.
Essential Job Responsibilities:

To perform this job successfully, an individual must be able to perform each essential function satisfactorily, with or without reasonable accommodation.
Assist in the design, preparation and distribution of reports and dashboards for end-users, management, and key stakeholders.
Use statistical methods to ensure metrics are well defined and match to the customerâs goals and desired outcomes documented during the discovery process.
Support the standardization of reporting for end-users.
Serve as internal expert user of the reporting system for input to the ongoing improvement and development of reporting tools.
Communicate reporting enhancements to internal and external users through presentations and documentation.
Seek and analyze trends and patterns across communities, populations, and contacts to assist in product management and development.

Working Conditions:
Ability to work well in a fast-paced environment.
Enjoys working with people to address their needs, and being a partner in achieving good health.
Ability to work at a desk and to use a phone and computer.
Ability to use office equipment and machinery effectively.
Ability to work effectively with frequent interruptions.","Bachelorâs degree in Engineering, Mathematics, Computer Science or related field Clinical and healthcare relevance; a Clinical degree isnât required, but a deep understanding of the healthcare market is.  Solid knowledge of SQL queries and relational databases Good understanding of math and statistics Deep experience with Excel and/or other data manipulation tools Familiarity with data visualization tools  e.g. Looker, Tableau  Programming experience with R/Python for data analysis preferred  Assist in the design, preparation and distribution of reports and dashboards for end-users, management, and key stakeholders. Use statistical methods to ensure metrics are well defined and match to the customerâs goals and desired outcomes documented during the discovery process. Support the standardization of reporting for end-users. Serve as internal expert user of the reporting system for input to the ongoing improvement and development of reporting tools. Communicate reporting enhancements to internal and external users through presentations and documentation. Seek and analyze trends and patterns across communities, populations, and contacts to assist in product management and development.  Bachelorâs degree in Engineering, Mathematics, Computer Science or related field Clinical and healthcare relevance; a Clinical degree isnât required, but a deep understanding of the healthcare market is.  Bachelorâs degree in Engineering, Mathematics, Computer Science or related field Clinical and healthcare relevance; a Clinical degree isnât required, but a deep understanding of the healthcare market is. ","Bachelorâs degree in Engineering, Mathematics, Computer Science or related field Clinical and healthcare relevance; a isnât required, but deep understanding of the market is. Solid knowledge SQL queries relational databases Good math statistics Deep experience with Excel and/or other data manipulation tools Familiarity visualization e.g. Looker, Tableau Programming R/Python for analysis preferred Assist design, preparation distribution reports dashboards end-users, management, key stakeholders. Use statistical methods to ensure metrics are well defined match customerâs goals desired outcomes documented during discovery process. Support standardization reporting end-users. Serve as internal expert user system input ongoing improvement development tools. Communicate enhancements external users through presentations documentation. Seek analyze trends patterns across communities, populations, contacts assist product management development.","Bachelorâs degree Engineering, Mathematics, Computer Science related field Clinical healthcare relevance; isnât required, deep understanding market is. Solid knowledge SQL queries relational databases Good math statistics Deep experience Excel and/or data manipulation tools Familiarity visualization e.g. Looker, Tableau Programming R/Python analysis preferred Assist design, preparation distribution reports dashboards end-users, management, key stakeholders. Use statistical methods ensure metrics well defined match customerâs goals desired outcomes documented discovery process. Support standardization reporting end-users. Serve internal expert user system input ongoing improvement development tools. Communicate enhancements external users presentations documentation. Seek analyze trends patterns across communities, populations, contacts assist product management development."
160,Data Engineer,Principal Data Engineer,"Austin, TX",Austin,TX,"Crowdskout is looking for a Lead Data Engineer that can both architect and lead efforts around our expanding data pipeline infrastructure. Crowdskout's product has most recently been centered in the CRM space, but we are looking to expand far beyond that. Currently, we process millions of data points through multiple data pipelines to feed a suite of datastores and applications. We are preparing for +10x growth both in the volume of data processed and the speed in which that data can be available and actionable. To accomplish this we are looking for someone who can architect and lead the effort to build out highly scalable data solutions.

If you are highly motivated, super passionate about democracy, and want to join a close-knit team that is looking to build great things together, Crowdskout may be for you. This is a full-time position in Austin, TX.

Responsibilities:

Lead efforts to design, build, scale, and maintain multiple data pipelines
Architect highly scalable data solutions
Be a technical thought leader within the org
Work closely with business owners and external stakeholders to provide actionable data
Ensure data accuracy and reliability

Requirements:

Experience building large scale streaming and batch data pipelines
Experience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)
Mastery of multiple databases (e.g. MongoDB, MySQL, etc.)
Understanding of data security best practices

Extras:

AWS data technologies (e.g. Kinesis, Glue, RDS, Athena, Redshift, etc.)
Experience building out data warehouse infrastructure
DevOps or System Admin experience
Data Science exploration and modeling

Crowdskout is an equal opportunity employer that encourages diversity across all spectrums in its hiring, without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, or any other protected factor. With that being said, we wouldn't be able to accommodate candidates in need of work sponsorship at this time since we are a small company. If you find this role interesting and you hit on the elements above, please apply!","   Lead efforts to design, build, scale, and maintain multiple data pipelines Architect highly scalable data solutions Be a technical thought leader within the org Work closely with business owners and external stakeholders to provide actionable data Ensure data accuracy and reliability    Experience building large scale streaming and batch data pipelines Experience using Big Data technologies  Spark, EMR, hadoop, data lakes, etc.  Mastery of multiple databases  e.g. MongoDB, MySQL, etc.  Understanding of data security best practices ","Lead efforts to design, build, scale, and maintain multiple data pipelines Architect highly scalable solutions Be a technical thought leader within the org Work closely with business owners external stakeholders provide actionable Ensure accuracy reliability Experience building large scale streaming batch using Big Data technologies Spark, EMR, hadoop, lakes, etc. Mastery of databases e.g. MongoDB, MySQL, Understanding security best practices","Lead efforts design, build, scale, maintain multiple data pipelines Architect highly scalable solutions Be technical thought leader within org Work closely business owners external stakeholders provide actionable Ensure accuracy reliability Experience building large scale streaming batch using Big Data technologies Spark, EMR, hadoop, lakes, etc. Mastery databases e.g. MongoDB, MySQL, Understanding security best practices"
161,Data Engineer,Senior Data Engineer,"Austin, TX",Austin,TX,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Hihg
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Mastery in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or customer-facing role     ","Mastery in at least one of the following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing more languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role","Mastery least one following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role"
162,Data Engineer,Principal Data Engineer,"Austin, TX 78723",Austin,TX,"At Texas Mutual, we're creating a stronger, safer Texas. Here, you will explore and create new data sets and work with business and IT partners to support our corporate data strategy. Join one of the best companies to work for in Texas, reporting into the Chief Data Office. Located in the Mueller development, we offer modern sit/stand workstations, free on-site fitness center, and free garage parking.
Responsibilities & Qualifications
In this Role:
Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server
Play a lead role in migrating data from legacy systems to cloud platforms
Create custom data sets for use by business analysts and data scientists
Rapidly prototype new data sets for exploratory analysis
Use SQL skills to manage data
Monitor database performance and tuning to improve query performance
Design and automate data pipelines to integrate different data sources using SSIS
Collaborate with IT partners to move data prototypes into production
Develop real-time data integrations in MS SQL Server
Establish techniques to monitor data quality and implement remediation procedures
Working directly with non-technical users to identify complex needs/requirements and translate into technical solutions
Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler
Create data training programs
Train analyst community on data sources and best practices
Lead other staff on SSIS, performance tuning, and database administration
Required Qualifications:
At least 10 years of related data experience
Bachelor's degree in a related field
Experience in the design, development, implementation and support of SQL Server
Experience with SSIS, Alteryx or similar ETL tools ï»¿ï»¿ ï»¿ï»¿
Our Benefits:
Day one health, dental, and vision insurance
Performance bonus
401k plan with 4% basic employer contribution and 100% employer match contribution up to 6%
Vacation, sick, holiday and volunteer time off
Life and disability insurance
Flexible spending account
Free on-site gym and fitness classes
Professional development
Tuition reimbursement
Pet insurance
Free identity theft protection
Company-sponsored social and philanthropy events
Texas Mutual Insurance Company is an Equal Employment Opportunity employer.","Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server Play a lead role in migrating data from legacy systems to cloud platforms Create custom data sets for use by business analysts and data scientists Rapidly prototype new data sets for exploratory analysis Use SQL skills to manage data Monitor database performance and tuning to improve query performance Design and automate data pipelines to integrate different data sources using SSIS Collaborate with IT partners to move data prototypes into production Develop real-time data integrations in MS SQL Server Establish techniques to monitor data quality and implement remediation procedures Working directly with non-technical users to identify complex needs/requirements and translate into technical solutions Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler Create data training programs Train analyst community on data sources and best practices Lead other staff on SSIS, performance tuning, and database administration   Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server Play a lead role in migrating data from legacy systems to cloud platforms Create custom data sets for use by business analysts and data scientists Rapidly prototype new data sets for exploratory analysis Use SQL skills to manage data Monitor database performance and tuning to improve query performance Design and automate data pipelines to integrate different data sources using SSIS Collaborate with IT partners to move data prototypes into production Develop real-time data integrations in MS SQL Server Establish techniques to monitor data quality and implement remediation procedures Working directly with non-technical users to identify complex needs/requirements and translate into technical solutions Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler Create data training programs Train analyst community on data sources and best practices Lead other staff on SSIS, performance tuning, and database administration   ","Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server Play a lead role migrating from legacy systems to cloud platforms Create custom sets use by business analysts scientists Rapidly prototype new exploratory analysis Use skills manage Monitor database performance tuning improve query Design automate pipelines integrate different sources using SSIS Collaborate with IT partners move prototypes into production real-time integrations Establish techniques monitor quality implement remediation procedures Working directly non-technical users identify complex needs/requirements translate technical solutions Demonstrate competency T-SQL, including advanced functions tuning, job scheduler training programs Train analyst community on best practices Lead other staff SSIS, administration","Develop strategies data models, automated ETL processes, stored procedures, views MS SQL Server Play lead role migrating legacy systems cloud platforms Create custom sets use business analysts scientists Rapidly prototype new exploratory analysis Use skills manage Monitor database performance tuning improve query Design automate pipelines integrate different sources using SSIS Collaborate IT partners move prototypes production real-time integrations Establish techniques monitor quality implement remediation procedures Working directly non-technical users identify complex needs/requirements translate technical solutions Demonstrate competency T-SQL, including advanced functions tuning, job scheduler training programs Train analyst community best practices Lead staff SSIS, administration"
163,Data Engineer,Big Data Engineer,"Austin, TX",Austin,TX,"About Us

At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world's largest networks that powers trillions of requests per month. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare have all web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was recognized by the World Economic Forum as a Technology Pioneer and named to Entrepreneur Magazine's Top Company Cultures list.

We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!

About the department

Cloudflare is looking to build and grow our Business Intelligence team responsible for building large-scale enterprise data lake and EDW from different sources and enabling various product and business teams such as Marketing, Customer Support, Sales, Finance with key business dashboards/reporting, insights and recommendation models.

About the role

As part of this initiative, we are looking for a Big Data(EDW/Analytics) Engineers to come join Cloudflare and help us build a scalable petabyte scale data lake and EDW using modern tech stack from the ground up. Success in this role comes from marrying a strong data engineering background with product and business acumen to deliver scalable data pipeline and BI solutions that can enable self-service analytics at Cloudflare in a simple and standard manner. This person will also play a crucial role in hiring and growing the business intelligence team in Austin in a rapid manner.

What we look for: Agile Delivery & Execution, Engineering Excellence, Tech Savvy, Business & Product acumen, Creative Problem solver

Responsibilities:

Build and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies (Hadoop, Spark, Cloud, NoSQL etc.) in a agile manner.
Strong understanding of business and product data needs.
Close partnership with internal stakeholders and partners from Engineering, product, and business(Finance, Sales, Customer Experience, Marketing etc.).
Active role in hiring and growing the team in Austin with data Engineers, analysts, and data scientists.

Requirements:

Bachelor's or Master's Degree in Computer Science or Engineering or related experience required.
3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions.
Solid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka.
Experience in all aspects of data systems(both Big data and traditional) including data schema design, ETL, aggregation strategy, and performance optimization.
Capable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality.
Experience in hiring data Engineers preferred.
Experience in Internet security industry preferred.

What Makes Us Special

We're not just a highly ambitious, large-scale technology company. We're a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.

Project Galileo ( https://blog.cloudflare.com/protecting-free-expression-online/ ): We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare's enterprise customers--at no cost.

Project Athenian ( https://www.cloudflare.com/athenian/ ): We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.

Path Forward Partnership ( https://blog.cloudflare.com/tag/path-forward/ ): Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.

1.1.1.1 ( https://1.1.1.1/ ): We released 1.1.1.1 ( https://1.1.1.1/ ) to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here's the deal - we don't store client IP addresses never, ever. We will continue to abide by our privacy policy ( https://developers.cloudflare.com/1.1.1.1/commitment-to-privacy/privacy-policy/privacy-policy/ ) and ensure that no user data is sold to advertisers or used to target consumers.

Sound like something you'd like to be a part of? We'd love to hear from you!

Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.

Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.","   Build and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies  Hadoop, Spark, Cloud, NoSQL etc.  in a agile manner. Strong understanding of business and product data needs. Close partnership with internal stakeholders and partners from Engineering, product, and business Finance, Sales, Customer Experience, Marketing etc. . Active role in hiring and growing the team in Austin with data Engineers, analysts, and data scientists.    Bachelor's or Master's Degree in Computer Science or Engineering or related experience required. 3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions. Solid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka. Experience in all aspects of data systems both Big data and traditional  including data schema design, ETL, aggregation strategy, and performance optimization. Capable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality. Experience in hiring data Engineers preferred. Experience in Internet security industry preferred. ","Build and Support scalable reliable data solutions that can enable self-service reporting advanced analytics at Cloudflare using modern lake EDW technologies Hadoop, Spark, Cloud, NoSQL etc. in a agile manner. Strong understanding of business product needs. Close partnership with internal stakeholders partners from Engineering, product, Finance, Sales, Customer Experience, Marketing . Active role hiring growing the team Austin Engineers, analysts, scientists. Bachelor's or Master's Degree Computer Science Engineering related experience required. 3+ years development Big space working Petabytes building large scale solutions. Solid Google Cloud Platform, Python, Hive, Kafka. Experience all aspects systems both traditional including schema design, ETL, aggregation strategy, performance optimization. Capable closely teams to ensure are aligned initiatives high quality. Engineers preferred. Internet security industry","Build Support scalable reliable data solutions enable self-service reporting advanced analytics Cloudflare using modern lake EDW technologies Hadoop, Spark, Cloud, NoSQL etc. agile manner. Strong understanding business product needs. Close partnership internal stakeholders partners Engineering, product, Finance, Sales, Customer Experience, Marketing . Active role hiring growing team Austin Engineers, analysts, scientists. Bachelor's Master's Degree Computer Science Engineering related experience required. 3+ years development Big space working Petabytes building large scale solutions. Solid Google Cloud Platform, Python, Hive, Kafka. Experience aspects systems traditional including schema design, ETL, aggregation strategy, performance optimization. Capable closely teams ensure aligned initiatives high quality. Engineers preferred. Internet security industry"
164,Data Engineer,Sr. Data Engineer,"Austin, TX",Austin,TX,"About the Role:
As the Data Engineer, you will join our engineering team and build world-class data solutions for RealMassive. You will have oversight of the commercial real estate data pipeline process and develop best practices and recommendations for the data acquisition and data delivery method for the company. You will be the expert on the entire data solution from data collection, transformation, ingestion, and data delivery.
This role reports directly to the VP of Engineering and is located in Austin, TX.

Responsibilities:
Overall responsibility for day-to-day data pipeline operations and manage database acquisition and data delivery methods effectively
Serve as a lead data engineer to manage the development of web services for data posting and delivery
Provide technical leadership and expertise on data integration and data delivery
Perform data quality procedures to ensure data consistency and data integrity
Ensure proper documentation of data posting and related API objects
Work directly with product and engineering team to understand data needs and provide end-to-end data solutions that address customer requirements
Perform other duties as assigned
Requirements
Required Qualifications:
6+ years of professional experience as a data engineer or in a similar role
4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL
Hands-on experience writing SQL, Perform SQL optimization and tuning
Strong work experience with Python scripting or equivalent
Working knowledge with AWS RDS, Data Lakes and data analytics
Must be able to work in a diverse team environment
Must possess problem-solving skills and ability to multitask
Outstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude
Bachelorâs degree in Computer Science or Engineering, or equivalent experience
Preferred Qualifications:
Knowledge of advanced analytics tools and Agile development methodologies
AWS RDS PostgreSQL skills and experience is a plus
Experience working with AWS Glue, EMR, and Kinesis
Experience with data science and machine learning
Working knowledge with big data and advanced programming languages
Experience in working with data visualization tools such as Tableau
Benefits
Best-in-class benefits: medical, dental, and vision
Company sponsored long-term disability
Unlimited vacation
Maternity/paternity leave
Flexible working options
Suite of optional benefits include HSA, FSA, and short-term disability
Enjoy catered lunch several times per week + snacks + fancy coffees
Why join us:
At RealMassive, our strength is our knowledgeable, passionate, and creative people. We are commercial real estateâs only real-time, big data solution and property marketplace dedicated to providing innovative technology solutions to modernize a $15 Trillion industry. Our products are designed to break down the barriers that have traditionally isolated data and insights from those who benefit the most from an open data environment. We strive to make the lives of our customers easier through information and access, which in turn helps local communities around the country grow and thrive.
Whether you work in engineering, data science, product, sales or operations, at RealMassive, we are all focused on the same mission and vision â to be the worldâs source of commercial real estate data and insights. We are focused, dedicated and committed to improving the lives of our customers by staying experimental, agile and innovative.
Our culture is built on inclusion, integrity and ideation. We value the voices, thoughts and passions of every employee and believe that success is collaborative, not individual. When you join RealMassive, youâll instantly help our customers make better decisions, create opportunities, move faster and improve the commercial real estate transaction experience.
About RealMassive:
RealMassive, commercial real estateâs real-time data provider and marketplace, is the industryâs source for searching, finding and evaluating commercial property and markets around the country. The marketplace makes it easy for business owners, entrepreneurs, brokers, developers and investors to find the perfect property to start and grow their business in small rural communities or major metropolitan areas.
With innovative data solutions, RealMassive is the industryâs largest data source of commercial property data that is updated and verified weekly. The data collected, standardized and enriched by RealMassive provides our customers with faster, more effective information to make decisions relating to leasing, acquiring and dispositioning commercial properties.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
How to apply:
If your career preference is to work in a fast-paced entrepreneurial environment, where your success is measured by your contribution to the teamâwe should talk.
Please complete the online application as well as the behavioral assessment at the following link: https://assess.predictiveindex.com/Gvjan","6+ years of professional experience as a data engineer or in a similar role 4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL Hands-on experience writing SQL, Perform SQL optimization and tuning Strong work experience with Python scripting or equivalent Working knowledge with AWS RDS, Data Lakes and data analytics Must be able to work in a diverse team environment Must possess problem-solving skills and ability to multitask Outstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude Bachelorâs degree in Computer Science or Engineering, or equivalent experience   Overall responsibility for day-to-day data pipeline operations and manage database acquisition and data delivery methods effectively Serve as a lead data engineer to manage the development of web services for data posting and delivery Provide technical leadership and expertise on data integration and data delivery Perform data quality procedures to ensure data consistency and data integrity Ensure proper documentation of data posting and related API objects Work directly with product and engineering team to understand data needs and provide end-to-end data solutions that address customer requirements Perform other duties as assigned   6+ years of professional experience as a data engineer or in a similar role 4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL Hands-on experience writing SQL, Perform SQL optimization and tuning Strong work experience with Python scripting or equivalent Working knowledge with AWS RDS, Data Lakes and data analytics Must be able to work in a diverse team environment Must possess problem-solving skills and ability to multitask Outstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude Bachelorâs degree in Computer Science or Engineering, or equivalent experience ","6+ years of professional experience as a data engineer or in similar role 4+ database development with relational databases such Oracle/MS SQL/PostgreSQL Hands-on writing SQL, Perform SQL optimization and tuning Strong work Python scripting equivalent Working knowledge AWS RDS, Data Lakes analytics Must be able to diverse team environment possess problem-solving skills ability multitask Outstanding analytical abilities, drive for results, attention quality detail, collaborative attitude Bachelorâs degree Computer Science Engineering, Overall responsibility day-to-day pipeline operations manage acquisition delivery methods effectively Serve lead the web services posting Provide technical leadership expertise on integration procedures ensure consistency integrity Ensure proper documentation related API objects Work directly product engineering understand needs provide end-to-end solutions that address customer requirements other duties assigned","6+ years professional experience data engineer similar role 4+ database development relational databases Oracle/MS SQL/PostgreSQL Hands-on writing SQL, Perform SQL optimization tuning Strong work Python scripting equivalent Working knowledge AWS RDS, Data Lakes analytics Must able diverse team environment possess problem-solving skills ability multitask Outstanding analytical abilities, drive results, attention quality detail, collaborative attitude Bachelorâs degree Computer Science Engineering, Overall responsibility day-to-day pipeline operations manage acquisition delivery methods effectively Serve lead web services posting Provide technical leadership expertise integration procedures ensure consistency integrity Ensure proper documentation related API objects Work directly product engineering understand needs provide end-to-end solutions address customer requirements duties assigned"
165,Data Engineer,Sr. Data Engineer - Payments Systems Risk,"Austin, TX",Austin,TX,"Job Description

To ensure that Visaâs payment technology is truly available to everyone, everywhere requires the success of our key bank or merchant partners and internal business units. The Global Data Science group supports these partners by using our extraordinarily rich data set that spans more than 3 billion cards globally and captures more than 100 billion transactions in a single year.
Are you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.
As a Senior Data Engineer, you will be responsible for helping to blueprint and deliver modelled attributes, data assets, and self-serve workflows that solve clients' business objectives. You will get the chance to leverage your business acumen and technical knowledge of big data and data mining techniques. Based on deep understanding and knowledge of big-data engineering techniques, you will develop and maintain data and tools to enable data scientists to draw fact based insights and build models This function is critical in building market-relevant client solutions and intellectual property for Visa.
Essential Functions
Work with manager and clients to fully understand business requirements and desired business outcomes
Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions
Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visaâs data scientists
Perform other tasks on R&D, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis
Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users
Execute data engineering projects ranging from small to large either individually or as part of a project team
Ensure project delivery within timelines and budget requirements
Provide coaching and mentoring to junior team members

Qualifications

Basic Qualifications:
2 years of work experience with a Bachelorâs Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)
Preferred Qualifications:
Minimum of 2-3 yearsâ experience in production ETL pipelines, utilizing big data engineering techniques that enable statistical solutions to solve business problems
Post graduate degree in Computer Science/ Engineering, Information Science or a related discipline with strong technical experiences highly desired
Previous exposure to financial services, credit cards or merchant analytics is a plus, but not required
Extensive experience with SQL and big data technologies (Hadoop, Python , Java, Spark, Hive etc.) tools for large scale data processing, data transformation and machine learning pipelines
Experience with data visualization and business intelligence tools like Tableau, Microstrategy, or other programs highly desired
Experience with SAS as a statistical package is preferred
Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred
Additional Information

Work Hours
The incumbent must make themselves available during core business hours.
Travel Requirements
The position requires the incumbent to travel for work 5% of the time.
Physical Requirements
This position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers, reach with hands and arms, and bend or lift up to 25 pounds.
Visa will consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law."," Minimum of 2-3 yearsâ experience in production ETL pipelines, utilizing big data engineering techniques that enable statistical solutions to solve business problems Post graduate degree in Computer Science/ Engineering, Information Science or a related discipline with strong technical experiences highly desired Previous exposure to financial services, credit cards or merchant analytics is a plus, but not required Extensive experience with SQL and big data technologies  Hadoop, Python , Java, Spark, Hive etc.  tools for large scale data processing, data transformation and machine learning pipelines Experience with data visualization and business intelligence tools like Tableau, Microstrategy, or other programs highly desired Experience with SAS as a statistical package is preferred Familiarity or experience with data mining and statistical modeling  e.g., regression modeling, clustering techniques, decision trees, etc.  is preferred    ","Minimum of 2-3 yearsâ experience in production ETL pipelines, utilizing big data engineering techniques that enable statistical solutions to solve business problems Post graduate degree Computer Science/ Engineering, Information Science or a related discipline with strong technical experiences highly desired Previous exposure financial services, credit cards merchant analytics is plus, but not required Extensive SQL and technologies Hadoop, Python , Java, Spark, Hive etc. tools for large scale processing, transformation machine learning pipelines Experience visualization intelligence like Tableau, Microstrategy, other programs SAS as package preferred Familiarity mining modeling e.g., regression modeling, clustering techniques, decision trees,","Minimum 2-3 yearsâ experience production ETL pipelines, utilizing big data engineering techniques enable statistical solutions solve business problems Post graduate degree Computer Science/ Engineering, Information Science related discipline strong technical experiences highly desired Previous exposure financial services, credit cards merchant analytics plus, required Extensive SQL technologies Hadoop, Python , Java, Spark, Hive etc. tools large scale processing, transformation machine learning pipelines Experience visualization intelligence like Tableau, Microstrategy, programs SAS package preferred Familiarity mining modeling e.g., regression modeling, clustering techniques, decision trees,"
166,Data Engineer,Senior Data Engineer,"Austin, TX",Austin,TX,"NarrativeDx is expanding our software development team and we are looking for smart, talented engineers that have a history of getting products complete and in the hands of customers. For this position we are searching for a data engineer with experience architecting data processing pipelines and working alongside data scientists to adapt data models into production systems. The position requires experience developing web applications in python and experience deploying applications on AWS. This position will be involved with a wide variety of development tasks on the engineering team and will be the primary link between our research team and our application development team. Candidates must have experience with: python server application development, web application development with exposure to frontend visualizations of datasets, SQL databases and query optimization, and scaling data processing.
Requirements
B.S. in Computer Science/Engineering and 5 years of professional software development experience or equivalent.
3+ years experience with Python using Django or Flask
Experience with Python Celery or other task/job management frameworks
5+ years of experience in a software development environment
Experience with AWS services
Experience with data modeling techniques
3+ years experience with PostgreSQL or other SQL server
Nice to haves:
Experience with natural language processing
Experience with sentiment classification
Experience with computational linguistics
Benefits
NarrativeDx is a 6 year old company with VC backing. We have a great office in downtown Austin with good parking, a great healthcare package, 401K Match and flexible schedules. We are a small close-knit team where we have fun and work smart and we keep our team small by hiring smart experienced people!",     B.S. in Computer Science/Engineering and 5 years of professional software development experience or equivalent. 3+ years experience with Python using Django or Flask Experience with Python Celery or other task/job management frameworks 5+ years of experience in a software development environment Experience with AWS services Experience with data modeling techniques 3+ years experience with PostgreSQL or other SQL server ,B.S. in Computer Science/Engineering and 5 years of professional software development experience or equivalent. 3+ with Python using Django Flask Experience Celery other task/job management frameworks 5+ a environment AWS services data modeling techniques PostgreSQL SQL server,B.S. Computer Science/Engineering 5 years professional software development experience equivalent. 3+ Python using Django Flask Experience Celery task/job management frameworks 5+ environment AWS services data modeling techniques PostgreSQL SQL server
167,Data Engineer,SQL Data Engineer,"Austin, TX",Austin,TX,"Overview
ProSphere is seeking an experienced SQL Data Engineer to provide highly specialized applications and operational analysis. The Engineer assists with planning and supporting network and computing infrastructure and has knowledge of networking technologies. The Engineer is cognizant of all phases of software development with emphasis on the planning, analysis, modeling, simulation, testing, integration, documentation, and presentation phases.

This is full-time position located in Austin, TX. Veterans are encouraged to apply.
Responsibilities
Develop, implement and maintain a scalable data management architecture to support the storage and querying of large datasets
Create and maintain data pipelines to automate the processing of large data sets
Help design and maintain efficient data collection workflows with other groups within the company
Manage and perform data analysis to identify data quality issues
Propose new technologies that could improve the way data is handled
Manage data security and provide efficient access to engineering teams
Communicate technical data and approaches to both technical and non-technical audiences
Perform Database maintenance
Building and analyzing dashboards and reports
Evaluating and defining metrics and perform exploratory analysis
Monitoring key product metrics and understanding root causes of changes in metrics
Empower and assist operation and product teams through building key data sets and data-based recommendations
Automating analyses and authoring pipelines via SQL/python based ETL framework
Qualifications
Bachelor's Degree in Computer Science, Engineering, Math or related technical field (8 years of additional experience can be substituted for education)
5+ years' relevant experience
Experience in Data Platform Languages such as SSIS and TSQL
Experience in Data Platform Tools such as SSIS, SSDT, SSMS and/or Visual Studio
Experience working in an environment using Agile methodology
Experience in Data Engineering concepts such as ETL, ELT or performance tuning
Hands on experience writing SQL scripts
Strong Communication, Presentation and Facilitation Skills. Must be able to explain data quality issues and impacts to a non-technical audience
Physical Demands
Typical office environment. Ability to sit and stand for extended periods of time
Ability to lift 5-20 lbs.

ProSphere offers full-time employees a comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.

It is ProSphereâs policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability or any other characteristic protected by applicable federal, state or local law.","Bachelor's Degree in Computer Science, Engineering, Math or related technical field  8 years of additional experience can be substituted for education  5+ years' relevant experience Experience in Data Platform Languages such as SSIS and TSQL Experience in Data Platform Tools such as SSIS, SSDT, SSMS and/or Visual Studio Experience working in an environment using Agile methodology Experience in Data Engineering concepts such as ETL, ELT or performance tuning Hands on experience writing SQL scripts Strong Communication, Presentation and Facilitation Skills. Must be able to explain data quality issues and impacts to a non-technical audience  Develop, implement and maintain a scalable data management architecture to support the storage and querying of large datasets Create and maintain data pipelines to automate the processing of large data sets Help design and maintain efficient data collection workflows with other groups within the company Manage and perform data analysis to identify data quality issues Propose new technologies that could improve the way data is handled Manage data security and provide efficient access to engineering teams Communicate technical data and approaches to both technical and non-technical audiences Perform Database maintenance Building and analyzing dashboards and reports Evaluating and defining metrics and perform exploratory analysis Monitoring key product metrics and understanding root causes of changes in metrics Empower and assist operation and product teams through building key data sets and data-based recommendations Automating analyses and authoring pipelines via SQL/python based ETL framework  ","Bachelor's Degree in Computer Science, Engineering, Math or related technical field 8 years of additional experience can be substituted for education 5+ years' relevant Experience Data Platform Languages such as SSIS and TSQL Tools SSIS, SSDT, SSMS and/or Visual Studio working an environment using Agile methodology Engineering concepts ETL, ELT performance tuning Hands on writing SQL scripts Strong Communication, Presentation Facilitation Skills. Must able to explain data quality issues impacts a non-technical audience Develop, implement maintain scalable management architecture support the storage querying large datasets Create pipelines automate processing sets Help design efficient collection workflows with other groups within company Manage perform analysis identify Propose new technologies that could improve way is handled security provide access engineering teams Communicate approaches both audiences Perform Database maintenance Building analyzing dashboards reports Evaluating defining metrics exploratory Monitoring key product understanding root causes changes Empower assist operation through building data-based recommendations Automating analyses authoring via SQL/python based ETL framework","Bachelor's Degree Computer Science, Engineering, Math related technical field 8 years additional experience substituted education 5+ years' relevant Experience Data Platform Languages SSIS TSQL Tools SSIS, SSDT, SSMS and/or Visual Studio working environment using Agile methodology Engineering concepts ETL, ELT performance tuning Hands writing SQL scripts Strong Communication, Presentation Facilitation Skills. Must able explain data quality issues impacts non-technical audience Develop, implement maintain scalable management architecture support storage querying large datasets Create pipelines automate processing sets Help design efficient collection workflows groups within company Manage perform analysis identify Propose new technologies could improve way handled security provide access engineering teams Communicate approaches audiences Perform Database maintenance Building analyzing dashboards reports Evaluating defining metrics exploratory Monitoring key product understanding root causes changes Empower assist operation building data-based recommendations Automating analyses authoring via SQL/python based ETL framework"
168,Data Engineer,Data Engineer,"Austin, TX",Austin,TX,"Come join our team at Cerity! We are a cutting edge insure tech company working with some really cool technologies. We have an opening for a data engineer on our growing team. We are looking for highly motivated, startup minded engineers to join our team of stellar engineers. Our data engineers work with latest big data tech including NoSQL (DynamoDB), AWS Aurora, AWS Data pipeline, Kinesis and Snowflake DB. Come help us revolutionize insurance technology.

Responsibilities will include:Designing and Implementing RDBMS Data ModelsDesigning and Implementing NoSQL Data ModelsImplementing ETL solutionsWorking with DevOps to establish infrastructure as codeFixing any issues that arise with data functionality
Requirements7+ Years Data Engineering5+ Years RDBMS Management2+ Years of NoSQLAWS Ecosystem knowledgeSolid Data Modeling/Design Experience
BenefitsCompetitive SalariesAnnual Bonus ProgramGreat Health and other BenefitsUnlimited PTO",    7+ Years Data Engineering5+ Years RDBMS Management2+ Years of NoSQLAWS Ecosystem knowledgeSolid Data Modeling/Design Experience,7+ Years Data Engineering5+ RDBMS Management2+ of NoSQLAWS Ecosystem knowledgeSolid Modeling/Design Experience,7+ Years Data Engineering5+ RDBMS Management2+ NoSQLAWS Ecosystem knowledgeSolid Modeling/Design Experience
169,Data Engineer,Data Engineer,"Austin, TX",Austin,TX,"LMI is currently seeking a data engineer within LMIâs Advanced Analytics service line to support the design and implementation of business critical data management & engineering solutions.

This position is located in Austin, TX
Responsibilities
The ideal candidate will have direct, applied experience with one or more of the following areas:
Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.
Qualifications
Bachelorâs degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with big data tools: Hadoop, Spark, Kafka
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with data governance tools: Collibra, Immuta
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
Secret or Top Secret clearance is preferred.
#LI-SH1","Bachelorâs degree in a quantitative field  e.g., engineering, statistics, mathematics, information technology, etc.  is preferred. Master's degree is desired. Must have at least 3 years of experience, preferably with a federal government customer. Experience with big data tools  Hadoop, Spark, Kafka Experience with relational SQL and NoSQL databases  Postgres, Cassandra, MongoDB Experience with data governance tools  Collibra, Immuta Experience with AWS cloud services  EC2, EMR, RDS, Redshift Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala Must possess strong written and verbal communication skills. Secret or Top Secret clearance is preferred.  Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.  ","Bachelorâs degree in a quantitative field e.g., engineering, statistics, mathematics, information technology, etc. is preferred. Master's desired. Must have at least 3 years of experience, preferably with federal government customer. Experience big data tools Hadoop, Spark, Kafka relational SQL and NoSQL databases Postgres, Cassandra, MongoDB governance Collibra, Immuta AWS cloud services EC2, EMR, RDS, Redshift object-oriented/object function scripting languages Python, Java, C++, Scala possess strong written verbal communication skills. Secret or Top clearance Develop structures systems to support the generation business insightsKnowledge experience overall ETL processesMaintain infrastructure develop scripts for regular processesDefine, design, flow diagrams, dictionaries, logical physical modelsDefine requirements, document elements, capture maintain metadetaIdentify clean incomplete, incorrect, inaccurate irrelevant dataIdentify new opportunities use improve performanceCommunicate present by developing reports using Tableau Business Intelligence toolsAdhere compliance audit requirements storage, architecture, cybersecurity,","Bachelorâs degree quantitative field e.g., engineering, statistics, mathematics, information technology, etc. preferred. Master's desired. Must least 3 years experience, preferably federal government customer. Experience big data tools Hadoop, Spark, Kafka relational SQL NoSQL databases Postgres, Cassandra, MongoDB governance Collibra, Immuta AWS cloud services EC2, EMR, RDS, Redshift object-oriented/object function scripting languages Python, Java, C++, Scala possess strong written verbal communication skills. Secret Top clearance Develop structures systems support generation business insightsKnowledge experience overall ETL processesMaintain infrastructure develop scripts regular processesDefine, design, flow diagrams, dictionaries, logical physical modelsDefine requirements, document elements, capture maintain metadetaIdentify clean incomplete, incorrect, inaccurate irrelevant dataIdentify new opportunities use improve performanceCommunicate present developing reports using Tableau Business Intelligence toolsAdhere compliance audit requirements storage, architecture, cybersecurity,"
170,Data Engineer,Tech Consulting Senior - Big Data Engineer,"Austin, TX 78701",Austin,TX,"EY delivers unparalleled service in big data, business intelligence, and digital analytics built on a blend of custom-developed methods related to customer analytics, data visualization, and optimization. We leverage best practices and a high degree of business acumen that has been compiled over years of experience to ensure the highest level of execution and satisfaction for our clients. At EY, our methods are not tied to any specific platforms but rather arrived at by analyzing business needs and making sure that the solutions delivered meet all client goals.
The opportunity
You will help our clients navigate the complex world of modern data analytics. Weâll look to you to provide our clients with a unique business perspective on how Big Data analytics can transform and improve their entire organization - starting with key business issues they face. This is a high growth, high visibility area with plenty of opportunities to enhance your skillset and build your career.
Your key responsibilities
Youâll spend most of your time working with a wide variety of clients to deliver the latest big data technologies and practices to design, build and maintain scalable and robust solutions that unify, enrich and analyse data from multiple sources.
Skills and attributes for success
Designing, Architecting, and Developing solutions leveraging big data technology (Open Source, AWS, or Microsoft) to ingest, process and analyze large, disparate data sets to exceed business requirements
Unifying, enriching, and analyzing customer data to derive insights and opportunities
Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements
Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions
Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches
Seeking out information to learn about emerging methodologies and technologies
Clarifying problems by driving to understand the true issue
Looking for opportunities for improving methods and outcomes
Applying data driven approach (KPIs) in tying technology solutions to specific business outcomes
Collaborating, influencing and building consensus through constructive relationships and effective listening
Solving problems by incorporating data into decision making
To qualify for the role you must have
A bachelor's degree and approximately three years of related work experience; or a master's degree and approximately two years of related work experience
At least five years hands-on experience with various Big Data technologies in one or more ecosystems: Open Source, Microsoft, or AWS:
Hadoop, Spark, NoSQL, Streaming, Atlas, Sqoop, HIVE
AWS, EMR, Hortonworks, Cassandra, Mongo, Redshift, Kafka
Azure, HDInsight, Azure DocumentDB, SQL Server
Proficiency coding in Java, C#, C++, or Scala
Experienced organizing, aggregating, querying, and analyzing large data sets
Communication is essential, must be able to listen and understand the question and develop and deliver clear insights.
Outstanding team player.
Independent and able to manage and prioritize workload.
Ability to quickly and positively adapt to change.
A valid driverâs license in the US; willingness and ability to travel to meet client needs.
Ideally, youâll also have
Bachelorâs Degree or above in mathematics, information systems, statistics, computer science, or related disciplines
What we look for
Weâre interested in passionate leaders with strong vision and a desire to stay on top of trends in the Big Data industry. If you have a genuine passion for helping businesses achieve the full potential of their data, this role is for you.
What working at EY offers
We offer a competitive compensation package where youâll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, both pension and 401(k) plans, a minimum of 15 days of vacation plus ten observed holidays and three paid personal days, and a range of programs and benefits designed to support your physical, financial and social well-being. Plus, we offer:
Opportunities to develop new skills and progress your career
A collaborative environment where everyone works together to create a better working world
Excellent training and development prospects, both through established programs and on-the-job training
About EY
As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.
Join us in building a better working world. Apply now.

EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status."," Designing, Architecting, and Developing solutions leveraging big data technology  Open Source, AWS, or Microsoft  to ingest, process and analyze large, disparate data sets to exceed business requirements Unifying, enriching, and analyzing customer data to derive insights and opportunities Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches Seeking out information to learn about emerging methodologies and technologies Clarifying problems by driving to understand the true issue Looking for opportunities for improving methods and outcomes Applying data driven approach  KPIs  in tying technology solutions to specific business outcomes Collaborating, influencing and building consensus through constructive relationships and effective listening Solving problems by incorporating data into decision making    ","Designing, Architecting, and Developing solutions leveraging big data technology Open Source, AWS, or Microsoft to ingest, process analyze large, disparate sets exceed business requirements Unifying, enriching, analyzing customer derive insights opportunities Leveraging in-house platforms as needed recommending building new platforms/solutions required Clearly communicating findings, recommendations, improve systems Demonstrating deep understanding of technology, concepts, tools, features, functions benefits different approaches Seeking out information learn about emerging methodologies technologies Clarifying problems by driving understand the true issue Looking for improving methods outcomes Applying driven approach KPIs in tying specific Collaborating, influencing consensus through constructive relationships effective listening Solving incorporating into decision making","Designing, Architecting, Developing solutions leveraging big data technology Open Source, AWS, Microsoft ingest, process analyze large, disparate sets exceed business requirements Unifying, enriching, analyzing customer derive insights opportunities Leveraging in-house platforms needed recommending building new platforms/solutions required Clearly communicating findings, recommendations, improve systems Demonstrating deep understanding technology, concepts, tools, features, functions benefits different approaches Seeking information learn emerging methodologies technologies Clarifying problems driving understand true issue Looking improving methods outcomes Applying driven approach KPIs tying specific Collaborating, influencing consensus constructive relationships effective listening Solving incorporating decision making"
171,Data Engineer,Data Engineer Python Programmer,"Austin, TX 78716",Austin,TX,"Do you want to do something meaningful with Big Data? At Optum, we?re leading the fastest moving industry in the world. Health care is transforming and we?re innovating to help make the health care system work better for everyone. Our technology teams are working on projects with national and international visibility and impact. As a Principal Data Engineer, you will join a company and a team that is finding solutions to transform terabytes of healthcare information into actionable data. Join us. Let innovation and performance fuel your life?s best work.(sm)

Optum Analytics solutions create a longitudinal view of both individual patients and patient populations. We gather, normalize, and analyze data from disparate sources that, uniquely, span the continuum of care-including EHRs, Practice Management Systems and claims. Our EHR data alone accounts for over 80M patient lives across the U.S.

We are looking for a Principal Data Engineer who is eager to tackle the challenges of processing vast amounts of EHR data originating from multiple sources. You will be the driving force, and thought-leader behind helping us build services based on NLP products.

Your primary responsibilities will be to design and maintain data pipelines and services using best practices for data management and governance, and deploy machine learning and NLP applications in production. You will be working with EHR data and working across teams with ETL, NLP engineers and data scientists, researchers and clinicians to provide data services with high data quality control standards. You will need to develop a deep understanding of the data and drive efforts to maintain and improve data quality and usability. You should understand the importance and value of writing maintainable, documented, and well-tested code throughout the entire product lifecycle. Above all, you should be curious about what is possible in healthcare with the right tools and infrastructure.

Youâll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.


Required Qualifications:
Degree in computer science or related field
5+ years current programming experience operating as individual contributor/hands on developer with programming projects as primary part of job
3+ Python experience (years can include advanced degree python projects)
3+ years building and maintaining data pipelines and data assets
2+ years working with distributed data processing frameworks such as Spark, Hive, and MapReduce
Demonstrated knowledge of data management best practices
Strong prioritization skills; ability to manage ad-hoc requests in parallel with ongoing projects
Attention to detail, intellectual curiosity, collaborative attitude and strong communication skills
Willingness to pick up new platforms and technologies and strong curiosity about new technologies
Preferred Qualifications:
Experience running machine learning or NLP applications at scale
Experience with data pipeline frameworks such as Airflow, Luigi or Oozie
Experience with search engines (Elasticsearch, Solr)
Experience with cloud-based computing (AWS, Azure)
Experience with Scala, in particular with Spark Scala API
Familiarity with EHR data and standards (HL7, FHIR)
Experience with HBase or other non-relational data bases
Experience with explaining, educating, presenting and/or training non-engineers on engineering concepts and processes
Experience with ETL
Experience with continuous integration and delivery

Careers with Optum. Here's the idea. We built an entire organization around one giant objective; make health care work better for everyone. So when it comes to how we use the world?s large accumulation of health-related information, or guide health and lifestyle choices or manage pharmacy benefits for millions, our first goal is to leap beyond the status quo and uncover new ways to serve. Optum, part of the UnitedHealth Group family of businesses, brings together some of the greatest minds and most advanced ideas on where health care has to go in order to reach its fullest potential. For you, that means working on high performance teams against sophisticated challenges that matter. Optum, incredible ideas in one incredible company and a singular opportunity to do your life's best work.(sm)

Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.

UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.","Degree in computer science or related field 5+ years current programming experience operating as individual contributor/hands on developer with programming projects as primary part of job 3+ Python experience  years can include advanced degree python projects  3+ years building and maintaining data pipelines and data assets 2+ years working with distributed data processing frameworks such as Spark, Hive, and MapReduce Demonstrated knowledge of data management best practices Strong prioritization skills; ability to manage ad-hoc requests in parallel with ongoing projects Attention to detail, intellectual curiosity, collaborative attitude and strong communication skills Willingness to pick up new platforms and technologies and strong curiosity about new technologies    ","Degree in computer science or related field 5+ years current programming experience operating as individual contributor/hands on developer with projects primary part of job 3+ Python can include advanced degree python building and maintaining data pipelines assets 2+ working distributed processing frameworks such Spark, Hive, MapReduce Demonstrated knowledge management best practices Strong prioritization skills; ability to manage ad-hoc requests parallel ongoing Attention detail, intellectual curiosity, collaborative attitude strong communication skills Willingness pick up new platforms technologies curiosity about","Degree computer science related field 5+ years current programming experience operating individual contributor/hands developer projects primary part job 3+ Python include advanced degree python building maintaining data pipelines assets 2+ working distributed processing frameworks Spark, Hive, MapReduce Demonstrated knowledge management best practices Strong prioritization skills; ability manage ad-hoc requests parallel ongoing Attention detail, intellectual curiosity, collaborative attitude strong communication skills Willingness pick new platforms technologies curiosity"
172,Data Engineer,Senior Data Engineer,"Austin, TX 78746",Austin,TX,"What youâll be called: Senior Data Engineer

Where youâll work: KWRI HeadquartersâAustin, TX

Named a Happiest Company to Work for in 2019; one of the Best Places to Work in Austin, TX; and featured on the Training Magazine Training 125 list seven times, Keller Williams Realty International (KWRI) thrives within a creative and collaborative culture where transforming the real estate industry through technology is our primary goal.

KW Technology is the foremost provider of real estate solutions, offering the most comprehensive end-to-end portfolio of products, services and training in the industry. Our Data Engineering team converts agent and consumer challenges into intuitive, insight-enhanced technology and consumer experiences using tools such as Python, Hadoop, Spark, MySQL, MongoDB and Snaplogic.

What youâll do:

Design, develop and implement data infrastructure and pipelines that collect, connect, centralize and curate data from various internal and external data sources. You will ensure that architectures support the needs of the business, and recommend ways to improve data reliability, efficiency and quality.

Essential Duties and Responsibilities:

Participate in data architecture discussions to understand target data structures, required data transformations and inform architectural approach based on best practices for data processing.
Lead detailed exploration of new internal and external source data to advise strategic initiatives led by the Product, Artificial Intelligence and Business Intelligence teams.
Influence technical and business strategy by making insightful contributions to team priorities and overall data processing approach.
Work in close collaboration with data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.
Participate in the hiring and mentoring of other data engineers.
Minimum Qualifications:

Bachelorâs degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.
6 or more years of experience as a data engineer on enterprise-level data solutions.
Experience in SQL and scripting for automation with Python, Perl or Ruby.
Experience working with relational and unstructured databases and enterprise data warehouses, including MySQL, PostgreSQL, MongoDB, SQL Server or Oracle.
Preferred Qualifications:

Masterâs degree in Information Management, Data Science, Analytics or related field.
Expert in SQL and Python for scripting automation.
Experience building open source data pipeline systems such as AirFlow, Hadoop or Kafka.
Experience with Spark, Presto, Hive or other map/reduce ""big data"" systems and services.
Who are we?

Austin, Texas-based Keller Williams, the world's largest real estate franchise by agent count, has more than 1,000 offices and 180,000 associates. The franchise is also No. 1 in units and sales volume in the United States. In 2015, Keller Williams began its evolution into a technology company, now building the real estate platform that agents' buyers and sellers prefer. Since 1983, the company has cultivated an agent-centric, technology-driven and education-based culture that rewards agents as stakeholders."," Bachelorâs degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience. 6 or more years of experience as a data engineer on enterprise-level data solutions. Experience in SQL and scripting for automation with Python, Perl or Ruby. Experience working with relational and unstructured databases and enterprise data warehouses, including MySQL, PostgreSQL, MongoDB, SQL Server or Oracle.    Participate in data architecture discussions to understand target data structures, required data transformations and inform architectural approach based on best practices for data processing. Lead detailed exploration of new internal and external source data to advise strategic initiatives led by the Product, Artificial Intelligence and Business Intelligence teams. Influence technical and business strategy by making insightful contributions to team priorities and overall data processing approach. Work in close collaboration with data-minded colleagues focused on back-end  microservice  development, business intelligence reporting, machine learning and artificial intelligence models. Participate in the hiring and mentoring of other data engineers.   ","Bachelorâs degree in Computer Science, Information Management, Data Analytics or related field equivalent experience. 6 more years of experience as a data engineer on enterprise-level solutions. Experience SQL and scripting for automation with Python, Perl Ruby. working relational unstructured databases enterprise warehouses, including MySQL, PostgreSQL, MongoDB, Server Oracle. Participate architecture discussions to understand target structures, required transformations inform architectural approach based best practices processing. Lead detailed exploration new internal external source advise strategic initiatives led by the Product, Artificial Intelligence Business teams. Influence technical business strategy making insightful contributions team priorities overall processing approach. Work close collaboration data-minded colleagues focused back-end microservice development, intelligence reporting, machine learning artificial models. hiring mentoring other engineers.","Bachelorâs degree Computer Science, Information Management, Data Analytics related field equivalent experience. 6 years experience data engineer enterprise-level solutions. Experience SQL scripting automation Python, Perl Ruby. working relational unstructured databases enterprise warehouses, including MySQL, PostgreSQL, MongoDB, Server Oracle. Participate architecture discussions understand target structures, required transformations inform architectural approach based best practices processing. Lead detailed exploration new internal external source advise strategic initiatives led Product, Artificial Intelligence Business teams. Influence technical business strategy making insightful contributions team priorities overall processing approach. Work close collaboration data-minded colleagues focused back-end microservice development, intelligence reporting, machine learning artificial models. hiring mentoring engineers."
173,Data Engineer,Data Engineer,"Austin, TX",Austin,TX,"Our mission at Civitas Learning is to partner with forward-thinking colleges and universities, harnessing the power of insight and action analytics to help students learn well and finish strong. Data and predictive models are at our core to achieve this mission. Are you amazing at SQL and PHP? Curious about data transformation for data science? We are looking for a smart and dedicated data integration specialists to join our data engineering services team.

The Data Engineering role at Civitas Learning is critical to onboarding colleges and universities onto our cloud platform and making the process smoother, more flexible, and faster. We work with people who are passionate about our mission, enjoy working with customers, and are eager to use their technical skills. We work hard, but also like to have fun! If this sounds like you, keep reading.

Primary Responsibilities:

Collaborate directly with external customers to understand their student success goals, specify and design data solutions and commission products into production.
Develop ETL transformations to map client data systems into our canonical data model.
Collaborate with teams across Civitas to drive innovation and best practices in our data and data science platform.
Up to 20% Travel to visit external clients for technical discovery and/or UAT/QA of data mappings.
Design, Implement and Maintain Database Models.
Build and operationalize data science models on AWS.

Minimum Qualifications

Bachelor's degree plus 3 years experience designing, developing, testing, and implementing complex ETL solutions using enterprise ETL tools
Expertise in writing complex database SQL queries with a focus on Postgres and Redshift.
Strong understanding of ETL best practices.
Proficient manipulating and processing text files with command line build tools and/or writing scripts to manipulate text files
Experience with database design and function
Expertise in working with technical and business teams to extract and document data integration/exchange requirements
Ability to handle multiple projects and deadlines with minimal supervision.
Ability to work independently and learn on the job as well as in a cross-functional team environment, collaborating with others and sharing tools, skills, and knowledge
Strong organizational skills and ability to meet deadlines, prioritize workload, and manage time effectively
Solid problem-solving and analysis skills that demonstrate resourcefulness and attention to detail
Strong customer service focus and a comfort with engaging with a customer via email, phone, and in person
Ability to express complex technical concepts effectively, both verbally and in writing.

Must have experience in some of the following:

Writing SQL and using command line tools (preferably in a linux or linux like environment)
Experience with at least one major RDBMS (preferably postGres, RedShift, or MySQL)
Python/PHP or other programming language

Preferred experience in the following:

Version Control System (preferably Github)
Experience with Higher Education Student Information Systems or Learning Management Systems (Ellucian Banner, Peoplesoft, Blackboard LMS, Canvas, etc.)
JIRA

About Civitas Learning:
Civitas Learning partners with universities and colleges dedicated to helping more students learn well and finish strong. We provide tools and services for educators that bring together and make the most of their diverse and disconnected data streams; personalize information and support for their students; and deepen understanding of the impact of their student-success initiatives. Through our work together, our partners are empowering leaders, advisors, faculty, & studentsâand measurably improving enrollment, persistence, and graduation outcomes.

Today, Civitas Learning has more than 350 colleges and universities as customers, serving nearly 8 million students. Together with our growing community of customers, Civitas Learning is making the most of the worldâs learning data to graduate a million more students per year by 2025.

Civitas Learning is located in the heart of downtown Austin, directly across from Lady Bird Lake and the Austin Hike and Bike Trail. Civitas offers a comprehensive benefits package including medical, dental, vision, disability insurance, onsite paid parking, 401-K Program and a flexible paid time off policy. Civitas Learning is an equal opportunity employer and values diversity. We do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability.","   Collaborate directly with external customers to understand their student success goals, specify and design data solutions and commission products into production. Develop ETL transformations to map client data systems into our canonical data model. Collaborate with teams across Civitas to drive innovation and best practices in our data and data science platform. Up to 20% Travel to visit external clients for technical discovery and/or UAT/QA of data mappings. Design, Implement and Maintain Database Models. Build and operationalize data science models on AWS.   ","Collaborate directly with external customers to understand their student success goals, specify and design data solutions commission products into production. Develop ETL transformations map client systems our canonical model. teams across Civitas drive innovation best practices in science platform. Up 20% Travel visit clients for technical discovery and/or UAT/QA of mappings. Design, Implement Maintain Database Models. Build operationalize models on AWS.","Collaborate directly external customers understand student success goals, specify design data solutions commission products production. Develop ETL transformations map client systems canonical model. teams across Civitas drive innovation best practices science platform. Up 20% Travel visit clients technical discovery and/or UAT/QA mappings. Design, Implement Maintain Database Models. Build operationalize models AWS."
174,Data Engineer,Data Engineer â Warehouse and B.I.,"Austin, TX",Austin,TX,"WHO WE ARE:


The Teacher Retirement System of Texas is the largest public retirement system in Texas, serving more than 1.5 million people. Innovation, technology, and collaboration make the difference as we strive to continue earning your trust every day. TRS improves the retirement security of Texas public education employees through our 'best in class' investment management and delivery of pension and health care benefits.


Our Mission: Improving the retirement security of TRS members by prudently investing and managing the Trust assets and delivering benefits that make a positive difference in their lives.


Teacher Retirement System of Texas (TRS) is developing groundbreaking solutions to manage retirement & healthcare services as well as maximizing investment returns for the stateâs public education employees. Succeeding in life is about commitment and hard workâmaybe a favorite school teacher imparted that idea to you along the way. At TRS, we take that to heart and believe life can also be a balance between giving back while excelling professionally.
The Data Engineer â Warehouse and Business Intelligence, performs advanced (senior-level) data analysis and architectural work for TRSâ Financial department. Work involves leading initiatives to develop and implement data analytics-driven solutions to meet critical business objectives. This position serves as liaison and technical expert between IT technical teams and the internal customers (business intelligence and data analytics team) of the data warehouse. May supervise the work of others. Works under minimal supervision with extensive latitude for the use of initiative and independent judgement.

State Classification
Data Analyst VI: 0655/B28

Duties and Responsibilities

Data Engineer, Warehouse and B.I Works closely with internal customers regarding their specific data needs to develop the requirements for data subject areas needed in the Data Warehouse. Captures the inventory of data sources and dashboards to prepare and manage integrated data. Acts as the IT knowledge leader on data warehousing and data analytics to the business. Ensures data warehouse implementations meet business expectations and coordinates customer acceptance testing and training. Ensures the customer can exploit the data warehouse solutions and helps identify additional possible uses of information; anticipates future needs and opportunities. Assists in the identification and integration of potential new data sources. Designs and develops ETL pipelines that extract data from various sources and load into the data warehouse or other systems. Ensures that controls to verify the accuracy and consistency of data are implemented and monitored. Provides ongoing operational support of the enterprise data warehouse, continued development and enhancement of the data warehouse, automation of daily data extracts and external system feeds, and development and enhancement of current dashboards.
Performs related work as assigned

Minimum Required Qualifications
Education: Graduation from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information systems or information science or a related field. High School diploma or equivalent and additional directly related experience may substitute for the required education on a year-for-year basis.

Experience: Three (3) years of experience in financial/accounting data warehouse, including dimensional modeling, ETL pipeline design & development, data management, data analysis, process measurement, and metrics management. Three (3) years of experience working closely with business analytics teams and data analysts. Two (2) years of experience with PowerBI or equivalent modern data visualization package. One (1) Experience with Talend Studio and Data Integration or equivalent modern ETL package. Experience with metadata-driven ETL templates. Successful track record of Business Intelligence/Data Warehouse solution implementations (design, implementation, data visualization and ongoing support/maintenance) with extensive interaction with business users. Demonstrated record of accomplishment working in a cross-functional team environment; including project management skills utilizing business and technical resources. Ability to apply data warehouse architecture theories and concepts to create business solutions. Thorough understanding of ETL (extract/transform/load) processing methods, underlying data warehouse data models, security, and BI products.

Registration, Certification, or Licensure: None.

Preferred QualificationsExperienced in Agile methodologies & DevOps approach to maintaining pipelines and databases. Recent experience with PowerBI (both cloud and on-premises). Recent experience with Talend. SQL and T-SQL knowledge.

Knowledge, Skills and Abilities
Knowledge of: Emerging data and analytics technologies (i.e. Hadoop, Spark, MongoDB, Azure Data Lake, etc.) Cloud platforms and development patterns (i.e. AWS, Azure, MapReduce, etc.) Machine-learning, statistical analysis, artificial intelligence, predictive analytics. Relational and non-relational data structures, theories, principles, and practices. Metadata management and associated processes. Web services (REST, SOAP, XML, WSDL, JSON). Data encryption and secure transmission practices (SSL, SSH, SFTP, Certificates, PKI, OAUTH2).

Skill in: Highly complex problem solving and critical thinking, and operating computers and applicable computer software. Planning, organizing, and coordinating work assignments to effectively meet frequent and/or multiple deadlines, handling multiple tasks simultaneously, and managing conflicting priorities and demands. Project management and system development life cycle concepts. Client/user interaction to determine system requirements. Strong written and oral communication skills. Strong presentation and interpersonal skills. Ability to present ideas in user-friendly language. Strong technical zeal with a passion for solving complex problems.

Ability to: Establish and maintain harmonious working relationships with co-workers, agency staff, and external contacts. Work effectively in a professional team environment. Work in an Agile development environment.

Physical Requirements and/or Working Conditions
Work is performed in a standard office environment and requires: normal cognitive abilities including the ability to learn, recall, and apply certain practices and policies; marginal or corrected visual and auditory requirements; constant use of personal computers, copiers, printers, and telephones; the ability to move about the office to access file cabinets and office machinery; frequent sitting and/or remaining in a stationary position; and the ability to work under deadlines, as a team member, and in direct contact with others.

Workforce Expectations
Must be able to: regularly, reliably, and punctually attend work; work extended hours as necessary; travel occasionally for work assignments and trainings; show flexibility and adaptability toward changes in assignments and work schedules; adhere to the agencyâs internal management policies and procedures; and exhibit work behaviors consistent with agency core values.



Military Occupational Specialty (MOS) Codes:

Veterans, Reservists or Guardsmen with experience in the Military Occupational Specialty ( http://www.hr.sao.texas.gov/Compensation/MilitaryCrosswalk/MOSC_AdministrativeSupport.pdf ) along with the minimum qualifications listed above may meet the minimum requirements and are highly encouraged to apply. Please contact Talent Acquisition at careers@trs.texas.gov with questions or for additional information.


To view all job vacancies, visit www.trs.texas.gov/careers or www.trs.csod.com/careersite.


For more information, visit www.trs.texas.gov."," Graduation from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information systems or information science or a related field. High School diploma or equivalent and additional directly related experience may substitute for the required education on a year-for-year basis.  Emerging data and analytics technologies  i.e. Hadoop, Spark, MongoDB, Azure Data Lake, etc.  Cloud platforms and development patterns  i.e. AWS, Azure, MapReduce, etc.  Machine-learning, statistical analysis, artificial intelligence, predictive analytics. Relational and non-relational data structures, theories, principles, and practices. Metadata management and associated processes. Web services  REST, SOAP, XML, WSDL, JSON . Data encryption and secure transmission practices  SSL, SSH, SFTP, Certificates, PKI, OAUTH2 .  Works closely with internal customers regarding their specific data needs to develop the requirements for data subject areas needed in the Data Warehouse. Captures the inventory of data sources and dashboards to prepare and manage integrated data. Acts as the IT knowledge leader on data warehousing and data analytics to the business. Ensures data warehouse implementations meet business expectations and coordinates customer acceptance testing and training. Ensures the customer can exploit the data warehouse solutions and helps identify additional possible uses of information; anticipates future needs and opportunities. Assists in the identification and integration of potential new data sources. Designs and develops ETL pipelines that extract data from various sources and load into the data warehouse or other systems. Ensures that controls to verify the accuracy and consistency of data are implemented and monitored. Provides ongoing operational support of the enterprise data warehouse, continued development and enhancement of the data warehouse, automation of daily data extracts and external system feeds, and development and enhancement of current dashboards.  Graduation from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information systems or information science or a related field. High School diploma or equivalent and additional directly related experience may substitute for the required education on a year-for-year basis.  normal cognitive abilities including the ability to learn, recall, and apply certain practices and policies; marginal or corrected visual and auditory requirements; constant use of personal computers, copiers, printers, and telephones; the ability to move about the office to access file cabinets and office machinery; frequent sitting and/or remaining in a stationary position; and the ability to work under deadlines, as a team member, and in direct contact with others.","Graduation from an accredited four-year college or university with major coursework in computer information systems, science, data management, systems science a related field. High School diploma equivalent and additional directly experience may substitute for the required education on year-for-year basis. Emerging analytics technologies i.e. Hadoop, Spark, MongoDB, Azure Data Lake, etc. Cloud platforms development patterns AWS, Azure, MapReduce, Machine-learning, statistical analysis, artificial intelligence, predictive analytics. Relational non-relational structures, theories, principles, practices. Metadata management associated processes. Web services REST, SOAP, XML, WSDL, JSON . encryption secure transmission practices SSL, SSH, SFTP, Certificates, PKI, OAUTH2 Works closely internal customers regarding their specific needs to develop requirements subject areas needed Warehouse. Captures inventory of sources dashboards prepare manage integrated data. Acts as IT knowledge leader warehousing business. Ensures warehouse implementations meet business expectations coordinates customer acceptance testing training. can exploit solutions helps identify possible uses information; anticipates future opportunities. Assists identification integration potential new sources. Designs develops ETL pipelines that extract various load into other systems. controls verify accuracy consistency are implemented monitored. Provides ongoing operational support enterprise warehouse, continued enhancement automation daily extracts external system feeds, current dashboards. normal cognitive abilities including ability learn, recall, apply certain policies; marginal corrected visual auditory requirements; constant use personal computers, copiers, printers, telephones; move about office access file cabinets machinery; frequent sitting and/or remaining stationary position; work under deadlines, team member, direct contact others.","Graduation accredited four-year college university major coursework computer information systems, science, data management, systems science related field. High School diploma equivalent additional directly experience may substitute required education year-for-year basis. Emerging analytics technologies i.e. Hadoop, Spark, MongoDB, Azure Data Lake, etc. Cloud platforms development patterns AWS, Azure, MapReduce, Machine-learning, statistical analysis, artificial intelligence, predictive analytics. Relational non-relational structures, theories, principles, practices. Metadata management associated processes. Web services REST, SOAP, XML, WSDL, JSON . encryption secure transmission practices SSL, SSH, SFTP, Certificates, PKI, OAUTH2 Works closely internal customers regarding specific needs develop requirements subject areas needed Warehouse. Captures inventory sources dashboards prepare manage integrated data. Acts IT knowledge leader warehousing business. Ensures warehouse implementations meet business expectations coordinates customer acceptance testing training. exploit solutions helps identify possible uses information; anticipates future opportunities. Assists identification integration potential new sources. Designs develops ETL pipelines extract various load systems. controls verify accuracy consistency implemented monitored. Provides ongoing operational support enterprise warehouse, continued enhancement automation daily extracts external system feeds, current dashboards. normal cognitive abilities including ability learn, recall, apply certain policies; marginal corrected visual auditory requirements; constant use personal computers, copiers, printers, telephones; move office access file cabinets machinery; frequent sitting and/or remaining stationary position; work deadlines, team member, direct contact others."
175,Data Engineer,Azure Data Engineer,"Austin, TX 78727",Austin,TX,"Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, âas isâ and âto beâ scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ","At least 5 years of consulting or client service delivery experience on Azure DevOps an platform Proven ability to build, manage and foster a team-oriented environment","At least 5 years consulting client service delivery experience Azure DevOps platform Proven ability build, manage foster team-oriented environment"
176,Data Engineer,Data Engineer,"Austin, TX",Austin,TX,"Crowdskout is looking for a Data Engineer that can help us expand our data pipeline infrastructure. Crowdskout's product has most recently been centered in the CRM space, but we are looking to change that. Currently, we process millions of data points through multiple data pipelines to feed into a suite of databases. We are preparing for 10x growth both in the volume of data processed and the speed in which that data can be available and actionable. To accomplish this we are looking for someone who can build out highly scalable data solutions.

If you are highly motivated, super passionate about democracy, and want to join a close-knit team that is looking to build great things together, Crowdskout may be for you. This is a full-time position in Washington, DC; Sacramento, CA; Austin, TX; Raleigh-Durham, NC; Salt Lake City, UT; or Chicago, IL.

Responsibilities:

Design, build, scale, and maintain multiple data pipelines
Work closely with business owners and external stakeholders to provide actionable data
Ensure data accuracy and reliability

Requirements:

Experience building large scale streaming and batch data pipelines
Experience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)
Mastery of multiple databases (e.g. MongoDB, MySQL, etc.)
Understanding of data security best practices

Extras:

AWS data technologies (e.g. Kenesis, Glue, RDS, Athena, etc.)
Experience building out data warehouse infrastructure
Software development using PHP
DevOps or System Admin experience
Data Science exploration and modeling

Crowdskout is an equal opportunity employer that encourages diversity across all spectrums in its hiring, without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, or any other protected factor. With that being said, we wouldn't be able to accommodate candidates in need of work sponsorship at this time since we are a small company. If you find this role interesting and you hit on the elements above, please apply!","   Design, build, scale, and maintain multiple data pipelines Work closely with business owners and external stakeholders to provide actionable data Ensure data accuracy and reliability    Experience building large scale streaming and batch data pipelines Experience using Big Data technologies  Spark, EMR, hadoop, data lakes, etc.  Mastery of multiple databases  e.g. MongoDB, MySQL, etc.  Understanding of data security best practices ","Design, build, scale, and maintain multiple data pipelines Work closely with business owners external stakeholders to provide actionable Ensure accuracy reliability Experience building large scale streaming batch using Big Data technologies Spark, EMR, hadoop, lakes, etc. Mastery of databases e.g. MongoDB, MySQL, Understanding security best practices","Design, build, scale, maintain multiple data pipelines Work closely business owners external stakeholders provide actionable Ensure accuracy reliability Experience building large scale streaming batch using Big Data technologies Spark, EMR, hadoop, lakes, etc. Mastery databases e.g. MongoDB, MySQL, Understanding security best practices"
177,Data Engineer,Data Engineer,"Austin, TX",Austin,TX,"General Information
Ref #: 27449
Functional Area: Technology
Employee Type: Full Time
Location: Austin
Experienced Required: Please See Below
Education Required: Bachelors Degree
Job Posting Shift: 1st
Date published: 26-Jun-2019
About Us:
We are PIMCO, a leading global asset management firm. We manage investments and develop solutions across the full spectrum of asset classes, strategies and vehicles: fixed income, equities, commodities, asset allocation, ETFs, hedge funds and private equity. PIMCO is one of the largest investment managers, actively managing more than $1.84 trillion in assets for clients around the world. PIMCO has over 2,700 employees in 17 offices globally. PIMCO is recognized as an innovator, industry thought leader and trusted advisor to our clients.

PIMCO is one of the worldâs premier fixed income investment managers with thousands of professionals around the world united in a single purpose: creating opportunities for our clients in every environment. Since 1971, we have brought innovation and expertise to our partnership with the institutions, financial advisors and millions of individual investors who entrust us with their assets. We aspire to cultivate performance and leadership through empowering our people, diversity of thought, and a commitment to an inclusive culture that engages in our global communities.
Position Description:
As a data engineer on our team, we will be working on a number of high-profile projects that will require you to collaborate with key partners and develop data solutions that enable insights into our clients through an evolving data architecture and drive our next generation data platform. In this role you will meet with relevant partners, understand and model data assets, and constantly find opportunities to optimize and evolve the underlining data platform. You will work with software developers, data architects, data analysts, and data scientists to solve complex business problems. Demonstrating lifelong learning and collaboration to fill gaps in knowledge is essential.
Position Requirements:

2+ years of experience with building end-to-end scalable production-grade data pipelines
Knowledge of data warehousing
Understanding of modern data architecture, data modeling, and data management principles
Experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)
Good foundation in data structures and algorithms
Understanding with Relational and NoSQL databases to help teams best organize their data for analysis
Experienced in OOP design and development, preferably in Python
Knowledge in writing, understanding and tuning PL/SQL and/or T-SQL
BS/BA degree in Computer Science, Engineering or related fields or equivalent experience
PREFERRED QUALIFICATIONS
Domain knowledge of Financial Services
Hands-on experience with working with Cloud technologies, including AWS
Experience with CI/CD methodologies
Experience with *nix environments, including shell script development

We are PIMCO, a global investment management firm with a singular focus on preserving and enhancing investorsâ assets. We manage investments for institutions, financial advisors and individuals, helping millions of people around the world meet their financial goals.

Our technology powers the firmâs global trading platform. We employ sophisticated and cutting edge technology tools that support PIMCOâs core investment management strategy.

Why PIMCO? At PIMCO you will join a dynamic, constantly evolving global firm that pushes you to grow, lead and innovate. You will be client- focused and work on technologies that will be put to immediate use. You will be part of a team whose members are encouraged to speak up with an idea or challenge existing views, regardless of title or tenure. You will have the opportunity to receive competitive compensation and other attractive benefits (Please see below).

PIMCOâs Technology Team is organized in small, focused, agile groups, that either work closely with business units to deliver value or develop core technologies that lever the product teams. Our environment fosters innovation and promotes entrepreneurial spirit, and we use top of the line tools. PIMCO recognizes the paramount role of tech now and in the future and invests in technology accordingly. Technology careers are available in Newport Beach, Austin, New York, London, Munich, Singapore and Tokyo.
Benefits:
PIMCO is committed to offering a comprehensive portfolio of employee benefits designed to support the health and wellbeing of you and your family. Benefits vary by location but may include:
Medical, dental, and vision coverage
Life insurance and travel coverage
401(k) (defined contribution) retirement savings, retirement plan, pension contribution from your first day of employment
Work/life programs such as flexible work arrangements, parental leave and support, employee assistance plan, commuter benefits, health club discounts, and educational/CFA certification reimbursement programs
Community involvement opportunities with The PIMCO Foundation in each PIMCO office","   2+ years of experience with building end-to-end scalable production-grade data pipelines Knowledge of data warehousing Understanding of modern data architecture, data modeling, and data management principles Experience with modern data pipeline technologies  Spark, Flink, Airflow, Beam, etc.  Good foundation in data structures and algorithms Understanding with Relational and NoSQL databases to help teams best organize their data for analysis Experienced in OOP design and development, preferably in Python Knowledge in writing, understanding and tuning PL/SQL and/or T-SQL BS/BA degree in Computer Science, Engineering or related fields or equivalent experience  2+ years of experience with building end-to-end scalable production-grade data pipelines Knowledge of data warehousing Understanding of modern data architecture, data modeling, and data management principles Experience with modern data pipeline technologies  Spark, Flink, Airflow, Beam, etc.  Good foundation in data structures and algorithms Understanding with Relational and NoSQL databases to help teams best organize their data for analysis Experienced in OOP design and development, preferably in Python Knowledge in writing, understanding and tuning PL/SQL and/or T-SQL BS/BA degree in Computer Science, Engineering or related fields or equivalent experience ","2+ years of experience with building end-to-end scalable production-grade data pipelines Knowledge warehousing Understanding modern architecture, modeling, and management principles Experience pipeline technologies Spark, Flink, Airflow, Beam, etc. Good foundation in structures algorithms Relational NoSQL databases to help teams best organize their for analysis Experienced OOP design development, preferably Python writing, understanding tuning PL/SQL and/or T-SQL BS/BA degree Computer Science, Engineering or related fields equivalent","2+ years experience building end-to-end scalable production-grade data pipelines Knowledge warehousing Understanding modern architecture, modeling, management principles Experience pipeline technologies Spark, Flink, Airflow, Beam, etc. Good foundation structures algorithms Relational NoSQL databases help teams best organize analysis Experienced OOP design development, preferably Python writing, understanding tuning PL/SQL and/or T-SQL BS/BA degree Computer Science, Engineering related fields equivalent"
178,Data Engineer,Data Engineer,"Austin, TX",Austin,TX,"Design, develop and build out data pipelines to ingest data into our proprietary data structures, and be a key collaborator in the data discovery and exploratory analysis process during our client engagements.
Responsibilities
Build pipelines to ingest and maintain complex data sets into Cerebri AIâs proprietary data stores for use in machine learning modeling
Develop and maintain data ontologies for key market segments
Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data
Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data
Write quality documentation on the discovery process and software projects
Work equally well in a team environment and on your own.
Communicate complex ideas clearly with both team members and clients
Travel up to 25%
Qualifications
At least one (1) year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models
At least two (2) years of experience in Scala, Python, Apache Spark and SQL
Experience working directly with relational database structures and flat files
Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices
Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.
Good verbal and written communication skills, with both technical and non-technical stakeholders
Nice to Haves
Experience in Java and/or Scala
Experience with data management processing tools such as Kafka, Elasticsearch and Logstash
Experience with NoSQL distributed databases such as Cassandra.
Experience in business intelligence visualization tools such as Grafana, Superset, Redash or Tableau.
Experience with Microsoft Azure or similar cloud computing solutions
Masterâs degree or higher in a relevant quantitative subject"," At least one  1  year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models At least two  2  years of experience in Scala, Python, Apache Spark and SQL Experience working directly with relational database structures and flat files Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations. Good verbal and written communication skills, with both technical and non-technical stakeholders   Build pipelines to ingest and maintain complex data sets into Cerebri AIâs proprietary data stores for use in machine learning modeling Develop and maintain data ontologies for key market segments Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data Write quality documentation on the discovery process and software projects Work equally well in a team environment and on your own. Communicate complex ideas clearly with both team members and clients Travel up to 25%  ","At least one 1 year of experience designing and building data processing solutions ETL pipelines for varied formats, ideally at a company that leverages machine learning models two 2 years in Scala, Python, Apache Spark SQL Experience working directly with relational database structures flat files Ability to write efficient queries, functions views include complex joins the identification development custom indices Knowledge professional software engineering practices best full life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration development, operations. Good verbal written communication skills, both technical non-technical stakeholders Build ingest maintain sets into Cerebri AIâs proprietary stores use modeling Develop ontologies key market segments Collaborate scientists perform exploratory analysis map fields find signals client clients develop pipeline infrastructure, ask appropriate questions gain deep understanding Write quality documentation on discovery process projects Work equally well team environment your own. Communicate ideas clearly members Travel up 25%","At least one 1 year experience designing building data processing solutions ETL pipelines varied formats, ideally company leverages machine learning models two 2 years Scala, Python, Apache Spark SQL Experience working directly relational database structures flat files Ability write efficient queries, functions views include complex joins identification development custom indices Knowledge professional software engineering practices best full life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration development, operations. Good verbal written communication skills, technical non-technical stakeholders Build ingest maintain sets Cerebri AIâs proprietary stores use modeling Develop ontologies key market segments Collaborate scientists perform exploratory analysis map fields find signals client clients develop pipeline infrastructure, ask appropriate questions gain deep understanding Write quality documentation discovery process projects Work equally well team environment own. Communicate ideas clearly members Travel 25%"
179,Data Engineer,Data Engineer,"Austin, TX 78723",Austin,TX,"Weâre excited youâre considering joining a great place to work! At Texas Mutual, we value our employees. Our service-inspired culture, great compensation and benefits package, award-winning wellness program and excellent career opportunities make Texas Mutual a great place to work. In the Data Engineer role, you will use advanced technical skills and knowledge to develop data models, automated ETL processes, stored procedures, and views in MS SQL Server. You will provide technical expertise to staff and end-users.
Responsibilities & Qualifications
Essential Functions:
Use advanced SQL skills to manage data
Monitor database performance and tuning to improve query performance
Design and automate data pipelines and integrate different data sources using SSIS
Apply advanced skills to develop real-time data integrations in MS SQL Server
Establish techniques to monitor data quality and implement remediation procedures
Partner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions
Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler
Provide advanced technical expertise to staff and end-users
Requir ed Qualifications:
Bachelorâs degree in a related field.
At least five years of professional data experience for a Data Engineer; at least seven years of professional data experience for a Senior Data Engineer or any equivalent combination of education, training , and experience that provides the skills necessary to perform the essential function of the job.
Preferred Qualifications:
Experience managing cloud data assets
Data preparation using Python or R
Experience building data pipelines for machine learning
NoSQL database experience
Our Benefits:
Day one health, dental, and vision insurance
Performance bonus
401k plan with 4% basic employer contribution and 100% employer match contribution up to 6%
Vacation, sick, holiday and volunteer time off
Life and disability insurance
Flexible spending account
Free on-site gym and fitness classes
Professional development
Tuition reimbursement
Pet insurance
Free identity theft protection
Company-sponsored social and philanthropy events
Texas Mutual Insurance Company is an Equal Employment Opportunity employer.","Use advanced SQL skills to manage data Monitor database performance and tuning to improve query performance Design and automate data pipelines and integrate different data sources using SSIS Apply advanced skills to develop real-time data integrations in MS SQL Server Establish techniques to monitor data quality and implement remediation procedures Partner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler Provide advanced technical expertise to staff and end-users   Use advanced SQL skills to manage data Monitor database performance and tuning to improve query performance Design and automate data pipelines and integrate different data sources using SSIS Apply advanced skills to develop real-time data integrations in MS SQL Server Establish techniques to monitor data quality and implement remediation procedures Partner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler Provide advanced technical expertise to staff and end-users   ","Use advanced SQL skills to manage data Monitor database performance and tuning improve query Design automate pipelines integrate different sources using SSIS Apply develop real-time integrations in MS Server Establish techniques monitor quality implement remediation procedures Partner with non-technical users identify needs/requirements then translating the requirements into technical solutions Demonstrate competency T-SQL, including functions tuning, job scheduler Provide expertise staff end-users","Use advanced SQL skills manage data Monitor database performance tuning improve query Design automate pipelines integrate different sources using SSIS Apply develop real-time integrations MS Server Establish techniques monitor quality implement remediation procedures Partner non-technical users identify needs/requirements translating requirements technical solutions Demonstrate competency T-SQL, including functions tuning, job scheduler Provide expertise staff end-users"
180,Data Engineer,Senior Data Engineer,"Austin, TX",Austin,TX,"General Information
Ref #: 27451
Functional Area: Technology
Employee Type: Full Time
Location: Austin
Experienced Required: Please See Below
Education Required: Bachelors Degree
Job Posting Shift: 1st
Date published: 28-Jun-2019
About Us:
We are PIMCO, a leading global asset management firm. We manage investments and develop solutions across the full spectrum of asset classes, strategies and vehicles: fixed income, equities, commodities, asset allocation, ETFs, hedge funds and private equity. PIMCO is one of the largest investment managers, actively managing more than $1.84 trillion in assets for clients around the world. PIMCO has over 2,700 employees in 17 offices globally. PIMCO is recognized as an innovator, industry thought leader and trusted advisor to our clients.

PIMCO is one of the worldâs premier fixed income investment managers with thousands of professionals around the world united in a single purpose: creating opportunities for our clients in every environment. Since 1971, we have brought innovation and expertise to our partnership with the institutions, financial advisors and millions of individual investors who entrust us with their assets. We aspire to cultivate performance and leadership through empowering our people, diversity of thought, and a commitment to an inclusive culture that engages in our global communities.
Position Description:
As a Sr. Data Engineer on our team we will be working on a number of high-profile projects that will require you to collaborate with key partners and develop data solutions that enable insights into our clients through an evolving data architecture and drive our next generation data platform. A data engineer will meet with relevant partners, understand and model data assets, and constantly find opportunities to optimize and evolve the underlining data platform. You will work with software developers, data architects, data analysts, and data scientists to solve complex business problems. Demonstrating lifelong learning and collaboration to fill gaps in knowledge is essential.
Position Requirements:
5+ years of experience with building end-to-end scalable production-grade data pipelines
In-depth knowledge of data warehousing and master data management
Expertise with modern data architecture, data modeling, and data management principles
Hands-on experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)
Solid foundation in data structures, algorithms, and software design
Expertise with Relational and NoSQL databases to help teams best organize their data for analysis
Skilled in OOP design and development, preferably in Python
Advanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL
BS/BA degree in Computer Science, Engineering or related fields or equivalent experience

PREFERRED QUALIFICATIONS
Domain knowledge of Financial Services
Hands-on experience with working with Cloud technologies, including AWS
Skilled at designing and implementing ETL frameworks
Experience with CI/CD methodologies
Experience with *nix environments, including shell script development

We are PIMCO, a global investment management firm with a singular focus on preserving and enhancing investorsâ assets. We manage investments for institutions, financial advisors and individuals, helping millions of people around the world meet their financial goals.

Our technology powers the firmâs global trading platform. We employ sophisticated and cutting edge technology tools that support PIMCOâs core investment management strategy.

Why PIMCO? At PIMCO you will join a dynamic, constantly evolving global firm that pushes you to grow, lead and innovate. You will be client- focused and work on technologies that will be put to immediate use. You will be part of a team whose members are encouraged to speak up with an idea or challenge existing views, regardless of title or tenure. You will have the opportunity to receive competitive compensation and other attractive benefits (Please see below).

PIMCOâs Technology Team is organized in small, focused, agile groups, that either work closely with business units to deliver value or develop core technologies that lever the product teams. Our environment fosters innovation and promotes entrepreneurial spirit, and we use top of the line tools. PIMCO recognizes the paramount role of tech now and in the future and invests in technology accordingly. Technology careers are available in Newport Beach, Austin, New York, London, Munich, Singapore and Tokyo.
Benefits:
PIMCO is committed to offering a comprehensive portfolio of employee benefits designed to support the health and wellbeing of you and your family. Benefits vary by location but may include:
Medical, dental, and vision coverage
Life insurance and travel coverage
401(k) (defined contribution) retirement savings, retirement plan, pension contribution from your first day of employment
Work/life programs such as flexible work arrangements, parental leave and support, employee assistance plan, commuter benefits, health club discounts, and educational/CFA certification reimbursement programs
Community involvement opportunities with The PIMCO Foundation in each PIMCO office","   5+ years of experience with building end-to-end scalable production-grade data pipelines In-depth knowledge of data warehousing and master data management Expertise with modern data architecture, data modeling, and data management principles Hands-on experience with modern data pipeline technologies  Spark, Flink, Airflow, Beam, etc.  Solid foundation in data structures, algorithms, and software design Expertise with Relational and NoSQL databases to help teams best organize their data for analysis Skilled in OOP design and development, preferably in Python Advanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL BS/BA degree in Computer Science, Engineering or related fields or equivalent experience 5+ years of experience with building end-to-end scalable production-grade data pipelines In-depth knowledge of data warehousing and master data management Expertise with modern data architecture, data modeling, and data management principles Hands-on experience with modern data pipeline technologies  Spark, Flink, Airflow, Beam, etc.  Solid foundation in data structures, algorithms, and software design Expertise with Relational and NoSQL databases to help teams best organize their data for analysis Skilled in OOP design and development, preferably in Python Advanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL BS/BA degree in Computer Science, Engineering or related fields or equivalent experience","5+ years of experience with building end-to-end scalable production-grade data pipelines In-depth knowledge warehousing and master management Expertise modern architecture, modeling, principles Hands-on pipeline technologies Spark, Flink, Airflow, Beam, etc. Solid foundation in structures, algorithms, software design Relational NoSQL databases to help teams best organize their for analysis Skilled OOP development, preferably Python Advanced writing, understanding tuning PL/SQL and/or T-SQL BS/BA degree Computer Science, Engineering or related fields equivalent","5+ years experience building end-to-end scalable production-grade data pipelines In-depth knowledge warehousing master management Expertise modern architecture, modeling, principles Hands-on pipeline technologies Spark, Flink, Airflow, Beam, etc. Solid foundation structures, algorithms, software design Relational NoSQL databases help teams best organize analysis Skilled OOP development, preferably Python Advanced writing, understanding tuning PL/SQL and/or T-SQL BS/BA degree Computer Science, Engineering related fields equivalent"
181,Data Engineer,Data Engineer,"Austin, TX 78746",Austin,TX,"What youâll be called: Data Engineer

Where youâll work: KWRI HeadquartersâAustin, TX

Named a Happiest Company to Work for in 2019; one of the Best Places to Work in Austin, TX; and featured on the Training Magazine Training 125 list seven times, Keller Williams Realty International (KWRI) thrives within a creative and collaborative culture where transforming the real estate industry through technology is our primary goal.

KW Technology is the foremost provider of real estate solutions, offering the most comprehensive end-to-end portfolio of products, services and training in the industry. Our Data Engineering team converts agent and consumer challenges into intuitive, insight-enhanced technology and consumer experiences using tools such as Python, Hadoop, Spark, MySQL, MongoDB and Snaplogic.

What youâll do:

Design, develop and implement data infrastructure and best-in-class pipelines that collect, connect, centralize and curate data from various internal and external data sources. You will ensure that architectures support the needs of the business, and recommend ways to improve data reliability, efficiency.

Essential Duties and Responsibilities:

Design, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources
Participate in data architecture discussions to understand target data structures, required data transformations and deliver data pipelines/ETL loading processes that meet requirements.
Perform detailed exploration of new internal and external source data to perform source-to-target mapping to inform the development of new data pipelines/flows.
Work in close collaboration with your data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.
Investigate the root cause of data-related issues and implement viable, sustainable solutions to correct issues.
Perform database administration activities such as refreshes, updates, migrations, etc. in support of data pipeline maintenance.
Minimum Qualifications:

Bachelorâs degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.
3 or more years of experience as a data engineer on enterprise-level data solutions, specifically as a Data Engineer or ETL Developer.
2 or more years of experience working with relational and unstructured databases and enterprise data warehouses, such as work with MySQL, PostgreSQL, MongoDB, SQL Server, or Oracle.
Experience with Spark, Presto, Hive and/or other map/reduce ""big data"" systems and services.
Experience in SQL and Python for scripting automation.
Preferred Qualifications:

Masterâs degree in Information Management, Data Science, Analytics or related field.
Experience building open source data pipeline systems such as AirFlow, Hadoop or Kafka.
Familiar working in a Cloud environment (AWS or GCP) with a subset of the following tools or their equivalent - Redshift, RDS, S3, EC2, Lambda, Kinesis, Elasticsearch, EMR, BigQuery, GCS.
Who are we?

Keller Williams Realty Inc. is the largest real estate company by agent count across the globe and is number one in units and volume in the United States. Founded in 1983, we pride ourselves on an agent-centric, technology-driven and education-based culture that rewards agents as stakeholders. Keller Williams Realty International (KWRI), is the companyâs corporate headquarters located in Austin, TX. Here, through a focus on cutting edge technology, education, and products and services, we support our agents and associates to create careers worth having, businesses worth owning, lives worth living, experiences worth giving and legacies worth leaving."," Bachelorâs degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience. 3 or more years of experience as a data engineer on enterprise-level data solutions, specifically as a Data Engineer or ETL Developer. 2 or more years of experience working with relational and unstructured databases and enterprise data warehouses, such as work with MySQL, PostgreSQL, MongoDB, SQL Server, or Oracle. Experience with Spark, Presto, Hive and/or other map/reduce ""big data"" systems and services. Experience in SQL and Python for scripting automation.    Design, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources Participate in data architecture discussions to understand target data structures, required data transformations and deliver data pipelines/ETL loading processes that meet requirements. Perform detailed exploration of new internal and external source data to perform source-to-target mapping to inform the development of new data pipelines/flows. Work in close collaboration with your data-minded colleagues focused on back-end  microservice  development, business intelligence reporting, machine learning and artificial intelligence models. Investigate the root cause of data-related issues and implement viable, sustainable solutions to correct issues. Perform database administration activities such as refreshes, updates, migrations, etc. in support of data pipeline maintenance.   ","Bachelorâs degree in Computer Science, Information Management, Data Analytics or related field equivalent experience. 3 more years of experience as a data engineer on enterprise-level solutions, specifically Engineer ETL Developer. 2 working with relational and unstructured databases enterprise warehouses, such work MySQL, PostgreSQL, MongoDB, SQL Server, Oracle. Experience Spark, Presto, Hive and/or other map/reduce ""big data"" systems services. Python for scripting automation. Design, develop, implement infrastructure pipelines that collect, connect, centralize, curate from various internal external sources Participate architecture discussions to understand target structures, required transformations deliver pipelines/ETL loading processes meet requirements. Perform detailed exploration new source perform source-to-target mapping inform the development pipelines/flows. Work close collaboration your data-minded colleagues focused back-end microservice development, business intelligence reporting, machine learning artificial models. Investigate root cause data-related issues viable, sustainable solutions correct issues. database administration activities refreshes, updates, migrations, etc. support pipeline maintenance.","Bachelorâs degree Computer Science, Information Management, Data Analytics related field equivalent experience. 3 years experience data engineer enterprise-level solutions, specifically Engineer ETL Developer. 2 working relational unstructured databases enterprise warehouses, work MySQL, PostgreSQL, MongoDB, SQL Server, Oracle. Experience Spark, Presto, Hive and/or map/reduce ""big data"" systems services. Python scripting automation. Design, develop, implement infrastructure pipelines collect, connect, centralize, curate various internal external sources Participate architecture discussions understand target structures, required transformations deliver pipelines/ETL loading processes meet requirements. Perform detailed exploration new source perform source-to-target mapping inform development pipelines/flows. Work close collaboration data-minded colleagues focused back-end microservice development, business intelligence reporting, machine learning artificial models. Investigate root cause data-related issues viable, sustainable solutions correct issues. database administration activities refreshes, updates, migrations, etc. support pipeline maintenance."
182,Data Engineer,Data Engineer,"Austin, TX",Austin,TX,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ","Expertise in at least one of the following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing more languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting other customer-facing role","Expertise least one following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role"
183,Data Engineer,"Data Engineer, Research","Austin, TX",Austin,TX,"The Research group at Dimensional is essential both in the successful daily functioning of the firm and helping develop Dimensionalâs long-term strategy. The team produces high-quality, expert research on investments and financial markets that is of interest to and helps educate clients. Research is also involved in the design of the firmâs investment approach and the application of that approach through portfolio management and trading.
This position is responsible for the management and development of our proprietary global security-level database. This database is used by the Research team to enhance our understanding of long-term, short-term and intra-day drivers of expected returns and to help develop new investment strategies to meet the needs, goals and preferences of our clients. It will be primarily project-based work, extending existing code infrastructure and creating new ones. This role will be based in our Austin, TX headquarters and reports to a senior member of our Research team.
Responsibilities:
Design, compile and manage efficiently large financial databases from a variety of financial data vendors.
Develop sophisticated code in object-oriented programming languages such as C#, C++ or Python for historical portfolio simulations and tools for investment and performance analysis.
Conduct data analysis for projects related to research on equities and fixed income markets, retirement research, and other investment research.
Qualifications:
Bachelorâs or Masterâs in Computer Science, Computer Engineering, Electrical Engineering, Computer Information Systems, MIS, or relevant technology degree from a top-tier school.
Minimum of 5+ years SQL Server Development experience preferred.
Demonstrate knowledge and ability of database development skills including physical structure, overall architecture, and database analysis.
Solid software development skills in an object-oriented programming language (C#, C++, Java, etc.).
Programming skills with statistical software including Python, R, SAS or Matlab a plus.
Proven database design and implementation experience with the ability to provide end-to-end database solutions and resolve complex database issues.
Experience in database query optimization, performance tuning, and monitoring.
Expert knowledge of best practices in database design.
Strong time management skills with the ability to participate in multiple projects/work streams simultaneously.
Detail-oriented, organized, highly motivated and able to work independently and in a team environment.
Excellent verbal and written communications skills.
Self-starter who is capable of managing multiple projects and meeting deadlines.
Experience with research in securities and financial markets preferred.
Knowledge of finance/asset pricing is preferred.
It is the policy of the Company to provide equal employment opportunity for all applicants and employees. The Company does not unlawfully discriminate on the basis of race, color, religion, creed, sex, gender, gender identity, gender expression, national origin, age, disability, genetic information, ancestry, medical condition, marital status, covered veteran status, citizenship status, sexual orientation, or any other protected status. This policy applies to all areas of employment including recruitment, hiring, training, job assignment, promotion, compensation, benefits, transfer, discipline, termination, and social and recreational programs.","Bachelorâs or Masterâs in Computer Science, Computer Engineering, Electrical Engineering, Computer Information Systems, MIS, or relevant technology degree from a top-tier school. Minimum of 5+ years SQL Server Development experience preferred. Demonstrate knowledge and ability of database development skills including physical structure, overall architecture, and database analysis. Solid software development skills in an object-oriented programming language  C , C++, Java, etc. . Programming skills with statistical software including Python, R, SAS or Matlab a plus. Proven database design and implementation experience with the ability to provide end-to-end database solutions and resolve complex database issues. Experience in database query optimization, performance tuning, and monitoring. Expert knowledge of best practices in database design. Strong time management skills with the ability to participate in multiple projects/work streams simultaneously. Detail-oriented, organized, highly motivated and able to work independently and in a team environment. Excellent verbal and written communications skills. Self-starter who is capable of managing multiple projects and meeting deadlines. Experience with research in securities and financial markets preferred. Knowledge of finance/asset pricing is preferred.   Design, compile and manage efficiently large financial databases from a variety of financial data vendors. Develop sophisticated code in object-oriented programming languages such as C , C++ or Python for historical portfolio simulations and tools for investment and performance analysis. Conduct data analysis for projects related to research on equities and fixed income markets, retirement research, and other investment research.   ","Bachelorâs or Masterâs in Computer Science, Engineering, Electrical Information Systems, MIS, relevant technology degree from a top-tier school. Minimum of 5+ years SQL Server Development experience preferred. Demonstrate knowledge and ability database development skills including physical structure, overall architecture, analysis. Solid software an object-oriented programming language C , C++, Java, etc. . Programming with statistical Python, R, SAS Matlab plus. Proven design implementation the to provide end-to-end solutions resolve complex issues. Experience query optimization, performance tuning, monitoring. Expert best practices design. Strong time management participate multiple projects/work streams simultaneously. Detail-oriented, organized, highly motivated able work independently team environment. Excellent verbal written communications skills. Self-starter who is capable managing projects meeting deadlines. research securities financial markets Knowledge finance/asset pricing Design, compile manage efficiently large databases variety data vendors. Develop sophisticated code languages such as C++ Python for historical portfolio simulations tools investment Conduct analysis related on equities fixed income markets, retirement research, other research.","Bachelorâs Masterâs Computer Science, Engineering, Electrical Information Systems, MIS, relevant technology degree top-tier school. Minimum 5+ years SQL Server Development experience preferred. Demonstrate knowledge ability database development skills including physical structure, overall architecture, analysis. Solid software object-oriented programming language C , C++, Java, etc. . Programming statistical Python, R, SAS Matlab plus. Proven design implementation provide end-to-end solutions resolve complex issues. Experience query optimization, performance tuning, monitoring. Expert best practices design. Strong time management participate multiple projects/work streams simultaneously. Detail-oriented, organized, highly motivated able work independently team environment. Excellent verbal written communications skills. Self-starter capable managing projects meeting deadlines. research securities financial markets Knowledge finance/asset pricing Design, compile manage efficiently large databases variety data vendors. Develop sophisticated code languages C++ Python historical portfolio simulations tools investment Conduct analysis related equities fixed income markets, retirement research, research."
184,Data Engineer,Tech Consulting Manager - Big Data Engineer,"Austin, TX 78701",Austin,TX,"EY delivers unparalleled service in big data, business intelligence, and digital analytics built on a blend of custom-developed methods related to customer analytics, data visualization, and optimization. We leverage best practices and a high degree of business acumen that has been compiled over years of experience to ensure the highest level of execution and satisfaction for our clients. At EY, our methods are not tied to any specific platforms but rather arrived at by analyzing business needs and making sure that the solutions delivered meet all client goals.
The opportunity
You will help our clients navigate the complex world of modern data analytics. Weâll look to you to provide our clients with a unique business perspective on how Big Data analytics can transform and improve their entire organization - starting with key business issues they face. This is a high growth, high visibility area with plenty of opportunities to enhance your skillset and build your career.
Your key responsibilities
Youâll spend most of your time working with a wide variety of clients to deliver the latest big data technologies and practices to design, build and maintain scalable and robust solutions that unify, enrich and analyse data from multiple sources.
Skills and attributes for success
Designing, Architecting, and Developing solutions leveraging big data technology (Open Source, AWS, or Microsoft) to ingest, process and analyze large, disparate data sets to exceed business requirements
Unifying, enriching, and analyzing customer data to derive insights and opportunities
Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements
Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions
Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches
Seeking out information to learn about emerging methodologies and technologies
Clarifying problems by driving to understand the true issue
Looking for opportunities for improving methods and outcomes
Applying data driven approach (KPIs) in tying technology solutions to specific business outcomes
Collaborating, influencing and building consensus through constructive relationships and effective listening
Solving problems by incorporating data into decision making
To qualify for the role you must have
A bachelor's degree and approximately six years of related work experience; or a master's degree and approximately five years of related work experience
At least five years hands-on experience with various Big Data technologies in one or more ecosystems: Open Source, Microsoft, or AWS:
Hadoop, Spark, NoSQL, Streaming, Atlas, Sqoop, HIVE
AWS, EMR, Hortonworks, Cassandra, Mongo, Redshift, Kafka
Azure, HDInsight, Azure DocumentDB, SQL Server
Proficiency coding in Java, C#, C++, or Scala
Experienced organizing, aggregating, querying, and analyzing large data sets
Communication is essential, must be able to listen and understand the question and develop and deliver clear insights.
Outstanding team player.
Independent and able to manage and prioritize workload.
Ability to quickly and positively adapt to change.
A valid driverâs license in the US; willingness and ability to travel to meet client needs.
Ideally, youâll also have
Bachelorâs Degree or above in mathematics, information systems, statistics, computer science, or related disciplines
What we look for
Weâre interested in passionate leaders with strong vision and a desire to stay on top of trends in the Big Data industry. If you have a genuine passion for helping businesses achieve the full potential of their data, this role is for you.
What working at EY offers
We offer a competitive compensation package where youâll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, both pension and 401(k) plans, a minimum of 15 days of vacation plus ten observed holidays and three paid personal days, and a range of programs and benefits designed to support your physical, financial and social well-being. Plus, we offer:
Opportunities to develop new skills and progress your career
A collaborative environment where everyone works together to create a better working world
Excellent training and development prospects, both through established programs and on-the-job training
About EY
As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.
Join us in building a better working world. Apply now.

EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status."," Designing, Architecting, and Developing solutions leveraging big data technology  Open Source, AWS, or Microsoft  to ingest, process and analyze large, disparate data sets to exceed business requirements Unifying, enriching, and analyzing customer data to derive insights and opportunities Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches Seeking out information to learn about emerging methodologies and technologies Clarifying problems by driving to understand the true issue Looking for opportunities for improving methods and outcomes Applying data driven approach  KPIs  in tying technology solutions to specific business outcomes Collaborating, influencing and building consensus through constructive relationships and effective listening Solving problems by incorporating data into decision making    ","Designing, Architecting, and Developing solutions leveraging big data technology Open Source, AWS, or Microsoft to ingest, process analyze large, disparate sets exceed business requirements Unifying, enriching, analyzing customer derive insights opportunities Leveraging in-house platforms as needed recommending building new platforms/solutions required Clearly communicating findings, recommendations, improve systems Demonstrating deep understanding of technology, concepts, tools, features, functions benefits different approaches Seeking out information learn about emerging methodologies technologies Clarifying problems by driving understand the true issue Looking for improving methods outcomes Applying driven approach KPIs in tying specific Collaborating, influencing consensus through constructive relationships effective listening Solving incorporating into decision making","Designing, Architecting, Developing solutions leveraging big data technology Open Source, AWS, Microsoft ingest, process analyze large, disparate sets exceed business requirements Unifying, enriching, analyzing customer derive insights opportunities Leveraging in-house platforms needed recommending building new platforms/solutions required Clearly communicating findings, recommendations, improve systems Demonstrating deep understanding technology, concepts, tools, features, functions benefits different approaches Seeking information learn emerging methodologies technologies Clarifying problems driving understand true issue Looking improving methods outcomes Applying driven approach KPIs tying specific Collaborating, influencing consensus constructive relationships effective listening Solving incorporating decision making"
185,Data Engineer,AWS Data Engineer,"Austin, TX 78727",Austin,TX,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
186,Data Engineer,Sr. Data Engineer,"Austin, TX 78728",Austin,TX,"Job Title: Data Engineer

Location: San Francisco, Chicago, San Jose, Palo Alto, Austin, TX

Terms: Full-time, Contract, Contract-2-Hire

About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.

What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.

As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do

Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:

Cloud
Analytics
Digitization
Infrastructure
Security

Sr. Data Engineer
Job Description
Responsibilities
Ingestion of data from multiple, unstructured sources using multiple analytics tools
Implementing ETL process
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies

Requirements
3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct query performance tuning
Strong skills in one of the scripting language (Python, Ruby, Bash)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)

We are Growing Rapidly: 2019 Highlights

Trianz is growing rapidly. Here are some highlights.

Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.

Won the âCustomer Obsession Awardâ from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.

Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.

Featured by IDC in their Spotlight series under the theme of âOperationalizing Strategies through Execution Excellence: A New Paradigms in Technology Deliveryâ.

Achieved 50%+ revenue and employee growth compared to prior yearâs exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.

Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is whatâs fundamental for everyone at Trianz.
 We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
 Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).","   Ingestion of data from multiple, unstructured sources using multiple analytics tools Implementing ETL process Monitoring performance and advising any necessary infrastructure changes Defining data retention policies   Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.","Ingestion of data from multiple, unstructured sources using multiple analytics tools Implementing ETL process Monitoring performance and advising any necessary infrastructure changes Defining retention policies Voted significantly above other services firms by 90% + clients for business impact, execution predictability organizational commitment in the recent Trianz wide client satisfaction survey.","Ingestion data multiple, unstructured sources using multiple analytics tools Implementing ETL process Monitoring performance advising necessary infrastructure changes Defining retention policies Voted significantly services firms 90% + clients business impact, execution predictability organizational commitment recent Trianz wide client satisfaction survey."
187,Data Engineer,Staff Data Engineer,"Austin, TX",Austin,TX,"At SailPoint, we do things differently. We understand that a fun-loving work environment can be highly motivating and productive. When smart people work on intriguing problems, and they enjoy coming to work each day, they accomplish great things together. With that philosophy, weâve assembled the best identity team in the world that is passionate about the power of identity.
As the fastest-growing, independent identity and access management (IAM) provider, SailPoint helps hundreds of global organizations securely and effectively deliver and manage user access from any device to data and applications residing in the data center, on mobile devices, and in the cloud. The companyâs innovative product portfolio offers customers an integrated set of core services including identity governance, provisioning, and access management delivered on-premises or from the cloud (IAM-as-a-service).
SailPoint is seeking a Sr/Staff Data Software Engineer to help build a new cloud-based identity analytics product incorporating real-time data pipelines, machine learning algorithms and multi-tenancy support. We are looking for well-rounded backend or full stack engineers who are passionate about building and delivering reliable, scalable microservices and infrastructure for SaaS products.
Responsibilities
Collaborate with peers on requirements, designs, code reviews, and testing
Produce designs and rough estimates, and implement features based on product requirements
Deliver efficient, maintainable, robust Java/Scala based microservices
Produce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features
Productize and operationalize machine learning algorithms
Actively engage in technology discovery that can be applied to the product
Requirements
7-10 years of professional software development experience
2+ years of data engineering or related experience
Strong Java and/or Scala experience
Experience with Agile development practices and continuous delivery
Proficient understanding of distributed computing principles. microservice architectures and patterns
Experience with integration of data from multiple data sources
Experience writing unit and integration tests
Great communication skills
BS in Computer Science or a related experience
Preferred
Experience with Cloud computing architectures (AWS, Google Cloud)
Experience with Kafka, Flink/Spark, Elasticsearch technologies or related
Experience integrating data pipelines for machine learning
Experience with container technologies (Docker, Kubernetes, etc.)
Experience with NoSQL databases, such as Redshift, Cassandra, DynamoDB
Experience instrumenting code for gathering production performance metrics
Compensation and benefits
Experience a Small-company Atmosphere with Big-company Benefits
Competitive pay, 401(k) and comprehensive medical, dental and vision plans
Recharge your batteries with a flexible vacation policy and paid holidays
Grow with us with both technical and career growth opportunities
Enjoy a healthy work-life balance with flexible hours, family-friendly company events and charitable work
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.","  Collaborate with peers on requirements, designs, code reviews, and testing Produce designs and rough estimates, and implement features based on product requirements Deliver efficient, maintainable, robust Java/Scala based microservices Produce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features Productize and operationalize machine learning algorithms Actively engage in technology discovery that can be applied to the product   7-10 years of professional software development experience 2+ years of data engineering or related experience Strong Java and/or Scala experience Experience with Agile development practices and continuous delivery Proficient understanding of distributed computing principles. microservice architectures and patterns Experience with integration of data from multiple data sources Experience writing unit and integration tests Great communication skills BS in Computer Science or a related experience ","Collaborate with peers on requirements, designs, code reviews, and testing Produce designs rough estimates, implement features based product requirements Deliver efficient, maintainable, robust Java/Scala microservices unit end-to-end tests to improve quality maximize coverage for new existing Productize operationalize machine learning algorithms Actively engage in technology discovery that can be applied the 7-10 years of professional software development experience 2+ data engineering or related Strong Java and/or Scala Experience Agile practices continuous delivery Proficient understanding distributed computing principles. microservice architectures patterns integration from multiple sources writing Great communication skills BS Computer Science a","Collaborate peers requirements, designs, code reviews, testing Produce designs rough estimates, implement features based product requirements Deliver efficient, maintainable, robust Java/Scala microservices unit end-to-end tests improve quality maximize coverage new existing Productize operationalize machine learning algorithms Actively engage technology discovery applied 7-10 years professional software development experience 2+ data engineering related Strong Java and/or Scala Experience Agile practices continuous delivery Proficient understanding distributed computing principles. microservice architectures patterns integration multiple sources writing Great communication skills BS Computer Science"
188,Data Engineer,Data Engineer - Associate,"Austin, TX",Austin,TX,"Data Engineer (early career)
Austin or Chicago
(Visa sponsorship not currently offered)

Mattersight is a leader in enterprise analytics focused on customer and employee interactions and behaviors. Mattersight's Behavioral Analytics service captures and analyzes customer and employee interactions, employee desktop data, and other contextual information to improve operational performance and predict future customer and employee outcomes. Mattersightâs analytics are based on millions of proprietary algorithms and the application of unique behavioral models. The company's SaaS+ delivery model combines analytics in the cloud with deep customer partnerships to drive significant business value. Mattersight's applications are used by leading companies in Healthcare, Insurance, Financial Services, Telecommunications, Cable, Utilities and Government. See What Mattersâ¢ by visiting www.Mattersight.com.

Data Engineer Role & Responsibilities:
The Data Engineer will be part of the Routing Analytics R&D team, which provides the data sources and analytical tools used to help our clients derive maximum value from our Behavioral Routing solution. This position will support the day to day reporting needs of the Routing clients, including Business Monitoring, Insights, Product Development, Analysis & Testing, and others. Responsibilities include troubleshooting issues that occur with existing report deliverables as well as developing new reports and integrating them into the overall service catalog. This role will require development, testing, and configuration management of all BI deliverables in coordination with Data Engineers, Software Engineers, and Testers within the organization. Additionally, the Data Engineer will be responsible for maintaining any documentation and training materials required to support the various business units the group serves.

This individual will also support the Data Warehouse Specialist to troubleshoot issues relating to the warehouse, especially as they impact the reporting environment.

To summarize, the Data Engineer will be responsible for, but not limited to, the following tasks:

Troubleshoot and resolve issues as they arise related to all BI Tools
Manage iteration and release cycles and deployments
Assist in data modeling and design sessions
Proactively maintain documentation and training materials

Because of the data-centric culture and rapid growth of NICE Mattersight, a rich career path exists for the Data Engineer within Mattersight.

Preferred Skills/Attributes
Experience with RDBMS applications (SQL Server preferred)
Good communication skills and experience working with cross-functional teams
Exposure to the concepts of data warehouse design
SQL programming familiarity in large RDBMS systems (T-SQL preferred)
Exposure to ETL and data integration processes

Required Knowledge, Skills & Abilities

Previous Experience
For this role, Mattersight is not requiring previous work experience in a technical role though some experience in a Business Intelligence environment working with BI visualization tools, relational databases, and/or data warehousing systems is helpful. Experience with ETL applications and data modeling/UML software is also helpful. Required is an interest in data pipelines, data aggregations, and creatively solving data-related problems as well as the motivation to dive deeper into the data engineering world. Weâd also like to see a candidate who displays evidence of strong communication skills and the ability to work under pressure.
Ideal CandidateAnalyticalDetail OrientedStrong CommunicatorEntrepreneurialResults OrientedTask AgilityOperations-MindedAdept Time ManagerProblem SolverTeam PlayerSeeker of ExcellenceHigh Knowledge Bandwidth

Company Culture & Facts
Corporate Culture
Mattersight values diversity amongst its employees. Employees from all levels of experience and backgrounds are mingled together and are encouraged to learn about projects others are working on. Mattersight fosters teamwork as well as self motivation.
Eligibility & Location
Mattersight seeks candidates authorized to work in the United States. The Routing Analytics Data Engineer role will be based out of Mattersightâs Austin location; candidates should anticipate little to no travel.
Compensation
Mattersight is prepared to offer a highly competitive benefits and compensation package for the ideal candidate.
For more information about Mattersight, visit http://www.mattersight.com/. Mattersight is committed to equal opportunity and affirmative action in all employment matters: Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, ancestry, marital or domestic partner status, national origin, disability or medical condition, pregnancy, veteran or military status, sexual orientation, gender identity, or on account of membership or affiliation with anyone in any of the foregoing categories, or any other protected category under federal, state or local law. Although particular legal provisions may differ in various locations in which we do business, our principles are the same worldwide.",  Experience with RDBMS applications  SQL Server preferred  Good communication skills and experience working with cross-functional teams Exposure to the concepts of data warehouse design SQL programming familiarity in large RDBMS systems  T-SQL preferred   Troubleshoot and resolve issues as they arise related to all BI Tools Manage iteration and release cycles and deployments Assist in data modeling and design sessions Proactively maintain documentation and training materials  ,Experience with RDBMS applications SQL Server preferred Good communication skills and experience working cross-functional teams Exposure to the concepts of data warehouse design programming familiarity in large systems T-SQL Troubleshoot resolve issues as they arise related all BI Tools Manage iteration release cycles deployments Assist modeling sessions Proactively maintain documentation training materials,Experience RDBMS applications SQL Server preferred Good communication skills experience working cross-functional teams Exposure concepts data warehouse design programming familiarity large systems T-SQL Troubleshoot resolve issues arise related BI Tools Manage iteration release cycles deployments Assist modeling sessions Proactively maintain documentation training materials
189,Data Engineer,Senior Data Engineer,"Austin, TX",Austin,TX,"Job Posting Title:
Senior Data Engineer
-
Hiring Department:
IQ - Information Quest
-
Position Open To:
All Applicants
-
Weekly Scheduled Hours:
40
-
FLSA Status:
Exempt
-
Earliest Start Date:
Immediately
-
Position Duration:
Expected to Continue
-
Location:
UT MAIN CAMPUS
-
Job Description:
You will be part of a team building the next generation data warehouse platform and will design, develop, and maintain complex extract, transform, and load (ETL) data pipelines using large heterogeneous datasets. You will also build data engineering solutions for complex data models that express academic and administrative business processes. Your expertise with leading technologies and tools such as Oracle, Postgres, Python, etc. will result in a valuable modern data warehouse that supports critical business decisions and data analysis processes. Your collaboration and communication skills will help to establish stakeholder relationships and ensure that your work products are in alignment with project goals. Most importantly, you will be passionate about working with data and will be a significant contributor to our university mission: To transform lives for the benefit of society.
-
Job Details:
General Notes
Senior level technical role that builds and supports cloud-based Enterprise Data Warehouse ecosystem.
Responsibilities
Design, develop, and automate scalable data engineering solutions by leveraging cloud infrastructure. Extend or migrate existing data pipelines to new cloud environment.
Lead technical projects involving design and development of data pipelines for complex datasets. Document project plans, outline tasks and milestones, provide estimation of effort.
Work closely with business partners to devise and manage data pipelines, load frequency, data delivery mechanisms, and performance tuning.
Identify and implement best practices for data engineering and software development to ensure quality delivery of enterprise solutions.
Help enable team alignment by participating in code reviews, change management and team meetings.
Develop and maintain detailed technical documentation of data engineering solutions.
Collaborate with key stakeholders, both internal and external, including enterprise data architect, data modelers, and subject matter experts (SMEs).
Required Qualifications
Five or more years of professional experience as data engineer. Bachelorâs degree in Computer Science or equivalent experience.
Demonstrated experience in data warehousing and ETL development.
Experience building complex data pipelines using large, disparate data sources.
Demonstrated expert knowledge in SQL.
Demonstrated experience working with relational databases such as Oracle, Postgres and other modern database technologies.
Proficiency in modern programming languages such as Python, R, Java.
Thorough understanding of data movement and transformation tools, such as Informatica, Datastage or equivalent.
Demonstrated experience in selecting tools, methods, techniques, and evaluation criteria for designing optimal data engineering solutions.
Demonstrated experience in leading complex technical projects, including assigning tasks and selecting team members.
Ability to make technical presentations to teams, focus groups, management, and governance committees.
Excellent customer service, communication and collaboration skills.
Preferred Qualifications
Five years or more experience as data engineer designing and implementing complex data pipelines.
Masterâs degree in Computer Science, Information Technology or related field.
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.).
Experience with AWS technologies.
Salary Range
$105,000 + depending on qualifications
Working Conditions
May work around standard office conditions.
Repetitive use of a keyboard at a workstation.
May require occasional off-hours work.
Required Materials
Resume/CV
3 work references with their contact information; at least one reference should be from a supervisor
Letter of interest
Important for applicants who are NOT current university employees or contingent workers: You will be prompted to submit your resume in the first step of the online job application process. Then, any additional Required Materials will be uploaded in the My Experience section; you can multi-select the additional files or click the Upload button for each file. Before submitting your online job application, ensure that ALL Required Materials have been uploaded. Once your job application has been submitted, you cannot make changes.
Important for Current university employees and contingent workers: As a current university employee or contingent worker, you MUST apply within Workday by searching for Find Jobs. Before you apply though, log-in to Workday, navigate to your Worker Profile, click the Career link in the left hand navigation menu and then update the sections in your Professional Profile. This information will be pulled in to your application. The application is one page and you will need to click the Upload button multiple times in order to attach your Resume, References and any additional Required Materials noted above.
-
Employment Eligibility:
Regular staff who have been employed in their current position for the last six continuous months are eligible for openings being recruited for through University-Wide or Open Recruiting, to include both promotional opportunities and lateral transfers. Staff who are promotion/transfer eligible may apply for positions without supervisor approval.
-
Retirement Plan Eligibility:
The retirement plan for this position is Teacher Retirement System of Texas (TRS), subject to the position being at least 20 hours per week and at least 135 days in length.
-
Background Checks:
A criminal history background check will be required for finalist(s) under consideration for this position.
-
Equal Opportunity Employer:
The University of Texas at Austin, as an equal opportunity/affirmative action employer , complies with all applicable federal and state laws regarding nondiscrimination and affirmative action. The University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, or veteran status in employment, educational programs and activities, and admissions.
-
Pay Transparency:
The University of Texas at Austin will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractorâs legal duty to furnish information.
-
Employment Eligibility Verification:
If hired, you will be required to complete the federal Employment Eligibility Verification I-9 form. You will be required to present acceptable and original documents to prove your identity and authorization to work in the United States. Documents need to be presented no later than the third day of employment. Failure to do so will result in loss of employment at the university.
-
E-Verify:
The University of Texas at Austin use E-Verify to check the work authorization of all new hires effective May 2015. The universityâs company ID number for purposes of E-Verify is 854197.","Five or more years of professional experience as data engineer. Bachelorâs degree in Computer Science or equivalent experience. Demonstrated experience in data warehousing and ETL development. Experience building complex data pipelines using large, disparate data sources. Demonstrated expert knowledge in SQL. Demonstrated experience working with relational databases such as Oracle, Postgres and other modern database technologies. Proficiency in modern programming languages such as Python, R, Java. Thorough understanding of data movement and transformation tools, such as Informatica, Datastage or equivalent. Demonstrated experience in selecting tools, methods, techniques, and evaluation criteria for designing optimal data engineering solutions. Demonstrated experience in leading complex technical projects, including assigning tasks and selecting team members. Ability to make technical presentations to teams, focus groups, management, and governance committees. Excellent customer service, communication and collaboration skills.   Design, develop, and automate scalable data engineering solutions by leveraging cloud infrastructure. Extend or migrate existing data pipelines to new cloud environment. Lead technical projects involving design and development of data pipelines for complex datasets. Document project plans, outline tasks and milestones, provide estimation of effort. Work closely with business partners to devise and manage data pipelines, load frequency, data delivery mechanisms, and performance tuning. Identify and implement best practices for data engineering and software development to ensure quality delivery of enterprise solutions. Help enable team alignment by participating in code reviews, change management and team meetings. Develop and maintain detailed technical documentation of data engineering solutions. Collaborate with key stakeholders, both internal and external, including enterprise data architect, data modelers, and subject matter experts  SMEs .   ","Five or more years of professional experience as data engineer. Bachelorâs degree in Computer Science equivalent experience. Demonstrated warehousing and ETL development. Experience building complex pipelines using large, disparate sources. expert knowledge SQL. working with relational databases such Oracle, Postgres other modern database technologies. Proficiency programming languages Python, R, Java. Thorough understanding movement transformation tools, Informatica, Datastage equivalent. selecting methods, techniques, evaluation criteria for designing optimal engineering solutions. leading technical projects, including assigning tasks team members. Ability to make presentations teams, focus groups, management, governance committees. Excellent customer service, communication collaboration skills. Design, develop, automate scalable solutions by leveraging cloud infrastructure. Extend migrate existing new environment. Lead projects involving design development datasets. Document project plans, outline milestones, provide estimation effort. Work closely business partners devise manage pipelines, load frequency, delivery mechanisms, performance tuning. Identify implement best practices software ensure quality enterprise Help enable alignment participating code reviews, change management meetings. Develop maintain detailed documentation Collaborate key stakeholders, both internal external, architect, modelers, subject matter experts SMEs .","Five years professional experience data engineer. Bachelorâs degree Computer Science equivalent experience. Demonstrated warehousing ETL development. Experience building complex pipelines using large, disparate sources. expert knowledge SQL. working relational databases Oracle, Postgres modern database technologies. Proficiency programming languages Python, R, Java. Thorough understanding movement transformation tools, Informatica, Datastage equivalent. selecting methods, techniques, evaluation criteria designing optimal engineering solutions. leading technical projects, including assigning tasks team members. Ability make presentations teams, focus groups, management, governance committees. Excellent customer service, communication collaboration skills. Design, develop, automate scalable solutions leveraging cloud infrastructure. Extend migrate existing new environment. Lead projects involving design development datasets. Document project plans, outline milestones, provide estimation effort. Work closely business partners devise manage pipelines, load frequency, delivery mechanisms, performance tuning. Identify implement best practices software ensure quality enterprise Help enable alignment participating code reviews, change management meetings. Develop maintain detailed documentation Collaborate key stakeholders, internal external, architect, modelers, subject matter experts SMEs ."
190,Data Engineer,"Cloud Data Engineer, Google Professional Services","Atlanta, GA",Atlanta,GA,"Note: By applying to this position your application is automatically submitted to the following locations: Chicago, IL, USA; Atlanta, GA, USA
Minimum qualifications:

BA/BS degree in Computer Science, Mathematics or related technical field, or equivalent practical experience.
Experience with data processing software (such as Hadoop, Spark, Pig, Hive) and with data processing algorithms (MapReduce, Flume).
Experience in writing software in one or more languages such as Java, C++, Python, Go and/or JavaScript.
Experience managing internal or client-facing projects to completion; experience troubleshooting clients' technical issues; experience working with engineering teams, sales, services, and customers.

Preferred qualifications:
Experience working data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments.
Experience in technical consulting.
Experience working with big data, information retrieval, data mining or machine learning as well as experience in building multi-tier high availability applications with modern web technologies (such as NoSQL, MongoDB, SparkML, Tensorflow).
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments.
About the job
As a Cloud Data Engineer, you'll guide customers on how to ingest, store, process, analyze and explore/visualize data on the Google Cloud Platform. You will work on data migrations and transformational projects, and with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform challenges.
In this role you are the Google Engineer working with Google's most strategic Cloud customers. Together with the team you will support customer implementation of Google Cloud products through: architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more.
The Google Cloud Platform team helps customers transform and evolve their business through the use of Googleâs global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of businesses of all sizes use technology to connect with customers, employees and partners.
Responsibilities
Act as a trusted technical advisor to customers and solve complex Big Data challenges.
Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.
Travel up to 30% of the time.
Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.
At Google, we donât just accept differenceâwe celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.","   Act as a trusted technical advisor to customers and solve complex Big Data challenges. Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders. Travel up to 30% of the time. Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.  ","Act as a trusted technical advisor to customers and solve complex Big Data challenges. Create deliver best practices recommendations, tutorials, blog articles, sample code, presentations adapting different levels of key business stakeholders. Travel up 30% the time. Communicate effectively via video conferencing for meetings, reviews onsite delivery activities.","Act trusted technical advisor customers solve complex Big Data challenges. Create deliver best practices recommendations, tutorials, blog articles, sample code, presentations adapting different levels key business stakeholders. Travel 30% time. Communicate effectively via video conferencing meetings, reviews onsite delivery activities."
191,Data Engineer,AWS Data Engineer,"Atlanta, GA 30303",Atlanta,GA,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
192,Data Engineer,Senior Big Data Engineer,"Atlanta, GA",Atlanta,GA,"Location: San Jose, CA or Atlanta, GA
For over 10 years, Zscaler has been disrupting and transforming the security industry. Our 100% purpose built cloud platform delivers the entire gateway security stack as a service through 150 global data centers to securely connect users to their applications, regardless of device, location, or network in over 185 countries protecting over 3,500 companies and 100 Million threats detected a day.
We work in a fast paced, dynamic and make it happen culture. Our people are some of the brightest and passionate in the industry that thrive on being the first to solve problems. We are always looking to hire highly passionate, collaborative and humble people that want to make a difference.
As a Big Data Engineer, you will work on building the next generation of Zscaler's security analytics platform. You will play a crucial role in building a platform to collect and ingest several billion (and growing) log events from Zscaler's globally distributed security infrastructure and provide actionable insights to customers and Zscaler's security researchers.
Responsibilities:
You will design and create multi-tenant systems capable of loading and transforming a large volume of structured and semi-structured fast moving data
Build robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external users
Build Data Pipelines
Run ETL into Hadoop/Elastic Search
Required:
3+ years of experience in Python or Java development a must (Strong Scala would skills would be acceptable as well)
3+ years experience in application big data development (Spark, Kafka, Storm, Kinesis, & building data pipelines)
Ability to troubleshoot and find complex performance issues with queries on the Spark platform (Spark SQL)
Familiarity with implementing services following REST model
Excellent interpersonal, technical and communication skills
Ability to learn, evaluate and adopt new technologies
Bachelor's Degree in computer science or equivalent experience
Highly Desirable:
Experience working with data processing infrastructure
Experience with data serialization techniques and data stores for persisting events
What You Can Expect From Us:
An environment where you will be working on cutting edge technologies and architectures
A fun, passionate and collaborative workplace
Competitive salary and benefits, including equity
The pace and excitement of working for a Silicon Valley Unicorn
Why Zscaler?
People who excel at Zscaler are smart, motivated and share our values. Ask yourself: Do you want to team with the best talent in the industry? Do you want to work on disruptive technology? Do you thrive in a fluid work environment? Do you appreciate a company culture that enables individual and group success and celebrates achievement?

If you said yes, weâd love to talk to you about joining our award-winning team!
Learn more at zscaler.com or follow us on Twitter @zscaler. Additional information about Zscaler (NASDAQ : ZS ) is available at http://www.zscaler.com. All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability.
#LI-JM1",   You will design and create multi-tenant systems capable of loading and transforming a large volume of structured and semi-structured fast moving data Build robust and scalable data infrastructure  both batch processing and real-time  to support needs from internal and external users Build Data Pipelines Run ETL into Hadoop/Elastic Search  ,You will design and create multi-tenant systems capable of loading transforming a large volume structured semi-structured fast moving data Build robust scalable infrastructure both batch processing real-time to support needs from internal external users Data Pipelines Run ETL into Hadoop/Elastic Search,You design create multi-tenant systems capable loading transforming large volume structured semi-structured fast moving data Build robust scalable infrastructure batch processing real-time support needs internal external users Data Pipelines Run ETL Hadoop/Elastic Search
193,Data Engineer,Advanced Data Engineer,"Atlanta, GA 30309",Atlanta,GA,"Invesco is one of the world's leading global investment managers, entrusted with managing $1.2 trillion* in assets on behalf of clients worldwide. We are the 6th largest US retail asset manager and the 13th largest investment manager globally, and our more than 8,000 employees worldwide are dedicated to delivering an investment experience that helps people get more out of life. We are purely focused on managing a comprehensive range of active, passive and alternative investment capabilities, which we draw on to provide customized solutions aligned to client needs, our most important benchmark. (*As of May 31, 2019)

Job Purpose (Job Summary):

Weâre seeking an Advanced Data Engineer to join a fast-paced agile development team building-out enterprise grade data platforms that support Client Experience, Regulatory, and investment teams. Youâll be a part of a dynamic, collaborative team that wants to hear your input because you know the leading methods, tools, and theories in data engineering. The focus of this role will be to source data sets, both internal and external, that support our business clients. Youâll be working alongside our data science and machine learning teams leveraging the Invesco enterprise data lake architecture. Candidates will be expected to ingest, curate, and provide access to structured and unstructured data sets.

Key Responsibilities / Duties:
Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies
Responsible to collect, process, and compute business metrics from activity & persisted data using Python/Spark
Process, cleanse, and verify the integrity of data used for analysis; optimize data for consumption
Build scalable OLAP backend storage for data in PB scale
Develop data set processes for data discovery, modeling, mining, and archival
Work Experience / Knowledge:
2+ years of experience focused in big data engineering
2+ years of experience with ETL/SQL design build and tuning
Experience with data services using Amazon Web Services (AWS), Azure, and / or Google Cloud
Expertise in at least one of the following areas:
Experience with parallel computing, batch processing, and stream processing using tools such as Kinesis or Kafka
Experience with data processing technologies such as Hadoop, Spark, Hive / Pig, and Java / MapReduce
Experience with data warehousing and columnar databases such as Redshift as well as NoSQL databases such as AWS S3, MongoDB, Cassandra, HBase, DynamoDB
Experience with programming languages such as Java, Python or Scala
Experience with micro-services based architecture and design/build of RESTful APIâs is a plus
Experience operationalizing data sourcing/loading including automating/scheduling data ingestion
Familiar with Agile software development (Scrum is a plus)
DevOps knowledge is a plus
Skills / Other Personal Attributes Required:
Strong analytical and critical thinking skills
Autonomous personality. Weâll help guide you, but we wonât micromanage you. We expect integrity and results. Your work and deliverables will speak for themselves
Strong written and verbal communication skills
Enjoy challenging and thought-provoking work and have a strong desire to learn and progress
Ability to manage multiple tasks and requests
Must demonstrate a positive, team-focused attitude
Ability to react positively under pressure to meet tight deadlines
You listen to the input of your team members and take diverse perspectives into account to approach challenges from multiple angles
Structured, disciplined approach to work, with attention to detail
Flexible â able to meet changing requirements and priorities
Maintenance of up-to-date knowledge in the appropriate technical areas
Able to work in a global, multicultural environment
Formal Education: (minimum requirement to perform job duties)
Bachelorâs or Master's Degree of Statistics, Computer Science or other similar advanced degrees from a top tier educational institution preferred
Working Conditions:
Normal office environment with little exposure to noise, dust and temperatures
The ability to lift, carry or otherwise move objects of up to 10 pounds is also necessary
Normally works a regular schedule of hours, however hours may vary depending upon the project or assignment
Hours may include evenings and/or weekends and may include 24 hour a day on call support by pager and/or cell phone
Able and willing to travel both domestically and internationally. Frequency and duration to be determined by manager. Estimate: 10-15%
FLSA (US Only): Nonexempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.

Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment."," Strong analytical and critical thinking skills Autonomous personality. Weâll help guide you, but we wonât micromanage you. We expect integrity and results. Your work and deliverables will speak for themselves Strong written and verbal communication skills Enjoy challenging and thought-provoking work and have a strong desire to learn and progress Ability to manage multiple tasks and requests Must demonstrate a positive, team-focused attitude Ability to react positively under pressure to meet tight deadlines You listen to the input of your team members and take diverse perspectives into account to approach challenges from multiple angles Structured, disciplined approach to work, with attention to detail Flexible â able to meet changing requirements and priorities Maintenance of up-to-date knowledge in the appropriate technical areas Able to work in a global, multicultural environment Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Responsible to collect, process, and compute business metrics from activity & persisted data using Python/Spark Process, cleanse, and verify the integrity of data used for analysis; optimize data for consumption Build scalable OLAP backend storage for data in PB scale Develop data set processes for data discovery, modeling, mining, and archival  Bachelorâs or Master's Degree of Statistics, Computer Science or other similar advanced degrees from a top tier educational institution preferred ","Strong analytical and critical thinking skills Autonomous personality. Weâll help guide you, but we wonât micromanage you. We expect integrity results. Your work deliverables will speak for themselves written verbal communication Enjoy challenging thought-provoking have a strong desire to learn progress Ability manage multiple tasks requests Must demonstrate positive, team-focused attitude react positively under pressure meet tight deadlines You listen the input of your team members take diverse perspectives into account approach challenges from angles Structured, disciplined work, with attention detail Flexible â able changing requirements priorities Maintenance up-to-date knowledge in appropriate technical areas Able global, multicultural environment Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Responsible collect, process, compute business metrics activity & persisted Python/Spark Process, cleanse, verify used analysis; optimize consumption scalable OLAP backend storage PB scale Develop set processes discovery, modeling, mining, archival Bachelorâs Master's Degree Statistics, Computer Science similar advanced degrees top tier educational institution preferred","Strong analytical critical thinking skills Autonomous personality. Weâll help guide you, wonât micromanage you. We expect integrity results. Your work deliverables speak written verbal communication Enjoy challenging thought-provoking strong desire learn progress Ability manage multiple tasks requests Must demonstrate positive, team-focused attitude react positively pressure meet tight deadlines You listen input team members take diverse perspectives account approach challenges angles Structured, disciplined work, attention detail Flexible â able changing requirements priorities Maintenance up-to-date knowledge appropriate technical areas Able global, multicultural environment Build robust data pipelines public Cloud using AWS Kinesis, Kafka, Lambda technologies Responsible collect, process, compute business metrics activity & persisted Python/Spark Process, cleanse, verify used analysis; optimize consumption scalable OLAP backend storage PB scale Develop set processes discovery, modeling, mining, archival Bachelorâs Master's Degree Statistics, Computer Science similar advanced degrees top tier educational institution preferred"
194,Data Engineer,Senior Big Data Engineer,"Atlanta, GA 30309",Atlanta,GA,"Invesco is one of the worldâs leading global investment managers, entrusted with managing $1.2 trillion* in assets on behalf of clients worldwide. We are the 6th largest US retail asset manager and the 13th largest investment manager globally, and our more than 8,000 employees worldwide are dedicated to delivering an investment experience that helps people get more out of life. We are purely focused on managing a comprehensive range of active, passive and alternative investment capabilities, which we draw on to provide customized solutions aligned to client needs, our most important benchmark. (*As of May 31,2019)

Job Purpose (Job Summary):

Weâre seeking a Senior Data Engineer to join a fast-paced agile development team building-out enterprise grade data platforms that support Client Experience, Regulatory, and investment teams. Youâll be a part of a dynamic, collaborative team that wants to hear your input because you know the leading methods, tools, and theories in data engineering. The focus of this role will be to source data sets, both internal and external, that support our business clients. Youâll be working alongside our data science and machine learning teams leveraging the Invesco enterprise data lake architecture. Candidates will be expected to ingest, curate, and provide access to structured and unstructured data sets.

Key Responsibilities / Duties:
Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies
Responsible to collect, process, and compute business metrics from activity & persisted data using Python/Spark
Process, cleanse, and verify the integrity of data used for analysis; optimize data for consumption
Build scalable OLAP backend storage for data in PB scale
Develop data set processes for data discovery, modeling, mining, and archival
Work Experience / Knowledge:
3+ years of experience focused in big data engineering with 6+ years overall working experience
3+ years of experience with ETL/SQL including fundamental and optimization query techniques, normal forms, and processing semi-structured data such as CSV, XML, XPath, XQuery
3+ years of experience with data services using Amazon Web Services (AWS), Azure, and / or Google Cloud
Expertise in at least one of the following areas:
Experience with parallel computing, batch processing, and stream processing using tools such as Kinesis or Kafka
Experience with data processing technologies such as Hadoop, Spark, Hive / Pig, and Java / MapReduce
Experience with data warehousing and columnar databases such as Redshift as well as NoSQL databases such as AWS S3, MongoDB, Cassandra, HBase, DynamoDB
3-5 years of experience with programming languages such as Java, Python or Scala
Experience with micro-services-based architecture and design/build of RESTful APIâs is a plus
Experience operationalizing data sourcing/loading including automating/scheduling data ingestion
Familiar with Agile software development (Scrum is a plus)
Expertise in building out DevOps pipelines is a plus
Skills / Other Personal Attributes Required:
Strong analytical and critical thinking skills
Autonomous personality. Weâll help guide you, but we wonât micromanage you. We expect integrity and results. Your work and deliverables will speak for themselves
Strong written and verbal communication skills
Enjoy challenging and thought-provoking work and have a strong desire to learn and progress
Ability to manage multiple tasks and requests
Must demonstrate a positive, team-focused attitude
Ability to react positively under pressure to meet tight deadlines
You listen to the input of your team members and take diverse perspectives into account to approach challenges from multiple angles
Structured, disciplined approach to work, with attention to detail
Flexible â able to meet changing requirements and priorities
Maintenance of up-to-date knowledge in the appropriate technical areas
Able to work in a global, multicultural environment
Formal Education: (minimum requirement to perform job duties)
Bachelorâs or masterâs degree of Statistics, Computer Science or other similar advanced degrees from a top tier educational institution preferred
Working Conditions:
Normal office environment with little exposure to noise, dust and temperatures
The ability to lift, carry or otherwise move objects of up to 10 pounds is also necessary
Normally works a regular schedule of hours, however hours may vary depending upon the project or assignment
Hours may include evenings and/or weekends and may include 24 hour a day on call support by pager and/or cell phone
Able and willing to travel both domestically and internationally. Frequency and duration to be determined by manager. Estimate: 10-15%
FLSA (US Only): Exempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.

Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment."," Strong analytical and critical thinking skills Autonomous personality. Weâll help guide you, but we wonât micromanage you. We expect integrity and results. Your work and deliverables will speak for themselves Strong written and verbal communication skills Enjoy challenging and thought-provoking work and have a strong desire to learn and progress Ability to manage multiple tasks and requests Must demonstrate a positive, team-focused attitude Ability to react positively under pressure to meet tight deadlines You listen to the input of your team members and take diverse perspectives into account to approach challenges from multiple angles Structured, disciplined approach to work, with attention to detail Flexible â able to meet changing requirements and priorities Maintenance of up-to-date knowledge in the appropriate technical areas Able to work in a global, multicultural environment Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Responsible to collect, process, and compute business metrics from activity & persisted data using Python/Spark Process, cleanse, and verify the integrity of data used for analysis; optimize data for consumption Build scalable OLAP backend storage for data in PB scale Develop data set processes for data discovery, modeling, mining, and archival  Bachelorâs or masterâs degree of Statistics, Computer Science or other similar advanced degrees from a top tier educational institution preferred ","Strong analytical and critical thinking skills Autonomous personality. Weâll help guide you, but we wonât micromanage you. We expect integrity results. Your work deliverables will speak for themselves written verbal communication Enjoy challenging thought-provoking have a strong desire to learn progress Ability manage multiple tasks requests Must demonstrate positive, team-focused attitude react positively under pressure meet tight deadlines You listen the input of your team members take diverse perspectives into account approach challenges from angles Structured, disciplined work, with attention detail Flexible â able changing requirements priorities Maintenance up-to-date knowledge in appropriate technical areas Able global, multicultural environment Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Responsible collect, process, compute business metrics activity & persisted Python/Spark Process, cleanse, verify used analysis; optimize consumption scalable OLAP backend storage PB scale Develop set processes discovery, modeling, mining, archival Bachelorâs masterâs degree Statistics, Computer Science similar advanced degrees top tier educational institution preferred","Strong analytical critical thinking skills Autonomous personality. Weâll help guide you, wonât micromanage you. We expect integrity results. Your work deliverables speak written verbal communication Enjoy challenging thought-provoking strong desire learn progress Ability manage multiple tasks requests Must demonstrate positive, team-focused attitude react positively pressure meet tight deadlines You listen input team members take diverse perspectives account approach challenges angles Structured, disciplined work, attention detail Flexible â able changing requirements priorities Maintenance up-to-date knowledge appropriate technical areas Able global, multicultural environment Build robust data pipelines public Cloud using AWS Kinesis, Kafka, Lambda technologies Responsible collect, process, compute business metrics activity & persisted Python/Spark Process, cleanse, verify used analysis; optimize consumption scalable OLAP backend storage PB scale Develop set processes discovery, modeling, mining, archival Bachelorâs masterâs degree Statistics, Computer Science similar advanced degrees top tier educational institution preferred"
195,Data Engineer,Lead Big Data Engineer,"Atlanta, GA 30308",Atlanta,GA,"Keysight launched a Software Design Center in Atlanta to create a new software platform for electronic product design and test. The new center is located in Atlantaâs growing Midtown district and features an open environment to foster collaboration and agile software development. We are seeking a lead developer to build upon a solid working knowledge of big data technologies such as MapReduce, Apache Spark and NoSQL databases on both structured and unstructured data. The focus of this position will be implementation of scalable computing technologies for data analytics. Along with a team of researchers and developers, you will help define, investigate, develop and implement, state-of-the-art informatics, data analysis and data visualization technology.
Job Qualifications
REQUIRED QUALIFICATIONS:M.S. in Computer Science, Informatics, Mathematics, Electronic/Electrical Engineering or other relevant field with an emphasis on data analytics.Experience with big data technologies such as Hadoop, Apache Spark, NoSQL databases.Strong computer science grounding, with knowledge of data structures, algorithms and computer architectures.Proficiency developing in one or more languages such as C++, Python or Java.Self-starting, requiring minimal supervision with strong problem-solving skills.Excellent communication and teamwork skills.

DESIRED QUALIFICATIONS:Hands-on experience with Cloud environments, such as AWS, Google Cloud, or AzureExperience with Cloudera
Job Function
R&D
___________________________________________________________________________________
Privacy Statement
***Keysight is an Equal Opportunity Employer.***
Keysight Technologies Inc. is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other protected categories under all applicable laws.

Candidates can be considered to work from the following locations:
Americas : United States : Georgia : Atlanta
Job ID : 391","M.S. in Computer Science, Informatics, Mathematics, Electronic/Electrical Engineering or other relevant field with an emphasis on data analytics.Experience with big data technologies such as Hadoop, Apache Spark, NoSQL databases.Strong computer science grounding, with knowledge of data structures, algorithms and computer architectures.Proficiency developing in one or more languages such as C++, Python or Java.Self-starting, requiring minimal supervision with strong problem-solving skills.Excellent communication and teamwork skills.    ","M.S. in Computer Science, Informatics, Mathematics, Electronic/Electrical Engineering or other relevant field with an emphasis on data analytics.Experience big technologies such as Hadoop, Apache Spark, NoSQL databases.Strong computer science grounding, knowledge of structures, algorithms and architectures.Proficiency developing one more languages C++, Python Java.Self-starting, requiring minimal supervision strong problem-solving skills.Excellent communication teamwork skills.","M.S. Computer Science, Informatics, Mathematics, Electronic/Electrical Engineering relevant field emphasis data analytics.Experience big technologies Hadoop, Apache Spark, NoSQL databases.Strong computer science grounding, knowledge structures, algorithms architectures.Proficiency developing one languages C++, Python Java.Self-starting, requiring minimal supervision strong problem-solving skills.Excellent communication teamwork skills."
196,Data Engineer,Senior Engineer - Data & Analytics,"Atlanta, GA 30326",Atlanta,GA,"About Slalom Build
Slalom Build is a highly-scalable, high-velocity Build as a Service firm. We work with clients to close the distance between dream and reality, imagined possibility and technical realization.

We do this by blending design, product engineering, analytics, and automation to build the modern technology products of tomorrow.

Nearly 1000 builders strong in seven Build Centers across North America, Slalom Build leverages a foundation of innovation inherited from Slalom Consulting. Weâre intensely proud to partner with future-focused clients committed to disrupting their industries.

About Slalom
Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to over 6,000 employees. We were named one of Fortuneâs 100 Best Companies to Work For in 2017 and are regularly recognized by our employees as a best place to work. You can find us in 27 cities across the U.S., U.K., and Canada.

Job Title: Senior Data Engineer
As a Senior Data Engineer for Slalom Build, youâll work in small teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core cloud data warehouse tools, Hadoop, Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, youâll be working with some of the most forward-thinking organizations in data and analytics.

Responsibilities:Work as part of a team to develop Cloud Data and Analytics solutionsParticipate in development of cloud data warehouses and business intelligence solutionsData wrangling of heterogeneous data and explore and discover new insightsGain hands-on experience with new data platforms and programming languages (e.g. Python, Hive, Spark)Willingness to travel up to 50%, at peak times of projects

Qualifications:3+ years of related work experience in Data Engineering or Data WarehousingHands-on experience with leading commercial Cloud platforms, including AWS, Azure, and GoogleProven experience with data warehousing, data ingestion, and data profilingProficient in SQLStrong aptitude for learning new technologies and analytics techniquesHighly self-motivated and able to work independently as well as in a team environmentUnderstanding of agile project approaches and methodologiesProficient in a source code control system, such as GitProficient in the Linux shell, including utilities such as SSH

Preferred Experience:Familiarity with implementing analytics solutions with one or more Hadoop distributions (Cloudera, Hortonworks, MapR, HDInsight, EMR)Failiarity with streaming data ingestionProficient in Python and/or JavaConsulting experienceFamiliarity or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)

Slalom is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.",3+ years of related work experience in Data Engineering or Data Warehousing  Work as part of a team to develop Cloud Data and Analytics solutions  ,3+ years of related work experience in Data Engineering or Warehousing Work as part a team to develop Cloud and Analytics solutions,3+ years related work experience Data Engineering Warehousing Work part team develop Cloud Analytics solutions
197,Data Engineer,Senior Big Data Engineer,"Alpharetta, GA",Alpharetta,GA,"Company Overview
Fractal Analytics is a strategic AI partner to Fortune 500 companies with a vision to power every human decision in the enterprise. Fractal is building a world where individual choices, freedom, and diversity are the greatest assets. An ecosystem where human imagination is at the heart of every decision. Where no possibility is written off, only challenged to get better. We believe that a true Fractalite is the one who empowers imagination with intelligence.
We are seeking for a strong candidate with advanced analytics experience to fill an exciting Big Data Engineer position, in Alpharetta, GA. In this role, you will be a valuable expert and will help design and build analytics methodologies, solutions, and products to deliver value to our clients in collaboration with cross-functional teams. As an exceptional candidate, you will show an analytical curiosity.
Responsibilities
Understand problems from a client's point of view, build and execute solid analytics work plans, gather and organize large and complex data sets, perform relevant analyses (data exploration and statistical modeling), manage priorities and deadlines, foster teamwork in interactions, develop client relationships with client counterparts, and communicate hypotheses and findings in a structured way
Partner with business teams in identifying business requirements and developing advanced analytical solutions to complex problems by utilizing statistical models and machine learning techniques and algorithms
Qualifications
A bachelor's degree or related field with at least 3 â 8 years of experience with Big Data or Hadoop tools such as Spark, Hive, Kafka and MapReduce, Spark, R, Python
Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasks
Experience in AI/ML on big data platforms preferred:
Slid applied statistics skills, such as identifying distributions, statistical testing, regression, etc.
Prficiency querying both structured and unstructured data
Experience with Pythn and Deep Learning packages (Keras, Tensorflow, mxnet) and NLP packages (nltk, spacy) is a plus
Experience deplying models in production environments
Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space
Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage
Excellent written and oral communication skills; must be able to effectively articulate technical concepts to non-technical audiences
Software development experience using:
Database prgramming using any flavor of SQL
Expertise in relatinal and dimensional modelling
Nice to Have:
Operating knowledge of cloud computing platforms (AWS, especially EMR, EC2, S3, SWF services and the AWS CLI)
Experience building recommendation engines (both offline and online validation metrics)
z8yMgRjHjP"," A bachelor's degree or related field with at least 3 â 8 years of experience with Big Data or Hadoop tools such as Spark, Hive, Kafka and MapReduce, Spark, R, Python Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasks Experience in AI/ML on big data platforms preferred  Slid applied statistics skills, such as identifying distributions, statistical testing, regression, etc. Prficiency querying both structured and unstructured data Experience with Pythn and Deep Learning packages  Keras, Tensorflow, mxnet  and NLP packages  nltk, spacy  is a plus Experience deplying models in production environments Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space Hands-on experience with Apache Spark and its components  Streaming, SQL, MLLib  is a strong advantage Excellent written and oral communication skills; must be able to effectively articulate technical concepts to non-technical audiences Software development experience using  Database prgramming using any flavor of SQL Expertise in relatinal and dimensional modelling   Understand problems from a client's point of view, build and execute solid analytics work plans, gather and organize large and complex data sets, perform relevant analyses  data exploration and statistical modeling , manage priorities and deadlines, foster teamwork in interactions, develop client relationships with client counterparts, and communicate hypotheses and findings in a structured way Partner with business teams in identifying business requirements and developing advanced analytical solutions to complex problems by utilizing statistical models and machine learning techniques and algorithms  ","A bachelor's degree or related field with at least 3 â 8 years of experience Big Data Hadoop tools such as Spark, Hive, Kafka and MapReduce, R, Python Experience working within a Linux computing environment, use command line including knowledge shell/Python scripting for automating common tasks in AI/ML on big data platforms preferred Slid applied statistics skills, identifying distributions, statistical testing, regression, etc. Prficiency querying both structured unstructured Pythn Deep Learning packages Keras, Tensorflow, mxnet NLP nltk, spacy is plus deplying models production environments Strong System Integration, Application Development Data-Warehouse projects, across technologies used the enterprise space Hands-on Apache Spark its components Streaming, SQL, MLLib strong advantage Excellent written oral communication skills; must be able to effectively articulate technical concepts non-technical audiences Software development using Database prgramming any flavor SQL Expertise relatinal dimensional modelling Understand problems from client's point view, build execute solid analytics work plans, gather organize large complex sets, perform relevant analyses exploration modeling , manage priorities deadlines, foster teamwork interactions, develop client relationships counterparts, communicate hypotheses findings way Partner business teams requirements developing advanced analytical solutions by utilizing machine learning techniques algorithms","A bachelor's degree related field least 3 â 8 years experience Big Data Hadoop tools Spark, Hive, Kafka MapReduce, R, Python Experience working within Linux computing environment, use command line including knowledge shell/Python scripting automating common tasks AI/ML big data platforms preferred Slid applied statistics skills, identifying distributions, statistical testing, regression, etc. Prficiency querying structured unstructured Pythn Deep Learning packages Keras, Tensorflow, mxnet NLP nltk, spacy plus deplying models production environments Strong System Integration, Application Development Data-Warehouse projects, across technologies used enterprise space Hands-on Apache Spark components Streaming, SQL, MLLib strong advantage Excellent written oral communication skills; must able effectively articulate technical concepts non-technical audiences Software development using Database prgramming flavor SQL Expertise relatinal dimensional modelling Understand problems client's point view, build execute solid analytics work plans, gather organize large complex sets, perform relevant analyses exploration modeling , manage priorities deadlines, foster teamwork interactions, develop client relationships counterparts, communicate hypotheses findings way Partner business teams requirements developing advanced analytical solutions utilizing machine learning techniques algorithms"
198,Data Engineer,Sr. Data Engineer,"Atlanta, GA",Atlanta,GA,"Must Have qualifications: Top tier consultant firm experience 8-10 years building products and architecting solutions. Experience advising clients on Data Modernization initiatives. Ability to deal with structured, semi-structured, unstructured and streaming data. Ability to lead proofs-of-concepts and then effectively transition and scale those concepts into production at scale through, engineering, deployment and commercialization. Serve as an expert; envision and integrate emerging data technologies, anticipate new trends to solve complex business and technical problems. Experience in sales, pre-sales functions, leading pursuits, proposal development, and statement of works. Ability to work in the United States without visa sponsorship now or in the future

TECHNICAL REQUIREMENTS:
 Experience designing, developing, optimizing and troubleshooting complex data-intensive applications using Spark, HDFS, Kafka, MapReduce, MongoDB and other big data related technologies. Must know PySpark, Python and SQL; comfortable with Java and/or Scala. Comfortable designing and implementing data warehouse and pipelinesâsuch as ETL, data integration, and streamingâto support teams focused in Analytics. Experienced in AWS or/and Azure Cloud Platform. Ability to engineer for performance, scalability, latency, & reliability. DevOps and automation experience highly desired
Candidates should be flexible / willing to work across this delivery landscape which includes and not limited to Agile Applications Development, Support and Deployment.
Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.

Qualifications

Responsible for programming and software development using various programming languages and related tools and frameworks, reviewing code written by other programmers, requirement gathering, bug fixing, testing, documenting and implementing software systems. Experienced programmers are also responsible for interpreting architecture and design, code reviews, mentoring, guiding and monitoring programmers, ensuring adherence to programming and documentation policies, software development, testing and release.

Required Skills and Experience:

You assign, coordinate, and review work and activities of programming personnel. Collaborate with computer manufacturers and other users to develop new programming methods. Supervise, train, mentor junior level programmers in programming and program coding. Represent team in project meetings. Work with business and functional analysts, and software & solution architects in ensuring that programs and systems function as intended Supervise, mentor and manage large teams of programmers in one or more projects. Represent project teams in project/program meetings or in meetings with sponsor.
Qualifications: 7 â 10 (3 years min relevant experience in the role) years experience, Bachelorâs Degree.Must have experience in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Lifecycle and Data Management.Should be proficient in Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Click the following link for more information on your rights as an Applicant - http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law


About Capgemini

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clientsâ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.

Visit us at www.capgemini.com. People matter, results count.","Qualifications  7 â 10  3 years min relevant experience in the role  years experience, Bachelorâs Degree.Must have experience in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Lifecycle and Data Management.Should be proficient in Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.    ","Qualifications 7 â 10 3 years min relevant experience in the role experience, Bachelorâs Degree.Must have Software Engineering Techniques, Architecture, Lifecycle and Data Management.Should be proficient Business Analysis, Knowledge, Leadership, Architecture Knowledge Technical Solution Design.","Qualifications 7 â 10 3 years min relevant experience role experience, Bachelorâs Degree.Must Software Engineering Techniques, Architecture, Lifecycle Data Management.Should proficient Business Analysis, Knowledge, Leadership, Architecture Knowledge Technical Solution Design."
199,Data Engineer,Lead Data Engineer,"Atlanta, GA 30309",Atlanta,GA,"Invesco is one of the worldâs leading global investment managers, entrusted with managing $1.2 trillion* in assets on behalf of clients worldwide. We are the 6th largest US retail asset manager and the 13th largest investment manager globally, and our more than 8,000 employees worldwide are dedicated to delivering an investment experience that helps people get more out of life. We are purely focused on managing a comprehensive range of active, passive and alternative investment capabilities, which we draw on to provide customized solutions aligned to client needs, our most important benchmark. (*As of May 31,2019)

Job Purpose (Job Summary):

As a Team Lead at Invesco, you would align business outcomes to technology goals and objectives. You will be responsible for managing decision sciences and advanced analytics initiatives, data integration/engineering. Interacts with Data Scientists, Business Owners, Business Data Analysts, Data Modelers, Architects, and Application Developers, to design, build and manage large-scale batch and real-time data pipelines utilizing various data analytics processing frameworks in support of Data Science practice. The position requires building strong technical hands-on-skills and experiences as well as establishing close business relationships across the enterprise. Working closely with marketing and sales stakeholders, you will establish and drive long term advance analytics roadmap manage increasing demands for advance analytics in Sales and Marketing. You will help steer the strategic technology direction, define target state architecture, technology roadmaps and mentor others. A successful candidate must have a demonstrable background with experience in development of high performance, distributed computing tasks using Big Data technologies such as Hadoop (platform level), NoSQL, text mining and other distributed environment technologies based on the needs of the organization. Responsible for analyzing, designing, programing, debugging and modifying software enhancements and/or new products used in distributed, large scale analytics solutions. Strong visualization skills and experience with BI tools (ex. Tableau)

Key Responsibilities / Duties:
Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities
Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities
Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies
Assist in the decision-making process related to the selection of software architecture solutions
Implement architectures to handle web-scale data and its organization
Execute strategies that inform data design and architecture partnering with enterprise standard
Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies
Assist in creating documents that ensure consistency in development across the online organization. Implements and improves core software infrastructure
Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments
Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools (such as control charts and scorecards), readiness and adoption of data w/in the organization
Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding
Serve as strong advocate to improve analytical capability across the organization
Communicate with various business areas and to gather and prioritize their business requirements
Support various reporting and BI solutions (Tableau, Power BI, Salesforce Einstein)
Manage application and data integration platforms (Informatica, Talend)
Manage the full life cycle of development/reporting/integration projects: planning, design, develop, testing and rollout
Manage solution providers, define sourcing approach and manage the providers
Create and manage data, applications and technology architecture documentation and design artifacts
Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement
Gain adoption of architecture processes, standards and procedures
Work Experience / Knowledge:
7+ years of experience in data modeling, data warehousing, and big data architectures
5+ years of experience in a data engineering role
Proficient in application/software architecture (Definition, Business Process Modeling, etc.)
Deep expertise in (at least one) SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments
The role will be responsible for providing innovative operational solutions and best practices
Advanced knowledge in SQL/Hive, Spark, NoSQL, (Java/Python is a plus)
Strong programming skills and ability to utilize a variety of software/languages/tools; e.g., Spark, R, Python, Scala, Java, Hive, SQL, SAS, Tableau, etc.
Experience with microservice development, Docker, Kubernetes
Develop software to run on cloud native big data infrastructure built on AWS using Spark, Lambda, S3, and other cloud native services
Designs and develops complex and large-scale data structures and pipelines to organize, collect and standardize data to generate insight
3+ years of experience in data integration platforms (Informatica, Talend)
Strong analytics and reporting skills â hands on experience in BI Tools like Tableau, Power BI, Salesforce Einstein
Hands on experience in self-service data preparation tool like Alteryx
Experience using GitHub, Bit Bucket, or other code repository solution
DevOps experience is a plus
Skills / Other Personal Attributes Required:
Previous management experience and successfully leading a team of direct reports
Proven track record of effectively leading and managing employees, managing performance, setting goals for team/individuals and provide mentoring.
Strong Tableau, Adobe Analytics other BI solution
Expert Oracle and Vertica skillsets
Experience using JIRA and Agile Project Management software
Experience using GitHub, Bit Bucket, or other code repository solution
Strong written, verbal communication and presentation skills
Ability to explain complex technical issues in a way that non-technical people may understand
Able to work in a global, multicultural environment
Self-motivated. Capable of working with little or no supervision
Ability to react positively under pressure to meet tight deadlines
Able to work independently or as a team player
Enjoy challenging and thought provoking work and have a strong desire to learn and progress
Formal Education: (minimum requirement to perform job duties)
BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience
FLSA (US Only): Exempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.

Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment."," Previous management experience and successfully leading a team of direct reports Proven track record of effectively leading and managing employees, managing performance, setting goals for team/individuals and provide mentoring. Strong Tableau, Adobe Analytics other BI solution Expert Oracle and Vertica skillsets Experience using JIRA and Agile Project Management software Experience using GitHub, Bit Bucket, or other code repository solution Strong written, verbal communication and presentation skills Ability to explain complex technical issues in a way that non-technical people may understand Able to work in a global, multicultural environment Self-motivated. Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player Enjoy challenging and thought provoking work and have a strong desire to learn and progress Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Implement architectures to handle web-scale data and its organization Execute strategies that inform data design and architecture partnering with enterprise standard Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Assist in creating documents that ensure consistency in development across the online organization. Implements and improves core software infrastructure Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools  such as control charts and scorecards , readiness and adoption of data w/in the organization Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding Serve as strong advocate to improve analytical capability across the organization Communicate with various business areas and to gather and prioritize their business requirements Support various reporting and BI solutions  Tableau, Power BI, Salesforce Einstein  Manage application and data integration platforms  Informatica, Talend  Manage the full life cycle of development/reporting/integration projects  planning, design, develop, testing and rollout Manage solution providers, define sourcing approach and manage the providers Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement Gain adoption of architecture processes, standards and procedures  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience ","Previous management experience and successfully leading a team of direct reports Proven track record effectively managing employees, performance, setting goals for team/individuals provide mentoring. Strong Tableau, Adobe Analytics other BI solution Expert Oracle Vertica skillsets Experience using JIRA Agile Project Management software GitHub, Bit Bucket, or code repository written, verbal communication presentation skills Ability to explain complex technical issues in way that non-technical people may understand Able work global, multicultural environment Self-motivated. Capable working with little no supervision react positively under pressure meet tight deadlines independently as player Enjoy challenging thought provoking have strong desire learn progress Work development teams project leaders/stakeholders solutions enable business capabilities Supports the data science community by enabling availability environments advanced analytical Maintains broad understanding implementation, integration, inter-connectivity emerging technologies define strategies Assist decision-making process related selection architecture Implement architectures handle web-scale its organization Execute inform design partnering enterprise standard Build robust pipelines on public Cloud AWS Kinesis, Kafka, Lambda creating documents ensure consistency across online organization. Implements improves core infrastructure Deep expertise SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. writing programs, implementing architectures, automation these Develop maintain reporting, ensuring reliability delivery performance tools such control charts scorecards , readiness adoption w/in Consolidate, standardize changes capacity metric definitions, ownership, accountability taxonomy alignment Serve advocate improve capability Communicate various areas gather prioritize their requirements Support reporting Power BI, Salesforce Einstein Manage application integration platforms Informatica, Talend full life cycle development/reporting/integration projects planning, design, develop, testing rollout providers, sourcing approach manage providers Create data, applications technology documentation artifacts deliver meaningful reference outline principles best practices advancement Gain processes, standards procedures BS Computer Science, Applied Mathematics, Physics, Statistics area study sciences mining relevant","Previous management experience successfully leading team direct reports Proven track record effectively managing employees, performance, setting goals team/individuals provide mentoring. Strong Tableau, Adobe Analytics BI solution Expert Oracle Vertica skillsets Experience using JIRA Agile Project Management software GitHub, Bit Bucket, code repository written, verbal communication presentation skills Ability explain complex technical issues way non-technical people may understand Able work global, multicultural environment Self-motivated. Capable working little supervision react positively pressure meet tight deadlines independently player Enjoy challenging thought provoking strong desire learn progress Work development teams project leaders/stakeholders solutions enable business capabilities Supports data science community enabling availability environments advanced analytical Maintains broad understanding implementation, integration, inter-connectivity emerging technologies define strategies Assist decision-making process related selection architecture Implement architectures handle web-scale organization Execute inform design partnering enterprise standard Build robust pipelines public Cloud AWS Kinesis, Kafka, Lambda creating documents ensure consistency across online organization. Implements improves core infrastructure Deep expertise SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. writing programs, implementing architectures, automation Develop maintain reporting, ensuring reliability delivery performance tools control charts scorecards , readiness adoption w/in Consolidate, standardize changes capacity metric definitions, ownership, accountability taxonomy alignment Serve advocate improve capability Communicate various areas gather prioritize requirements Support reporting Power BI, Salesforce Einstein Manage application integration platforms Informatica, Talend full life cycle development/reporting/integration projects planning, design, develop, testing rollout providers, sourcing approach manage providers Create data, applications technology documentation artifacts deliver meaningful reference outline principles best practices advancement Gain processes, standards procedures BS Computer Science, Applied Mathematics, Physics, Statistics area study sciences mining relevant"
200,Data Engineer,Senior Digital Lead Data Engineer,"Atlanta, GA 30309",Atlanta,GA,"Invesco is one of the world's leading global investment managers, entrusted with managing $1.2 trillion* in assets on behalf of clients worldwide. We are the 6th largest US retail asset manager and the 13th largest investment manager globally, and our more than 8,000 employees worldwide are dedicated to delivering an investment experience that helps people get more out of life. We are purely focused on managing a comprehensive range of active, passive and alternative investment capabilities, which we draw on to provide customized solutions aligned to client needs, our most important benchmark. (*As of May 31,2019)

About Invesco Technology
At Invesco Technology, we are strategic problem solvers. Our mission is to create world-class technology solutions to enable global operations, lead in the innovative use of data and emerging technologies to redefine the investment experience, and help our clients âget more out of life."" This mission is fueled by our high-performing teams, which thrive on collaboration, operate on shared trust, and leverage diversity of thought to deliver valuable results every day to Invesco, clients, and partners.

We wholeheartedly believe that our success is driven by our people. That is why we invest heavily in our top talent, providing opportunities for continuous learning and professional development. Our employees are encouraged and supported in taking advantage of development opportunities tied to their goals and are recognized for employing new skills to make an impact beyond the scope of their daily roles.

To continue building our high-performing, OneTech Team, we are seeking candidates who champion innovation, operate effectively in an agile environment, challenge the status quo and are empowered to take risks.

Job Purpose (Job Summary):

Join us in realizing Invesco's Digital Transformation! We are on a journey to reimagine what is into what could be. Think of us as like a tribe of digital activists who focus disruptive innovation who believe that digital operations in Investment Management can be done better. We have MASSIVE aspirations however we are focused on four goals:
Streamline content capture across the enterprise and create a one source of truth
Fully exploit the tremendous re-use potential through seamless accessibility to content
Provide Invesco with a highly scalable and dynamic platform that enables rapid growth
To shake-up the norm by developing new digital operating models to create a ""new world order""
Our vision is simple. To create strategic advantage by being the best at digital operations. We are highly motivated, not your usual financial services stereotype, and thrive within an industry resistant to change. As a team we respect and trust each other and celebrate our successes AND failures. Welcome to the new world order!
Right now, we are transforming the way we operate and to that note, we are looking for a super-strong and super-passionate Digital Lead Data Engineer. In our digital transformation, data is key. In this role, you will build technical solutions to help improve the scalability and performance of our data stores and our overall systems. As a Lead Engineer, you will focus on efforts that will provide increased flexibility and accessibility to our data, such as our cloud-based data lake and digital pipeline initiatives.
The Digital Lead Data Engineer acts as a senior level technical and functional expert, collaborating with business and Tech stakeholders at all levels to identify architectural solution options and map value streams. This position is responsible for using data engineering practices and techniques to deliver measurable business outcomes in a large-scale, complex organization.

Key Responsibilities / Duties:
Design and work with Agile teams to implement highly scalable Enterprise approaches for metadata, taxonomy, tagging, folder structures providing the lightweight controls to ensure sustainable data quality and efficiencies
Defines non-functional requirements including data cleansing and validation
Mentors and coaches other members of the agile and\or Run team
Interfaces with the Product Owner and Technology partners at the Program level to define and estimate features for agile teams
Works within the SAFe Agile framework and employs ITIL best practices
Client-facing, senior strategic support
Content assessment. Providing content revision/creation recommendations that help the business and its customers achieve their goals (based on brand documentation, competitive assessments, audience segmentation, SEO data, and site metrics)
Generating and/or overseeing an inventory of relevant client content assets
Creating and/or reviewing site structure and nomenclature for the most intuitive presentation of content, usually partnering with UX
Creating taxonomy and tagging strategies
Ensuring content is structured properly for any relevant backend systems including CMS, DAM, eCommerce and/or PIM
Contributing to technical system evaluations (such as CMS, DAM, and/or PIM) from a content perspective in terms of requirements
Establishing and/or maintaining editorial standards and accuracy/quality of content
Overseeing content migration and creating or reviewing associated documentation
Contributing to definition and management of, as well as periodic updates to, content governance and workflow (including content creation, content entry, publication, and decommission) as well as localization strategies
Responsible for management of, as well as periodic updates to, personalization and content tagging strategies for the site
Assists in production support and maintenance of applications as needed

Work Experience / Knowledge:
Minimum 8+ years' experience in digital content strategy or similar
Agency or consulting experience a plus
Proven track record of Innovation and expertise in Data Engineering
Understanding of agile development methods including: core values, guiding principles, and key agile practices
Proven ability to get stuff done, driven to see your team succeed.

Skills / Other Personal Attributes Required:
Exceptional interpersonal skills, including teamwork, facilitation, and negotiation.
Communicates IT requirements and guidelines to vendor partners.
Applies multiple technical solutions to business problems.
Quickly comprehends the functions and capabilities of new technology.
Excellent written and oral communications skills.

Formal Education: (minimum requirement to perform job duties)
Bachelor's degree in English, Library Science, Journalism, Technical Writing, Marketing or equivalent military experience preferred

Working Conditions:
An entrepreneurial working environment. If you can make a case for a project that helps achieve our goals or fits within our vision, we would encourage and pave the way for you to deliver it
A team that supports and collaborates with you
A fun environment that is not your typical Financial Services way-of-working
Challenges that will not only positively impact our organization and clients, however will also be a first in our industry
A competitive salary
Training to further your career and development. We want you to stay progressive and relevant and therefore we will invest in you to do just that!

FLSA (US Only): Exempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.

Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment.

#Ll-SD1"," Exceptional interpersonal skills, including teamwork, facilitation, and negotiation. Communicates IT requirements and guidelines to vendor partners. Applies multiple technical solutions to business problems. Quickly comprehends the functions and capabilities of new technology. Excellent written and oral communications skills. Design and work with Agile teams to implement highly scalable Enterprise approaches for metadata, taxonomy, tagging, folder structures providing the lightweight controls to ensure sustainable data quality and efficiencies Defines non-functional requirements including data cleansing and validation Mentors and coaches other members of the agile and\or Run team Interfaces with the Product Owner and Technology partners at the Program level to define and estimate features for agile teams Works within the SAFe Agile framework and employs ITIL best practices Client-facing, senior strategic support Content assessment. Providing content revision/creation recommendations that help the business and its customers achieve their goals  based on brand documentation, competitive assessments, audience segmentation, SEO data, and site metrics  Generating and/or overseeing an inventory of relevant client content assets Creating and/or reviewing site structure and nomenclature for the most intuitive presentation of content, usually partnering with UX Creating taxonomy and tagging strategies Ensuring content is structured properly for any relevant backend systems including CMS, DAM, eCommerce and/or PIM Contributing to technical system evaluations  such as CMS, DAM, and/or PIM  from a content perspective in terms of requirements Establishing and/or maintaining editorial standards and accuracy/quality of content Overseeing content migration and creating or reviewing associated documentation Contributing to definition and management of, as well as periodic updates to, content governance and workflow  including content creation, content entry, publication, and decommission  as well as localization strategies Responsible for management of, as well as periodic updates to, personalization and content tagging strategies for the site Assists in production support and maintenance of applications as needed  Bachelor's degree in English, Library Science, Journalism, Technical Writing, Marketing or equivalent military experience preferred ","Exceptional interpersonal skills, including teamwork, facilitation, and negotiation. Communicates IT requirements guidelines to vendor partners. Applies multiple technical solutions business problems. Quickly comprehends the functions capabilities of new technology. Excellent written oral communications skills. Design work with Agile teams implement highly scalable Enterprise approaches for metadata, taxonomy, tagging, folder structures providing lightweight controls ensure sustainable data quality efficiencies Defines non-functional cleansing validation Mentors coaches other members agile and\or Run team Interfaces Product Owner Technology partners at Program level define estimate features Works within SAFe framework employs ITIL best practices Client-facing, senior strategic support Content assessment. Providing content revision/creation recommendations that help its customers achieve their goals based on brand documentation, competitive assessments, audience segmentation, SEO data, site metrics Generating and/or overseeing an inventory relevant client assets Creating reviewing structure nomenclature most intuitive presentation content, usually partnering UX taxonomy tagging strategies Ensuring is structured properly any backend systems CMS, DAM, eCommerce PIM Contributing system evaluations such as from a perspective in terms Establishing maintaining editorial standards accuracy/quality Overseeing migration creating or associated documentation definition management of, well periodic updates to, governance workflow creation, entry, publication, decommission localization Responsible personalization Assists production maintenance applications needed Bachelor's degree English, Library Science, Journalism, Technical Writing, Marketing equivalent military experience preferred","Exceptional interpersonal skills, including teamwork, facilitation, negotiation. Communicates IT requirements guidelines vendor partners. Applies multiple technical solutions business problems. Quickly comprehends functions capabilities new technology. Excellent written oral communications skills. Design work Agile teams implement highly scalable Enterprise approaches metadata, taxonomy, tagging, folder structures providing lightweight controls ensure sustainable data quality efficiencies Defines non-functional cleansing validation Mentors coaches members agile and\or Run team Interfaces Product Owner Technology partners Program level define estimate features Works within SAFe framework employs ITIL best practices Client-facing, senior strategic support Content assessment. Providing content revision/creation recommendations help customers achieve goals based brand documentation, competitive assessments, audience segmentation, SEO data, site metrics Generating and/or overseeing inventory relevant client assets Creating reviewing structure nomenclature intuitive presentation content, usually partnering UX taxonomy tagging strategies Ensuring structured properly backend systems CMS, DAM, eCommerce PIM Contributing system evaluations perspective terms Establishing maintaining editorial standards accuracy/quality Overseeing migration creating associated documentation definition management of, well periodic updates to, governance workflow creation, entry, publication, decommission localization Responsible personalization Assists production maintenance applications needed Bachelor's degree English, Library Science, Journalism, Technical Writing, Marketing equivalent military experience preferred"
201,Data Engineer,Data Engineer - Java / Python & Spark,"Atlanta, GA",Atlanta,GA,"Data Engineer
Java/Python + Spark
Overall experience of around 5-8 years with Data integration, data migration
Job description:
Good understanding of ETL/ELT concepts
Hands on working experience with one of the programming languages (Java, Python)
Experience working with big data tools and technologies (Spark, Kafka)
Should be able to interpret business requirements, understand design and identify mechanism to test
Resource should know how to figure out what should be tested, with a focus on data quality
Experience with data reconciliation, testing to ensure quality of migrated data.
Understanding of Cloud Architecture
Experience with NoSQL databases (Cassandra, Couchbase)
Understanding of Cloud Data Warehouse solutions (AWS)
Resource must be comfortable working in an Agile environment


Candidates should be flexible / willing to work across this delivery landscape which includes and not limited to Agile Applications Development, Support and Deployment.

Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.

Qualifications

Responsible for programming and software development using various programming languages and related tools and frameworks, reviewing code written by other programmers, requirement gathering, bug fixing, testing, documenting and implementing software systems. Experienced programmers are also responsible for interpreting architecture and design, code reviews, mentoring, guiding and monitoring programmers, ensuring adherence to programming and documentation policies, software development, testing and release.
Required Skills and Experience:
Write software programs using specific programming languages/platforms such as Java or MS .NET, and related tools, platform and environment. Write, update, and maintain computer programs or software packages to handle specific jobs, such as tracking inventory, storing or retrieving data, or controlling other equipment. Consult with managerial, engineering, and technical personnel to clarify program intent, identify problems, and suggest changes. Perform or direct revision, repair, or expansion of existing programs to increase operating efficiency or adapt to new requirements. Write, analyze, review, and rewrite programs, using workflow chart and diagram, and applying knowledge of computer capabilities, subject matter, and symbolic logic. Write or contribute to instructions or manuals to guide end users. Correct errors by making appropriate changes and then rechecking the program to ensure that the desired results are produced. Conduct trial runs of programs and software applications to be sure they will produce the desired information and that the instructions are correct. Compile and write documentation of program development and subsequent revisions, inserting comments in the coded instructions so others can understand the program. Investigate whether networks, workstations, the central processing unit of the system, and/or peripheral equipment are responding to a program's instructions. Prepare detailed workflow charts and diagrams that describe input, output, and logical operation, and convert them into a series of instructions coded in a computer language. Perform systems analysis and programming tasks to maintain and control the use of computer systems software as a systems programmer. Consult with and assist computer operators or system analysts to define and resolve problems in running computer programs. Perform unit testing Assist in system and user testing Fix errors and bugs that are identified in the course of testing.
Qualifications: 3-7 years (2 years min relevant experience in the role) experience; Bachelorâs degreeShould be proficient in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Life cycle and Data Management.Should have progressing skills on Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.

This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Click the following link for more information on your rights as an Applicant - http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law

About Capgemini
A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clientsâ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.

Visit us at www.capgemini.com. People matter, results count.","Qualifications  3-7 years  2 years min relevant experience in the role  experience; Bachelorâs degreeShould be proficient in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Life cycle and Data Management.Should have progressing skills on Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.    ","Qualifications 3-7 years 2 min relevant experience in the role experience; Bachelorâs degreeShould be proficient Software Engineering Techniques, Architecture, Life cycle and Data Management.Should have progressing skills on Business Analysis, Knowledge, Leadership, Architecture Knowledge Technical Solution Design.","Qualifications 3-7 years 2 min relevant experience role experience; Bachelorâs degreeShould proficient Software Engineering Techniques, Architecture, Life cycle Data Management.Should progressing skills Business Analysis, Knowledge, Leadership, Architecture Knowledge Technical Solution Design."
202,Data Engineer,Data Engineer,"Atlanta, GA 30340",Atlanta,GA,"Job Description

Accountabilities in this Role:
Ã Develop, construct, test and maintain architectures
Ã Data acquisition / verifies it meets business and data privacy regulations
Ã Deploy sophisticated analytics programs, machine learning and statistical methods
Ã Acts as primary backup for ERP (Syspro) Administration
Ã Ownership / support of company intranet (SharePoint) along with workflows
Ã Web Master (Website Development) / Website Support (back-end)

Qualifications

Technical Skills:
Ã Understanding of object-oriented design, client-server architecture and relational database design
Ã Knowledge of .NET Framework and ability to work with C#, ASP.NET, WCF, MVC and ADO.NET
Ã Experience using T-SQL with the ability to write SQL queries and stored procedures
Ã Knowledge of client-side technologies such as JavaScript, jQuery, HTML5 and CSS
Ã Experience with Microsoft Visual Studio and SQL Server Management Studio
Ã Experience using Team Foundation Server for source control and build management
Ã SharePoint development experience with SharePoint 2010 / 2013
Ã Experience with SharePoint Solutions (SharePoint Server 2013, InfoPath, Forms Services, Excel Services, Search, Workflows, Content Management, Metadata, Business Data Catalog and Web Services)
Ã SharePoint experience including SharePoint workflows, effective use of the data view web part (DVWP), content query web part, data form web part, navigation customization, and some branding customization
Ã Web development experience (HTML, CSS, XSL, XSLT, JavaScript, AngularJS).
Ã Experience with SharePoint templates (site templates, list templates, master page customization)
Ã Strong experience in a scripting language such as PowerShell
Ã ERP system support and implementation in the finance, manufacturing and logistics arena. SYSPRO experience
Ã Database architecture and design; optimize T-SQL code and stored procedures
Ã SQL Server Reporting services (SSRS) to write reports not possible using SYSPRO report writer
Ã Use Automate software to print journals and distribution reports from all SYSPRO modules then post in the GL
Ã SYSPRO application administration with MS SQL Server administration and development
Ã Working knowledge of Storage (SAN) platforms, specifically HP.
Ã Understanding of Windows 2012 R2 Microsoft Hyper-V technology
Additional Preferred Qualifications:
Ã Bachelorâs degree in a computer/accounting/finance or related field.
Ã Understands accounting, purchasing, manufacturing and distribution methodologies through an ERP system. The company uses SYSPRO.
Ã Self-starter and requires minimal supervision, but works well in a group.
Ã Ability to interpret what non-computer science personnel need and then develop the ideas to solve the situation.
Additional Information

The Rewards:

Ã Fantastic, casual working environmentMonthly employee social functionsOn premises gymDaily afternoon yoga stretchingFriendly
Ã Comprehensive benefit package401(k) with company matchExtremely competitive health insurance planVacation and Personal DaysCompany paid holidays

All qualified applicants will receive consideration for employment without regard to race, color, gender identity or expression, age, religion, intellectual disability, mental disability, physical disability, including but not limited to blindness, unless it is shown that such disability prevents performance of the work involved, medical condition, handicap, national origin, ancestry, sexual orientation, marital status, domestic partnership status, parental status, military status, veteran or military discharge status, source of income or housing status or any other status protected by applicable law.
GF Health Products, Inc. is a drug free workplace",Monthly employee social functionsOn premises gymDaily afternoon yoga stretchingFriendly Monthly employee social functionsOn premises gymDaily afternoon yoga stretchingFriendly   ,Monthly employee social functionsOn premises gymDaily afternoon yoga stretchingFriendly,Monthly employee social functionsOn premises gymDaily afternoon yoga stretchingFriendly
203,Data Engineer,Big Data Engineer,"Atlanta, GA",Atlanta,GA,"The big data Senior Technical person (4 to 10 years of experience) responsible for managing the full life-cycle of a Hadoop solution

This includes creating the requirements analysis, the platform selection, design of the technical architecture, design of the application design and development, testing, and deployment of the proposed solution

Analytical and problem solving skills, applied to big data domain

Good communication skill

Worked on Hadoop, Hive, Spark, Scala, Kafka, NiFi, Scoop, Oozie


Candidates should be flexible / willing to work across this delivery landscape which includes and not limited to Agile Applications Development, Support and Deployment.
Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.
Qualifications
Software Engineers perform requirements analysis. They then design, develop or maintain the physical application (components) or the application environment, based on the Software Architecture (models and principles). Activities include coding, integrating, implementing, installing or changing frameworks and standard components, or technical and functional application management. A Software Engineer also develops languages, methods, frameworks and tools, and/or undertakes activities in support of server-based databases in development, test and production environments.
Required Skills and Experience:
You are an experienced Software Engineer. You have received training and mastered at least one technology environment. You are good at elaborating technology areas and have an ability to position them within the scope of an overall project. You are a member of at least one community.
Qualification: 3-7 years (2 years min relevant experience in the role) experience, Bachelorâs DegreeCertification: Should have or seeking SE Level 1Should have progressing knowledge in Business Analysis, Business Knowledge, Software Engineering, Testing, Data Management, Architecture Knowledge and Technical Solution Design

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Click the following link for more information on your rights as an Applicant - http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
About Capgemini
A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clientsâ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.
Visit us at www.capgemini.com. People matter, results count.","Qualification  3-7 years  2 years min relevant experience in the role  experience, Bachelorâs DegreeCertification  Should have or seeking SE Level 1Should have progressing knowledge in Business Analysis, Business Knowledge, Software Engineering, Testing, Data Management, Architecture Knowledge and Technical Solution Design    ","Qualification 3-7 years 2 min relevant experience in the role experience, Bachelorâs DegreeCertification Should have or seeking SE Level 1Should progressing knowledge Business Analysis, Knowledge, Software Engineering, Testing, Data Management, Architecture Knowledge and Technical Solution Design","Qualification 3-7 years 2 min relevant experience role experience, Bachelorâs DegreeCertification Should seeking SE Level 1Should progressing knowledge Business Analysis, Knowledge, Software Engineering, Testing, Data Management, Architecture Knowledge Technical Solution Design"
204,Data Engineer,Senior Azure Data Engineer,"Atlanta, GA 30338",Atlanta,GA,"Overview
At Perficient youâll deliver mission-critical technology and business solutions to Fortune 500 companies and some of the most recognized brands on the planet. And youâll do it with cutting-edge technologies, thanks to our close partnerships with the worldâs biggest vendors. Our network of offices across North America, as well as locations in India and China, will give you the opportunity to spread your wings, too.

Weâre proud to be publicly recognized as a âTop Workplaceâ year after year. This is due, in no small part, to our entrepreneurial attitude and collaborative spirit that sets us apart and keeps our colleagues impassioned, driven, and fulfilled.

Perficient currently has a career opportunity for a Senior Azure Data Engineer.

Job Overview:
As a Senior Azure Data Engineer, you will technically lead cloud-focused software engineering teams to deliver workloads for our clients. You will participate in all aspects of the software development lifecycle, which includes estimating, technical design, implementation, documentation, testing, and deployment. We are looking for multi-talented professionals with deep technical backgrounds, experience building world-class digital experiences, and excellent team leadership skills.

You will provision and set up data platform technologies that are on-premises and in the cloud. You will manage and secure the flow of structured and unstructured data from multiple sources. The data platforms you use can include relational databases, nonrelational databases, data streams, and file stores. You will also ensure that data services securely and seamlessly integrate with other data platform technologies or application.

As part of our cloud migration, app modernization, and cloud-native development services, you will focus on data-related tasks in Azure. Your primary responsibilities include using services and tools to ingest, egress, and transform data from multiple sources. Azure data engineers collaborate with business stakeholders to identify and meet data requirements. They design and implement solutions, manage, monitor, and ensure the security and privacy of data to satisfy business needs.

Strong experience in designing data centric-systems in Azure leveraging Data Lake Storage, Blob Storage, Azure SQL Database, Azure SQL Data Warehouse, Cosmos DB, Azure Databricks, Azure Data Factory, and Stream Analytics is desired. Comprehension of modern solution architectures, solid .NET development practices, common API / service implementations, cloud/hybrid networking, and Azure IaaS experiences are a plus.

Platform Technologies: Document DBs, Graph Databases, SQL Databases, ELT and ETL tools, Microsoft Azure: Azure SQL Database, Cosmos DB, Data Factory, SQL Data Warehouse, Analysis Services, Power BI, Azure Search, App Services (Web, API, Logic apps), Azure Functions, Azure WebJobs, networking & security, Azure storage, Service Bus, ExpressRoute/VPN, Azure SQL Database, Redis Cache, Azure Search, Application Insights, Azure Active Directory, Visual Studio Team Services, Azure ARM templates, Informatica BDM, Hadoop, HDInsight

Delivery Technologies: SQL, PolyBase, Python, C#, Web API, JavaScript, Entity Framework, RESTful services, microservices, Visual Studio, WCF, PowerShell, Bash
Responsibilities
Technical and thought-leadership of Azure data design and implementation engagements
Contribute to customer elaboration and discovery sessions to inform solution design
Lead data architecture design, solution structures and component design
Craft high-quality Visio documents to communicate thoroughness of vision for system architecture topology, component design, networking and security/authentication of proposed solutions
Ensure that technical software development process is followed on the project, be familiar with industry best practices for software development and agile practices
Understand a broad spectrum of technology in order to provide part or all of a detailed technical design which meets customer requirements
Act as lead or technical architect on customer projects, closely aligned with the Microsoft Azure Practice to deliver a wide-range of PaaS-focused solutions
Provide technical leadership for on-premises and external integration points
Communicate across the clientâs community â consistently viewed as adding value
Contribute knowledge, tools, and positivity to the greater Perficient culture and community
Serve as a technical leader and mentor
Qualifications
3+ years of professional solution delivery experience and a Bachelor of Computer Science, MIS or equivalent degree; without a degree, three additional years of relevant professional experience (6+ years in total)
2+ years designing and building modern data pipelines and data streams
2+ years designing Azure data storage solutions (SQL Database, SQL Data Warehouse, Cosmos DB, Data Lake Storage)
2+ years developing data ingestion, data processing and data optimization (Databricks, Data Factory, Informatica, PolyBase)
Understating of Master Data Management (MDM) and data quality tools and processes
Understanding of Lambda and Kappa architecture patterns
Understanding of DevOps including CI/CD
Experience developing software architectures and key software components
High-level understanding of common authentication patterns and flow including single sign-on and OAuth
Experience with Agile/Scrum methodology
Ability to apply technology and consulting to solve a client business problem
Sufficient depth and breadth of technical knowledge to be individually responsible for the design and scope of deliverables within a field of expertise
Able to communicate and present complex issues with preciseness, assurance, and confidence
A disciplined approach to software development and problem solving
Passion for technology and a high technical aptitude
Insightful and always looking for breakthrough ideas
Self-sufficient, high integrity, more than just competent
Demonstrates the use of consulting skills including: questioning, listening, ideas development, permission and rapport, and influencing
Ability to conduct/lead oral status/technical interchange meetings with clients
Owns and produces customer documentation. Ability to translate technical details into concise and easy to understand written form
Ability to write relevant components of a proposal document (e.g. participate in RFIs and RFPs including answering specific technology related questions and coming up with initial high-level technical design and architecture including any necessary Visio diagrams and PowerPoint slides)
Ability to translate verbal requirements from face to face client meetings into requirements documents, statements of work, and proposals
Perficient full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and an outstanding benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs including billable bonus opportunities. Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Perficient a great place to work.

More About Perficient

Perficient is the leading digital transformation consulting firm serving Global 2000 and enterprise customers throughout North America. With unparalleled information technology, management consulting and creative capabilities, Perficient and its Perficient Digital agency deliver vision, execution and value with outstanding digital experience, business optimization and industry solutions.

Our work enables clients to improve productivity and competitiveness; grow and strengthen relationships with customers, suppliers and partners; and reduce costs. Perficient's professionals serve clients from a network of offices across North America and offshore locations in India and China. Traded on the Nasdaq Global Select Market, Perficient is a member of the Russell 2000 index and the S&P SmallCap 600 index.
Perficient is an award-winning IBM Premier Business Partner, a Microsoft National Service Provider and Gold Certified Partner, an Oracle Platinum Partner, an Adobe Business Solution Partner, and a Salesforce Gold Consulting Partner.

Perficient is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.
#LI-KG1","3+ years of professional solution delivery experience and a Bachelor of Computer Science, MIS or equivalent degree; without a degree, three additional years of relevant professional experience  6+ years in total  2+ years designing and building modern data pipelines and data streams 2+ years designing Azure data storage solutions  SQL Database, SQL Data Warehouse, Cosmos DB, Data Lake Storage  2+ years developing data ingestion, data processing and data optimization  Databricks, Data Factory, Informatica, PolyBase  Understating of Master Data Management  MDM  and data quality tools and processes Understanding of Lambda and Kappa architecture patterns Understanding of DevOps including CI/CD Experience developing software architectures and key software components High-level understanding of common authentication patterns and flow including single sign-on and OAuth Experience with Agile/Scrum methodology Ability to apply technology and consulting to solve a client business problem Sufficient depth and breadth of technical knowledge to be individually responsible for the design and scope of deliverables within a field of expertise Able to communicate and present complex issues with preciseness, assurance, and confidence A disciplined approach to software development and problem solving Passion for technology and a high technical aptitude Insightful and always looking for breakthrough ideas Self-sufficient, high integrity, more than just competent Demonstrates the use of consulting skills including  questioning, listening, ideas development, permission and rapport, and influencing Ability to conduct/lead oral status/technical interchange meetings with clients Owns and produces customer documentation. Ability to translate technical details into concise and easy to understand written form Ability to write relevant components of a proposal document  e.g. participate in RFIs and RFPs including answering specific technology related questions and coming up with initial high-level technical design and architecture including any necessary Visio diagrams and PowerPoint slides  Ability to translate verbal requirements from face to face client meetings into requirements documents, statements of work, and proposals  Technical and thought-leadership of Azure data design and implementation engagements Contribute to customer elaboration and discovery sessions to inform solution design Lead data architecture design, solution structures and component design Craft high-quality Visio documents to communicate thoroughness of vision for system architecture topology, component design, networking and security/authentication of proposed solutions Ensure that technical software development process is followed on the project, be familiar with industry best practices for software development and agile practices Understand a broad spectrum of technology in order to provide part or all of a detailed technical design which meets customer requirements Act as lead or technical architect on customer projects, closely aligned with the Microsoft Azure Practice to deliver a wide-range of PaaS-focused solutions Provide technical leadership for on-premises and external integration points Communicate across the clientâs community â consistently viewed as adding value Contribute knowledge, tools, and positivity to the greater Perficient culture and community Serve as a technical leader and mentor  ","3+ years of professional solution delivery experience and a Bachelor Computer Science, MIS or equivalent degree; without degree, three additional relevant 6+ in total 2+ designing building modern data pipelines streams Azure storage solutions SQL Database, Data Warehouse, Cosmos DB, Lake Storage developing ingestion, processing optimization Databricks, Factory, Informatica, PolyBase Understating Master Management MDM quality tools processes Understanding Lambda Kappa architecture patterns DevOps including CI/CD Experience software architectures key components High-level understanding common authentication flow single sign-on OAuth with Agile/Scrum methodology Ability to apply technology consulting solve client business problem Sufficient depth breadth technical knowledge be individually responsible for the design scope deliverables within field expertise Able communicate present complex issues preciseness, assurance, confidence A disciplined approach development solving Passion high aptitude Insightful always looking breakthrough ideas Self-sufficient, integrity, more than just competent Demonstrates use skills questioning, listening, development, permission rapport, influencing conduct/lead oral status/technical interchange meetings clients Owns produces customer documentation. translate details into concise easy understand written form write proposal document e.g. participate RFIs RFPs answering specific related questions coming up initial high-level any necessary Visio diagrams PowerPoint slides verbal requirements from face documents, statements work, proposals Technical thought-leadership implementation engagements Contribute elaboration discovery sessions inform Lead design, structures component Craft high-quality documents thoroughness vision system topology, networking security/authentication proposed Ensure that process is followed on project, familiar industry best practices agile Understand broad spectrum order provide part all detailed which meets Act as lead architect projects, closely aligned Microsoft Practice deliver wide-range PaaS-focused Provide leadership on-premises external integration points Communicate across clientâs community â consistently viewed adding value knowledge, tools, positivity greater Perficient culture Serve leader mentor","3+ years professional solution delivery experience Bachelor Computer Science, MIS equivalent degree; without degree, three additional relevant 6+ total 2+ designing building modern data pipelines streams Azure storage solutions SQL Database, Data Warehouse, Cosmos DB, Lake Storage developing ingestion, processing optimization Databricks, Factory, Informatica, PolyBase Understating Master Management MDM quality tools processes Understanding Lambda Kappa architecture patterns DevOps including CI/CD Experience software architectures key components High-level understanding common authentication flow single sign-on OAuth Agile/Scrum methodology Ability apply technology consulting solve client business problem Sufficient depth breadth technical knowledge individually responsible design scope deliverables within field expertise Able communicate present complex issues preciseness, assurance, confidence A disciplined approach development solving Passion high aptitude Insightful always looking breakthrough ideas Self-sufficient, integrity, competent Demonstrates use skills questioning, listening, development, permission rapport, influencing conduct/lead oral status/technical interchange meetings clients Owns produces customer documentation. translate details concise easy understand written form write proposal document e.g. participate RFIs RFPs answering specific related questions coming initial high-level necessary Visio diagrams PowerPoint slides verbal requirements face documents, statements work, proposals Technical thought-leadership implementation engagements Contribute elaboration discovery sessions inform Lead design, structures component Craft high-quality documents thoroughness vision system topology, networking security/authentication proposed Ensure process followed project, familiar industry best practices agile Understand broad spectrum order provide part detailed meets Act lead architect projects, closely aligned Microsoft Practice deliver wide-range PaaS-focused Provide leadership on-premises external integration points Communicate across clientâs community â consistently viewed adding value knowledge, tools, positivity greater Perficient culture Serve leader mentor"
205,Data Engineer,Data Analyst III - Data Engineer,"Atlanta, GA 30328",Atlanta,GA,"Cox Communications is the largest private telecom company in America, and we proudly serve six million homes and businesses across 18 states. At Cox, we are committed to creating meaningful moments of human connection, not only with our products and services, but also with our career opportunities. Come connect with us, and lets build a better future together.

Role Summary

Looking for a Senior Data Analyst to join an enterprise reporting and analytics team supporting Cox Communications Enterprise Finance and Accounting as well as our business partners. This person should be comfortable in a data engineering role building data pipelines and ETL processes, have enough business acumen to understand context and intent, and apply these concepts to data solutions. This role will assist in data strategy and design, and develop data-sets for the Finance BI ecosystem to support reporting, analysis & analytical modeling.

Recognized as a Subject Matter Expert (SME) and authority on issues related to Business Intelligence applications reporting, analysis, and metrics. This person will exercise an inquisitive mindset to transform business questions into actionable data exploration exercises and data sets using data analysis, modeling, automation, and optimization techniques. Operates with considerable latitude for independent judgment. Provides influence and expertise to cross-functional teams and other stakeholders on ideas and solutions that impact corporate results.

Primary Responsibilities and Essential Functions
 Supports and/or leads discussions with multidisciplinary teams to collect functional business requirements, scoping analytical projects, manage expectations and deadlines, and translate needs into technical specifications. Supports technical development of solutions to expedite delivery of new datasets, process automation, production of complex models and analyses, including gap assessments, and works to deliver strategies. Serves as an organizational consultant on matters relating to data and databases by providing expertise to assist users in meeting their needs. Initiates the identification of actionable insights and contribute to the development of business recommendations through effective presentations and communication of results. Develops processes and solutions to speed up / expedite the development of datasets, report automation, production of dashboards, complex models, and analyses. Extract and manipulate data from a variety of cloud and on-premise based systems for reporting and analytical purposes, including: Oracle databases, Essbase and data cubes, Tableau, SQL Server, and various data sources as needed. Report automation and self-service dashboard solutions using combination of tools such as SQL, Tableau, Prep, Alteryx, Informatica and Python. Develops automated processes that preserve data integrity by managing the alignment of data availability and integration processes. Expertise in creating and optimizing Tableau Datasets and Visualizations. Identifies, researches, and resolves discrepancies in an analytical procedure or cross-functional methods. Establish and maintain design and development best practices including keeping written procedures to document data processes and ensure best data governance practices are maintained. Liaises with CCI Technology/EDS partners for both data-sourcing needs and for âpromotionâ of data-sets into the enterprise BI layer when/as-needed. Leads data collection, cleansing, and validation. Conducts day-to-day activities with minimum supervision.

Qualifications:
Skills and Qualifications

Minimum
 5 or more years of experience required in related field (developing and implementing analytical solutions in Finance, Marketing, Sales or Operations). 3 or more years of experience required if candidate possesses a related advanced degree. Requires strong skills in SQL writing and query optimization. Requires experience building data workflows, manipulation of large data sets, or developing data pipelines in analytical tools such as Informatica, Alteryx, Tableau Prep, SQL, SSIS, etc. Requires strong skills and experience with reporting and data visualization in analytical tools such as Tableau, Prep, SQL, etc. Requires effective proficiency in teamwork, communication, presentation, and time management to work effectively with teams throughout organization, including strong verbal and written communication. Experience manipulating large datasets and the ability to extrapolate conclusions from the data. Demonstrated problem solving and analytical thinking skills. Excellent interpersonal, leadership, presentation, and collaborative skills to work effectively with teams throughout organization. BS/BA degree in related discipline

Preferred
 Master's degree in a related discipline preferred Working knowledge of Tableau and/or visual analytics software tools and best practices Working knowledge of Alteryx and/or data transformation tools and best practices 3 or more years using Tableau for analysis, visualization & dashboard development 3 or more years of experience in database administration and SQL, including SQL tuning/performance optimization 2 or more years of experience in Data Warehousing, ETL Development, Data Management tools such as SSIS, Informatica, Datastage, etc 1 or more years of experience using Oracle Essbase or integration with data cubes 2 or more years of experience using Alteryx for data preparation & modeling 1 or more years of experience in OBIEE report and RPD development 1 or more years of experience in Java, Python or other programming or scripting languages Knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience within telecom, consumer package goods, retail, financial services, or consulting industries Operational analytics including application for Call Center, Technical Support, Collections, and Customer Experience Analytics
#LI-355

About Cox Communications
Cox Communications is committed to creating meaningful moments of human connection through broadband applications and services. The largest private telecom company in America, we proudly serve six million homes and businesses across 18 states. We're dedicated to empowering others to build a better future and celebrate diverse products, people, suppliers, communities and the characteristics that makes each one unique. Cox Communications is the largest division of Cox Enterprises, a family-owned business founded in 1898 by Governor James M. Cox.
Cox is an Equal Employment Opportunity employer - All qualified applicants/employees will receive consideration for employment without regard to that individual's age, race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender, gender identity, physical or mental disability, veteran status, genetic information, ethnicity, citizenship, or any other characteristic protected by law.
Statement to ALL Third-Party Agencies and Similar Organizations: Cox accepts resumes only from agencies with which we formally engage their services. Please do not forward resumes to our applicant tracking system, Cox employees, Cox hiring manager, or send to any Cox facility. Cox is not responsible for any fees or charges associated with unsolicited resumes."," 5 or more years of experience required in related field  developing and implementing analytical solutions in Finance, Marketing, Sales or Operations . 3 or more years of experience required if candidate possesses a related advanced degree. Requires strong skills in SQL writing and query optimization. Requires experience building data workflows, manipulation of large data sets, or developing data pipelines in analytical tools such as Informatica, Alteryx, Tableau Prep, SQL, SSIS, etc. Requires strong skills and experience with reporting and data visualization in analytical tools such as Tableau, Prep, SQL, etc. Requires effective proficiency in teamwork, communication, presentation, and time management to work effectively with teams throughout organization, including strong verbal and written communication. Experience manipulating large datasets and the ability to extrapolate conclusions from the data. Demonstrated problem solving and analytical thinking skills. Excellent interpersonal, leadership, presentation, and collaborative skills to work effectively with teams throughout organization. BS/BA degree in related discipline    ","5 or more years of experience required in related field developing and implementing analytical solutions Finance, Marketing, Sales Operations . 3 if candidate possesses a advanced degree. Requires strong skills SQL writing query optimization. building data workflows, manipulation large sets, pipelines tools such as Informatica, Alteryx, Tableau Prep, SQL, SSIS, etc. with reporting visualization Tableau, effective proficiency teamwork, communication, presentation, time management to work effectively teams throughout organization, including verbal written communication. Experience manipulating datasets the ability extrapolate conclusions from data. Demonstrated problem solving thinking skills. Excellent interpersonal, leadership, collaborative organization. BS/BA degree discipline","5 years experience required related field developing implementing analytical solutions Finance, Marketing, Sales Operations . 3 candidate possesses advanced degree. Requires strong skills SQL writing query optimization. building data workflows, manipulation large sets, pipelines tools Informatica, Alteryx, Tableau Prep, SQL, SSIS, etc. reporting visualization Tableau, effective proficiency teamwork, communication, presentation, time management work effectively teams throughout organization, including verbal written communication. Experience manipulating datasets ability extrapolate conclusions data. Demonstrated problem solving thinking skills. Excellent interpersonal, leadership, collaborative organization. BS/BA degree discipline"
206,Data Engineer,Data Engineer,"Duluth, GA 30097",Duluth,GA,"The Data Engineer will be responsible for expanding and optimizing our data capabilities for Data Warehousing, Master Data, Data Services as well as Business Intelligence. The ideal candidate is an experienced data wrangler who enjoys optimizing data systems and can build them from the ground up. He/she must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The ideal candidate will be excited by the prospect of optimizing or even re-designing our companyâs data architecture to support our next generation of products and data initiatives.
Responsibilities
Drive design and development of internal data pipeline architecture to service BI and Analytics capabilities
Work with key stakeholders including Product, Sales, Quality Engineering and Marketing to assist with data-related technical needs and support their data infrastructure needs.
Ensure operational resilience, stability, and scalability of our enterprise data platform by conduct continuous hardening activates that increase uptime, data quality and reduce cost
Ensure tight data platform security during data transport and while at rest
Build hybrid cloud/prem data platform to enable self-service Business Intelligence and Analytics functions
Conduct data profiling and analysis of complex data sets to discover how to meet functional / non-functional requirements
Institute data quality monitoring and alerting platform operations
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Drive solutions for meta data and data lineage management
Drive innovation by recommending and driving adoption of new technologies that provide competitive data advantages for enterprise
Contributes to agile team alignment and is committed to constant improvement efforts by participating in team ceremonies sprint planning, stand-ups, backlog grooming and retrospectives
Ensure technical delivery of detailed feature/story level solutions that satisfies the IT roadmapâs acceptance criteria
Maintains professional and technical knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; participating in professional societies
Minimum Work Experience
8+ years of proven experience in the design, development and deployment of end-to-end data pipeline solutions including data processing (Batch, Micro-Batch, Streaming), data preparation (ETL, ELT), data modeling (STAR, OLAP, MPP)
5+ year of Business Intelligence Development and Administration
Experience of building data solutions for back office applications that source data from CRM and ERP applications
Advanced experience working with SQL Server databases
Strong grasp of master data management, data quality monitoring and metadata management
Proficiency in data centric shell scripting, ETL, Python, and/or .NET Programming skills
A comfortable and confident communicator with technical staff but also able to speak with customers concisely to translate technical concepts into business terminology and impacts
Exposure to continuous integration implementations that utilize DevOps style tools (such as Jenkins, Chef, Docker, Terraform, etc.)
Proven team player with the ability to multi-task in a fast-paced dynamic agile work environment
Has previously supported a metrics-driven data culture to drive accountability and transparency
Passionate problem solver and motivated self-starter including ability to analyze situation and recommend sound solutions and implementation strategies
Additional Desired Skills:
Technology certifications such as Amazon Certified Solutions Architect
Proficiency in programming in Spark, R and/or ML packages
Exposure to applications developed to support manufacturing quality
Experience with MS Dynamics CRM, MicroStrategy BI tools, SAP ERP
Consulting experience
Education: Bachelorâs degree in Computer Science, Information Systems, or combination of education and experience.
Location: Duluth, Georgia
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractorâs legal duty to furnish information.","  Technology certifications such as Amazon Certified Solutions Architect Proficiency in programming in Spark, R and/or ML packages Exposure to applications developed to support manufacturing quality Experience with MS Dynamics CRM, MicroStrategy BI tools, SAP ERP Consulting experience  Drive design and development of internal data pipeline architecture to service BI and Analytics capabilities Work with key stakeholders including Product, Sales, Quality Engineering and Marketing to assist with data-related technical needs and support their data infrastructure needs. Ensure operational resilience, stability, and scalability of our enterprise data platform by conduct continuous hardening activates that increase uptime, data quality and reduce cost Ensure tight data platform security during data transport and while at rest Build hybrid cloud/prem data platform to enable self-service Business Intelligence and Analytics functions Conduct data profiling and analysis of complex data sets to discover how to meet functional / non-functional requirements Institute data quality monitoring and alerting platform operations Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics Drive solutions for meta data and data lineage management Drive innovation by recommending and driving adoption of new technologies that provide competitive data advantages for enterprise Contributes to agile team alignment and is committed to constant improvement efforts by participating in team ceremonies sprint planning, stand-ups, backlog grooming and retrospectives Ensure technical delivery of detailed feature/story level solutions that satisfies the IT roadmapâs acceptance criteria Maintains professional and technical knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; participating in professional societies  ","Technology certifications such as Amazon Certified Solutions Architect Proficiency in programming Spark, R and/or ML packages Exposure to applications developed support manufacturing quality Experience with MS Dynamics CRM, MicroStrategy BI tools, SAP ERP Consulting experience Drive design and development of internal data pipeline architecture service Analytics capabilities Work key stakeholders including Product, Sales, Quality Engineering Marketing assist data-related technical needs their infrastructure needs. Ensure operational resilience, stability, scalability our enterprise platform by conduct continuous hardening activates that increase uptime, reduce cost tight security during transport while at rest Build hybrid cloud/prem enable self-service Business Intelligence functions Conduct profiling analysis complex sets discover how meet functional / non-functional requirements Institute monitoring alerting operations analytics tools utilize the provide actionable insights into customer acquisition, efficiency other business performance metrics solutions for meta lineage management innovation recommending driving adoption new technologies competitive advantages Contributes agile team alignment is committed constant improvement efforts participating ceremonies sprint planning, stand-ups, backlog grooming retrospectives delivery detailed feature/story level satisfies IT roadmapâs acceptance criteria Maintains professional knowledge attending educational workshops; reviewing publications; establishing personal networks; societies","Technology certifications Amazon Certified Solutions Architect Proficiency programming Spark, R and/or ML packages Exposure applications developed support manufacturing quality Experience MS Dynamics CRM, MicroStrategy BI tools, SAP ERP Consulting experience Drive design development internal data pipeline architecture service Analytics capabilities Work key stakeholders including Product, Sales, Quality Engineering Marketing assist data-related technical needs infrastructure needs. Ensure operational resilience, stability, scalability enterprise platform conduct continuous hardening activates increase uptime, reduce cost tight security transport rest Build hybrid cloud/prem enable self-service Business Intelligence functions Conduct profiling analysis complex sets discover meet functional / non-functional requirements Institute monitoring alerting operations analytics tools utilize provide actionable insights customer acquisition, efficiency business performance metrics solutions meta lineage management innovation recommending driving adoption new technologies competitive advantages Contributes agile team alignment committed constant improvement efforts participating ceremonies sprint planning, stand-ups, backlog grooming retrospectives delivery detailed feature/story level satisfies IT roadmapâs acceptance criteria Maintains professional knowledge attending educational workshops; reviewing publications; establishing personal networks; societies"
207,Data Engineer,"Lead Data Engineer, Assessment Analytics & Visualization","Atlanta, GA 30309",Atlanta,GA,"Invesco is one of the worldâs leading global investment managers, entrusted with managing $1.2 trillion* in assets on behalf of clients worldwide. We are the 6th largest US retail asset manager and the 13th largest investment manager globally, and our more than 8,000 employees worldwide are dedicated to delivering an investment experience that helps people get more out of life. We are purely focused on managing a comprehensive range of active, passive and alternative investment capabilities, which we draw on to provide customized solutions aligned to client needs, our most important benchmark. (*As of May 31,2019)

Job Purpose (Job Summary):

As a Team Lead at Invesco, you would align business outcomes to technology goals and objectives. You will be responsible for managing decision sciences and advanced analytics initiatives, data integration/engineering. Interacts with Data Scientists, Business Owners, Business Data Analysts, Data Modelers, Architects, and Application Developers, to design, build and manage large-scale batch and real-time data pipelines utilizing various data analytics processing frameworks in support of Data Science practice. The position requires building strong technical hands-on-skills and experiences as well as establishing close business relationships across the enterprise. Working closely with marketing and sales stakeholders, you will establish and drive long term advance analytics roadmap manage increasing demands for advance analytics in Sales and Marketing. You will help steer the strategic technology direction, define target state architecture, technology roadmaps and mentor others. A successful candidate must have a demonstrable background with experience in development of high performance, distributed computing tasks using Big Data technologies such as Hadoop (platform level), NoSQL, text mining and other distributed environment technologies based on the needs of the organization. Responsible for analyzing, designing, programing, debugging and modifying software enhancements and/or new products used in distributed, large scale analytics solutions. Strong visualization skills and experience with BI tools (ex. Tableau)

Key Responsibilities / Duties:
Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities
Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities
Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies
Assist in the decision-making process related to the selection of software architecture solutions
Implement architectures to handle web-scale data and its organization
Execute strategies that inform data design and architecture partnering with enterprise standard
Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies
Assist in creating documents that ensure consistency in development across the online organization. Implements and improves core software infrastructure
Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments
Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools (such as control charts and scorecards), readiness and adoption of data w/in the organization
Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding
Serve as strong advocate to improve analytical capability across the organization
Communicate with various business areas and to gather and prioritize their business requirements
Support various reporting and BI solutions (Tableau, Power BI, Salesforce Einstein)
Manage application and data integration platforms (Informatica, Talend)
Manage the full life cycle of development/reporting/integration projects: planning, design, develop, testing and rollout
Manage solution providers, define sourcing approach and manage the providers
Create and manage data, applications and technology architecture documentation and design artifacts
Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement
Gain adoption of architecture processes, standards and procedures
Work Experience / Knowledge:
7+ years of experience in data modeling, data warehousing, and big data architectures
5+ years of experience in a data engineering role
Proficient in application/software architecture (Definition, Business Process Modeling, etc.)
Deep expertise in (at least one) SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments
The role will be responsible for providing innovative operational solutions and best practices
Advanced knowledge in SQL/Hive, Spark, NoSQL, (Java/Python is a plus)
Strong programming skills and ability to utilize a variety of software/languages/tools; e.g., Spark, R, Python, Scala, Java, Hive, SQL, SAS, Tableau, etc.
Experience with microservice development, Docker, Kubernetes
Develop software to run on cloud native big data infrastructure built on AWS using Spark, Lambda, S3, and other cloud native services
Designs and develops complex and large-scale data structures and pipelines to organize, collect and standardize data to generate insight
3+ years of experience in data integration platforms (Informatica, Talend)
Strong analytics and reporting skills â hands on experience in BI Tools like Tableau, Power BI, Salesforce Einstein
Hands on experience in self-service data preparation tool like Alteryx
Experience using GitHub, Bit Bucket, or other code repository solution
DevOps experience is a plus
Skills / Other Personal Attributes Required:
Previous management experience and successfully leading a team of direct reports
Proven track record of effectively leading and managing employees, managing performance, setting goals for team/individuals and provide mentoring.
Strong Tableau, Adobe Analytics other BI solution
Expert Oracle and Vertica skillsets
Experience using JIRA and Agile Project Management software
Experience using GitHub, Bit Bucket, or other code repository solution
Strong written, verbal communication and presentation skills
Ability to explain complex technical issues in a way that non-technical people may understand
Able to work in a global, multicultural environment
Self-motivated. Capable of working with little or no supervision
Ability to react positively under pressure to meet tight deadlines
Able to work independently or as a team player
Enjoy challenging and thought provoking work and have a strong desire to learn and progress
Formal Education: (minimum requirement to perform job duties)
BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience
FLSA (US Only): Exempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.

Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment."," Previous management experience and successfully leading a team of direct reports Proven track record of effectively leading and managing employees, managing performance, setting goals for team/individuals and provide mentoring. Strong Tableau, Adobe Analytics other BI solution Expert Oracle and Vertica skillsets Experience using JIRA and Agile Project Management software Experience using GitHub, Bit Bucket, or other code repository solution Strong written, verbal communication and presentation skills Ability to explain complex technical issues in a way that non-technical people may understand Able to work in a global, multicultural environment Self-motivated. Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player Enjoy challenging and thought provoking work and have a strong desire to learn and progress Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Implement architectures to handle web-scale data and its organization Execute strategies that inform data design and architecture partnering with enterprise standard Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Assist in creating documents that ensure consistency in development across the online organization. Implements and improves core software infrastructure Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools  such as control charts and scorecards , readiness and adoption of data w/in the organization Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding Serve as strong advocate to improve analytical capability across the organization Communicate with various business areas and to gather and prioritize their business requirements Support various reporting and BI solutions  Tableau, Power BI, Salesforce Einstein  Manage application and data integration platforms  Informatica, Talend  Manage the full life cycle of development/reporting/integration projects  planning, design, develop, testing and rollout Manage solution providers, define sourcing approach and manage the providers Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement Gain adoption of architecture processes, standards and procedures  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience ","Previous management experience and successfully leading a team of direct reports Proven track record effectively managing employees, performance, setting goals for team/individuals provide mentoring. Strong Tableau, Adobe Analytics other BI solution Expert Oracle Vertica skillsets Experience using JIRA Agile Project Management software GitHub, Bit Bucket, or code repository written, verbal communication presentation skills Ability to explain complex technical issues in way that non-technical people may understand Able work global, multicultural environment Self-motivated. Capable working with little no supervision react positively under pressure meet tight deadlines independently as player Enjoy challenging thought provoking have strong desire learn progress Work development teams project leaders/stakeholders solutions enable business capabilities Supports the data science community by enabling availability environments advanced analytical Maintains broad understanding implementation, integration, inter-connectivity emerging technologies define strategies Assist decision-making process related selection architecture Implement architectures handle web-scale its organization Execute inform design partnering enterprise standard Build robust pipelines on public Cloud AWS Kinesis, Kafka, Lambda creating documents ensure consistency across online organization. Implements improves core infrastructure Deep expertise SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. writing programs, implementing architectures, automation these Develop maintain reporting, ensuring reliability delivery performance tools such control charts scorecards , readiness adoption w/in Consolidate, standardize changes capacity metric definitions, ownership, accountability taxonomy alignment Serve advocate improve capability Communicate various areas gather prioritize their requirements Support reporting Power BI, Salesforce Einstein Manage application integration platforms Informatica, Talend full life cycle development/reporting/integration projects planning, design, develop, testing rollout providers, sourcing approach manage providers Create data, applications technology documentation artifacts deliver meaningful reference outline principles best practices advancement Gain processes, standards procedures BS Computer Science, Applied Mathematics, Physics, Statistics area study sciences mining relevant","Previous management experience successfully leading team direct reports Proven track record effectively managing employees, performance, setting goals team/individuals provide mentoring. Strong Tableau, Adobe Analytics BI solution Expert Oracle Vertica skillsets Experience using JIRA Agile Project Management software GitHub, Bit Bucket, code repository written, verbal communication presentation skills Ability explain complex technical issues way non-technical people may understand Able work global, multicultural environment Self-motivated. Capable working little supervision react positively pressure meet tight deadlines independently player Enjoy challenging thought provoking strong desire learn progress Work development teams project leaders/stakeholders solutions enable business capabilities Supports data science community enabling availability environments advanced analytical Maintains broad understanding implementation, integration, inter-connectivity emerging technologies define strategies Assist decision-making process related selection architecture Implement architectures handle web-scale organization Execute inform design partnering enterprise standard Build robust pipelines public Cloud AWS Kinesis, Kafka, Lambda creating documents ensure consistency across online organization. Implements improves core infrastructure Deep expertise SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. writing programs, implementing architectures, automation Develop maintain reporting, ensuring reliability delivery performance tools control charts scorecards , readiness adoption w/in Consolidate, standardize changes capacity metric definitions, ownership, accountability taxonomy alignment Serve advocate improve capability Communicate various areas gather prioritize requirements Support reporting Power BI, Salesforce Einstein Manage application integration platforms Informatica, Talend full life cycle development/reporting/integration projects planning, design, develop, testing rollout providers, sourcing approach manage providers Create data, applications technology documentation artifacts deliver meaningful reference outline principles best practices advancement Gain processes, standards procedures BS Computer Science, Applied Mathematics, Physics, Statistics area study sciences mining relevant"
208,Data Engineer,Data Engineer,"Atlanta, GA",Atlanta,GA,"Company Summary:
Headquartered in Atlanta, Ga, Jvion is the leader in Artificial Intelligence (AI)-enabled prescriptive analytics that helps healthcare organizations lower rates of preventable harm and reduce costs. Charting the future with our proprietary solution, Jvion is partnering with healthcare organizations to empower clinicians with AI that more effectively identifies at risk patients, the factors driving that risk, and the interventions that will lead to better outcomes. As the leader in healthcare AI, Jvion is making healthcare data meaningful, bringing leading edge technology to the front lines of care, tackling socioeconomic barriers to health and ultimately improving patients' lives.

Position Summary:
As a key member ofâ¯Jvion'sâ¯technology team, the Data Engineer is responsible for managing, optimizing, analyzing, overseeing, and monitoring data retrieval, storage, and distribution. This individual will prepare the ""big data"" infrastructure to be analyzed by theâ¯Jvionâ¯Machine.â¯Theâ¯Data Engineer participates asâ¯aâ¯memberâ¯of technical professionals in the development, delivery and support of cutting-edge Artificial Intelligence, cloud-based prescriptive solutions to healthcare clients that deliver high levels of customer satisfaction.

Responsibilities and Essential Functions:

Create data ingestion pipelines and manage and maintain master data specification documents which reflect necessary data elements. Cleanse, analyze, and maintain incoming client data. Develop features relevant to healthcare use cases/vectors.
Generate data visualizations and presentations, including the design of interactive and intuitive dashboards.
Participate in process adherence and continuance through automation of workflow schedules and cyclic support to ensure stable production environment.

Experience/Education:

Bachelor's degreeâ¯in Information Technology/Computer Engineering
3+ years of experience in development and deployment of Big Data technologies with a focus on Data Engineering (SQL, AWS/Azure Cloud platform tools, Data handling, Shell Scripting, Airflow, Python, etc.)
Expertise in cloud platforms (AWS, Azure)
Experience in data interchange standards like EDI and HL7 is a plus
Familiarity in Agile delivery framework
Awareness of Artificial Intelligence; data science concepts a plus
Experience working in healthcare setting preferred

Work Skills / Personal Characteristics:

You are passionate aboutâ¯making a difference in the world and the impactâ¯technology canâ¯have in helping people get better faster
You drive results effectively through cross functional teams
You are flexible and adaptiveâ¯andâ¯can adjust quicklyâ¯in the face of changing situations and challenges
You are disciplined and able to manage multiple responsibilities simultaneously
You are a strong team player whoâ¯is happy to share knowledge and mentor others to grow their skills
You work effectively with minimal supervision and can be relied on to deliver on commitments effectively

Jvionâ¯is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.","  You are passionate aboutâ¯making a difference in the world and the impactâ¯technology canâ¯have in helping people get better faster You drive results effectively through cross functional teams You are flexible and adaptiveâ¯andâ¯can adjust quicklyâ¯in the face of changing situations and challenges You are disciplined and able to manage multiple responsibilities simultaneously You are a strong team player whoâ¯is happy to share knowledge and mentor others to grow their skills You work effectively with minimal supervision and can be relied on to deliver on commitments effectively   Create data ingestion pipelines and manage and maintain master data specification documents which reflect necessary data elements. Cleanse, analyze, and maintain incoming client data. Develop features relevant to healthcare use cases/vectors. Generate data visualizations and presentations, including the design of interactive and intuitive dashboards. Participate in process adherence and continuance through automation of workflow schedules and cyclic support to ensure stable production environment.   Bachelor's degreeâ¯in Information Technology/Computer Engineering 3+ years of experience in development and deployment of Big Data technologies with a focus on Data Engineering  SQL, AWS/Azure Cloud platform tools, Data handling, Shell Scripting, Airflow, Python, etc.  Expertise in cloud platforms  AWS, Azure  Experience in data interchange standards like EDI and HL7 is a plus Familiarity in Agile delivery framework Awareness of Artificial Intelligence; data science concepts a plus Experience working in healthcare setting preferred  ","You are passionate aboutâ¯making a difference in the world and impactâ¯technology canâ¯have helping people get better faster drive results effectively through cross functional teams flexible adaptiveâ¯andâ¯can adjust quicklyâ¯in face of changing situations challenges disciplined able to manage multiple responsibilities simultaneously strong team player whoâ¯is happy share knowledge mentor others grow their skills work with minimal supervision can be relied on deliver commitments Create data ingestion pipelines maintain master specification documents which reflect necessary elements. Cleanse, analyze, incoming client data. Develop features relevant healthcare use cases/vectors. Generate visualizations presentations, including design interactive intuitive dashboards. Participate process adherence continuance automation workflow schedules cyclic support ensure stable production environment. Bachelor's degreeâ¯in Information Technology/Computer Engineering 3+ years experience development deployment Big Data technologies focus SQL, AWS/Azure Cloud platform tools, handling, Shell Scripting, Airflow, Python, etc. Expertise cloud platforms AWS, Azure Experience interchange standards like EDI HL7 is plus Familiarity Agile delivery framework Awareness Artificial Intelligence; science concepts working setting preferred","You passionate aboutâ¯making difference world impactâ¯technology canâ¯have helping people get better faster drive results effectively cross functional teams flexible adaptiveâ¯andâ¯can adjust quicklyâ¯in face changing situations challenges disciplined able manage multiple responsibilities simultaneously strong team player whoâ¯is happy share knowledge mentor others grow skills work minimal supervision relied deliver commitments Create data ingestion pipelines maintain master specification documents reflect necessary elements. Cleanse, analyze, incoming client data. Develop features relevant healthcare use cases/vectors. Generate visualizations presentations, including design interactive intuitive dashboards. Participate process adherence continuance automation workflow schedules cyclic support ensure stable production environment. Bachelor's degreeâ¯in Information Technology/Computer Engineering 3+ years experience development deployment Big Data technologies focus SQL, AWS/Azure Cloud platform tools, handling, Shell Scripting, Airflow, Python, etc. Expertise cloud platforms AWS, Azure Experience interchange standards like EDI HL7 plus Familiarity Agile delivery framework Awareness Artificial Intelligence; science concepts working setting preferred"
209,Data Engineer,Senior Data Engineer,"Atlanta, GA",Atlanta,GA,"Company Summary:
Headquartered in Atlanta, GA, Jvion is the leader in Artificial Intelligence (AI)-enabled prescriptive analytics that helps healthcare organizations lower rates of preventable harm and reduce costs. Charting the future with our proprietary solution, Jvion is partnering with healthcare organizations to empower clinicians with AI that more effectively identifies at risk patients, the factors driving that risk, and the interventions that will lead to better outcomes. As the leader in healthcare AI, Jvion is making healthcare data meaningful, bringing leading edge technology to the front lines of care, tackling socioeconomic barriers to health and ultimately improving patients' lives.

Position Summary:
As a key member ofâ¯Jvion'sâ¯technology team, the Senior Data Engineer is responsible for managing, optimizing, analyzing, overseeing, and monitoring data retrieval, storage, and distribution. This individual will prepare the ""big data"" infrastructure to be analyzed by theâ¯Jvionâ¯Machine.â¯The Seniorâ¯Data Engineer participates asâ¯aâ¯memberâ¯of technical professionals in the development, delivery and support of cutting-edge Artificial Intelligence, cloud-based prescriptive solutions to healthcare clients that deliver high levels of customer satisfaction.

Responsibilities and Essential Functions:

Design and create data ingestion pipelines and manage and maintain master data specification documents which reflect necessary data elements. Cleanse, analyze, and maintain incoming client data. Design and develop features relevant to healthcare use cases/vectors.
Lead in generating data visualizations and presentations, including the design of interactive and intuitive dashboards.
Lead process adherence and continuance through automation of workflow schedules and cyclic support to ensure stable production environment.

Experience/Education:

Bachelor's degreeâ¯in Information Technology/Computer Engineering
5+ years of experience in development and deployment of Big Data technologies with a focus on Data Engineering (SQL, AWS/Azure Cloud platform tools, Data handling, Shell Scripting, Airflow, Python, etc.)
Expertise in cloud platforms (AWS, Azure)
Experience in data interchange standards like EDI and HL7 is a plus
Familiarity in Agile delivery framework
Awareness of Artificial Intelligence; data science concepts a plus
Experience working in healthcare setting preferred

Work Skills / Personal Characteristics:

You are passionate aboutâ¯making a difference in the world and the impactâ¯technology canâ¯have in helping people get better faster
You drive results effectively through cross functional teams
You are flexible and adaptiveâ¯andâ¯can adjust quicklyâ¯in the face of changing situations and challenges
You are disciplined and able to manage multiple responsibilities simultaneously
You are a strong team player whoâ¯is happy to share knowledge and mentor others to grow their skills
You work effectively with minimal supervision and can be relied on to deliver on commitments effectively

Jvionâ¯is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.","  You are passionate aboutâ¯making a difference in the world and the impactâ¯technology canâ¯have in helping people get better faster You drive results effectively through cross functional teams You are flexible and adaptiveâ¯andâ¯can adjust quicklyâ¯in the face of changing situations and challenges You are disciplined and able to manage multiple responsibilities simultaneously You are a strong team player whoâ¯is happy to share knowledge and mentor others to grow their skills You work effectively with minimal supervision and can be relied on to deliver on commitments effectively   Design and create data ingestion pipelines and manage and maintain master data specification documents which reflect necessary data elements. Cleanse, analyze, and maintain incoming client data. Design and develop features relevant to healthcare use cases/vectors. Lead in generating data visualizations and presentations, including the design of interactive and intuitive dashboards. Lead process adherence and continuance through automation of workflow schedules and cyclic support to ensure stable production environment.   Bachelor's degreeâ¯in Information Technology/Computer Engineering 5+ years of experience in development and deployment of Big Data technologies with a focus on Data Engineering  SQL, AWS/Azure Cloud platform tools, Data handling, Shell Scripting, Airflow, Python, etc.  Expertise in cloud platforms  AWS, Azure  Experience in data interchange standards like EDI and HL7 is a plus Familiarity in Agile delivery framework Awareness of Artificial Intelligence; data science concepts a plus Experience working in healthcare setting preferred  ","You are passionate aboutâ¯making a difference in the world and impactâ¯technology canâ¯have helping people get better faster drive results effectively through cross functional teams flexible adaptiveâ¯andâ¯can adjust quicklyâ¯in face of changing situations challenges disciplined able to manage multiple responsibilities simultaneously strong team player whoâ¯is happy share knowledge mentor others grow their skills work with minimal supervision can be relied on deliver commitments Design create data ingestion pipelines maintain master specification documents which reflect necessary elements. Cleanse, analyze, incoming client data. develop features relevant healthcare use cases/vectors. Lead generating visualizations presentations, including design interactive intuitive dashboards. process adherence continuance automation workflow schedules cyclic support ensure stable production environment. Bachelor's degreeâ¯in Information Technology/Computer Engineering 5+ years experience development deployment Big Data technologies focus SQL, AWS/Azure Cloud platform tools, handling, Shell Scripting, Airflow, Python, etc. Expertise cloud platforms AWS, Azure Experience interchange standards like EDI HL7 is plus Familiarity Agile delivery framework Awareness Artificial Intelligence; science concepts working setting preferred","You passionate aboutâ¯making difference world impactâ¯technology canâ¯have helping people get better faster drive results effectively cross functional teams flexible adaptiveâ¯andâ¯can adjust quicklyâ¯in face changing situations challenges disciplined able manage multiple responsibilities simultaneously strong team player whoâ¯is happy share knowledge mentor others grow skills work minimal supervision relied deliver commitments Design create data ingestion pipelines maintain master specification documents reflect necessary elements. Cleanse, analyze, incoming client data. develop features relevant healthcare use cases/vectors. Lead generating visualizations presentations, including design interactive intuitive dashboards. process adherence continuance automation workflow schedules cyclic support ensure stable production environment. Bachelor's degreeâ¯in Information Technology/Computer Engineering 5+ years experience development deployment Big Data technologies focus SQL, AWS/Azure Cloud platform tools, handling, Shell Scripting, Airflow, Python, etc. Expertise cloud platforms AWS, Azure Experience interchange standards like EDI HL7 plus Familiarity Agile delivery framework Awareness Artificial Intelligence; science concepts working setting preferred"
210,Data Engineer,Lead Data Engineer - Buckhead - 8491,"Atlanta, GA",Atlanta,GA,"At Cortland, you map the story of your success. We don't adhere to the status quo, we love outside industry perspective, and we thrive on exploring possibilities and reimagining solutions. As an innovative leader in multifamily, our high performance continues to drive exponential growth â and we invite you to join us on our journey towards real estate excellence. With tools and guidance to sharpen your skills, you can forge your own career path, love what you do, and let it show.
Key Responsibilites
Collaborate to architect and implement end-to-end cloud infrastructure (analytics, compute, databases, DevOps, identity, integration, management, networking, security, and storage)
Leverage technologies such as the Azure BI Stack, BI and GIS visualization platforms, and modern developer tools to execute innovative solutions that facilitate data-driven analysis, automation, and data science
Apply dimensional data modeling to solve business problems
Analyze, develop, and maintain data pipelines from internal and external sources, utilizing Python and Azure Data Factory
Profile and analyze data in designing scalable solutions
Apply and build automated test-driven development, continuous integration/delivery, and version control best practices
Preferred Qualifications
Degree in Computer Science, Engineering, Mathematics, Statistics or related quantitative field
5+ yearsâ experience in RDBMS systems, data warehousing, advanced SQL Server Analytical development, and sophisticated data analysis
Expertise with Azure Cloud Technologies (Data Factory, PowerShell, Data Lake and Data Lake Analytics)
Extensive experience with Data Modeling and ETL tools, Business Intelligence platforms, API Integration, and Object-Oriented Programming (OOP)
Ability to thrive in a cross-functional environment utilizing modern technologies (Python, Git, Jenkins, Octopus Deploy, Tensorflow, Domo, ArcGIS, E/R Studio, RedGate DLM Automation and other tools)
Experience with messaging/event processing tooling and frameworks such as Azure Event Hub, Kafka, Kinesis
Working knowledge of Azure HDInsight + Spark, Azure Databricks, Azure Stream Analytics
At Cortland, we create, reimagine, and manage apartment communities for residents nationwide. Headquartered in Atlanta, GA, we have communities and regional offices all over the country, as well as overseas. From product design and procurement to general contracting and property management, we do it all â to make sure our communities are the perfect setting for living life to its fullest.

Our success is fueled by our belief in a better life â where hospitality is always a given, each detail is worth a second thought, and every open door is a new opportunity to go beyond expectations. We come to work every day to create possibilities for people â possibilities that translate into superior living spaces and experiences designed to inspire our residents, associates, and investors to live a better life focused on what matters most to them.

Cortland is an equal opportunity employer, and weâre proud to support and celebrate diversity in the workplace. We are committed to equal consideration for all qualified applicants regardless of race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, protected veteran status, genetic information, or any other characteristic protected by applicable law. If you have a disability and need an accommodation or assistance with the application process and/or using our website, please email talentresources@cortland.com or call 404.965.3988.

Cortland is a drug-free workplace.

Cortland participates in e-verify to verify the employment status of all persons hired to work in the United States."," Degree in Computer Science, Engineering, Mathematics, Statistics or related quantitative field 5+ yearsâ experience in RDBMS systems, data warehousing, advanced SQL Server Analytical development, and sophisticated data analysis Expertise with Azure Cloud Technologies  Data Factory, PowerShell, Data Lake and Data Lake Analytics  Extensive experience with Data Modeling and ETL tools, Business Intelligence platforms, API Integration, and Object-Oriented Programming  OOP  Ability to thrive in a cross-functional environment utilizing modern technologies  Python, Git, Jenkins, Octopus Deploy, Tensorflow, Domo, ArcGIS, E/R Studio, RedGate DLM Automation and other tools  Experience with messaging/event processing tooling and frameworks such as Azure Event Hub, Kafka, Kinesis Working knowledge of Azure HDInsight + Spark, Azure Databricks, Azure Stream Analytics    ","Degree in Computer Science, Engineering, Mathematics, Statistics or related quantitative field 5+ yearsâ experience RDBMS systems, data warehousing, advanced SQL Server Analytical development, and sophisticated analysis Expertise with Azure Cloud Technologies Data Factory, PowerShell, Lake Analytics Extensive Modeling ETL tools, Business Intelligence platforms, API Integration, Object-Oriented Programming OOP Ability to thrive a cross-functional environment utilizing modern technologies Python, Git, Jenkins, Octopus Deploy, Tensorflow, Domo, ArcGIS, E/R Studio, RedGate DLM Automation other tools Experience messaging/event processing tooling frameworks such as Event Hub, Kafka, Kinesis Working knowledge of HDInsight + Spark, Databricks, Stream","Degree Computer Science, Engineering, Mathematics, Statistics related quantitative field 5+ yearsâ experience RDBMS systems, data warehousing, advanced SQL Server Analytical development, sophisticated analysis Expertise Azure Cloud Technologies Data Factory, PowerShell, Lake Analytics Extensive Modeling ETL tools, Business Intelligence platforms, API Integration, Object-Oriented Programming OOP Ability thrive cross-functional environment utilizing modern technologies Python, Git, Jenkins, Octopus Deploy, Tensorflow, Domo, ArcGIS, E/R Studio, RedGate DLM Automation tools Experience messaging/event processing tooling frameworks Event Hub, Kafka, Kinesis Working knowledge HDInsight + Spark, Databricks, Stream"
211,Data Engineer,Data Engineer w/ Snowflake,"Atlanta, GA",Atlanta,GA,"The Snowflake Developer will be responsible for designing and implementing Snowflake data warehouse infrastructure and pipelines.
Strong experience and comfort with relational database concepts (Databases, Schemas, Tabular/Semi-structured Data, Primary/Foreign keys, etc).At least 2 years of experience in data warehouse build projects including data loading, processing and transformationExpert-level Python and SQL (Scala and Java are a plus). At least 2 years of experience is requiredExperience and comfort with ETL/ELT concepts and frameworks and data modelling.Knowledge and hands on experience with Snowflake, familiarity with Snowflake's features and use cases is a plus.Exposure to either AWS or Azure cloud environments in a production settingHands on experience with Apache Airflow is a plus.


Candidates should be flexible / willing to work across this delivery landscape which includes and not limited to Agile Applications Development, Support and Deployment.
Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.

Qualifications

Responsible for programming and software development using various programming languages and related tools and frameworks, reviewing code written by other programmers, requirement gathering, bug fixing, testing, documenting and implementing software systems. Experienced programmers are also responsible for interpreting architecture and design, code reviews, mentoring, guiding and monitoring programmers, ensuring adherence to programming and documentation policies, software development, testing and release.
Required Skills and Experience:
Write software programs using specific programming languages/platforms such as Java or MS .NET, and related tools, platform and environment. Write, update, and maintain computer programs or software packages to handle specific jobs, such as tracking inventory, storing or retrieving data, or controlling other equipment. Consult with managerial, engineering, and technical personnel to clarify program intent, identify problems, and suggest changes. Perform or direct revision, repair, or expansion of existing programs to increase operating efficiency or adapt to new requirements. Write, analyze, review, and rewrite programs, using workflow chart and diagram, and applying knowledge of computer capabilities, subject matter, and symbolic logic. Write or contribute to instructions or manuals to guide end users. Correct errors by making appropriate changes and then rechecking the program to ensure that the desired results are produced. Conduct trial runs of programs and software applications to be sure they will produce the desired information and that the instructions are correct. Compile and write documentation of program development and subsequent revisions, inserting comments in the coded instructions so others can understand the program. Investigate whether networks, workstations, the central processing unit of the system, and/or peripheral equipment are responding to a program's instructions. Prepare detailed workflow charts and diagrams that describe input, output, and logical operation, and convert them into a series of instructions coded in a computer language. Perform systems analysis and programming tasks to maintain and control the use of computer systems software as a systems programmer. Consult with and assist computer operators or system analysts to define and resolve problems in running computer programs. Perform unit testing Assist in system and user testing Fix errors and bugs that are identified in the course of testing.
Qualifications: 3-7 years (2 years min relevant experience in the role) experience; Bachelorâs degreeShould be proficient in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Lifecycle and Data Management.Should have progressing skills on Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.

This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Click the following link for more information on your rights as an Applicant - http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law

About Capgemini
A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clientsâ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.

Visit us at www.capgemini.com. People matter, results count.","Qualifications  3-7 years  2 years min relevant experience in the role  experience; Bachelorâs degreeShould be proficient in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Lifecycle and Data Management.Should have progressing skills on Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.    ","Qualifications 3-7 years 2 min relevant experience in the role experience; Bachelorâs degreeShould be proficient Software Engineering Techniques, Architecture, Lifecycle and Data Management.Should have progressing skills on Business Analysis, Knowledge, Leadership, Architecture Knowledge Technical Solution Design.","Qualifications 3-7 years 2 min relevant experience role experience; Bachelorâs degreeShould proficient Software Engineering Techniques, Architecture, Lifecycle Data Management.Should progressing skills Business Analysis, Knowledge, Leadership, Architecture Knowledge Technical Solution Design."
212,Data Engineer,Data Engineer,"Atlanta, GA 30326",Atlanta,GA,"Data Engineer

Job Description

CoStar Group is currently looking for a Data Engineer to join our Atlanta-based Apartments Network team. This exciting opportunity contributes to the growth of CoStarâs apartment rental brands including Apartments.com, ForRent.com, ApartmentFinder.com, ApartmentHomeLiving.com, Apartamentos.com, Cozy.co, WestsideRentals.com, AFTER55.com, ForRentUniversity.com, and CorporateHousing.com.
The ideal candidate is ambitious, a self-starter, loves challenges and thrives in a fast-paced, dynamic environment. This excellent problem solver takes pride in being best in class at leveraging their technical depth and breadth to design, build and support data solutions. This candidate is never satisfied with the status quo and is strong communicator who can actively engage in conversations about data needs and opportunities.
Responsibilities:
Proactively drive and facilitate conversations about data needs with stakeholders across the company
Build and maintain data pipelines and ETL process
Design, develop, maintain and enhance data collection procedures and analytic systems
Understand, unify and integrate data from internal and third-party data sources using industry best practices for scalability, quality, simplicity, and maintainability
Designing appropriate indexes for new tables, and analyze existing indexes for improvement
Conduct performance analysis and optimize systems to ensure products provide optimal performance
Ownership of end-to-end data and analytical solutions including internal and third-party systems and software components
Design, build and manage the deployment of various big data applications
Drive self-service analytics initiatives
Requirements and Qualifications:
BS or MS degree in Computer Science or a related technical experience
3+ years of experience in custom or structured ETL design, implementation, maintenance and support
3+ years of experience with one or more programming language: Python, Scala, Java, R, etc.
3+ years of experience with SQL, data definition, and data manipulation
3+ years of experience designing, implementing and maintaining SSIS packages
3+ years of experience managing ETL with data warehouses (AWS Redshift, Google BigQuery, etc)
Strong experience working with both structured and unstructured data
Experience building and optimizing data pipelines and big data sets
Fluency running automated jobs to manipulate and store data from APIs via Google Compute Engine, Google App Engine, Google Cloud Storage, EC2, AWS Lambda, S3, etc.
Experience with Spark
Experience with container applications like Docker
Expertise creating and managing dashboards in data visualization platforms such as Tableau, Microsoft PowerBI, Google Data Studio, SSRS, etc.
Excellent written and oral communication skills including facilitation, project management, and working with others in team communicate data-driven insights
Nice to Have
Experience with workflow management tools like Airflow
Understanding of supervised and unsupervised machine learning techniques and algorithms, such as k-NN, K-Means, Naive Bayes, SVM, Decision Forests, Neural Networks, etc.
Experience developing recommendation engines using different techniques like collaborative filtering, content-based filtering, deep learning-based approaches, etc.
What We Offer
You work hard to connect with our customers and communities every day. As part of your total rewards package, CoStar is committed to offering you valuable health, wellness and financial benefits to support you and your family, both now and in the future. At CoStar, we help our clients succeed by providing the most cutting-edge, reliable information and tools in the business. And we want our employees to succeed too. We strive to hire the best talent and reward you with top-notch, market-leading pay, benefits and career opportunities. As an example of our commitment to excellence, CoStar ranks in the 90th percentile of employers who offer competitive and affordable medical plans.
Join the CoStar Team
Join the team that creates CoStarâs winning technology. If youâre an engineer you have an incredible opportunity at CoStar â named one of Forbesâ Most Innovative Growth Companies for four consecutive years. Weâre always looking for talent to help build the tools and analytics that harvest our big data and power our research operations and shape the marketplaces that serve tens of millions of people each month. Our culture of innovation and excellence attracts and encourages the best and brightest in a broad range of disciplines, which makes CoStar a fun and supportive place to work.
Company Overview:
CoStar Group, Inc. (NASDAQ: CSGP) is the leading provider of commercial real estate information, analytics and online marketplaces. Founded in 1987, CoStar conducts expansive, ongoing research to produce and maintain the largest and most comprehensive database of commercial real estate information. Our suite of online services enables clients to analyze, interpret and gain unmatched insight on commercial property values, market conditions and current availabilities. LoopNet is the most heavily trafficked commercial real estate marketplace online with approximately 5 million monthly unique visitors per month. Realla is the UK's most comprehensive commercial property digital marketplace. Apartments.com, ApartmentFinder.com, ForRent.com, ApartmentHomeLiving.com, Westside Rentals, AFTER55.com, CorporateHousing.com, ForRentUniversity.com, Cozy and Apartamentos.com form the premier online apartment resource for renters seeking great apartment homes and provide property managers and owners a proven platform for marketing their properties. CoStar Group's websites attracted an average of approximately 45 million unique monthly visitors in aggregate in the third quarter of 2018. Headquartered in Washington, DC, CoStar maintains offices throughout the U.S. and in Europe and Canada with a staff of over 3,600 worldwide, including the industry's largest professional research organization.LI-AM2

CoStar Group is an Equal Employment Opportunity Employer; we maintain a drug-free workplace and perform pre-employment substance abuse testing"," BS or MS degree in Computer Science or a related technical experience 3+ years of experience in custom or structured ETL design, implementation, maintenance and support 3+ years of experience with one or more programming language  Python, Scala, Java, R, etc. 3+ years of experience with SQL, data definition, and data manipulation 3+ years of experience designing, implementing and maintaining SSIS packages 3+ years of experience managing ETL with data warehouses  AWS Redshift, Google BigQuery, etc  Strong experience working with both structured and unstructured data Experience building and optimizing data pipelines and big data sets Fluency running automated jobs to manipulate and store data from APIs via Google Compute Engine, Google App Engine, Google Cloud Storage, EC2, AWS Lambda, S3, etc. Experience with Spark Experience with container applications like Docker Expertise creating and managing dashboards in data visualization platforms such as Tableau, Microsoft PowerBI, Google Data Studio, SSRS, etc. Excellent written and oral communication skills including facilitation, project management, and working with others in team communicate data-driven insights   Proactively drive and facilitate conversations about data needs with stakeholders across the company Build and maintain data pipelines and ETL process Design, develop, maintain and enhance data collection procedures and analytic systems Understand, unify and integrate data from internal and third-party data sources using industry best practices for scalability, quality, simplicity, and maintainability   BS or MS degree in Computer Science or a related technical experience 3+ years of experience in custom or structured ETL design, implementation, maintenance and support 3+ years of experience with one or more programming language  Python, Scala, Java, R, etc. 3+ years of experience with SQL, data definition, and data manipulation 3+ years of experience designing, implementing and maintaining SSIS packages 3+ years of experience managing ETL with data warehouses  AWS Redshift, Google BigQuery, etc  Strong experience working with both structured and unstructured data Experience building and optimizing data pipelines and big data sets Fluency running automated jobs to manipulate and store data from APIs via Google Compute Engine, Google App Engine, Google Cloud Storage, EC2, AWS Lambda, S3, etc. Experience with Spark Experience with container applications like Docker Expertise creating and managing dashboards in data visualization platforms such as Tableau, Microsoft PowerBI, Google Data Studio, SSRS, etc. Excellent written and oral communication skills including facilitation, project management, and working with others in team communicate data-driven insights","BS or MS degree in Computer Science a related technical experience 3+ years of custom structured ETL design, implementation, maintenance and support with one more programming language Python, Scala, Java, R, etc. SQL, data definition, manipulation designing, implementing maintaining SSIS packages managing warehouses AWS Redshift, Google BigQuery, etc Strong working both unstructured Experience building optimizing pipelines big sets Fluency running automated jobs to manipulate store from APIs via Compute Engine, App Cloud Storage, EC2, Lambda, S3, Spark container applications like Docker Expertise creating dashboards visualization platforms such as Tableau, Microsoft PowerBI, Data Studio, SSRS, Excellent written oral communication skills including facilitation, project management, others team communicate data-driven insights Proactively drive facilitate conversations about needs stakeholders across the company Build maintain process Design, develop, enhance collection procedures analytic systems Understand, unify integrate internal third-party sources using industry best practices for scalability, quality, simplicity, maintainability","BS MS degree Computer Science related technical experience 3+ years custom structured ETL design, implementation, maintenance support one programming language Python, Scala, Java, R, etc. SQL, data definition, manipulation designing, implementing maintaining SSIS packages managing warehouses AWS Redshift, Google BigQuery, etc Strong working unstructured Experience building optimizing pipelines big sets Fluency running automated jobs manipulate store APIs via Compute Engine, App Cloud Storage, EC2, Lambda, S3, Spark container applications like Docker Expertise creating dashboards visualization platforms Tableau, Microsoft PowerBI, Data Studio, SSRS, Excellent written oral communication skills including facilitation, project management, others team communicate data-driven insights Proactively drive facilitate conversations needs stakeholders across company Build maintain process Design, develop, enhance collection procedures analytic systems Understand, unify integrate internal third-party sources using industry best practices scalability, quality, simplicity, maintainability"
213,Data Engineer,"Data Engineer, Decision Sciences","Atlanta, GA 30309",Atlanta,GA,"Invesco is one of the worldâs leading global investment managers, entrusted with managing $1.2 trillion* in assets on behalf of clients worldwide. We are the 6th largest US retail asset manager and the 13th largest investment manager globally, and our more than 8,000 employees worldwide are dedicated to delivering an investment experience that helps people get more out of life. We are purely focused on managing a comprehensive range of active, passive and alternative investment capabilities, which we draw on to provide customized solutions aligned to client needs, our most important benchmark. (*As of May 31,2019)

Job Purpose (Job Summary):

As a Data Engineer at Invesco, you would align business outcomes to technology goals and objectives. You will be responsible for managing decision sciences and advanced analytics initiatives, data integration/engineering. Interacts with Data Scientists, Business Owners, Business Data Analysts, Data Modelers, Architects, and Application Developers, to design, build and manage large-scale batch and real-time data pipelines utilizing various data analytics processing frameworks in support of Data Science practice. The position requires building strong technical hands-on-skills and experiences as well as establishing close business relationships across the enterprise. Working closely with marketing and sales stakeholders, you will execute long term advance analytics roadmap manage increasing demands for advance analytics in Sales and Marketing. A successful candidate must have a demonstrable background with experience in development of high performance, distributed computing tasks using Big Data technologies such as Hadoop (platform level), NoSQL, text mining and other distributed environment technologies based on the needs of the organization. Responsible for analyzing, designing, programing, debugging and modifying software enhancements and/or new products used in distributed, large scale analytics solutions. Strong visualization skills and experience with BI tools (ex. Tableau)

Key Responsibilities / Duties:
Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities
Serve as a key driver on various new technology initiatives and programs on behalf of IT organization, including big data, machine learning and AI
Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities
Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies
Assist in the decision-making process related to the selection of software architecture solutions
Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies
Assist in creating documents that ensure consistency in development across the online organization. Implements and improves core software infrastructure
Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments
Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools (such as control charts and scorecards), readiness and adoption of data w/in the organization
Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding
Serve as strong advocate to improve analytical capability across the organization
Support various reporting and BI solutions (Tableau, Power BI, Salesforce Einstein)
Manage application and data integration platforms (Informatica, Talend)
Create and manage data, applications and technology architecture documentation and design artifacts
Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement
Gain adoption of architecture processes, standards and procedures
Work Experience / Knowledge:
2+ years of experience in data modeling, data warehousing, and big data architectures
2+ years of experience in a data engineering role
Proficient in application/software architecture (Definition, Business Process Modeling, etc.)
Deep expertise in (at least one) SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments
The role will be responsible for providing innovative operational solutions and best practices
Advanced knowledge in SQL/Hive, Spark, NoSQL, (Java/Python is a plus)
Strong programming skills and ability to utilize a variety of software/languages/tools; e.g., Spark, R, Python, Scala, Java, Hive, SQL, SAS, Tableau, etc.
Experience with microservice development, Docker, Kubernetes
Develop software to run on cloud native big data infrastructure built on AWS using Spark, Lambda, S3, and other cloud native services
Designs and develops complex and large-scale data structures and pipelines to organize, collect and standardize data to generate insight
2+ years of experience in data integration platforms (Informatica, Talend)
Strong analytics and reporting skills â hands on experience in BI Tools like Tableau, Power BI, Salesforce Einstein
Experience using JIRA and Agile Project Management software
Experience using GitHub, Bit Bucket, or other code repository solution
DevOps experience is a plus
Skills / Other Personal Attributes Required:
Strong Tableau, Adobe Analytics other BI solution
Expert Oracle and Vertica skillsets
Experience using JIRA and Agile Project Management software
Experience using GitHub, Bit Bucket, or other code repository solution
Strong written, verbal communication and presentation skills
Ability to explain complex technical issues in a way that non-technical people may understand
Able to work in a global, multicultural environment
Self-motivated. Capable of working with little or no supervision
Ability to react positively under pressure to meet tight deadlines
Able to work independently or as a team player
Enjoy challenging and thought provoking work and have a strong desire to learn and progress
Formal Education: (minimum requirement to perform job duties)
BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience
FLSA (US Only): Nonexempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.

Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment."," Strong Tableau, Adobe Analytics other BI solution Expert Oracle and Vertica skillsets Experience using JIRA and Agile Project Management software Experience using GitHub, Bit Bucket, or other code repository solution Strong written, verbal communication and presentation skills Ability to explain complex technical issues in a way that non-technical people may understand Able to work in a global, multicultural environment Self-motivated. Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player Enjoy challenging and thought provoking work and have a strong desire to learn and progress Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities Serve as a key driver on various new technology initiatives and programs on behalf of IT organization, including big data, machine learning and AI Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Assist in creating documents that ensure consistency in development across the online organization. Implements and improves core software infrastructure Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools  such as control charts and scorecards , readiness and adoption of data w/in the organization Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding Serve as strong advocate to improve analytical capability across the organization Support various reporting and BI solutions  Tableau, Power BI, Salesforce Einstein  Manage application and data integration platforms  Informatica, Talend  Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement Gain adoption of architecture processes, standards and procedures  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience ","Strong Tableau, Adobe Analytics other BI solution Expert Oracle and Vertica skillsets Experience using JIRA Agile Project Management software GitHub, Bit Bucket, or code repository written, verbal communication presentation skills Ability to explain complex technical issues in a way that non-technical people may understand Able work global, multicultural environment Self-motivated. Capable of working with little no supervision react positively under pressure meet tight deadlines independently as team player Enjoy challenging thought provoking have strong desire learn progress Work development teams project leaders/stakeholders provide solutions enable business capabilities Serve key driver on various new technology initiatives programs behalf IT organization, including big data, machine learning AI Supports the data science community by enabling availability environments advanced analytical Maintains broad understanding implementation, integration, inter-connectivity emerging technologies define strategies Assist decision-making process related selection architecture Build robust pipelines public Cloud AWS Kinesis, Kafka, Lambda creating documents ensure consistency across online organization. Implements improves core infrastructure Deep expertise SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. experience writing programs, implementing architectures, automation these Develop maintain reporting, ensuring reliability performance, delivery performance management tools such control charts scorecards , readiness adoption w/in organization Consolidate, standardize changes capacity metric definitions, ownership, accountability taxonomy alignment advocate improve capability Support reporting Power BI, Salesforce Einstein Manage application integration platforms Informatica, Talend Create manage applications documentation design artifacts deliver meaningful reference architectures outline principles best practices for advancement Gain processes, standards procedures BS Computer Science, Applied Mathematics, Physics, Statistics area study sciences mining relevant","Strong Tableau, Adobe Analytics BI solution Expert Oracle Vertica skillsets Experience using JIRA Agile Project Management software GitHub, Bit Bucket, code repository written, verbal communication presentation skills Ability explain complex technical issues way non-technical people may understand Able work global, multicultural environment Self-motivated. Capable working little supervision react positively pressure meet tight deadlines independently team player Enjoy challenging thought provoking strong desire learn progress Work development teams project leaders/stakeholders provide solutions enable business capabilities Serve key driver various new technology initiatives programs behalf IT organization, including big data, machine learning AI Supports data science community enabling availability environments advanced analytical Maintains broad understanding implementation, integration, inter-connectivity emerging technologies define strategies Assist decision-making process related selection architecture Build robust pipelines public Cloud AWS Kinesis, Kafka, Lambda creating documents ensure consistency across online organization. Implements improves core infrastructure Deep expertise SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. experience writing programs, implementing architectures, automation Develop maintain reporting, ensuring reliability performance, delivery performance management tools control charts scorecards , readiness adoption w/in organization Consolidate, standardize changes capacity metric definitions, ownership, accountability taxonomy alignment advocate improve capability Support reporting Power BI, Salesforce Einstein Manage application integration platforms Informatica, Talend Create manage applications documentation design artifacts deliver meaningful reference architectures outline principles best practices advancement Gain processes, standards procedures BS Computer Science, Applied Mathematics, Physics, Statistics area study sciences mining relevant"
214,Data Engineer,Google Data Engineer,"Atlanta, GA 30303",Atlanta,GA,"Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet todayâs high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on GCP and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security (Cloud IAM, Data Loss Prevention API, etc)Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutionsMinimum of 3 years of hands-on experience in GCP and Big Data technologies such as Java, Node.js, C##, Python, PySpark, Spark/SparkSQL, Hadoop, Hive, Pig, Oozie and streaming technologies such as Kafka, Stream Ingestion API, Unix shell/Perl scripting etc.
Extensive experience providing practical direction with the GCP Native and Hadoop ecosystem
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Experience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps â Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plus
Understanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus


Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform. Multi-cloud experience a plus.   Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP DevOps an platform. Multi-cloud a plus. Proven ability to build, manage and foster team-oriented environment work creatively analytically in problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","Minimum 3 years previous Consulting client service delivery experience Google GCP DevOps platform. Multi-cloud plus. Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
215,Data Engineer,Senior Big Data Engineer- Java/Scala/Python,"Atlanta, GA 30327",Atlanta,GA,"Travelport are the only true travel commerce platform in the world. We are specialist solution providers and are committed to building leading technology that makes the experience of buying and managing travel continually better for the global travel and tourism industry. Come and be part of our mission to make sure that every trip is powered by Travelportâ¦
Role & Team
You will come on board at a truly exciting time, and as a member of the Big Data Team, you will play a pivotal and crucial part in finding unrivaled and creative ways to use the massive amount of data that we generate.
Travelportâs Big Data Engineers are tasked with designing and implementing big data solutions with terabytes and petabytes daily transaction volume. Travelport employees enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other Travelport developers, architects, and business architects.
Main Accountabilities:
Lead, design, develop, document, and test big data solutions.
Install and integrate various technologies in the big data echo system
Configure and troubleshoot issues in big data framework
Aid in the implementation of and implementing analytical models
Deliver solutions for using an Agile Development model
Create quality deliverables to communicate technical solutions to appropriate audiences.
Understand issues, problem solving and design/architect solutions
Build and collaborate with business and technical teams to deliver software
Learn continuously, leveraging training resources and self-directed training, sharing knowledge and skills with others.
Provide mentoring and leadership to more junior resources.
Passion for technology and willingness to learn is required
Have ability to work in a fast paced and dynamic work environment and be able to produce efficient and robust solution
High energy, confidence, and agility to drive a team
Candid and direct communication
A creative thinker who can bring in new ideas and innovations to the company.
Required Qualifications:
Bachelors/Master/PHD in computer science, engineering, information technology, or related degree and/or equivalent work experience
4+ years of Java development experience in large scale enterprise development for Undergraduates
2+ years of Java development experience in large scale enterprise development for Masters or PHD
Knowledge of multiple threading development and performance tuning
Knowledge of implementing efficient logic using collections and data structures
Knowledge of one or more Big Data Technologies like Elastic Search, Spark, Kafka, Map-Reduce, HDFS and Hive
Knowledge in Design Patterns, OOP/OOD, Software Architecture
Knowledge of how to assess the performance of data solutions, how to diagnose performance problems, and tools used to monitor and tune performance.
Excellent communication skills with both Technical and Business audience
Preferred Qualifications:
Strong programming skills in standard programming and/or algorithms
1+ years of Scala or Python programming
1+ years of development experience in the field of big data on Large Scale environment (e.g. Hadoop, MongoDB, Couch Base, Cassandra)
1+ years of Spark Based Technologies (e.g. Spark Dataframes, Spark Streaming and Spark SQL)
Knowledge of Big Data querying tools (e.g. Spark, Pig, Hive, and Impala)
Aid in the modeling and implementation of machine learning and deep learning solutions
Real time analytics using stream processing frameworks such like Spark / Storm
Experience in the following: C, C++, Perl, or PHP
Knowledge of designing and developing reusable components
Experience in web frontend development (e.g. Javascript, Jquery, AngularJS, ReactJS)
If this sounds like you, weâd love for you to get in touch.
Whatâs in it for you...
You will receive a competitive salary & benefits package accompanied by the opportunity to work in a fast-paced, dynamic and progressive organisation that cares about its people and promotes innovation.
We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."," Bachelors/Master/PHD in computer science, engineering, information technology, or related degree and/or equivalent work experience 4+ years of Java development experience in large scale enterprise development for Undergraduates 2+ years of Java development experience in large scale enterprise development for Masters or PHD Knowledge of multiple threading development and performance tuning Knowledge of implementing efficient logic using collections and data structures Knowledge of one or more Big Data Technologies like Elastic Search, Spark, Kafka, Map-Reduce, HDFS and Hive Knowledge in Design Patterns, OOP/OOD, Software Architecture Knowledge of how to assess the performance of data solutions, how to diagnose performance problems, and tools used to monitor and tune performance. Excellent communication skills with both Technical and Business audience    ","Bachelors/Master/PHD in computer science, engineering, information technology, or related degree and/or equivalent work experience 4+ years of Java development large scale enterprise for Undergraduates 2+ Masters PHD Knowledge multiple threading and performance tuning implementing efficient logic using collections data structures one more Big Data Technologies like Elastic Search, Spark, Kafka, Map-Reduce, HDFS Hive Design Patterns, OOP/OOD, Software Architecture how to assess the solutions, diagnose problems, tools used monitor tune performance. Excellent communication skills with both Technical Business audience","Bachelors/Master/PHD computer science, engineering, information technology, related degree and/or equivalent work experience 4+ years Java development large scale enterprise Undergraduates 2+ Masters PHD Knowledge multiple threading performance tuning implementing efficient logic using collections data structures one Big Data Technologies like Elastic Search, Spark, Kafka, Map-Reduce, HDFS Hive Design Patterns, OOP/OOD, Software Architecture assess solutions, diagnose problems, tools used monitor tune performance. Excellent communication skills Technical Business audience"
216,Data Engineer,Data Engineer,"Peachtree Corners, GA",Peachtree Corners,GA,"Excellence In Everything We Touch
Position Summary
Translating business requirements into callable data services, which may focus on issues such as reducing redundancy of data within the Crawford environment and improving the way in which it moves from one system to another. Will contribute technical and business knowledge expertise to design, develop, and implement integrated IT / business solutions focused primarily around the linkage to/from the Master Data Management (MDM) platform. Contributions will be on both next-generation and legacy solutions.
Responsibilities
Development and implementation of Master Data Web services/APIs as part of the Crawford Master Data Services Framework
Responsible for developing solutions in the Informatica MDM platform (also Salesforce and/or Mulesoft) by converting business requirements into quality technical solutions
Assist with the development and enforcement architecture standards
Serve as an expert developer, proficient in configuring, staging, loading, matching and merging processes and overseeing and promoting quality development standards
Capture and rationalize key business requirements data definitions for attributes in conceptual and logical models
Performing data extraction from legacy systems to map/load data into Informatica MDM performing validation routines and reviewing that all data was appropriately loaded
Engage with business partners to understand functional requirements to design best possible global solutions
Analyze business and data requirements and help define the best solution keeping our long term goal in mind of reducing data redundancy
Provide business and technical input in project activities and decision making processes
Performs analysis of cross-functional and complex business requirements.
Follow project management methodologies and ensure the timely delivery of project deliverables
Understand, influence and provide feedback on technical solution in support of end-to-end data, processes and assets
Flexibility and ability to work in a global and multi-cultural environment. Team members are in multiple geographies, resulting in time constraints due to time zones differences
Requirements
Bachelor's/Masterâs degree or equivalent in Information Technology, Computer Science, Engineering or related field.
1 â 3 years prior experience
Certified Data Management Professional (CDMP) preferred
Some experienced in extracting and loading large data volumes to-and-from platforms such a Microsoft Azure and Amazon Web Services (AWS)
Hands on experience working on UI development and configuration
Must have a minimum of exposure to tools such as and not limited to Java, C,C++, C#,VB, Sql Server/Oracle.
Experience with analysis and business intelligence tools (Tableau, Cognos, etc.)
Basic understanding of databases (RDBMSs) and database scripting.
Have some performance tuning and SQL query skills
Some experience with 3rd party package software (Oracle, Informatica, SAP)
Some knowledge and experience in software development life cycle (SDLC), software development methodologies and standards
About Us
People taking care of people. Itâs that simple. At Crawford & Company, we treat our clientsâ policyholders like our own, helping to restore and enhance lives, businesses and communities at all points of the claims management process. Combining a legacy of nearly 80 years of unmatched experience with global capabilities and industry-leading technology, Crawford is at the forefront of change, while also staying firmly rooted to our commitment to putting people first.
We are guided by our collective value system: RESTORE.
At Crawford, we:
Respect our culture of integrity and ethical behavior, while embracing the unique talents of the individual and encouraging an ownership mentality among everyone.
Are Empowered to advance the company mission and take ownership of our individual career progression.
Promote Sustainability through a corporate culture in which employees are good stewards of their communities.
Emphasize Training and an environment where employees continually seek and share knowledge and are engaged and satisfied with their work.
Are One Crawford, embracing a global mindset thatâs inclusive, agile, mission-focused, and customer-focused.
Give Recognition, participating in an environment where people are rewarded for jobs well done.
Embody an Entrepreneurial Spirit, sharing a passion to succeed, innovate, and outpace our competitors.
We believe in leading by example â at work and in our communities. We hail from more than 70 countries and speak dozens of languages, reflecting the global fabric of the audience we serve. Though our reach is vast, we proudly operate as One Crawford: united in mission, vision and values. Learn more at www.crawfordandcompany.com.
In addition to a competitive salary, Crawford offers you:
Career advancement potential locally, nationally and internationally. Crawford & Company has more than 700 locations in 70 countries
On-going training opportunities through every stage of your career
Strong benefits package including matching 401k; health, dental, and life insurance; employee stock purchase plans; tuition reimbursement and so much more.
Crawford & Company participates in E-Verify and is an Equal Opportunity Employer. M/F/D/V Crawford & Company is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at Crawford via-email, the Internet or in any form and/or method without a valid written Statement of Work in place for this position from Crawford HR/Recruitment will be deemed the sole property of Crawford. No fee will be paid in the event the candidate is hired by Crawford as a result of the referral or through other means.","  Development and implementation of Master Data Web services/APIs as part of the Crawford Master Data Services Framework Responsible for developing solutions in the Informatica MDM platform  also Salesforce and/or Mulesoft  by converting business requirements into quality technical solutions Assist with the development and enforcement architecture standards Serve as an expert developer, proficient in configuring, staging, loading, matching and merging processes and overseeing and promoting quality development standards Capture and rationalize key business requirements data definitions for attributes in conceptual and logical models Performing data extraction from legacy systems to map/load data into Informatica MDM performing validation routines and reviewing that all data was appropriately loaded Engage with business partners to understand functional requirements to design best possible global solutions Analyze business and data requirements and help define the best solution keeping our long term goal in mind of reducing data redundancy Provide business and technical input in project activities and decision making processes Performs analysis of cross-functional and complex business requirements. Follow project management methodologies and ensure the timely delivery of project deliverables Understand, influence and provide feedback on technical solution in support of end-to-end data, processes and assets Flexibility and ability to work in a global and multi-cultural environment. Team members are in multiple geographies, resulting in time constraints due to time zones differences  Bachelor's/Masterâs degree or equivalent in Information Technology, Computer Science, Engineering or related field. 1 â 3 years prior experience Certified Data Management Professional  CDMP  preferred Some experienced in extracting and loading large data volumes to-and-from platforms such a Microsoft Azure and Amazon Web Services  AWS  Hands on experience working on UI development and configuration Must have a minimum of exposure to tools such as and not limited to Java, C,C++, C ,VB, Sql Server/Oracle. Experience with analysis and business intelligence tools  Tableau, Cognos, etc.  Basic understanding of databases  RDBMSs  and database scripting. Have some performance tuning and SQL query skills Some experience with 3rd party package software  Oracle, Informatica, SAP  Some knowledge and experience in software development life cycle  SDLC , software development methodologies and standards","Development and implementation of Master Data Web services/APIs as part the Crawford Services Framework Responsible for developing solutions in Informatica MDM platform also Salesforce and/or Mulesoft by converting business requirements into quality technical Assist with development enforcement architecture standards Serve an expert developer, proficient configuring, staging, loading, matching merging processes overseeing promoting Capture rationalize key data definitions attributes conceptual logical models Performing extraction from legacy systems to map/load performing validation routines reviewing that all was appropriately loaded Engage partners understand functional design best possible global Analyze help define solution keeping our long term goal mind reducing redundancy Provide input project activities decision making Performs analysis cross-functional complex requirements. Follow management methodologies ensure timely delivery deliverables Understand, influence provide feedback on support end-to-end data, assets Flexibility ability work a multi-cultural environment. Team members are multiple geographies, resulting time constraints due zones differences Bachelor's/Masterâs degree or equivalent Information Technology, Computer Science, Engineering related field. 1 â 3 years prior experience Certified Management Professional CDMP preferred Some experienced extracting loading large volumes to-and-from platforms such Microsoft Azure Amazon AWS Hands working UI configuration Must have minimum exposure tools not limited Java, C,C++, C ,VB, Sql Server/Oracle. Experience intelligence Tableau, Cognos, etc. Basic understanding databases RDBMSs database scripting. Have some performance tuning SQL query skills 3rd party package software Oracle, Informatica, SAP knowledge life cycle SDLC ,","Development implementation Master Data Web services/APIs part Crawford Services Framework Responsible developing solutions Informatica MDM platform also Salesforce and/or Mulesoft converting business requirements quality technical Assist development enforcement architecture standards Serve expert developer, proficient configuring, staging, loading, matching merging processes overseeing promoting Capture rationalize key data definitions attributes conceptual logical models Performing extraction legacy systems map/load performing validation routines reviewing appropriately loaded Engage partners understand functional design best possible global Analyze help define solution keeping long term goal mind reducing redundancy Provide input project activities decision making Performs analysis cross-functional complex requirements. Follow management methodologies ensure timely delivery deliverables Understand, influence provide feedback support end-to-end data, assets Flexibility ability work multi-cultural environment. Team members multiple geographies, resulting time constraints due zones differences Bachelor's/Masterâs degree equivalent Information Technology, Computer Science, Engineering related field. 1 â 3 years prior experience Certified Management Professional CDMP preferred Some experienced extracting loading large volumes to-and-from platforms Microsoft Azure Amazon AWS Hands working UI configuration Must minimum exposure tools limited Java, C,C++, C ,VB, Sql Server/Oracle. Experience intelligence Tableau, Cognos, etc. Basic understanding databases RDBMSs database scripting. Have performance tuning SQL query skills 3rd party package software Oracle, Informatica, SAP knowledge life cycle SDLC ,"
217,Data Engineer,Data Engineer,"Duluth, GA 30095",Duluth,GA,"If you have these 7 bullet points we are interested in talking to you, please read the entire JD as well.


SSIS
ETL
Data analysis and profiling
Data Model
PowerBI
Reporting
Data Warehouse.

Here is the complete Job Description.

Job Details

Description

The Data Engineer will be responsible for expanding and optimizing our data capabilities for Data Warehousing, Master Data, Data Services as well as Business Intelligence. The ideal candidate is an experienced data wrangler who enjoys optimizing data systems and can build them from the ground up. He/she must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The ideal candidate will be excited by the prospect of optimizing or even re-designing our companys data architecture to support our next generation of products and data initiatives.

Responsibilities


Drive design and development of internal data pipeline architecture to service BI and Analytics capabilities
Work with key stakeholders including Product, Sales, Quality Engineering and Marketing to assist with data-related technical needs and support their data infrastructure needs.
Ensure operational resilience, stability, and scalability of our enterprise data platform by conduct continuous hardening activates that increase uptime, data quality and reduce cost
Ensure tight data platform security during data transport and while at rest
Build hybrid cloud/prem data platform to enable self-service Business Intelligence and Analytics functions
Conduct data profiling and analysis of complex data sets to discover how to meet functional / non-functional requirements
Institute data quality monitoring and alerting platform operations
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Drive solutions for meta data and data lineage management
Drive innovation by recommending and driving adoption of new technologies that provide competitive data advantages for enterprise
Contributes to agile team alignment and is committed to constant improvement efforts by participating in team ceremonies sprint planning, stand-ups, backlog grooming and retrospectives
Ensure technical delivery of detailed feature/story level solutions that satisfies the IT roadmaps acceptance criteria
Maintains professional and technical knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; participating in professional societies

Minimum Work Experience


8+ years of proven experience in the design, development and deployment of end-to-end data pipeline solutions including data processing (Batch, Micro-Batch, Streaming), data preparation (ETL, ELT), data modeling (STAR, OLAP, MPP)
5+ year of Business Intelligence Development and Administration
Experience of building data solutions for back office applications that source data from CRM and ERP applications
Advanced experience working with SQL Server databases
Strong grasp of master data management, data quality monitoring and metadata management
Proficiency in data centric shell scripting, ETL, Python, and/or .NET Programming skills
A comfortable and confident communicator with technical staff but also able to speak with customers concisely to translate technical concepts into business terminology and impacts
Exposure to continuous integration implementations that utilize DevOps style tools (such as Jenkins, Chef, Docker, Terraform, etc.)
Proven team player with the ability to multi-task in a fast-paced dynamic agile work environment
Has previously supported a metrics-driven data culture to drive accountability and transparency
Passionate problem solver and motivated self-starter including ability to analyze situation and recommend sound solutions and implementation strategies

Additional Desired Skills:

Technology certifications such as Amazon Certified Solutions Architect
Proficiency in programming in Spark, R and/or ML packages
Exposure to applications developed to support manufacturing quality
Experience with MS Dynamics CRM, MicroStrategy BI tools, SAP ERP
Consulting experience

Education: Bachelors degree in Computer Science, Information Systems, or combination of education and experience.

Location: Duluth, Georgia","  Technology certifications such as Amazon Certified Solutions Architect Proficiency in programming in Spark, R and/or ML packages Exposure to applications developed to support manufacturing quality Experience with MS Dynamics CRM, MicroStrategy BI tools, SAP ERP Consulting experience    ","Technology certifications such as Amazon Certified Solutions Architect Proficiency in programming Spark, R and/or ML packages Exposure to applications developed support manufacturing quality Experience with MS Dynamics CRM, MicroStrategy BI tools, SAP ERP Consulting experience","Technology certifications Amazon Certified Solutions Architect Proficiency programming Spark, R and/or ML packages Exposure applications developed support manufacturing quality Experience MS Dynamics CRM, MicroStrategy BI tools, SAP ERP Consulting experience"
218,Data Engineer,Senior Data Engineer,"Atlanta, GA 30309",Atlanta,GA,"Invesco is one of the worldâs leading global investment managers, entrusted with managing $1.2 trillion* in assets on behalf of clients worldwide. We are the 6th largest US retail asset manager and the 13th largest investment manager globally, and our more than 8,000 employees worldwide are dedicated to delivering an investment experience that helps people get more out of life. We are purely focused on managing a comprehensive range of active, passive and alternative investment capabilities, which we draw on to provide customized solutions aligned to client needs, our most important benchmark. (*As of May 31,2019)

Job Purpose (Job Summary):

As a Senior Data Engineer at Invesco, you would align business outcomes to technology goals and objectives. You will be responsible for managing decision sciences and advanced analytics initiatives, data integration/engineering. Interacts with Data Scientists, Business Owners, Business Data Analysts, Data Modelers, Architects, and Application Developers, to design, build and manage large-scale batch and real-time data pipelines utilizing various data analytics processing frameworks in support of Data Science practice. The position requires building strong technical hands-on-skills and experiences as well as establishing close business relationships across the enterprise. Working closely with marketing and sales stakeholders, you will establish and drive long term advance analytics roadmap manage increasing demands for advance analytics in Sales and Marketing. You will help steer the strategic technology direction, define target state architecture, technology roadmaps and mentor others. A successful candidate must have a demonstrable background with experience in development of high performance, distributed computing tasks using Big Data technologies such as Hadoop (platform level), NoSQL, text mining and other distributed environment technologies based on the needs of the organization. Responsible for analyzing, designing, programing, debugging and modifying software enhancements and/or new products used in distributed, large scale analytics solutions. Strong visualization skills and experience with BI tools (ex. Tableau)

Key Responsibilities / Duties:
Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities
Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities
Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies
Assist in the decision-making process related to the selection of software architecture solutions
Implement architectures to handle web-scale data and its organization
Execute strategies that inform data design and architecture partnering with enterprise standard
Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies
Assist in creating documents that ensure consistency in development across the online organization. Implements and improves core software infrastructure
Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments
Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools (such as control charts and scorecards), readiness and adoption of data w/in the organization
Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding
Serve as strong advocate to improve analytical capability across the organization
Communicate with various business areas and to gather and prioritize their business requirements
Support various reporting and BI solutions (Tableau, Power BI, Salesforce Einstein)
Manage application and data integration platforms (Informatica, Talend)
Manage the full life cycle of development/reporting/integration projects: planning, design, develop, testing and rollout
Manage solution providers, define sourcing approach and manage the providers
Create and manage data, applications and technology architecture documentation and design artifacts
Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement
Gain adoption of architecture processes, standards and procedures
Work Experience / Knowledge:
5+ years of experience in data modeling, data warehousing, and big data architectures
3+ years of experience in a data engineering role
Proficient in application/software architecture (Definition, Business Process Modeling, etc.)
Strong analytics and reporting skills â hands on experience in BI Tools like Tableau, Power BI, Salesforce Einstein
Deep expertise in (at least one) SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments
The role will be responsible for providing innovative operational solutions and best practices
Advanced knowledge in SQL/Hive, Spark, NoSQL, (Java/Python is a plus
Strong programming skills and ability to utilize a variety of software/languages/tools; e.g., Spark, R, Python, Scala, Java, Hive, SQL, SAS, Tableau, etc.
Experience with microservice development, Docker, Kubernetes
Develop software to run on cloud native big data infrastructure built on AWS using Spark, Lambda, S3, and other cloud native services
Designs and develops complex and large-scale data structures and pipelines to organize, collect and standardize data to generate insight
3+ years of experience in data integration platforms (Informatica, Talend)
Hands on experience in self-service data preparation tool like Alteryx
Experience using GitHub, Bit Bucket, or other code repository solution
DevOps experience is a plus
Skills / Other Personal Attributes Required:
Previous management experience and successfully leading a team of direct reports
Proven track record of effectively leading and managing employees, managing performance, setting goals for team/individuals and provide mentoring.
Strong Tableau, Adobe Analytics other BI solution
Expert Oracle and Vertica skillsets
Experience using JIRA and Agile Project Management software
Experience using GitHub, Bit Bucket, or other code repository solution
Strong written, verbal communication and presentation skills
Ability to explain complex technical issues in a way that non-technical people may understand
Able to work in a global, multicultural environment
Self-motivated. Capable of working with little or no supervision
Ability to react positively under pressure to meet tight deadlines
Able to work independently or as a team player
Enjoy challenging and thought provoking work and have a strong desire to learn and progress
Formal Education: (minimum requirement to perform job duties)
BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience
FLSA (US Only): Exempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.

Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment."," Previous management experience and successfully leading a team of direct reports Proven track record of effectively leading and managing employees, managing performance, setting goals for team/individuals and provide mentoring. Strong Tableau, Adobe Analytics other BI solution Expert Oracle and Vertica skillsets Experience using JIRA and Agile Project Management software Experience using GitHub, Bit Bucket, or other code repository solution Strong written, verbal communication and presentation skills Ability to explain complex technical issues in a way that non-technical people may understand Able to work in a global, multicultural environment Self-motivated. Capable of working with little or no supervision Ability to react positively under pressure to meet tight deadlines Able to work independently or as a team player Enjoy challenging and thought provoking work and have a strong desire to learn and progress Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies Assist in the decision-making process related to the selection of software architecture solutions Implement architectures to handle web-scale data and its organization Execute strategies that inform data design and architecture partnering with enterprise standard Build robust data pipelines on public Cloud using AWS Kinesis, Kafka, Lambda or other technologies Assist in creating documents that ensure consistency in development across the online organization. Implements and improves core software infrastructure Deep expertise in SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. Strong experience with writing complex programs, implementing architectures, and enabling automation in these environments Develop and maintain business reporting, ensuring reliability and performance, delivery of performance management tools  such as control charts and scorecards , readiness and adoption of data w/in the organization Consolidate, standardize and control changes to capacity management data and metric definitions, ownership, accountability and taxonomy to ensure alignment in understanding Serve as strong advocate to improve analytical capability across the organization Communicate with various business areas and to gather and prioritize their business requirements Support various reporting and BI solutions  Tableau, Power BI, Salesforce Einstein  Manage application and data integration platforms  Informatica, Talend  Manage the full life cycle of development/reporting/integration projects  planning, design, develop, testing and rollout Manage solution providers, define sourcing approach and manage the providers Create and manage data, applications and technology architecture documentation and design artifacts Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement Gain adoption of architecture processes, standards and procedures  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to data sciences and data mining or relevant experience ","Previous management experience and successfully leading a team of direct reports Proven track record effectively managing employees, performance, setting goals for team/individuals provide mentoring. Strong Tableau, Adobe Analytics other BI solution Expert Oracle Vertica skillsets Experience using JIRA Agile Project Management software GitHub, Bit Bucket, or code repository written, verbal communication presentation skills Ability to explain complex technical issues in way that non-technical people may understand Able work global, multicultural environment Self-motivated. Capable working with little no supervision react positively under pressure meet tight deadlines independently as player Enjoy challenging thought provoking have strong desire learn progress Work development teams project leaders/stakeholders solutions enable business capabilities Supports the data science community by enabling availability environments advanced analytical Maintains broad understanding implementation, integration, inter-connectivity emerging technologies define strategies Assist decision-making process related selection architecture Implement architectures handle web-scale its organization Execute inform design partnering enterprise standard Build robust pipelines on public Cloud AWS Kinesis, Kafka, Lambda creating documents ensure consistency across online organization. Implements improves core infrastructure Deep expertise SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. writing programs, implementing architectures, automation these Develop maintain reporting, ensuring reliability delivery performance tools such control charts scorecards , readiness adoption w/in Consolidate, standardize changes capacity metric definitions, ownership, accountability taxonomy alignment Serve advocate improve capability Communicate various areas gather prioritize their requirements Support reporting Power BI, Salesforce Einstein Manage application integration platforms Informatica, Talend full life cycle development/reporting/integration projects planning, design, develop, testing rollout providers, sourcing approach manage providers Create data, applications technology documentation artifacts deliver meaningful reference outline principles best practices advancement Gain processes, standards procedures BS Computer Science, Applied Mathematics, Physics, Statistics area study sciences mining relevant","Previous management experience successfully leading team direct reports Proven track record effectively managing employees, performance, setting goals team/individuals provide mentoring. Strong Tableau, Adobe Analytics BI solution Expert Oracle Vertica skillsets Experience using JIRA Agile Project Management software GitHub, Bit Bucket, code repository written, verbal communication presentation skills Ability explain complex technical issues way non-technical people may understand Able work global, multicultural environment Self-motivated. Capable working little supervision react positively pressure meet tight deadlines independently player Enjoy challenging thought provoking strong desire learn progress Work development teams project leaders/stakeholders solutions enable business capabilities Supports data science community enabling availability environments advanced analytical Maintains broad understanding implementation, integration, inter-connectivity emerging technologies define strategies Assist decision-making process related selection architecture Implement architectures handle web-scale organization Execute inform design partnering enterprise standard Build robust pipelines public Cloud AWS Kinesis, Kafka, Lambda creating documents ensure consistency across online organization. Implements improves core infrastructure Deep expertise SQL language, Python, Hadoop ecosystem and/or Spark ecosystem. writing programs, implementing architectures, automation Develop maintain reporting, ensuring reliability delivery performance tools control charts scorecards , readiness adoption w/in Consolidate, standardize changes capacity metric definitions, ownership, accountability taxonomy alignment Serve advocate improve capability Communicate various areas gather prioritize requirements Support reporting Power BI, Salesforce Einstein Manage application integration platforms Informatica, Talend full life cycle development/reporting/integration projects planning, design, develop, testing rollout providers, sourcing approach manage providers Create data, applications technology documentation artifacts deliver meaningful reference outline principles best practices advancement Gain processes, standards procedures BS Computer Science, Applied Mathematics, Physics, Statistics area study sciences mining relevant"
219,Data Engineer,Data Engineer,"Kennesaw, GA 30144",Kennesaw,GA,"Riskonnect is the leading integrated risk management software solution provider that empowers organizations to anticipate, manage and respond in real-time to strategic and operational risks across the extended enterprise. Riskonnect is the only provider ranked in the leadership and visionary quadrants by world renowned industry analysts - Gartner, Forrester and Advisen RMIS Review. We employ more than 500 risk professionals in the Americas, EMEA and Asia Pacific and serve over 900 customers across 6 continents. The combination of innovative risk technology, a customer success mindset, and employee-first belief makes Riskonnect a sought after place to work.
Responsibilities:
Develop strategy for new multi-platform data integration and analytics.
Develop strategy for new multi-platform-sourced data lake.
Contribute to API strategy to facilitate application connectivity and analytics.
Contribute to the maintenance and evolution of best practices.
Contribute to process documentation.
Perform multiple proofs of concept (POCs).
Contribute to implementation plan for decided-upon solution(s).

Required Qualifications:
Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.
Experience with Data Analytics.
Experience with Web Services and APIs.
Experience in the development of batch and real-time data integration and data consolidation processes.
Experience with machine learning, AI, and data lakes.
Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.
Strong analytical skills with ability for problem-solving.
Understands the importance of data provenance and the ability to demonstrate it to clients.
Detail oriented, organized, self-motivated.

Preferred Qualifications:
Experience with Salesforce.
Experience in the Risk Management, Healthcare, Financial, and/or Insurance industries is recommended.
Experience with Financial data sets, involving financial validation.","Experience with JavaScript/Java/ Python or Jitterbit and other developer languages. Experience with Data Analytics. Experience with Web Services and APIs. Experience in the development of batch and real-time data integration and data consolidation processes. Experience with machine learning, AI, and data lakes. Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views. Strong analytical skills with ability for problem-solving. Understands the importance of data provenance and the ability to demonstrate it to clients. Detail oriented, organized, self-motivated.    Develop strategy for new multi-platform data integration and analytics. Develop strategy for new multi-platform-sourced data lake. Contribute to API strategy to facilitate application connectivity and analytics. Contribute to the maintenance and evolution of best practices. Contribute to process documentation. Perform multiple proofs of concept  POCs . Contribute to implementation plan for decided-upon solution s .    ","Experience with JavaScript/Java/ Python or Jitterbit and other developer languages. Data Analytics. Web Services APIs. in the development of batch real-time data integration consolidation processes. machine learning, AI, lakes. Proficiency TSQL/PLSQL query-writing, stored procedure development, views. Strong analytical skills ability for problem-solving. Understands importance provenance to demonstrate it clients. Detail oriented, organized, self-motivated. Develop strategy new multi-platform analytics. multi-platform-sourced lake. Contribute API facilitate application connectivity maintenance evolution best practices. process documentation. Perform multiple proofs concept POCs . implementation plan decided-upon solution s","Experience JavaScript/Java/ Python Jitterbit developer languages. Data Analytics. Web Services APIs. development batch real-time data integration consolidation processes. machine learning, AI, lakes. Proficiency TSQL/PLSQL query-writing, stored procedure development, views. Strong analytical skills ability problem-solving. Understands importance provenance demonstrate clients. Detail oriented, organized, self-motivated. Develop strategy new multi-platform analytics. multi-platform-sourced lake. Contribute API facilitate application connectivity maintenance evolution best practices. process documentation. Perform multiple proofs concept POCs . implementation plan decided-upon solution"
220,Data Engineer,Azure Data Engineer,"Atlanta, GA 30303",Atlanta,GA,"Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, âas isâ and âto beâ scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ","At least 5 years of consulting or client service delivery experience on Azure DevOps an platform Proven ability to build, manage and foster a team-oriented environment","At least 5 years consulting client service delivery experience Azure DevOps platform Proven ability build, manage foster team-oriented environment"
221,Data Engineer,"Senior Data Engineer, Analytics","Warrendale, PA 15086",Warrendale,PA,"Description:
As a member of the Digital Solutions and Data and Analytics IT team, this person will be working with various teams to design solutions related to the data pipeline, with special emphasis on data modeling and design.

This role demands a strong knowledge and understanding of the data and the business requirements to build robust, scalable data models, which may be relational as well as dimensional. The role entails having the capability of relating and understanding existing and in-development data models/schemas and the business context they represent.

Responsibilities


Establish design principles for data models and create modeling standards
Document, create and maintain logical and physical database models as per the standards
Work with the business as well as the technical teams to understand the requirements, to create models that cater to the requirements
Work on understanding various data domains, to map and integrate disparate data from a variety of source systems
Support Analytics development from and to a variety of database systems
Facilitate data requirements conversation with business and technical stakeholders resolving conflicts to drive decisions and consensus
Establish and maintain comprehensive data model documentation including detailed descriptions of business entities, attributes, and data relationships as well as the definition of business rules governing the integrity, archiving, and audit requirements of the data
Analyze existing systems using manual or automated data analysis/profiling to reverse engineer data requirements
Determine suitable data modeling approach for each project based on business requirements for data capture and access
Assist developers with complex query development and performance optimization
Performs additional duties as required
Provide subject matter expertise (technical and business context) input and feedback to data/BI teams and development scrum teams as needed.
Identify, communicate, and mitigate risks related to including invalid data, invalid data calculations, formulas or derived insights, and misrepresentation of or otherwise confusing visual representations of data.

Requirements:
Technical


5 + years of experience designing data models
10 + years of experience implementing data models and data architecture
Experience with debugging, performance profiling and optimization of data at each phase of the data pipeline
Demonstrated analytical capabilities paired with a strong initiative to find ways to improve the analysis and communication of complex data
Specific experience with Microsoft SQL Server Reporting Services (SSRS) and SQL Server toolset. Candidates with extensive experience in other BI/reporting/analytics toolsets may be considered if they have a commitment to learning SSRS and a demonstrated ability to ramp up to proficiency rather quickly (within 2 months) in a primarily self-directed way
Proficiency in either Tableau, SSRS or Power BI is required
Capable of designing complex and performant SQL queries

Professional


Excellent interpersonal and organizational acumen by collaborating with both internal team members and external business stakeholders
Internally motivated, able to work proficiently both independently and in a team environment
Strong written and verbal communication skills with staff and leadership at all levels of seniority and technical experience
Bachelorâs Degree or higher (Computer Science, Engineering, Human Computer Interaction/Design, or similar/relevant field)
Motivated to âstay currentâ with technical change/commit to professional development to learn relevant new skills/tools/methodologies

Desired Additional Competencies


Experience with relational database design, SQL DDL, performance tuning relating to indexes, etc.
Experience with non-relational ânoSQLâ data sources
Experience with ETL processes and tools
Healthcare industry experience not required but a significant plus (supply chain, provider, payer, medical device, life-science, pharmacy, etc.)
Experience with other data/BI tools, especially within the Microsoft ecosystem

","     5 + years of experience designing data models 10 + years of experience implementing data models and data architecture Experience with debugging, performance profiling and optimization of data at each phase of the data pipeline Demonstrated analytical capabilities paired with a strong initiative to find ways to improve the analysis and communication of complex data Specific experience with Microsoft SQL Server Reporting Services  SSRS  and SQL Server toolset. Candidates with extensive experience in other BI/reporting/analytics toolsets may be considered if they have a commitment to learning SSRS and a demonstrated ability to ramp up to proficiency rather quickly  within 2 months  in a primarily self-directed way Proficiency in either Tableau, SSRS or Power BI is required Capable of designing complex and performant SQL queries ","5 + years of experience designing data models 10 implementing and architecture Experience with debugging, performance profiling optimization at each phase the pipeline Demonstrated analytical capabilities paired a strong initiative to find ways improve analysis communication complex Specific Microsoft SQL Server Reporting Services SSRS toolset. Candidates extensive in other BI/reporting/analytics toolsets may be considered if they have commitment learning demonstrated ability ramp up proficiency rather quickly within 2 months primarily self-directed way Proficiency either Tableau, or Power BI is required Capable performant queries","5 + years experience designing data models 10 implementing architecture Experience debugging, performance profiling optimization phase pipeline Demonstrated analytical capabilities paired strong initiative find ways improve analysis communication complex Specific Microsoft SQL Server Reporting Services SSRS toolset. Candidates extensive BI/reporting/analytics toolsets may considered commitment learning demonstrated ability ramp proficiency rather quickly within 2 months primarily self-directed way Proficiency either Tableau, Power BI required Capable performant queries"
222,Data Engineer,Data Engineer,"West Mifflin, PA 15122",West Mifflin,PA,"Description
The Naval Nuclear Laboratory seeks a talented and innovative Data Engineer to join a growing team dedicated to delivering data-driven solutions to the organization. As a data engineer, you will be responsible for developing, constructing, testing, and delivering Program data infrastructure needs. Specifically, the data engineer will help to assess existing data infrastructure hardware and software, and work with key stakeholders to identify, assess, implement, and test improvements to the data infrastructure. The data engineer will interface with data scientists and others at the Laboratory to ensure dataset reporting, querying, and analysis capabilities are based upon reliable, efficient, and scalable data infrastructure. The data engineer will participate in continuous data engineering training programs as well as develop and review corporate policies, instructions, and implementation principles related to data infrastructure and its use in the Program. We develop the world's best nuclear propulsion systems, train Sailors, to operate them, and provide full lifecycle support, from technology development through design to disposal.
Responsibilities:
Interface with technology and support organizations to create, adapt, maintain, and test optimal Program data pipeline architectures
Assemble large, complex datasets that meet functional business requirements
Identify, design, and implement internal data infrastructure process improvements such as automating manual processes, optimizing data delivery, redesigning infrastructure for greater scalability, and opportunities for data acquisition
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various technologies
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs
Create consistent data infrastructure solutions and access for users at multiple sites
Assist management in prioritizing projects
Deliver data engineering training
Continuous learning and improvement of skills needed to keep current with state-of-the-art data architectures
Develop and review corporate policies, instructions, and principles related to data infrastructure and its use in the Program
Requirements
Advanced:
BS degree in engineering or Bachelor's degree in a science related field from an accredited college or university and a minimum of four years relevant experience; or
MS degree in engineering or Master's degree in a science related field from an accredited college or university and a minimum of two years relevant experience
Experience with relational databases, query authoring, programming, and hardware/software troubleshooting.
Senior:
BS degree in engineering or Bachelor's degree in a science related field from an accredited college or university and a minimum of six years relevant experience; or
MS degree in engineering or Master's degree in a science related field from an accredited college or university and a minimum of four years relevant experience
Experience with relational databases, query authoring, programming, and hardware/software troubleshooting
Preferred Skills
Familiarity with building and optimizing data pipelines, architectures, and datasets
Familiarity with data virtualization, transformation, and various data structures
Familiarity with high performance computing
Developer technologies:
Software management tools including those for testing, continuous integration, version control, debugging, profiling, and compiler optimization
Environment Management:
Data management and file systems
System metrics and monitoring
Job scheduler, resource allocation and job/workflow management"," Familiarity with building and optimizing data pipelines, architectures, and datasets Familiarity with data virtualization, transformation, and various data structures Familiarity with high performance computing Developer technologies  Software management tools including those for testing, continuous integration, version control, debugging, profiling, and compiler optimization Environment Management  Data management and file systems System metrics and monitoring Job scheduler, resource allocation and job/workflow management   BS degree in engineering or Bachelor's degree in a science related field from an accredited college or university and a minimum of four years relevant experience; or MS degree in engineering or Master's degree in a science related field from an accredited college or university and a minimum of two years relevant experience Experience with relational databases, query authoring, programming, and hardware/software troubleshooting. ","Familiarity with building and optimizing data pipelines, architectures, datasets virtualization, transformation, various structures high performance computing Developer technologies Software management tools including those for testing, continuous integration, version control, debugging, profiling, compiler optimization Environment Management Data file systems System metrics monitoring Job scheduler, resource allocation job/workflow BS degree in engineering or Bachelor's a science related field from an accredited college university minimum of four years relevant experience; MS Master's two experience Experience relational databases, query authoring, programming, hardware/software troubleshooting.","Familiarity building optimizing data pipelines, architectures, datasets virtualization, transformation, various structures high performance computing Developer technologies Software management tools including testing, continuous integration, version control, debugging, profiling, compiler optimization Environment Management Data file systems System metrics monitoring Job scheduler, resource allocation job/workflow BS degree engineering Bachelor's science related field accredited college university minimum four years relevant experience; MS Master's two experience Experience relational databases, query authoring, programming, hardware/software troubleshooting."
223,Data Engineer,Data Engineer (PDP),"Pittsburgh, PA",Pittsburgh,PA,"This position will support our Postsecondary Data Partnership (PDP) project and will be located in our Pittsburgh Collaboration Center.

If you are a self-starter, thrive in a fast-paced environment, and get energized about driving new data driven services, then this job is for you!

The Clearinghouse is seeking a hands-on Data Engineer with a solid background in Data Warehouse architectures, metadata management, data profiling, data modeling, data quality and a proven ability to learn quickly, effectively collaborate with others, and successfully design and implement solutions. Using their excellent communication and presentation skills, the Engineer socializes technical concepts and builds consensus with internal stakeholders.

The Data Engineer reports to the Manager, Application Development and works with the business and development teams to design and implement the appropriate data solutions for Clearinghouse database-driven applications and services. The Engineer accomplishes this by performing data analysis of current and future systems and services, educating on industry best practices and policies, creating proposed database component designs and data models, and working with the architecture and development teams to refine and implement these designs.

How You Contribute:
Develop and maintain the PL/SQL Packages.
Develop and maintain ETL code.
Design Data Marts and create data models.
Work with Data owners to document and maintain Data Dictionary.
Develop scripts to maintain data quality and deploying data changes across different environments
Monitor, assess and optimize data processing performance.
Demonstrate NSCâs competencies, which align with our corporate values.
Core Competencies include: Customer Focus, Optimizes Work Processes, Collaborates, Communicates Effectively, and Be Open and Authentic.
Manager will provide more detail as needed.
Position may be required to perform other duties as required. These essential functions are representative of those that must be met by an employee to successfully perform the job. Reasonable accommodations will be made to enable individuals with disabilities to perform these essential functions.

What You Bring to the Table:
Bachelorâs degree required in Computer Science, Systems Engineering, or Information Systems, or some equivalent combination of education and experience, including through military service.
Proficient in:
PL/SQL programming, typically acquired through 4-7 years of experience programming with this technology.
Developing scripts using Shell, Perl or similar language.
Experience working on an Agile/Scrum development team.
Proficient with:
Data Warehouse production maintenance.
ETL development using products such as Oracle Data Integrator.
OLTP and data warehouse design.
Metadata management.
Creating and implementing data models.
Database performance monitoring and SQL optimization.
Excellent communication skills.
Demonstrated ability to work independently and directly with the data owners to gather requirements and architect change.
Live within a commutable distance of Pittsburgh, PA.
Additional Desired Requirements:

Experience with:
Oracle technologies including Oracle database and ODI
Cloud-based data warehouse technologies such as Snowflake and open source ETL technologies such as Snaplogic.
Physical Demands:
Use of a computer terminal and/or laptop computer for 8 or more hours a day.
Use of a copy machine, and telephone.
Frequently required to sit for 7 or more hours per day in close proximity to others in an open office environment.
Occasionally required to use hands and fingers to operate, handle, and reach.
Vision abilities include close vision and the ability to adjust focus.
NSC strives to hire, promote, and retain the best qualified individuals for our employment opportunities. Our policies are intended to provide equal employment opportunity for all employees and job applicants without regard to race, color, religion, gender, gender identity, sexual orientation, age, disability, national origin, protected veteran status, or any other status protected by law. NSC strives to have a culture that is diverse and equally welcoming to all. As a Federal contractor, NSC is subject to requirements to take affirmative action to employ and advance in employment protected Veterans and individuals with disabilities. NSC is committed to its outreach efforts and practices to promote employment and advancement of members of these groups. To read our entire policy, go to: https://studentclearinghouse.info/careers/human-resource-policies
PAY TRANSPARENCY POLICY NSC will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by NSC, or (c) consistent with the NSCâs legal duty to furnish information. 41 C.F.R. 60-1.35(c)
Please view Equal Employment Opportunity Posters provided by OFCCP here.",     Experience with  Oracle technologies including Oracle database and ODI Cloud-based data warehouse technologies such as Snowflake and open source ETL technologies such as Snaplogic. ,Experience with Oracle technologies including database and ODI Cloud-based data warehouse such as Snowflake open source ETL Snaplogic.,Experience Oracle technologies including database ODI Cloud-based data warehouse Snowflake open source ETL Snaplogic.
224,Data Engineer,Data Engineer,"Pittsburgh, PA",Pittsburgh,PA,"The Fort is the new hub for innovation and incubation at Fortive. We have the backing of a large company and the soul of a startup (a Fortune 500 startup, that is), and this is the place where that flame burns brightest.

At The Fort, we combine forces to deploy disruptive technologies and explore new ways of thinking and working. We get out of the office and into the wild to observe first-hand what our customers most need to accelerate progress for the world. We observe, listen, prototype, experiment, analyze, learn, and iterate with passion and speed to fuel Fortiveâs growth and build the future. We scout breakthrough ideas and emerging trends from an expansive network stretching from Seattle to Silicon Valley to Shanghai and beyond.

The Fort is built to make growth happen. We start up, we scale up, we shake things up. We face uncertainty head-on and wrestle it to the ground. We fail fast to move our customers forward. We move the needle so much it needs a new gauge. We turn What-if? into Whatâs next? and How might we? into Wow, we did that! If building a data-driven app in a matter of hours or pushing the frontiers of AI or IoT before lunch sounds like your kind of thing, The Fort just might be your kind of place.

Your Impact
You are collaborative, proactive, adaptable, and gritty. You excel at facilitating and reconciling inputs across teams. You balance a passion for deep understanding of innovation with the ability to deliver extraordinary results.

These are the traits we value:
Ability to Deliver Results: A track record for being able to set, meet and exceed expectations relative to data engineering.
Entrepreneurial Attitude: A proactive outlook that provides the chutzpah needed to overcome barriers, creatively problem solve, and challenge conventional thinking.

Comfort with Ambiguity: A willingness and aptitude for spending time in and thriving with deep uncertainty and environments where there is no clear âright answerâ.

Passion for Innovation: A demonstrated interest and desire to participate in innovation through personal study, on-the-job initiative, or other endeavors.
Positive Outlook: A desire to look past the challenged in search of the opportunity.

Empathy & a Teacherâs Mindset: The ability to teach and mentor effectively, successfully facilitate teams and different personalities in training sessions, and to care deeply about his/her colleagueâs growth, understanding, and experience.
Bias for Action: A need for speed; demonstrated ability to make decisions quickly and to act upon them.

Alongside a team of entrepreneurial, high-performing, curious people, youâll deliver breakthrough solutions to drive sustainable growth for Fortive. As a Data Engineer, youâll drive execution and play a vital role in building a culture of innovation at Fortive. You will build, manage, and inspire alongside a multi-functional team, that solves for our toughest growth challenges

Responsibilities:

Work with data scientists to build ML pipelines using heterogeneous sources and provide engineering services for data science applications
Design and implement data warehouses, real-time ETL, and batch processing of data to support modeling and reporting needs
Work with team to develop data expertise and resolve upstream issues relating to data quality
Define best practices and design for the management of data
With data scientists, build and maintain internal data processing and visualization tools
Create tools to serve data such as APIs and packages
Ensure automation through CI/CD across platforms both in cloud and on-premises
Ability to research and assess open source technologies and components to recommend and integrate into the design and implementation

Required Qualifications:

Bachelorâs degree in CS, IT/IS, or a field related to a computational science and a minimum two years experience working as a data engineer
Experience managing data ETL processes and making data available through service applications and databases.
Experience working with query authoring, relational databases, and a familiarity with a variety of databases (Cassandra or Elasticsearch preferred). Ability to write efficient SQL queries
Experience (3+ years) with programming languages (Python, Java, R, and/or Scala preferred)
Familiarity with a variety of data processing technologies (e.g. Spark, Kafka, Hadoop)
Excellent communication skills, including a knack for clear documentation
Experience with or knowledge of REST APIs and making data available through microservices.
Experience using version control (Git, Mercurial, SVN, etc.) for collaborative code development.
Experience with containerization and related technologies (e.g. Docker, Kubernetes)
Familiarity with core provider services from AWS, Azure or GCP, preferably having supported deployments on one or more of these platforms
Experience working with MongoDB, PostgreSQL, and Redis
Working knowledge of bash scripting and/or JavaScript
Experience with automation and configuration management
Expert level building pipelines using Apache Beam or Spark

Preferred Qualifications

MS or PhD in Computer Science or related technical field
Ability to architect data solutions. Knowledge of machine learning and data science processes
Experience supporting data science and analytical efforts is preferred
Some experience with frontend web-development
Experience defining and implementing APIs
System monitoring, alerting, and dashboarding experience
Experience developing, managing, and optimizing big data architectures and pipelines

Fortive (NYSE:FTV) is a diversified industrial growth company comprised of Professional Instrumentation and Industrial Technologies businesses that are recognized leaders in attractive markets. With 2018 revenues of $6.5 billion, $1.2 billion in operating profit, and a market cap of $26 billion, Fortiveâs well-known brands hold leading positions in field instrumentation, transportation, sensing, product realization, automation and specialty, and franchise distribution.

Fortive is headquartered in Everett, Washington and employs a team of more than 26,000 research and development, manufacturing, sales, distribution, service and administrative employees in more than 50 countries around the world. With a culture rooted in continuous improvement, the core of our companyâs operating model is the Fortive Business System.

Fortune Magazine ranks Fortive as a part of the âThe Future 50â companies with the best prospects for long-term growth. For more information please visit: www.fortive.com.

Fortive is a global family of more than 20 industry-leading industrial growth and technology companies, united by a shared purpose: to make the world stronger, safer and more effective by providing essential technology for the people who accelerate progress. We take on big challenges that have real impact in fast-moving fields like software development, robotics, transportation, energy and healthcare.

With more than $6.5 billion in annual revenues and a culture rooted in Kaizen, or continuous improvement, Fortive is well positioned to create essential, technology-based solutions to solve the worldâs most critical challenges. Our strong capability comes from a team of smart, motivated people who proudly deliver excellence in each of our outstanding brands in the areas of field instrumentation, transportation, sensing, product realization, automation and specialty, and franchise distribution.

Fortive is headquartered in Everett, Washington and employs a team of more than 26,000 research and development, sales, marketing, product development, innovation, and service employees in more than 50 countries around the world.
This is a place where people who share a drive and passion to make a personal difference can learn, grow, and achieve. And thatâs good... for you, for us, for growth.

For more information, please visit: www.fortive.com.
""The company in which you have expressed employment interest is a subsidiary or affiliate of Fortive Corporation. The subsidiary or affiliate is referred to as a Fortive Company. Fortive Corporation and all Fortive Companies are equal opportunity employers that evaluate qualified applicants without regard to race, color, national origin, religion, ancestry, sex (including pregnancy, childbirth and related medical conditions), age, marital status, disability, veteran status, citizenship status, sexual orientation, gender identity or expression, and other characteristics protected by law. The ""EEO is the Law"" poster is available at: http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf. Individuals who need a reasonable accommodation because of a disability for any part of the employment process should call 1-866-272-5573 or e-mail applyassistance@fortive.com to request accommodation."""," Bachelorâs degree in CS, IT/IS, or a field related to a computational science and a minimum two years experience working as a data engineer Experience managing data ETL processes and making data available through service applications and databases. Experience working with query authoring, relational databases, and a familiarity with a variety of databases  Cassandra or Elasticsearch preferred . Ability to write efficient SQL queries Experience  3+ years  with programming languages  Python, Java, R, and/or Scala preferred  Familiarity with a variety of data processing technologies  e.g. Spark, Kafka, Hadoop  Excellent communication skills, including a knack for clear documentation Experience with or knowledge of REST APIs and making data available through microservices. Experience using version control  Git, Mercurial, SVN, etc.  for collaborative code development. Experience with containerization and related technologies  e.g. Docker, Kubernetes  Familiarity with core provider services from AWS, Azure or GCP, preferably having supported deployments on one or more of these platforms Experience working with MongoDB, PostgreSQL, and Redis Working knowledge of bash scripting and/or JavaScript Experience with automation and configuration management Expert level building pipelines using Apache Beam or Spark    Work with data scientists to build ML pipelines using heterogeneous sources and provide engineering services for data science applications Design and implement data warehouses, real-time ETL, and batch processing of data to support modeling and reporting needs Work with team to develop data expertise and resolve upstream issues relating to data quality Define best practices and design for the management of data With data scientists, build and maintain internal data processing and visualization tools Create tools to serve data such as APIs and packages Ensure automation through CI/CD across platforms both in cloud and on-premises Ability to research and assess open source technologies and components to recommend and integrate into the design and implementation   ","Bachelorâs degree in CS, IT/IS, or a field related to computational science and minimum two years experience working as data engineer Experience managing ETL processes making available through service applications databases. with query authoring, relational databases, familiarity variety of databases Cassandra Elasticsearch preferred . Ability write efficient SQL queries 3+ programming languages Python, Java, R, and/or Scala Familiarity processing technologies e.g. Spark, Kafka, Hadoop Excellent communication skills, including knack for clear documentation knowledge REST APIs microservices. using version control Git, Mercurial, SVN, etc. collaborative code development. containerization Docker, Kubernetes core provider services from AWS, Azure GCP, preferably having supported deployments on one more these platforms MongoDB, PostgreSQL, Redis Working bash scripting JavaScript automation configuration management Expert level building pipelines Apache Beam Spark Work scientists build ML heterogeneous sources provide engineering Design implement warehouses, real-time ETL, batch support modeling reporting needs team develop expertise resolve upstream issues relating quality Define best practices design the With scientists, maintain internal visualization tools Create serve such packages Ensure CI/CD across both cloud on-premises research assess open source components recommend integrate into implementation","Bachelorâs degree CS, IT/IS, field related computational science minimum two years experience working data engineer Experience managing ETL processes making available service applications databases. query authoring, relational databases, familiarity variety databases Cassandra Elasticsearch preferred . Ability write efficient SQL queries 3+ programming languages Python, Java, R, and/or Scala Familiarity processing technologies e.g. Spark, Kafka, Hadoop Excellent communication skills, including knack clear documentation knowledge REST APIs microservices. using version control Git, Mercurial, SVN, etc. collaborative code development. containerization Docker, Kubernetes core provider services AWS, Azure GCP, preferably supported deployments one platforms MongoDB, PostgreSQL, Redis Working bash scripting JavaScript automation configuration management Expert level building pipelines Apache Beam Spark Work scientists build ML heterogeneous sources provide engineering Design implement warehouses, real-time ETL, batch support modeling reporting needs team develop expertise resolve upstream issues relating quality Define best practices design With scientists, maintain internal visualization tools Create serve packages Ensure CI/CD across cloud on-premises research assess open source components recommend integrate implementation"
225,Data Engineer,Data Engineer,"Pittsburgh, PA 15222",Pittsburgh,PA,"Overview
SDLC Partners is actively looking to hire Data Science/Data Engineering professionals. If you are an experienced Data Engineer or Data Scientist looking for a new opportunity in a growing organization, please apply.
Responsibilities
Build the infrastructure to support coding, testing, processing, and maintaining data resources in support of the Data Science, analytics and reporting organizations using SQL, SQOOP, Python, Google Big Query, Kafka and other Big Data technologies.
Collaborate with Data Scientists in the development of predictive models using machine learning, natural language and statistical analysis methods.
Identify, design and implement internal process improvements (automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc).
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.
Develop, refine and oversee data management standards, including establishing and enforcing governance procedures and ensuring data integrity across multiple functions.
Responsible for owning data quality metrics and meeting defined data accuracy goals according to industry best practices.
Qualifications
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Experience with relational database design methodologies and authoring complex SQL queries
Experience with NoSQL database technologies (MongoDB, Cassandra, etc).
Experience with Agile Development and Agile Deployment tools and versioning using Git or similar tools.
Experience with Hadoop and other Big Data technologies such as Spark, PySpark and Kafka.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with message queuing, stream processing, and highly scalable âbig dataâ data stores.
Experience building data pipelines utilizing Google Cloud platform.
Experience with git or other code repository tools Experience with Concourse or other CI/CD tools and methodologies.
3-5 years experience
Google Cloud Platform (GCS, BQ, etc), Apache Kafka, Python

EDUCATION
Masters/Bachelor's Degree in Computer Science, Software Engineering, Information Systems or Information Technology or related field required, or equivalent experience

SDLC Partners is a dynamic and fast-paced, privately held consulting firm with 400+ employees. We deliver customized digital solutions to transform organizations through our uniquely enabled talent, processes, and leadership. Through full scope partner, strategic, and improvement solutions, we give clients access to some of the best talent available across our service lines. We hire individuals who embody our goal to enable performance for our clients and we believe strongly in growing and developing talent.","Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, etc. Experience with relational database design methodologies and authoring complex SQL queries Experience with NoSQL database technologies  MongoDB, Cassandra, etc . Experience with Agile Development and Agile Deployment tools and versioning using Git or similar tools. Experience with Hadoop and other Big Data technologies such as Spark, PySpark and Kafka. Experience with data pipeline and workflow management tools  Azkaban, Luigi, Airflow, etc. Experience with message queuing, stream processing, and highly scalable âbig dataâ data stores. Experience building data pipelines utilizing Google Cloud platform. Experience with git or other code repository tools Experience with Concourse or other CI/CD tools and methodologies. 3-5 years experience Google Cloud Platform  GCS, BQ, etc , Apache Kafka, Python  Build the infrastructure to support coding, testing, processing, and maintaining data resources in support of the Data Science, analytics and reporting organizations using SQL, SQOOP, Python, Google Big Query, Kafka and other Big Data technologies. Collaborate with Data Scientists in the development of predictive models using machine learning, natural language and statistical analysis methods. Identify, design and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc . Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs. Develop, refine and oversee data management standards, including establishing and enforcing governance procedures and ensuring data integrity across multiple functions. Responsible for owning data quality metrics and meeting defined data accuracy goals according to industry best practices.  ","Experience with object-oriented/object function scripting languages Python, Java, C++, Scala, etc. relational database design methodologies and authoring complex SQL queries NoSQL technologies MongoDB, Cassandra, etc . Agile Development Deployment tools versioning using Git or similar tools. Hadoop other Big Data such as Spark, PySpark Kafka. data pipeline workflow management Azkaban, Luigi, Airflow, message queuing, stream processing, highly scalable âbig dataâ stores. building pipelines utilizing Google Cloud platform. git code repository Concourse CI/CD methodologies. 3-5 years experience Platform GCS, BQ, , Apache Kafka, Python Build the infrastructure to support coding, testing, maintaining resources in of Science, analytics reporting organizations SQL, SQOOP, Query, Kafka technologies. Collaborate Scientists development predictive models machine learning, natural language statistical analysis methods. Identify, implement internal process improvements automating manual processes, optimizing delivery, re-designing for greater scalability, that utilize provide actionable insights into customer acquisition, operational efficiency key business performance metrics. Work stakeholders assist data-related technical issues their needs. Develop, refine oversee standards, including establishing enforcing governance procedures ensuring integrity across multiple functions. Responsible owning quality metrics meeting defined accuracy goals according industry best practices.","Experience object-oriented/object function scripting languages Python, Java, C++, Scala, etc. relational database design methodologies authoring complex SQL queries NoSQL technologies MongoDB, Cassandra, etc . Agile Development Deployment tools versioning using Git similar tools. Hadoop Big Data Spark, PySpark Kafka. data pipeline workflow management Azkaban, Luigi, Airflow, message queuing, stream processing, highly scalable âbig dataâ stores. building pipelines utilizing Google Cloud platform. git code repository Concourse CI/CD methodologies. 3-5 years experience Platform GCS, BQ, , Apache Kafka, Python Build infrastructure support coding, testing, maintaining resources Science, analytics reporting organizations SQL, SQOOP, Query, Kafka technologies. Collaborate Scientists development predictive models machine learning, natural language statistical analysis methods. Identify, implement internal process improvements automating manual processes, optimizing delivery, re-designing greater scalability, utilize provide actionable insights customer acquisition, operational efficiency key business performance metrics. Work stakeholders assist data-related technical issues needs. Develop, refine oversee standards, including establishing enforcing governance procedures ensuring integrity across multiple functions. Responsible owning quality metrics meeting defined accuracy goals according industry best practices."
226,Data Engineer,"Data Engineer, National Robotics Engineering Center (NREC)","Pittsburgh, PA",Pittsburgh,PA,"The National Robotics Engineering Center (NREC) at Carnegie Mellon University is looking for data scientists to develop tools to support machine learning and data-intensive applications. We are people with a desire to make robust software using agile development processes. You will work on a variety of software for commercial and government organizations. You will bring together open-source, commercial, internal, and your own tools to support diverse data processing workflows. Some of our machine learning software keeps self-driving vehicles safe, automatically discovers new pharmaceuticals, and leads to less waste in agriculture. You will support programs in deep learning for agriculture, artificial intelligence for defense, and autonomous manipulation.

Why NREC?
You will have an impact in shaping the robotics revolution, collaborate with and learn from experts,and build your career in a very fast-growing field. As part of our team, you will develop solutions to solve industrial and government challenges, deploy your technology in real-world situations, work side-by-side with elite robotics experts, and develop a variety of cutting-edge technologies.

Have an Impact!
Remove waste from farming = more food (link)
Make industrial processes environmentally friendly (link)
Make hazardous jobs safer (link)
Improve efficiency in industry & manufacturing (link)
Accelerate screening of pharmaceuticals (link)

Take Control of Your Career!
Select the career pathway that interests you
Influence the direction of projects
Supportive of a non-standard schedule
Maintain work/life balance
Switch between part-time and full-time as life demands

NREC is at the center of the robotics ecosystem in Pittsburgh, PA. With over 60 robotics companies, Pittsburgh has become the robotics capital of the world. Geek Wire calls it Robotics Row; others call it Roboburgh. Join the leader in the most exciting time in robotics!

Join the best robotics R&D group
Join our talented team at NREC, an operating unit within the world-renowned Robotics Institute at Carnegie Mellon University. NREC has 20+ years of experience and is globally renowned for developing and deploying robots into many applications across multiple sectors, such as agriculture, mining, defense, energy, and manufacturing. We strive to provide solutions for real world challenges where automation and robots have greater impact on productivity and improve the safety and comfort of the labor force. Our unique expertise places us at the forefront of unmanned ground vehicle design, autonomy, sensing and perception, machine learning, machine vision, operator assistance, 3D mapping and position estimation.

With over 120 robotics professionals, we can solve challenges that no other organization can.
NREC also leads in educational outreach through its Robotics Academy, which builds robotics curricula and software for K-12 and college-level students.

Your primary responsibilities include:
Developing software for machine learning and data-science applications
Architecting big-data pipelines
Adapting and integrating proprietary and open source software packages and APIs
Data preparation, integration, verification
Participating in the software process: design, code reviews, etc.
Developing, documenting, testing, and fixing software
Supporting development and acquisition of training and inference hardware
Development in a Linux environment

You must be willing to travel for extended periods for field testing.

Qualifications:
B.S. in Computer Science, Engineering, Mathematics is required (Any more is a bonus)
3-5 years relevant experience is required.
Demonstrated understanding and use of software engineering concepts, practices, and procedures.
Proficient development skills (Python preferred)
Linux development experience
Technical communication skills
Ability to participate in a multi-disciplinary team

We especially want to hear from you if you have experience or qualifications in ANY of the following areas:
Data infrastructure tools (SQL, NoSQL, data version control, etc.)
High performance computing tools (Hadoop, AWS, Azure, etc.)
Model training infrastructure tools (tensorflow, caffe, Petuum, etc.)
Inference model deployment tools (Docker or other containerization, etc.)
Cloud, high performance, and distributed computing
Ingestion and integration of disparate sources
Data processing (pre-processing, augmentation, post-processing)
Tools and protocols for reproducible research and data analysis
Front end (Data visualization, exploration, labeling)
Proficient Matlab or R skills
Proficient C or C++ skills
Professional software development processes
Networking interfaces and applications
Mixed software and hardware architecture
CMake, Valgrind, and other development tools
Computer vision, robotics, machine learning, scientific computing, simulation, or graphics

At NREC, we value diversity, support it, and thrive on it for the benefits of our organization, our employees and our community.

More Information

Please visit âWhy Carnegie Mellonâ to learn more about becoming part of an institution inspiring innovations that change the world. http://www.cmu.edu/jobs/why-cmu/index.html.

A listing of employee benefits is available at: http://www.cmu.edu/jobs/benefits-at-a-glance/index.html.

Carnegie Mellon University is an Equal Opportunity Employer/Disability/Veteran.
Job Function: Engineering

Primary Location: United States-Pennsylvania-Pittsburgh

Time Type: Full Time

Organization: ROBOTICS

Minimum Education Level: Bachelor's Degree or equivalent

Preferred Education Level: Master's Degree or equivalent

Budgeted Base Pay: Negotiable"," B.S. in Computer Science, Engineering, Mathematics is required  Any more is a bonus  3-5 years relevant experience is required. Demonstrated understanding and use of software engineering concepts, practices, and procedures. Proficient development skills  Python preferred  Linux development experience Technical communication skills Ability to participate in a multi-disciplinary team    ","B.S. in Computer Science, Engineering, Mathematics is required Any more a bonus 3-5 years relevant experience required. Demonstrated understanding and use of software engineering concepts, practices, procedures. Proficient development skills Python preferred Linux Technical communication Ability to participate multi-disciplinary team","B.S. Computer Science, Engineering, Mathematics required Any bonus 3-5 years relevant experience required. Demonstrated understanding use software engineering concepts, practices, procedures. Proficient development skills Python preferred Linux Technical communication Ability participate multi-disciplinary team"
227,Data Engineer,SQL Data Engineer,"Pittsburgh, PA 15219",Pittsburgh,PA,"We are looking for a SQL Data Engineer who has strong SQL skills and the desire to build a sustainable environment that can scale as a business grows. This position will provide the opportunity to build an environment from the ground to customer facing and experience the value of all the time and effort to a solution. You will have the ability to help guide and teach others on SQL optimization as required to improve performance on results and for the betterment of the SQL Server. You will work with Data Analysts, Pricing Analysts and Business Stakeholders to develop solutions to meet data needs.

Key Responsibilities:
Strong Microsoft SQL Skills including but not limited to views, stored procedures and function creation and optimization
Ensure performance and availability of databases
ETL knowledge and capability
Data Modeling / Database Design
Perform baseline Microsoft SQL database administrative tasks
Job creations and failure monitoring
Evaluate datasets for quality and accuracy
SQL Training to others as needed
Bachelorâs degree in Computer Science, Information Systems, or equivalent degree or strong industry experience.
Strong verbal and written communication skills
5-10 years experience in database development/design
Knowledge about SQL Server Integration Services and SQL Agent
Experience handling multiple concurrent complex activities within a technical environment and ability to project risks and escalate to managers appropriately
Must have strong analytical and problem solving skills
Experience with Office 365 products a plus
Experience with Power Bi a plus
Experience with AWS a plus
Demonstrate Thermo Fisher Scientific values â Integrity, Intensity, Innovation and Involvement

At Thermo Fisher Scientific, each one of our 50,000 extraordinary minds has a unique story to tell. Join us and contribute to our singular missionâenabling our customers to make the world healthier, cleaner and safer. Apply today

Thermo Fisher Scientific is an EEO/Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status.




If you are an individual with a disability who requires reasonable accommodation to complete any part of our application process, for further assistance.
Thermo Fisher Scientific is an EEO/Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status.","  Strong Microsoft SQL Skills including but not limited to views, stored procedures and function creation and optimization Ensure performance and availability of databases ETL knowledge and capability Data Modeling / Database Design Perform baseline Microsoft SQL database administrative tasks Job creations and failure monitoring Evaluate datasets for quality and accuracy SQL Training to others as needed  ","Strong Microsoft SQL Skills including but not limited to views, stored procedures and function creation optimization Ensure performance availability of databases ETL knowledge capability Data Modeling / Database Design Perform baseline database administrative tasks Job creations failure monitoring Evaluate datasets for quality accuracy Training others as needed","Strong Microsoft SQL Skills including limited views, stored procedures function creation optimization Ensure performance availability databases ETL knowledge capability Data Modeling / Database Design Perform baseline database administrative tasks Job creations failure monitoring Evaluate datasets quality accuracy Training others needed"
228,Data Engineer,Senior Software Engineer - Data,"New York, NY",New York,NY,"Lucid is a market research platform that provides access to authentic, first-party data in over 90 countries. Our products and services enable anyone, in any industry, to ask questions of targeted audiences and find the answers they need â fast. These answers can be used to uncover consumer motivations, increase revenue, and measure the impact of digital advertising. Founded in 2010, Lucid is headquartered in New Orleans, LA with offices in Dallas, New York, London, Sydney, Singapore, Gurgaon, Prague, and Hamburg.


Apply for this Job

The Opportunity

The Senior Data Engineer will be a key part of our development team and work closely with development team peers, Product Management team, and business and other support teams.
Responsibilities
Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs
Adheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time
Able to define new architectures and improve existing ones
Can be the central focus for code reviews, architecture discussion and bug fixing
Demonstrates code and product ownership in production
Support the business teams and product managers in data extracts and data analysis
Performs as a true agile team leader and exhibits competencies in all layers of the application stack
Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration
Qualifications
Master's or bachelor's degree in Computer Science
At least 10 years of hands-on software development experience in Python, Golang, Java, C++ or Scala
Strong Object Oriented Programming skills
Deep knowledge in data structures, algorithms, and software design
Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources
Highly proficient with relational and non-relational data storages
Strong verbal and written communication skills
Preferred Qualifications
AWS experience with S3, RDS, Redshift, EMR, Kinesis, Lambda, Elastic Beanstalk and Elasticsearch
Experience with Spark, Hadoop and Hive on EMR and non-managed. Can build and run a basic cluster
Experience developing ETLs and running job schedulers (e.g. Airflow)
Experience with PostgreSQL
Experience with NoSQL data sources including Cassandra
Experience with Kafka, Redis
Experience creating Data Lakes


At Lucid we foster a collaborative and inspiring workplace. We pride ourselves in doing this by recruiting, hiring and retaining diverse, passionate, and forward-thinking talent. Lucid is committed to and encourages an inclusive environment and we are dedicated to providing equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know."," Master's or bachelor's degree in Computer Science At least 10 years of hands-on software development experience in Python, Golang, Java, C++ or Scala Strong Object Oriented Programming skills Deep knowledge in data structures, algorithms, and software design Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources Highly proficient with relational and non-relational data storages Strong verbal and written communication skills   Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs Adheres to the best practices of software engineering  testing, integration, clean design and concern separation  and helps improve those practices over time Able to define new architectures and improve existing ones Can be the central focus for code reviews, architecture discussion and bug fixing Demonstrates code and product ownership in production Support the business teams and product managers in data extracts and data analysis Performs as a true agile team leader and exhibits competencies in all layers of the application stack Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration  ","Master's or bachelor's degree in Computer Science At least 10 years of hands-on software development experience Python, Golang, Java, C++ Scala Strong Object Oriented Programming skills Deep knowledge data structures, algorithms, and design Experience with high volume performance applications dealing large amounts structured unstructured from multiple sources Highly proficient relational non-relational storages verbal written communication Work the team - Tech Product Managers executing product backlog, taking part its creation grooming, understanding stakeholders needs Adheres to best practices engineering testing, integration, clean concern separation helps improve those over time Able define new architectures existing ones Can be central focus for code reviews, architecture discussion bug fixing Demonstrates ownership production Support business teams managers extracts analysis Performs as a true agile leader exhibits competencies all layers application stack Demonstrate proficiency developing user interface, logic, modeling systems component integration","Master's bachelor's degree Computer Science At least 10 years hands-on software development experience Python, Golang, Java, C++ Scala Strong Object Oriented Programming skills Deep knowledge data structures, algorithms, design Experience high volume performance applications dealing large amounts structured unstructured multiple sources Highly proficient relational non-relational storages verbal written communication Work team - Tech Product Managers executing product backlog, taking part creation grooming, understanding stakeholders needs Adheres best practices engineering testing, integration, clean concern separation helps improve time Able define new architectures existing ones Can central focus code reviews, architecture discussion bug fixing Demonstrates ownership production Support business teams managers extracts analysis Performs true agile leader exhibits competencies layers application stack Demonstrate proficiency developing user interface, logic, modeling systems component integration"
229,Data Engineer,Data Engineer,"New York, NY 10022",New York,NY,"PromoteIQ delivers intelligent vendor marketing solutions built for the next generation of e-commerce. Our solutions help retailers implement, automate, and scale brand-funded marketing programs on e-commerce sites. PromoteIQ is a New York City-based technology company that works with the US's largest e-commerce retailers. Learn more about us at https://www.promoteiq.com.

Who weâre looking for
At PromoteIQ, data plays an integral role in our product, and software engineers on our data engineering team build the pipelines that power reporting and analytics for our e-commerce promotions platform. The infrastructure and applications that you'll build on the data engineering team will have broad and critical reach in powering real-time auction decisions, becoming multipliers on our revenues, and forecasting supply and demand for our customers.
Responsibilities
Ship high-quality, well-tested, secure, and maintainable code
Design, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions
Manage automated unit and integration test suites
Work collaboratively and communicate effectively with a small, motivated team of engineers and product managers
Experiment with and recommend new technologies that simplify or improve PromoteIQ's stack
Participate in an on-call rotation and work occasional off-hours
Qualifications
BS/MS in Computer Science or a related technical field
Seeking candidates with 1+ years of experience in:
Architecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services
Designing data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.)
Designing efficient data structures and database schemas
Working with distributed systems architecture
Incorporating data processing and workflow management tools into pipeline design (AWS EMR, Airflow, Kafka, etc.)
Using profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements
Developing for continuous integration and automated deployments
Utilizing a variety of data stores, including data warehouses (ideally Redshift), RDBMSes (ideally MySQL), in-memory caches (ideally Aerospike and Redis), and searchable document DBs (ideally Elasticseach)
Wrangling large-scale data sets

#MicrosoftAdvertising #PromoteIQ

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work."," BS/MS in Computer Science or a related technical field Seeking candidates with 1+ years of experience in  Architecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services Designing data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure  shell scripts, job schedulers, message queues, etc.  Designing efficient data structures and database schemas Working with distributed systems architecture Incorporating data processing and workflow management tools into pipeline design  AWS EMR, Airflow, Kafka, etc.  Using profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements Developing for continuous integration and automated deployments Utilizing a variety of data stores, including data warehouses  ideally Redshift , RDBMSes  ideally MySQL , in-memory caches  ideally Aerospike and Redis , and searchable document DBs  ideally Elasticseach  Wrangling large-scale data sets   Ship high-quality, well-tested, secure, and maintainable code Design, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions Manage automated unit and integration test suites Work collaboratively and communicate effectively with a small, motivated team of engineers and product managers Experiment with and recommend new technologies that simplify or improve PromoteIQ's stack Participate in an on-call rotation and work occasional off-hours  ","BS/MS in Computer Science or a related technical field Seeking candidates with 1+ years of experience Architecting, building, and maintaining end-to-end, high-throughput data systems their supporting services Designing that are secure, testable, modular, particularly Python, as well support infrastructure shell scripts, job schedulers, message queues, etc. efficient structures database schemas Working distributed architecture Incorporating processing workflow management tools into pipeline design AWS EMR, Airflow, Kafka, Using profiling tools, debugging logs, performance metrics, other sources to make code- application-level improvements Developing for continuous integration automated deployments Utilizing variety stores, including warehouses ideally Redshift , RDBMSes MySQL in-memory caches Aerospike Redis searchable document DBs Elasticseach Wrangling large-scale sets Ship high-quality, well-tested, maintainable code Design, develop, maintain pipelines back-end real-time decisioning, reporting, optimization, collection, functions Manage unit test suites Work collaboratively communicate effectively small, motivated team engineers product managers Experiment recommend new technologies simplify improve PromoteIQ's stack Participate an on-call rotation work occasional off-hours","BS/MS Computer Science related technical field Seeking candidates 1+ years experience Architecting, building, maintaining end-to-end, high-throughput data systems supporting services Designing secure, testable, modular, particularly Python, well support infrastructure shell scripts, job schedulers, message queues, etc. efficient structures database schemas Working distributed architecture Incorporating processing workflow management tools pipeline design AWS EMR, Airflow, Kafka, Using profiling tools, debugging logs, performance metrics, sources make code- application-level improvements Developing continuous integration automated deployments Utilizing variety stores, including warehouses ideally Redshift , RDBMSes MySQL in-memory caches Aerospike Redis searchable document DBs Elasticseach Wrangling large-scale sets Ship high-quality, well-tested, maintainable code Design, develop, maintain pipelines back-end real-time decisioning, reporting, optimization, collection, functions Manage unit test suites Work collaboratively communicate effectively small, motivated team engineers product managers Experiment recommend new technologies simplify improve PromoteIQ's stack Participate on-call rotation work occasional off-hours"
230,Data Engineer,Senior Data Engineer,"New York, NY",New York,NY,"Position Summary:
The New York County District Attorney's Office (DANY) has an opening for a Senior Data Engineer in its Information Technology (IT) Department. DANY's IT Department provides agency-wide IT solutions for investigations, prosecution support, and case management. The Senior Data Engineer will work with key data sets used in the prosecution process and other criminal justice initiatives. The Senior Data Engineer will also be a key force in the development of tools and appropriate environment for the effective and progressive use of the data to further criminal prosecutions and justice initiatives.

Responsibilities include but are not limited to:

Develop expertise in the DANY data model.
Develop data dictionaries and best practices for data definitions and use.
Coordinate with network administrative staff to develop, implement, and maintain a secure and agile sandbox environment.
Identify tools and procedures to effectively use data.
Research and promote options for quick development.
Document standards for security and maintainability.
Supervise developers hired for special projects.
Interact with Enterprise projects development team to coordinate data collection, standards, and use.
Perform related tasks as assigned.

Qualifications:

Experience with data dictionary concepts, data mapping, and data architecture.
Knowledge and experience with a wide range of tools and IT technologies.
Proven ability to research and learn tools, hardware, and languages quickly.
Understanding of data analysis strategies and concepts (e.g. business intelligence, time series analysis, project management).
Excellent interpersonal, organizational, and communication skills required.
Must be a self-starter with the ability to work independently and manage multiple long-term projects.
Strong attention to detail and high concern for data accuracy.
Ability to interact with all levels of staff, with a high regard for confidentiality and diplomacy.
Ability to work efficiently to meet deadlines.
Dependable team player who works collaboratively and cooperatively with staff in a team-oriented environment.
Ability to multi-task in a fast-paced environment, prioritize among competing needs and respond quickly to requests for information.
Ability to follow directions and apply proper policies, procedures and guidelines.
Resourcefulness, initiative, and good judgment essential.

Educational Requirements:

Bachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required.
Master's degree preferred.

Commitment:

One (1) year commitment to hiring department.

The New York County District Attorney's Office is an Equal Opportunity Employer"," Experience with data dictionary concepts, data mapping, and data architecture. Knowledge and experience with a wide range of tools and IT technologies. Proven ability to research and learn tools, hardware, and languages quickly. Understanding of data analysis strategies and concepts  e.g. business intelligence, time series analysis, project management . Excellent interpersonal, organizational, and communication skills required. Must be a self-starter with the ability to work independently and manage multiple long-term projects. Strong attention to detail and high concern for data accuracy. Ability to interact with all levels of staff, with a high regard for confidentiality and diplomacy. Ability to work efficiently to meet deadlines. Dependable team player who works collaboratively and cooperatively with staff in a team-oriented environment. Ability to multi-task in a fast-paced environment, prioritize among competing needs and respond quickly to requests for information. Ability to follow directions and apply proper policies, procedures and guidelines. Resourcefulness, initiative, and good judgment essential.    Develop expertise in the DANY data model. Develop data dictionaries and best practices for data definitions and use. Coordinate with network administrative staff to develop, implement, and maintain a secure and agile sandbox environment. Identify tools and procedures to effectively use data. Research and promote options for quick development. Document standards for security and maintainability. Supervise developers hired for special projects. Interact with Enterprise projects development team to coordinate data collection, standards, and use. Perform related tasks as assigned.   Bachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required. Master's degree preferred.   Bachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required. Master's degree preferred. ","Experience with data dictionary concepts, mapping, and architecture. Knowledge experience a wide range of tools IT technologies. Proven ability to research learn tools, hardware, languages quickly. Understanding analysis strategies concepts e.g. business intelligence, time series analysis, project management . Excellent interpersonal, organizational, communication skills required. Must be self-starter the work independently manage multiple long-term projects. Strong attention detail high concern for accuracy. Ability interact all levels staff, regard confidentiality diplomacy. efficiently meet deadlines. Dependable team player who works collaboratively cooperatively staff in team-oriented environment. multi-task fast-paced environment, prioritize among competing needs respond quickly requests information. follow directions apply proper policies, procedures guidelines. Resourcefulness, initiative, good judgment essential. Develop expertise DANY model. dictionaries best practices definitions use. Coordinate network administrative develop, implement, maintain secure agile sandbox Identify effectively use data. Research promote options quick development. Document standards security maintainability. Supervise developers hired special Interact Enterprise projects development coordinate collection, standards, Perform related tasks as assigned. Bachelor's degree Information Technology or Computer Science/Engineering, Machine Learning field Master's preferred.","Experience data dictionary concepts, mapping, architecture. Knowledge experience wide range tools IT technologies. Proven ability research learn tools, hardware, languages quickly. Understanding analysis strategies concepts e.g. business intelligence, time series analysis, project management . Excellent interpersonal, organizational, communication skills required. Must self-starter work independently manage multiple long-term projects. Strong attention detail high concern accuracy. Ability interact levels staff, regard confidentiality diplomacy. efficiently meet deadlines. Dependable team player works collaboratively cooperatively staff team-oriented environment. multi-task fast-paced environment, prioritize among competing needs respond quickly requests information. follow directions apply proper policies, procedures guidelines. Resourcefulness, initiative, good judgment essential. Develop expertise DANY model. dictionaries best practices definitions use. Coordinate network administrative develop, implement, maintain secure agile sandbox Identify effectively use data. Research promote options quick development. Document standards security maintainability. Supervise developers hired special Interact Enterprise projects development coordinate collection, standards, Perform related tasks assigned. Bachelor's degree Information Technology Computer Science/Engineering, Machine Learning field Master's preferred."
231,Data Engineer,"Senior Engineer, Data Engineering","New York, NY",New York,NY,"As a member of the Data Engineering Team, the Senior, Data Engineer, AdSmart will be directly responsible for the design, management, and development of parts of the software platform and products for NBCUniversalâs AdSmart. NBCUniversalâs AdSmart products will enable NBCUniversal to better understand it's brandâs audiences such as NBC News, Bravo, The Tonight Show, Saturday Night Live, and USA Network as well as audiences that cross brands. The goal is to ensure we know who is watching what, where and when. In turn, enabling NBCUniversalâs sales teams to properly align our audiences with the market advertisements that can benefit them the most.

Youâre a big thinker who can analyze and evangelize a long-range opportunity, architect a groundbreaking solution, and roll-up your sleeves to get code out the door when needed. You are data-driven and analytical. You understand the concept of a value proposition and evaluation criteria, and you know how to align them with low-level milestones to get the work done. You can apply domain knowledge from one technical subject, in order to quickly ramp and deliver on a new one. You know how to learn from failure until you succeed, and you are able to articulate and quantify the reasons for your decisions.

You will be part of the AdSmart Data Engineering team, participating in the data architecture that will drive both current and future data management initiatives within NBCUniversalâs AdSmart group.

Responsibilities:
Serve as a senior data engineer for AdSmart products.
Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and leadership
Develop and code the software components that are core to Audience Studio, under the leadership of the VP/Chief Architecture
Support product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.
Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership

Qualifications:
Bachelorâs degree in Computer Science or related field
3+ years of software development experience, as a developer
Fluency in Scala and/or Java programming languages
Strong OO & FP design patterns, data structure, and algorithm design skills
Extensive experience developing Apache Spark applications
2+ years of experience with both relational database design (SQL), non-relational (NoSQL) databases, big data, real-time technologies
Familiar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka
Experience with software containerization, such as Docker
Experience developing and/or consuming web interfaces (REST API) and associated skills (HTTP, web services)
Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment

Additional Job Requirements:
 Interested candidate must submit a resume/CV through www.nbcunicareers.com to be considered
Must be willing to work in New York, NY

Desired Requirements:
Experience as a development manager (with direct authority over development staff)
Experience with Cluster Management and Container Orchestration technologies such as Mesos, Kubernetes, Hadoop/Yarn
Experience with Apache Kafka or similar streaming technologies
Experience with digital advertising technologies.
Able and eager to learn new technologies
Able to easily transition between high-level strategy and day-to-day implementation
Excellent teamwork and collaboration skills
Results-oriented, high energy, self-motivated
Sub-BusinessTechnology
Career Level
Experienced
CityNew York
State/Province
New York
CountryUnited States
About Us
At NBCUniversal, we believe in the talent of our people. Itâs our passion and commitment to excellence that drives NBCUâs vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. Itâs what makes us uniquely NBCU. Here you can create the extraordinary. Join us.
Notices
NBCUniversalâs policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable."," Bachelorâs degree in Computer Science or related field 3+ years of software development experience, as a developer Fluency in Scala and/or Java programming languages Strong OO & FP design patterns, data structure, and algorithm design skills Extensive experience developing Apache Spark applications 2+ years of experience with both relational database design  SQL , non-relational  NoSQL  databases, big data, real-time technologies Familiar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka Experience with software containerization, such as Docker Experience developing and/or consuming web interfaces  REST API  and associated skills  HTTP, web services  Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment   Serve as a senior data engineer for AdSmart products. Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and leadership Develop and code the software components that are core to Audience Studio, under the leadership of the VP/Chief Architecture Support product with the overall roadmap and ensure updates to senior leadership are 100% technically correct. Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership    Interested candidate must submit a resume/CV through www.nbcunicareers.com to be considered Must be willing to work in New York, NY","Bachelorâs degree in Computer Science or related field 3+ years of software development experience, as a developer Fluency Scala and/or Java programming languages Strong OO & FP design patterns, data structure, and algorithm skills Extensive experience developing Apache Spark applications 2+ with both relational database SQL , non-relational NoSQL databases, big data, real-time technologies Familiar various cloud sources architectures such AWS/S3, HDFS, Kafka Experience containerization, Docker consuming web interfaces REST API associated HTTP, services Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable working effectively dynamic environment Serve senior engineer for AdSmart products. Participate in, execute, 12-36 month product roadmap input from the delivery team, stakeholders, leadership Develop code components that are core Audience Studio, under VP/Chief Architecture Support overall ensure updates 100% technically correct. Analyze report results adjust engineering strategy accordingly Interested candidate must submit resume/CV through www.nbcunicareers.com be considered Must willing work New York, NY","Bachelorâs degree Computer Science related field 3+ years software development experience, developer Fluency Scala and/or Java programming languages Strong OO & FP design patterns, data structure, algorithm skills Extensive experience developing Apache Spark applications 2+ relational database SQL , non-relational NoSQL databases, big data, real-time technologies Familiar various cloud sources architectures AWS/S3, HDFS, Kafka Experience containerization, Docker consuming web interfaces REST API associated HTTP, services Self-directed, ability multi-task, sharp analytical abilities, excellent communication skills, capable working effectively dynamic environment Serve senior engineer AdSmart products. Participate in, execute, 12-36 month product roadmap input delivery team, stakeholders, leadership Develop code components core Audience Studio, VP/Chief Architecture Support overall ensure updates 100% technically correct. Analyze report results adjust engineering strategy accordingly Interested candidate must submit resume/CV www.nbcunicareers.com considered Must willing work New York, NY"
232,Data Engineer,Engineer - Data,"New York, NY",New York,NY,"Lucid is a market research platform that provides access to authentic, first-party data in over 90 countries. Our products and services enable anyone, in any industry, to ask questions of targeted audiences and find the answers they need â fast. These answers can be used to uncover consumer motivations, increase revenue, and measure the impact of digital advertising. Founded in 2010, Lucid is headquartered in New Orleans, LA with offices in Dallas, New York, London, Sydney, Singapore, Gurgaon, Prague, and Hamburg.


Apply for this Job

The Opportunity

The Data Engineer will be part of our development team and work closely with development team peers, Product Management team, and business and other support teams.
Responsibilities
Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs
Adheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time
Collaborates with code reviews, architecture discussion and bug fixing
Demonstrates code and product ownership in production
Support the business teams and product managers in data extracts and data analysis
Performs as a true agile team member and exhibits competencies in all layers of the application stack. Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration
Qualifications
Master's or bachelor's degree in Computer Science
At least 5 years of hands-on software development experience in Python, Golang, Java, C++ or Scala
Strong Object Oriented Programming skills
Deep knowledge in data structures, algorithms, and software design
Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources
Highly proficient with relational and non-relational data storages
Strong verbal and written communication skills
Preferred Qualifications
AWS experience with S3, RDS, Redshift, EMR, Kinesis, Lambda, Elastic Beanstalk and Elasticsearch
Experience with Spark, Hadoop and Hive on EMR and non-managed. Can build and run a basic cluster
Experience developing ETLs and running job schedulers (e.g. Airflow)
Experience with PostgreSQL
Experience with NoSQL data sources including Cassandra
Experience with Kafka, Redis
Experience creating Data Lakes


At Lucid we foster a collaborative and inspiring workplace. We pride ourselves in doing this by recruiting, hiring and retaining diverse, passionate, and forward-thinking talent. Lucid is committed to and encourages an inclusive environment and we are dedicated to providing equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know."," Master's or bachelor's degree in Computer Science At least 5 years of hands-on software development experience in Python, Golang, Java, C++ or Scala Strong Object Oriented Programming skills Deep knowledge in data structures, algorithms, and software design Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources Highly proficient with relational and non-relational data storages Strong verbal and written communication skills  Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs Adheres to the best practices of software engineering  testing, integration, clean design and concern separation  and helps improve those practices over time Collaborates with code reviews, architecture discussion and bug fixing Demonstrates code and product ownership in production Support the business teams and product managers in data extracts and data analysis Performs as a true agile team member and exhibits competencies in all layers of the application stack. Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration  ","Master's or bachelor's degree in Computer Science At least 5 years of hands-on software development experience Python, Golang, Java, C++ Scala Strong Object Oriented Programming skills Deep knowledge data structures, algorithms, and design Experience with high volume performance applications dealing large amounts structured unstructured from multiple sources Highly proficient relational non-relational storages verbal written communication Work the team - Tech Product Managers executing product backlog, taking part its creation grooming, understanding stakeholders needs Adheres to best practices engineering testing, integration, clean concern separation helps improve those over time Collaborates code reviews, architecture discussion bug fixing Demonstrates ownership production Support business teams managers extracts analysis Performs as a true agile member exhibits competencies all layers application stack. Demonstrate proficiency developing for user interface, logic, modeling systems component integration","Master's bachelor's degree Computer Science At least 5 years hands-on software development experience Python, Golang, Java, C++ Scala Strong Object Oriented Programming skills Deep knowledge data structures, algorithms, design Experience high volume performance applications dealing large amounts structured unstructured multiple sources Highly proficient relational non-relational storages verbal written communication Work team - Tech Product Managers executing product backlog, taking part creation grooming, understanding stakeholders needs Adheres best practices engineering testing, integration, clean concern separation helps improve time Collaborates code reviews, architecture discussion bug fixing Demonstrates ownership production Support business teams managers extracts analysis Performs true agile member exhibits competencies layers application stack. Demonstrate proficiency developing user interface, logic, modeling systems component integration"
233,Data Engineer,AI Data Scientist / Data Engineer - Experienced Associate,"New York, NY 10017",New York,NY,"PwC Labs is focused on standardizing, automating, delivering tools and processes and exploring emerging technologies that drive efficiency and enable our people to reimagine the possible. Process improvement, transformation, effective use of innovative technology and data & analytics, and leveraging alternative delivery solutions are key areas of focus to drive additional value for our firm. The AI Lab focuses on implementing solutions that impact efficiency and effectiveness of our technology functions. Process improvement, transformation, effective use of technology and data & analytics, and leveraging alternative delivery are key areas to drive value and continue to be recognized as the leading professional services firm. AI Lab is focused on identifying and prioritizing emerging technologies to get the most out of our investments.

To really stand out and make us ?t for the future in a constantly changing world, each and every one of us at PwC needs to be an authentic and inclusive leader, at all grades/levels and in all lines of service. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.

As an Associate, youâll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:

Invite and provide evidence-based feedback in a timely and constructive manner.Share and collaborate effectively with others.Work with existing processes/systems whilst making constructive suggestions for improvements.Validate data and analysis for accuracy and relevance.Follow risk management and compliance procedures.Keep up-to-date with technical developments for business area.
- Communicate confidently in a clear, concise and articulate manner - verbally and in written form.
Seek opportunities to learn about other cultures and other parts of the business across the Network of PwC firms.Uphold the firmâs code of ethics and business conduct.

Our team is capability centric, focusing on AI and machine learning techniques that are broadly applicable across all industries. We work with a variety of data mediums including text, audio, imagery, sensory, and structured data. Our work involves the use of supervised/unsupervised machine learning algorithms, traditional statistical models, deep neural networks, terabyte scale data, and simulation modelling. Our work is having a tremendous impact on how PwC & our clients do business.
Job Requirements and Preferences:

Basic Qualifications:

Minimum Degree Required:
Bachelor Degree

Minimum Years of Experience:
1 year(s)

Preferred Qualifications:

Preferred Fields of Study:
Computer and Information Science, Computer Engineering, Computer and Information Science & Accounting, Economics, Economics and Finance, Economics and Finance & Technology, Engineering, Mathematics, Mathematical Statistics, Statistics

Preferred Knowledge/Skills:
Demonstrates some knowledge and/or a proven record of success in the following areas:
Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.
Demonstrates some abilities and/or a proven record of success learning and applying new skills quickly, including the following areas and technologies:
Programming: Python, R, Java, JavaScript, C++, Unix;
Hardware: sensors, robotics, GPU enabled machine learning, FPGAs, Raspberry Pis, etc.;
Data Storage Technologies: SQL, NoSQL, Hadoop, cloud-based databases such as GCP BigQuery, and different storage formats (e.g. Parquet, etc.);
Data Processing Tools: Python (Numpy, Pandas, etc.), Spark, cloud-based solutions such as GCP DataFlow;
Machine Learning Libraries: Python (scikit-learn, genism, etc.), TensorFlow, Keras, PyTorch, Spark MLlib;
Visualization: Python (Matplotlib, Seaborn, bokeh, etc.), JavaScript (d3); and,
Productionization and containerization technologies: GitHub, Flask, Docker, Kubernetes.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer."," Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding of machine learning algorithms  e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.  and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.  Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding of machine learning algorithms  e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.  and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.    Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding of machine learning algorithms  e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.  and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.","Exploring new analytical technologies and evaluating their technical commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts prototype models that can be demoed explained data scientists, internal stakeholders, clients; Testing rejecting hypotheses around processing machine learning model building; Demonstrating ability experiment, fail quickly, recognize when you need assistance vs. conclude a technology is not suitable for the task; Building pipelines ingest, clean data, make predictions; Staying abreast of AI research from leading labs by reading papers experimenting with code; Developing innovative solutions perspectives on published academic journals/arXiv shared continuously learn quickly evaluate viability; Applying techniques addressing variety problems e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding algorithms k-nearest neighbors, random forests, ensemble methods, deep neural networks, it appropriate use each technique; systems, interpreting output, communicating results; Moving development production plus; and, Conducting lab publishing work plus.","Exploring new analytical technologies evaluating technical commercial viability quickly; Working 4-week sprint cycles develop proof-of-concepts prototype models demoed explained data scientists, internal stakeholders, clients; Testing rejecting hypotheses around processing machine learning model building; Demonstrating ability experiment, fail quickly, recognize need assistance vs. conclude technology suitable task; Building pipelines ingest, clean data, make predictions; Staying abreast AI research leading labs reading papers experimenting code; Developing innovative solutions perspectives published academic journals/arXiv shared continuously learn quickly evaluate viability; Applying techniques addressing variety problems e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding algorithms k-nearest neighbors, random forests, ensemble methods, deep neural networks, appropriate use technique; systems, interpreting output, communicating results; Moving development production plus; and, Conducting lab publishing work plus."
234,Data Engineer,Senior Data Engineer,"New York, NY 10011",New York,NY,"Summary:

You have experience with client projects and in handling vast amounts of data â working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Data Science team. Lead on projects from a data engineering perspective, working with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches
Plan and execute secure, good practice data integration strategies and approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Create and manage data environments in the Cloud
Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models
Have a strong understanding of Information Security principles to ensure compliant handling and management of client data
This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science
Qualifications:
Commercial experience leading on client-facing projects, including working in close-knit teams
5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)
5+ years of experience working on projects within the cloud ideally AWS or Azure
5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent
Experience with open source tools like Apache Airflow and Griffin
Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform
Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift
Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models
Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma
Experience building automated data quality and testing into data pipelines
Experience with AI, NLP, Machine Learning, etc. is a plus
Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R
Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time
Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.
A deep personal motivation to always produce outstanding work for your clients and colleagues
Excel in team collaboration and working with others from diverse skill-sets and backgrounds
Cervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.

M4ahUGavPU"," Commercial experience leading on client-facing projects, including working in close-knit teams 5+ years of experience and interest in Big Data technologies  Hadoop / Spark / NoSQL DBs  5+ years of experience working on projects within the cloud ideally AWS or Azure 5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent Experience with open source tools like Apache Airflow and Griffin Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma Experience building automated data quality and testing into data pipelines Experience with AI, NLP, Machine Learning, etc. is a plus Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C , R Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner. A deep personal motivation to always produce outstanding work for your clients and colleagues Excel in team collaboration and working with others from diverse skill-sets and backgrounds    ","Commercial experience leading on client-facing projects, including working in close-knit teams 5+ years of and interest Big Data technologies Hadoop / Spark NoSQL DBs projects within the cloud ideally AWS or Azure with streaming architectures patterns like Kafka, Kinesis, Flink, Confluent Experience open source tools Apache Airflow Griffin DevOps DataOps Jenkins, Kubernetes, Docker, Terraform Warehousing products Snowflake, DW, Redshift building operational ETL data pipelines across a number sources, constructing relational dimensional models one more ETL/ELT Talend, Matillion, FiveTran, Alooma automated quality testing into AI, NLP, Machine Learning, etc. is plus Strong development background at least two scripting, object oriented functional programming language, SQL, Python, Java, Scala, C , R lively consulting setting, often different multiple same time Excellent interpersonal skills when interacting clients clear, timely, professional manner. A deep personal motivation to always produce outstanding work for your colleagues Excel team collaboration others from diverse skill-sets backgrounds","Commercial experience leading client-facing projects, including working close-knit teams 5+ years interest Big Data technologies Hadoop / Spark NoSQL DBs projects within cloud ideally AWS Azure streaming architectures patterns like Kafka, Kinesis, Flink, Confluent Experience open source tools Apache Airflow Griffin DevOps DataOps Jenkins, Kubernetes, Docker, Terraform Warehousing products Snowflake, DW, Redshift building operational ETL data pipelines across number sources, constructing relational dimensional models one ETL/ELT Talend, Matillion, FiveTran, Alooma automated quality testing AI, NLP, Machine Learning, etc. plus Strong development background least two scripting, object oriented functional programming language, SQL, Python, Java, Scala, C , R lively consulting setting, often different multiple time Excellent interpersonal skills interacting clients clear, timely, professional manner. A deep personal motivation always produce outstanding work colleagues Excel team collaboration others diverse skill-sets backgrounds"
235,Data Engineer,Data Engineer,"New York, NY 10018",New York,NY,"TEAM: Information Technology
REPORTS TO: MD, AI & Cognitive Services
LOCATION: New York, NY

THE ROLE
Are you passionate about Artificial Intelligence/Machine Learning, have coding experience and are looking for more progression & autonomy in your career? We are seeking an AI/ML Engineer to collaborate with some of techâs and analyticsâ sharpest minds to solve the firmâs ever-changing but exciting challenges and explore unique technical solutions. Working in tech here at Teach for America means that youâll always be presented with a variety of new possibilities as you continue to enhance your skills and contribute to our mission.

THE ORGANIZATION
There are more than 16 million children growing up in poverty in the U.S., and less than 10 percent of them will graduate from college. These statistics are not a reflection of our childrenâs potential; we know that children growing up in poverty can and do achieve at the highest levels. Rather, these statistics reflect the systemic lack of access and opportunity for children in low-income communities.

Teach For Americaâs (TFA) mission is to find, develop, and support a diverse network of leaders committed to expanding opportunity for children from classrooms, schools, and every sector and field that shapes the broader systems in which schools operate. We are seeking individuals who align with our mission, core values and commitment to Diversity Equity & Inclusiveness and are ready to join us in this global movement.


Qualifications:
THE PERSON
Hands on research and development
Find innovative solutions to difficult and unstructured problems that will support and expand our core business
Use analytical rigor to analyze large amounts of data, help extract actionable insights using data analysis, feature engineering, optimization tools and machine learning techniques
Develop and maintain scalable and reliable data pipelines for AI/ML processes
Participate in all aspects of Teach For Americaâs agile software development cycle
Other duties (research, presentations, communicating with other teams)

THE MUST HAVES
Prior Experience:
Intermediate or higher experience working with machine learning tools in Python (TensorFlow, Keras, fast.ai, etc)
Experience working with data solutions (data ingestion, preprocessing, analysis, predictive analytics)
Experience with Reporting and Advanced Analytics Solutions
Intermediate or higher experience working with Java (Spring, REST, JMS) and SQL
Comfortable working in a Unix environment
Experience with version control, containerization technologies
Experience working with Continuous Integration / Automation architectures
Experience interfacing and working with engineering teams throughout the product development lifecycle (leading projects and/or other developers)
Experience partnering with other teams to test and roll out cognitive & predictive analytics solutions
Optional but desired:
Experience developing applications for and deploying applications in cloud environments
Experience with big data and associated technologies (Hadoop, Databricks, Azure Machine Learning etc.)
Experience with statistical modeling in R
Skills:
Be able to communicate in a clear and effective manner with both technical and non-technical audiences
Be interested in staying up-to-date with recent advances in machine learning and predictive analytics
Be detail oriented, able to work under pressure and effectively manage competing priorities
Education:
At least a four-year degree in Computer Science, BSEE, MIS or a related field, or equivalent experience
Work Demands:
This position is located on site in our New York National Office
Limited travel may be required

THE TEAM
Our team loves to collaborate. We partner with every other team in the organization to create world-class technology solutions that staff and corps members use to more effectively and efficiently get all kids access to educational opportunity. Our team works very hard, but we also have a lot of fun. We enjoy game nights, quarterly trivia outings, and themed potlucks where we get together to eat and explore each other's cultures and favorite recipes.

THE PERKS
By joining staff, you join a network of individuals committed to pursuing equity for all students and developing themselves as professionals in the process. We as an organization value the longevity of our employees and offer a comprehensive and competitive benefits plan. The salary for this position is also competitive and depends on your prior work experience. Please be advised, you will have an opportunity to discuss salary in more detail after you begin the application process.

WE ARE DEEPLY COMMITTED TO DIVERSITY, EQUITY & INCLUSIVENESS
Teach For America encourages individuals of all ethnic, racial, and socioeconomic backgrounds to apply for this position. We are committed to maximizing the diversity of our organization, as we want to engage all those who can contribute to this effort.

Teach For America is committed to providing equal employment opportunities to all qualified individuals and does not discriminate on the basis of race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, parental status, genetic information or characteristics (or those of a family member) or any other basis prohibited by applicable law.

This job description reflects Teach For America's assignment of essential functions and qualifications of the role. Nothing in this herein restricts management's right to assign, reassign or eliminate duties and responsibilities to this role at any time.

NEXT STEPS
Interested in this position? Apply now! Scroll down to the bottom of the page to find the link to the online application. If you still have questions regarding the role, feel free to contact our recruitment team at staffing@teachforamerica.org or visit www.teachforamerica.org/about-us/careers.

Share||","THE PERSON Hands on research and development Find innovative solutions to difficult and unstructured problems that will support and expand our core business Use analytical rigor to analyze large amounts of data, help extract actionable insights using data analysis, feature engineering, optimization tools and machine learning techniques Develop and maintain scalable and reliable data pipelines for AI/ML processes Participate in all aspects of Teach For Americaâs agile software development cycle Other duties  research, presentations, communicating with other teams   THE MUST HAVES Prior Experience  Intermediate or higher experience working with machine learning tools in Python  TensorFlow, Keras, fast.ai, etc  Experience working with data solutions  data ingestion, preprocessing, analysis, predictive analytics  Experience with Reporting and Advanced Analytics Solutions Intermediate or higher experience working with Java  Spring, REST, JMS  and SQL Comfortable working in a Unix environment Experience with version control, containerization technologies Experience working with Continuous Integration / Automation architectures Experience interfacing and working with engineering teams throughout the product development lifecycle  leading projects and/or other developers  Experience partnering with other teams to test and roll out cognitive & predictive analytics solutions Optional but desired  Experience developing applications for and deploying applications in cloud environments Experience with big data and associated technologies  Hadoop, Databricks, Azure Machine Learning etc.  Experience with statistical modeling in R Skills  Be able to communicate in a clear and effective manner with both technical and non-technical audiences Be interested in staying up-to-date with recent advances in machine learning and predictive analytics Be detail oriented, able to work under pressure and effectively manage competing priorities Education  At least a four-year degree in Computer Science, BSEE, MIS or a related field, or equivalent experience Work Demands  This position is located on site in our New York National Office Limited travel may be required  THE TEAM Our team loves to collaborate. We partner with every other team in the organization to create world-class technology solutions that staff and corps members use to more effectively and efficiently get all kids access to educational opportunity. Our team works very hard, but we also have a lot of fun. We enjoy game nights, quarterly trivia outings, and themed potlucks where we get together to eat and explore each other's cultures and favorite recipes.  THE PERKS By joining staff, you join a network of individuals committed to pursuing equity for all students and developing themselves as professionals in the process. We as an organization value the longevity of our employees and offer a comprehensive and competitive benefits plan. The salary for this position is also competitive and depends on your prior work experience. Please be advised, you will have an opportunity to discuss salary in more detail after you begin the application process.  WE ARE DEEPLY COMMITTED TO DIVERSITY, EQUITY & INCLUSIVENESS Teach For America encourages individuals of all ethnic, racial, and socioeconomic backgrounds to apply for this position. We are committed to maximizing the diversity of our organization, as we want to engage all those who can contribute to this effort.  Teach For America is committed to providing equal employment opportunities to all qualified individuals and does not discriminate on the basis of race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, parental status, genetic information or characteristics  or those of a family member  or any other basis prohibited by applicable law.  This job description reflects Teach For America's assignment of essential functions and qualifications of the role. Nothing in this herein restricts management's right to assign, reassign or eliminate duties and responsibilities to this role at any time.  NEXT STEPS Interested in this position? Apply now! Scroll down to the bottom of the page to find the link to the online application. If you still have questions regarding the role, feel free to contact our recruitment team at staffing@teachforamerica.org or visit www.teachforamerica.org/about-us/careers.    ","THE PERSON Hands on research and development Find innovative solutions to difficult unstructured problems that will support expand our core business Use analytical rigor analyze large amounts of data, help extract actionable insights using data analysis, feature engineering, optimization tools machine learning techniques Develop maintain scalable reliable pipelines for AI/ML processes Participate in all aspects Teach For Americaâs agile software cycle Other duties research, presentations, communicating with other teams MUST HAVES Prior Experience Intermediate or higher experience working Python TensorFlow, Keras, fast.ai, etc ingestion, preprocessing, predictive analytics Reporting Advanced Analytics Solutions Java Spring, REST, JMS SQL Comfortable a Unix environment version control, containerization technologies Continuous Integration / Automation architectures interfacing engineering throughout the product lifecycle leading projects and/or developers partnering test roll out cognitive & Optional but desired developing applications deploying cloud environments big associated Hadoop, Databricks, Azure Machine Learning etc. statistical modeling R Skills Be able communicate clear effective manner both technical non-technical audiences interested staying up-to-date recent advances detail oriented, work under pressure effectively manage competing priorities Education At least four-year degree Computer Science, BSEE, MIS related field, equivalent Work Demands This position is located site New York National Office Limited travel may be required TEAM Our team loves collaborate. We partner every organization create world-class technology staff corps members use more efficiently get kids access educational opportunity. works very hard, we also have lot fun. enjoy game nights, quarterly trivia outings, themed potlucks where together eat explore each other's cultures favorite recipes. PERKS By joining staff, you join network individuals committed pursuing equity students themselves as professionals process. an value longevity employees offer comprehensive competitive benefits plan. The salary this depends your prior experience. Please advised, opportunity discuss after begin application WE ARE DEEPLY COMMITTED TO DIVERSITY, EQUITY INCLUSIVENESS America encourages ethnic, racial, socioeconomic backgrounds apply position. are maximizing diversity organization, want engage those who can contribute effort. providing equal employment opportunities qualified does not discriminate basis race, color, ethnicity, religion, sex, gender, gender identity expression, sexual orientation, national origin, disability, age, marital status, veteran pregnancy, parental genetic information characteristics family member any prohibited by applicable law. job description reflects America's assignment essential functions qualifications role. Nothing herein restricts management's right assign, reassign eliminate responsibilities role at time. NEXT STEPS Interested position? Apply now! Scroll down bottom page find link online application. If still questions regarding role, feel free contact recruitment staffing@teachforamerica.org visit www.teachforamerica.org/about-us/careers.","THE PERSON Hands research development Find innovative solutions difficult unstructured problems support expand core business Use analytical rigor analyze large amounts data, help extract actionable insights using data analysis, feature engineering, optimization tools machine learning techniques Develop maintain scalable reliable pipelines AI/ML processes Participate aspects Teach For Americaâs agile software cycle Other duties research, presentations, communicating teams MUST HAVES Prior Experience Intermediate higher experience working Python TensorFlow, Keras, fast.ai, etc ingestion, preprocessing, predictive analytics Reporting Advanced Analytics Solutions Java Spring, REST, JMS SQL Comfortable Unix environment version control, containerization technologies Continuous Integration / Automation architectures interfacing engineering throughout product lifecycle leading projects and/or developers partnering test roll cognitive & Optional desired developing applications deploying cloud environments big associated Hadoop, Databricks, Azure Machine Learning etc. statistical modeling R Skills Be able communicate clear effective manner technical non-technical audiences interested staying up-to-date recent advances detail oriented, work pressure effectively manage competing priorities Education At least four-year degree Computer Science, BSEE, MIS related field, equivalent Work Demands This position located site New York National Office Limited travel may required TEAM Our team loves collaborate. We partner every organization create world-class technology staff corps members use efficiently get kids access educational opportunity. works hard, also lot fun. enjoy game nights, quarterly trivia outings, themed potlucks together eat explore other's cultures favorite recipes. PERKS By joining staff, join network individuals committed pursuing equity students professionals process. value longevity employees offer comprehensive competitive benefits plan. The salary depends prior experience. Please advised, opportunity discuss begin application WE ARE DEEPLY COMMITTED TO DIVERSITY, EQUITY INCLUSIVENESS America encourages ethnic, racial, socioeconomic backgrounds apply position. maximizing diversity organization, want engage contribute effort. providing equal employment opportunities qualified discriminate basis race, color, ethnicity, religion, sex, gender, gender identity expression, sexual orientation, national origin, disability, age, marital status, veteran pregnancy, parental genetic information characteristics family member prohibited applicable law. job description reflects America's assignment essential functions qualifications role. Nothing herein restricts management's right assign, reassign eliminate responsibilities role time. NEXT STEPS Interested position? Apply now! Scroll bottom page find link online application. If still questions regarding role, feel free contact recruitment staffing@teachforamerica.org visit www.teachforamerica.org/about-us/careers."
236,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Who We Are:
Ocrolus is a Series B venture-backed FinTech company that uses Artificial Intelligence and crowdsourcing to automate financial review processes. The Company transforms e-statements, scans, and cell phone images, regardless of quality, into 99+% accurate digital data. By replacing tedious, imperfect human audits with sharp, AI-driven analyses, Ocrolus modernizes financial assessments in lending and a variety of other industries.





We are seeking a candidate with proven experience working as a Data Engineer, Full Stack Software Engineer with a Data focus, or similar role. The ideal candidate is comfortable in DevOps and Software Engineering.
Responsibilities
Manage competing priorities across the company
Maintain and automate reporting infrastructure
Manage the design and architecture of our Data Warehouse
Create Scripts to automate and manage ETL processes and Dependencies
Advise on the design of our application DB, machine learning components, and our data infrastructure.
Cleaning and restructuring datasets
Managing and optimizing reporting systems
Requirements
At least two years experience as a Data Engineer, or related Software or DevOps experience
Fluent in Python and SQL
Experience with Data Warehouses and Schema Design
Strong communication skills and experience communicating across business units
Experience working with Data Scientists and Data Analysts
Ability to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions
Machine Learning experience a plus
Analytical and BI skills a plus
Postgres and RedShift experience is a plus

Weâre a young and rapidly growing FinTech company - if you have ever wanted to jump on a rocket ship as itâs taking off, now is your chance!","   Manage competing priorities across the company Maintain and automate reporting infrastructure Manage the design and architecture of our Data Warehouse Create Scripts to automate and manage ETL processes and Dependencies Advise on the design of our application DB, machine learning components, and our data infrastructure. Cleaning and restructuring datasets Managing and optimizing reporting systems   At least two years experience as a Data Engineer, or related Software or DevOps experience Fluent in Python and SQL Experience with Data Warehouses and Schema Design Strong communication skills and experience communicating across business units Experience working with Data Scientists and Data Analysts Ability to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions Machine Learning experience a plus Analytical and BI skills a plus Postgres and RedShift experience is a plus","Manage competing priorities across the company Maintain and automate reporting infrastructure design architecture of our Data Warehouse Create Scripts to manage ETL processes Dependencies Advise on application DB, machine learning components, data infrastructure. Cleaning restructuring datasets Managing optimizing systems At least two years experience as a Engineer, or related Software DevOps Fluent in Python SQL Experience with Warehouses Schema Design Strong communication skills communicating business units working Scientists Analysts Ability create fast solutions problems introduced changing environment iteration towards optimal Machine Learning plus Analytical BI Postgres RedShift is","Manage competing priorities across company Maintain automate reporting infrastructure design architecture Data Warehouse Create Scripts manage ETL processes Dependencies Advise application DB, machine learning components, data infrastructure. Cleaning restructuring datasets Managing optimizing systems At least two years experience Engineer, related Software DevOps Fluent Python SQL Experience Warehouses Schema Design Strong communication skills communicating business units working Scientists Analysts Ability create fast solutions problems introduced changing environment iteration towards optimal Machine Learning plus Analytical BI Postgres RedShift"
237,Data Engineer,GCP Data Engineer,"New York, NY 10001",New York,NY,"Position: GCP Data Engineer
Location: New York, United States
Remuneration: $ 115.00 per hour
Who is hiring?
One of our Clients in Austin, TX will be hiring for a GCP Data Engineer to assist in building a recommendation software platform utilizing GCP as the main platform for their Data Visualization solution. This role will work closely with their Cloud Architect and Software Development teams thorughout the engagement and may require up to 25% - 50% travel.
What will you be doing?

Responsibilities will include:

Build and Deploy Data Pipelines on Google Cloud to enable AI & ML capabilities.
Drive the development of cloud-based and hybrid data warehouses & business intelligence platforms
Build Data Pipelines to ingest structured and Unstructured Data.
Gain hands-on experience with new data platforms and programming languages

Qualifications for This Role:

5+ years of experience consulting in Data Engineering or Data Warehousing
Hands-on experience with Google Cloud Platform
Experience leading data warehousing, data ingestion, and data profiling activities
Advanced SQL & Python skills
Hands-on experience with Google cloud platform technologies: Google Cloud Platform Pub/Sub, Cloud Functions, DataFlow, DataProc (Hadoop, Spark, Hive), Cloud Machine Learning, Cloud Data Store and BigTable, BigQuery, DataLab, and DataStudio
Migrating Data Pipelines to Google Cloud Platform (GCP)

Why you shouldnât miss this opportunity?
As a GCP Data Engineer through Third Republic (Recruitment Agency), you will work in teams to deliver innovative solutions on Google Cloud using core cloud data warehouse tools like Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in the advertisement space.
Data Science(Data Engineer), Google Cloud Platform (GCP), Data Engineer"," 5+ years of experience consulting in Data Engineering or Data Warehousing Hands-on experience with Google Cloud Platform Experience leading data warehousing, data ingestion, and data profiling activities Advanced SQL & Python skills Hands-on experience with Google cloud platform technologies  Google Cloud Platform Pub/Sub, Cloud Functions, DataFlow, DataProc  Hadoop, Spark, Hive , Cloud Machine Learning, Cloud Data Store and BigTable, BigQuery, DataLab, and DataStudio Migrating Data Pipelines to Google Cloud Platform  GCP     Build and Deploy Data Pipelines on Google Cloud to enable AI & ML capabilities. Drive the development of cloud-based and hybrid data warehouses & business intelligence platforms Build Data Pipelines to ingest structured and Unstructured Data. Gain hands-on experience with new data platforms and programming languages   ","5+ years of experience consulting in Data Engineering or Warehousing Hands-on with Google Cloud Platform Experience leading data warehousing, ingestion, and profiling activities Advanced SQL & Python skills cloud platform technologies Pub/Sub, Functions, DataFlow, DataProc Hadoop, Spark, Hive , Machine Learning, Store BigTable, BigQuery, DataLab, DataStudio Migrating Pipelines to GCP Build Deploy on enable AI ML capabilities. Drive the development cloud-based hybrid warehouses business intelligence platforms ingest structured Unstructured Data. Gain hands-on new programming languages","5+ years experience consulting Data Engineering Warehousing Hands-on Google Cloud Platform Experience leading data warehousing, ingestion, profiling activities Advanced SQL & Python skills cloud platform technologies Pub/Sub, Functions, DataFlow, DataProc Hadoop, Spark, Hive , Machine Learning, Store BigTable, BigQuery, DataLab, DataStudio Migrating Pipelines GCP Build Deploy enable AI ML capabilities. Drive development cloud-based hybrid warehouses business intelligence platforms ingest structured Unstructured Data. Gain hands-on new programming languages"
238,Data Engineer,EDL Big Data Engineer - Corporate- Information Technology (No Agencies),"New York, NY 10022",New York,NY,"Description


Position:
We are looking for an accomplished big data developer with strong experience in the cloud AWS data implementation to help us build and integrate data-driven intelligent cloud solutions for EDL. This role will involve a close collaboration with our team of passionate and innovative big data specialists, application developers and product managers.

This is a unique opportunity to be a member of our corporate CRM and Analytics Team, tackling our toughest and most exiting data lake challenges across multiple divisions in Jefferies.

CRM & Analytics Team Overview:

The CRM & Analytics team is a highly strategic and cross-functional team responsible for leading the firmâs global digitalization effort. This initiative, spanning all client-facing business units and corporate functions, will drive innovation and strategic change through technology, data science, and deep analytics. The team partners with key business leaders and industry experts to build transformational technology to drive revenue, maximize efficiency, and optimize the allocation of resources. The CRM & Analytics team is at the forefront of Jefferiesâ cloud initiative, leveraging best-in-class cloud-based technologies to replace legacy on-premises solutions to provide intelligent trend insights, actionable opportunities, decision support, and transparency into all client and business-related activities. This team is also responsible for Enterprise Data Lake.

Position Overview:
We are looking for an accomplished big data architect with strong experience in the cloud AWS data architecture and implementation.
This role will involve a close collaboration with our team of passionate and innovative big data specialists, application developers and product managers.
This is a unique opportunity to be a member of our corporate CRM and Analytics Team, tackling our toughest and most exiting data lake challenges across multiple divisions in Jefferies.


Enterprise Data Lake (EDL) Team Responsibilities:
The EDL team will oversee and support architecture and implementation of EDL for all Jefferies big data initiatives. It will drive the data governance and facilitate data onboarding. It will approve the design of data and software architecture, perform architecture review to pass EDL tollgates, evaluate and select cloud/AWS/Big Data tools for acceptance, and serve as a vendor liaison with data lake tool vendors and out internal infrastructure teams. Also, it will certify data for consumption, EDL patterns and processes, manage and govern data access controls, and will manage data lake and data governance training initiatives across enterprise.

The EDL team will become the center of excellence for the following EDL components and associated tools:
q EDL architecture and patterns
q EDL Data Stores
q EDL Governance
q EDL Data Discovery
q EDL Data Preparation
q EDL Reporting
q EDL Ingestion tools & other technologies and tools
q Educate teams to migrate and develop new cloud applications


Qualifications

Basic Requirements:
 A minimum of 5 years of hands-on technical experience with:
 big data implementation and technology offerings
 AWS/cloud big data modeling & data management
 analytics and ingestion architecture of big data
 data lake management and data architecture
 data lake design patterns & cloud best enterprise practices
 IoT and streaming, real time processing
 Big data related AWS technologies

Experience in AWS technologies such as Kinesis, Lambda, EC2, Redshift, RDS, Cloud formation, EMR, AWS S3, AWS Analytics, Spark, Databricks
Experience with at least one of the following languages Scala, Python, R and or Java
Experience with designing, developing, and implementing complex integration for end-to-end solutions at a middleware and app level with focus on performance optimization
Strong implementation skill in area of cloud development in AWS
Demonstrated ability in implementing cloud scalable, real time and high-performance data lake solutions (AWS)
Ability to quickly perform proof-of-concepts for validating new technology or approach
Ability to exercise independent judgment and creative problem-solving techniques in a highly complex environment using leading-edge technology and/or integrating with diverse application systems
Ability to lead and drive technology change in a fast-paced, dynamic environment and all phases of the entire software life cycle
Strong experience with data catalog, data governance, Collibra, MDM and/or Data Quality (IDQ) toolset
Strong experience with integration of diverse data sources (batch and real time) in the cloud
Lead the design and sustainment of data pipelines and data storage
Expertise in Structured, unstructured, SQL and No-SQL technologies
Expertise with identifying and understanding source data systems and mapping source system attributes to the target
Experience with design and automation of ETL\ELT processes
AWS and cloud performance tuning and optimization experience
Experience with effort estimation for new projects/proposals on an ongoing basis.
Excellent communication skills across all levels; ability to communicate with ease the complex and technical concepts.
Ability to work effectively in a fast-paced environment

Primary Location: US-NY-New York
Job: Information Technology
Organization: Corporate
Schedule: Full-time
Employee Status: Regular
Job Level: Non-Management
Job Posting: Jun 28, 2019, 2:58:28 PM", A minimum of 5 years of hands-on technical experience with    A minimum of 5 years of hands-on technical experience with    A minimum of 5 years of hands-on technical experience with ,A minimum of 5 years hands-on technical experience with,A minimum 5 years hands-on technical experience
239,Data Engineer,Senior Data Software Engineer - Java Map Reduce - Distributed Data Processing,"New York, NY",New York,NY,"Xandrâs mission is to Make Advertising Matter. We develop a technology platform that powers the real-time sale and purchase of digital & TV advertising. Our platform is engineered to provide one of the fastest, most reliable, and massively scaled advertising systems in the industry. As a transparent and independent partner for some of the largest publishers and advertisers, Xandr helps ensure that the Internet stays open and free.
As a Data Engineer on the Publisher Integrations capability, you will be responsible for creative solutions distilling data at a massive scale into insightful, actionable information for online publishers that use Xandr to drive demand.

Consider some of the problems we try to solve:
Getting Insight: Rapidly process data on an immense scale. Create reports that drive business outcomes and optimizations. Balance granularity and business value with resource cost and efficiency.Grow a Mature Platform: Reduce latency and error rates. Increase uptime. Maintain speed, availability, and reliability as our client base grows.

About the job:
Design and develop reliable, scalable and testable Map Reduce jobs and end-to-end data processing for reporting & analytics, real-time decision-making use casesCollaborate and communicate effectively with team members, product management, open source community members, and other stakeholdersBuild and maintain tools to automate regular testing and deploymentConsistently optimize and improve our systems, tools, and testing

QualificationsBA/BS degree and 2+ years of experience in software engineering OR MS degree and 1+ years of experience (Degree in Computer Science or related field preferred)Proficiency with large-scale distributed data processing systems like Hadoop, MR, Hive, Presto, and distributed data stores like Vertica and DruidExperience building large-scale multi-TB data processing systemsExperience supporting production systemsSolid experience in Java/Python/Scala and SQL and proficiency with Unix toolsNice to have: Experience with Spark, Kafka, NoSQL databases

More about you:
You are passionate about a culture of learning and teaching. You love challenging yourself to constantly improve, and sharing your knowledge to empower othersYou like to take risks when looking for novel solutions to complex problems. If faced with roadblocks, you continue to reach higher to make greatness happenYou care about solving big, systemic problems. You look beyond the surface to understand root causes so that you can build long-term solutions for the whole ecosystemYou believe in not only serving customers, but also empowering them by providing knowledge and tools","BA/BS degree and 2+ years of experience in software engineering OR MS degree and 1+ years of experience  Degree in Computer Science or related field preferred Proficiency with large-scale distributed data processing systems like Hadoop, MR, Hive, Presto, and distributed data stores like Vertica and DruidExperience building large-scale multi-TB data processing systemsExperience supporting production systems    ","BA/BS degree and 2+ years of experience in software engineering OR MS 1+ Degree Computer Science or related field preferred Proficiency with large-scale distributed data processing systems like Hadoop, MR, Hive, Presto, stores Vertica DruidExperience building multi-TB systemsExperience supporting production","BA/BS degree 2+ years experience software engineering OR MS 1+ Degree Computer Science related field preferred Proficiency large-scale distributed data processing systems like Hadoop, MR, Hive, Presto, stores Vertica DruidExperience building multi-TB systemsExperience supporting production"
240,Data Engineer,Senior Data Engineer,"New York, NY",New York,NY,"Vettery is changing the way people hire and get hired. We use machine learning and real-time data to match talented job-seekers with inspiring companies. Our goal is to enrich and automate the recruiting process, make hiring more rewarding for everyone, and create a happier and more accountable working world.

Since launching in 2015, we've made thousands of matches on our marketplace. We're currently working with over 45,000 job-seekers and 20,000 companies, from Fortune 500 giants to startups based out of co-working spaces. We've built powerful machine learning capabilities, and our matching algorithm is becoming more intelligent with each passing day. With an eye on the future, we're expanding our reach across major cities in the US, and around the globe.

Vettery engineers are working to build a highly innovative platform enabling an efficient experience for our users. Our engineers work across the full stack building highly scalable distributed solutions to enable exponential customer and business growth. Vettery engineers have input into the whole process of the company from business decisions to where our tech stack is going. Our engineering team works on a huge array of projects and are constantly incorporating new technologies into the stack.

Key responsibilities and expectations:

Design, develop and maintain end-to-end data pipelines across multiple data sources and systems of record.
Develop and design data models, data structures and ETL jobs for data science consumption
Manage and maintain cloud based data and analytics platform
Work in a highly agile development environment and practice Agile/Scrum methodology
Participate in hackathons, team outings, lunches and other team building events.

Candidate Qualifications:

Self-starter who can work in a highly demanding environment and maintains a positive attitude
4+ years of data engineering experience
Experienced in Big Data development using AWS EMR, SQOOP, Hadoop, Spark, and HDFS
Expert with one general purpose programming language, including but not limited to: Java, Scala, Python
Awareness of new and emerging Big Data technologies and trends
Advanced degree in computer science, engineering or a related field

Vettery has five key values that are the foundation of our company culture, which every employee embodies:


Positivity - We're positive when things get difficult so we can stay motivated and lift each other up. We're very team focused in everything we do so contagious positive energy is extremely impactful.
Ownership - We take pride in our work and take on a lot of responsibility from day one. All of us have the ability to see how our work and performance impact the success of the business.
Grit - We love getting in the trenches and building from the ground up. Even though we've significantly grown since our tiny startup days, we still have that scrappy mentality and love that there's still a lot to accomplish.
Awareness - We're focused, strategic, and constantly learning from our experiences. Each of us knows what's expected of us as a corporate citizen and within our teams.
Collaboration - We learn from one another and are constantly working with each other, within and across teams. Every team has an impact on others and we take pride in clear lines of communication.

Why you'll love working at Vettery:
We love coming to work on Monday. It's easy to love the work you do when you see the positive impact it has, and helping someone find their dream job can change their life forever. We believe in our mission, love the work, and have fun doing it together. Plus, coming to work in our sunny Flatiron office is easy when there are so many things to look forward to: Flag Football games, Thursday Game Night, Cross-Team lunches, company happy hours, volunteer events, adorable pups, ping-pong, and your favorite snacks.

We know life is about more than just work. We have an open vacation policy so you can take the time you need to relax and rejuvenate, contribute to the cost of insurance coverage (health, vision, and dental), and offer a fully paid 12-week parental leave, 401k, commuter benefits, and gym membership discounts.

We invest in your development. A company is only as strong as its team, and we want to help strengthen every member of our team. We give everyone the opportunities and support they need to reach that next professional level through company-sponsored General Assembly classes and conferences, in-house training, a culture of continuous feedback, and the chance to run with projects.

We're consistently recognized for our culture. We're listed #5 on Fortune's 60 Best Companies to Work ( https://click.api.drift.com/click/f53ee223-f672-4b64-b489-ab2d83a21bcc?u=https%3A%2F%2Ffortune.com%2F2019%2F07%2F16%2Fbest-companies-new-york-2019%2F%23bestcompanies&h=83cf0552f4bb19bbeff9cbafb15704e1 ) for in New York City this year, and have been previously honored at Crain's Best Places to Work Awards and included in Inc Best Workplaces.

Vettery values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status."," Self-starter who can work in a highly demanding environment and maintains a positive attitude 4+ years of data engineering experience Experienced in Big Data development using AWS EMR, SQOOP, Hadoop, Spark, and HDFS Expert with one general purpose programming language, including but not limited to  Java, Scala, Python Awareness of new and emerging Big Data technologies and trends Advanced degree in computer science, engineering or a related field     ","Self-starter who can work in a highly demanding environment and maintains positive attitude 4+ years of data engineering experience Experienced Big Data development using AWS EMR, SQOOP, Hadoop, Spark, HDFS Expert with one general purpose programming language, including but not limited to Java, Scala, Python Awareness new emerging technologies trends Advanced degree computer science, or related field","Self-starter work highly demanding environment maintains positive attitude 4+ years data engineering experience Experienced Big Data development using AWS EMR, SQOOP, Hadoop, Spark, HDFS Expert one general purpose programming language, including limited Java, Scala, Python Awareness new emerging technologies trends Advanced degree computer science, related field"
241,Data Engineer,"Data Engineer, Data Acquisition","Hoboken, NJ 07030",Hoboken,NJ,"Position Description
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the worldâs leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services â including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering
Minimum Qualifications
Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment.Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake.Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges.Mitigate Risks in our data infrastructure by developing the best in class tools and processes.Implement controls, policies, processes and best practices in the Data Engineering space.Evangelize an extremely high standard of code quality, system reliability, and performance.Help us improve our database deployment and change management process.Provide reliable and efficient Data services as part of the global data team.Work closely with the team on development best practices and standards.Be a mentor.
Who you are:
You have prior experience with leading data engineering efforts across a variety of data systemsYou have deep understanding of commercial data sources and understand database concepts and terminologyYou have a demonstrated track record of handling multiple complex sourcing projects and delivering results in the data engineering areaYou have strong SQL experience and the ability to work on multiple aspects of a data projects including ETL, tools integrations, data results and APIs.You are a team player, with the courage to drive change through disruption while maintaining a respect for the team
Requirements:
Very Strong engineering skills. Should have an analytical approach and have good programming skills.Provide business insights, while leveraging internal tools and systems, databases and industry dataMinimum of 5+ yearsâ experience. Experience in retail business will be a plus.Excellent written and verbal communication skills for varied audiences on engineering subject matterAbility to document requirements, data lineage, subject matter in both business and technical terminology.Guide and learn from other team members.Demonstrated ability to transform business requirements to code, specific analytical reports and toolsThis role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering teamExperience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.) and platforms such as HDP, Cloudera etc.Strong Hadoop scripting skills to process petabytes of dataExperience in Unix/Linux shell scripting or similar programming/scripting knowledgeReal time data ingestion (Kafka)Experience in ETL/ processes with exposure to one or more tools such as Nifi, Talend, Informatica, SSIS etc.
Additional Preferred Qualifications

Company Summary
The Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the worldâs largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.
Position Summary
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the worldâs leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services â including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering.","Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment.Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake.Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges.Mitigate Risks in our data infrastructure by developing the best in class tools and processes.Implement controls, policies, processes and best practices in the Data Engineering space.Evangelize an extremely high standard of code quality, system reliability, and performance.Help us improve our database deployment and change management process.Provide reliable and efficient Data services as part of the global data team.Work closely with the team on development best practices and standards.Be a mentor.    ","Play a pivotal design and hands on implementation role in improving the Data infrastructure project-oriented work environment.Influence cross functional architecture sprint planningGather process raw data at scale from internal external sources expose mechanisms for large parallel processingDesign, implement manage near real-time ingestion pipeline into warehouse Hadoop lake.Process unstructured form suitable analysis then empower state-of-the-art analysts, scientists, APIsSolve complex SQL Big Performance challenges.Mitigate Risks our by developing best class tools processes.Implement controls, policies, processes practices Engineering space.Evangelize an extremely high standard of code quality, system reliability, performance.Help us improve database deployment change management process.Provide reliable efficient services as part global team.Work closely with team development standards.Be mentor.","Play pivotal design hands implementation role improving Data infrastructure project-oriented work environment.Influence cross functional architecture sprint planningGather process raw data scale internal external sources expose mechanisms large parallel processingDesign, implement manage near real-time ingestion pipeline warehouse Hadoop lake.Process unstructured form suitable analysis empower state-of-the-art analysts, scientists, APIsSolve complex SQL Big Performance challenges.Mitigate Risks developing best class tools processes.Implement controls, policies, processes practices Engineering space.Evangelize extremely high standard code quality, system reliability, performance.Help us improve database deployment change management process.Provide reliable efficient services part global team.Work closely team development standards.Be mentor."
242,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Innovisk is a global underwriting platform with a strategy of growth and diversification. We are building market leading underwriting businesses through the application of modern technology, high quality data management, advanced analytics, and by attracting industry leading underwriting talent.
The Data Science & Actuarial team is part of an international core of integrated advisory, technology and analysis services supporting speciality & commercial insurance underwriters in a wide variety of lines including Commercial EL/PI, D&O, Energy, Environmental, Financial Lines, Inland Marine, M&A, Renewables and Surety.

We are looking for a Data Engineer to join our team of experienced data engineers, reporting engineers, data scientists and actuaries to help our underwriting businesses to evaluate, price and manage their risks & business operations.
The successful candidate will have a passion for creating and managing innovative data assets, and will work closely with the tech team, the underwriters and the operations team. You will also enjoy direct interaction with highly skilled and experienced underwriters, brokers, and insurance experts â the people who are pushing the industry forward.
The Role

Help to acquire, create and manage innovative data assets for the team. These assets draw from various sources â internal underwriting systems, external claims systems, flat files, databases, external APIs etc.
Alongside your UK-based counterpart, you will manage the pipelining (acquisition, ETL, storage, access), engineering (cleansing, feature creation / selection, warehousing, data marts, cubing), and understanding (documentation, exploration, communication) of this data
Work closely with the whole Data Science & Actuarial team to identify patterns, trends, outliers, exceptions in the underwriting businesses operational and performance data: help to understand root causes, predict future patterns and recommend management actions
Work closely with the Tech team to integrate new in-house underwriting systems and data assets, and advise on systems architecture and transactional database design
Help us develop streamlined external-facing commercial data products
As a new venture we have very little technical debt, so you will also have an opportunity to help shape our frameworks and methodologies.
Support the team in stakeholder engagement and business process
The Requirements

Minimum of a degree in a quantitative subject, ideally containing computer science, software engineering, database, or data analysis modules
3 to 5 yearsâ experience working as a data engineer or software engineer with a strong focus on data, ideally in financial services (insurance, banking, instech, fintech)
Strong and proven capability in dimensional data modelling: to design, implement and use dimensional database / datamart structures to ensure data is organised, aggregated and indexed effectively and ready for downstream reporting, analysis and statistical modelling
Strong and proven capability in data acquisition: to design data capture systems, collect or stream data from a variety of systems, to creatively transform, manipulate, and methodically describe it
Strong and proven capability with database administration: to maintain transactional and reference data, with migration alongside product development to ensure logical consistency
Strong and proven capability with data validation and verification: to understand the pitfalls in different data sources and to perform necessary provenance and lineage checks, and implement various data testing
A passion for quality software coding, data and analytics and some knowledge of statistics
Experience working in a fast-paced environment and desire for good documentation, exceptional delivery, effective organisation, and high-quality communication with team and customers
Strong and proven technical skills in MS SQL Server, SSIS, SSRS, Python, Git, Bash / Powershell
Desirable technical skills in MS Azure (inc DataFactory), Apache Airflow, PowerBI, Linux (Redhat / CentOS)
Equal Opportunity Employer/Vet/Disability","    Minimum of a degree in a quantitative subject, ideally containing computer science, software engineering, database, or data analysis modules 3 to 5 yearsâ experience working as a data engineer or software engineer with a strong focus on data, ideally in financial services  insurance, banking, instech, fintech  Strong and proven capability in dimensional data modelling  to design, implement and use dimensional database / datamart structures to ensure data is organised, aggregated and indexed effectively and ready for downstream reporting, analysis and statistical modelling Strong and proven capability in data acquisition  to design data capture systems, collect or stream data from a variety of systems, to creatively transform, manipulate, and methodically describe it Strong and proven capability with database administration  to maintain transactional and reference data, with migration alongside product development to ensure logical consistency Strong and proven capability with data validation and verification  to understand the pitfalls in different data sources and to perform necessary provenance and lineage checks, and implement various data testing A passion for quality software coding, data and analytics and some knowledge of statistics Experience working in a fast-paced environment and desire for good documentation, exceptional delivery, effective organisation, and high-quality communication with team and customers Strong and proven technical skills in MS SQL Server, SSIS, SSRS, Python, Git, Bash / Powershell Desirable technical skills in MS Azure  inc DataFactory , Apache Airflow, PowerBI, Linux  Redhat / CentOS  ","Minimum of a degree in quantitative subject, ideally containing computer science, software engineering, database, or data analysis modules 3 to 5 yearsâ experience working as engineer with strong focus on data, financial services insurance, banking, instech, fintech Strong and proven capability dimensional modelling design, implement use database / datamart structures ensure is organised, aggregated indexed effectively ready for downstream reporting, statistical acquisition design capture systems, collect stream from variety creatively transform, manipulate, methodically describe it administration maintain transactional reference migration alongside product development logical consistency validation verification understand the pitfalls different sources perform necessary provenance lineage checks, various testing A passion quality coding, analytics some knowledge statistics Experience fast-paced environment desire good documentation, exceptional delivery, effective organisation, high-quality communication team customers technical skills MS SQL Server, SSIS, SSRS, Python, Git, Bash Powershell Desirable Azure inc DataFactory , Apache Airflow, PowerBI, Linux Redhat CentOS","Minimum degree quantitative subject, ideally containing computer science, software engineering, database, data analysis modules 3 5 yearsâ experience working engineer strong focus data, financial services insurance, banking, instech, fintech Strong proven capability dimensional modelling design, implement use database / datamart structures ensure organised, aggregated indexed effectively ready downstream reporting, statistical acquisition design capture systems, collect stream variety creatively transform, manipulate, methodically describe administration maintain transactional reference migration alongside product development logical consistency validation verification understand pitfalls different sources perform necessary provenance lineage checks, various testing A passion quality coding, analytics knowledge statistics Experience fast-paced environment desire good documentation, exceptional delivery, effective organisation, high-quality communication team customers technical skills MS SQL Server, SSIS, SSRS, Python, Git, Bash Powershell Desirable Azure inc DataFactory , Apache Airflow, PowerBI, Linux Redhat CentOS"
243,Data Engineer,Data Engineer,"New York, NY 10036",New York,NY,"Kinetix Trading is seeking experienced Data Engineers to join our team in New York! This is the perfect opportunity for an experienced Engineer well versed in Big Data Technologies to build on top of a greenfield and deliver valuable solutions to our clients.

Basic Qualifications:

Bachelorâs Degree

At least 8 years of experience in application development

At least 3 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)


Preferred Qualifications:

Bachelors degree in Computer Science or Electrical Engineering

8+ years of experience in application development

3+ year experience working with Hadoop, Spark, Flume, HDFS, Zookeeper, and/or MongoDB

2+ years of experience with Agile engineering practices


3+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)

2+ years of experience developing Java based software solutions

2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)

2+ years of experience developing software solutions to solve complex business problems"," Bachelorâs Degree  At least 8 years of experience in application development  At least 3 years of experience in big data technologies  Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper      ","Bachelorâs Degree At least 8 years of experience in application development 3 big data technologies Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper","Bachelorâs Degree At least 8 years experience application development 3 big data technologies Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, Zookeeper"
244,Data Engineer,Data Engineer / Scientist,"New York, NY",New York,NY,"Skyline AI is redefining the worldâs approach to the $200 trillion commercial real estate industry by building data technology that will transform the market from the low-tech methods itâs been relying on for decades.
Here at Skyline AI, our data is our fuel. We are constantly collecting new data sources, analyzing our many different types of data and extracting valuable information from it.
We are looking for a versatile software engineer to join our data group. This is one of the broadest and most influential roles in the company - touching our data sources, data processing algorithms, ML research, and being responsible for the orchestration system on which it all relies.

About Skyline AI
Skyline AI is an asset management technology company for commercial real estate. Skyline AI partners with leading commercial real estate firms to establish next-generation investment vehicles augmented by artificial intelligence. Mining data from over 130 different sources and applying advanced AI models to detect and exploit market anomalies, identify superior risk-reward investments, and discover untapped value-add opportunities, Skyline AI is a new, more precise way to invest in real estate. Founded in 2017, Skyline AI is backed by Sequoia Capital, JLL (NYSE: JLL), Nyca Partners and others. The company has offices in New York and Tel Aviv.

Responsibilities
Maintain and improve our in-house git-inspired data pipeline orchestration system
Write applications that fetch data by various means, including PDF parsing, HTML scraping, and more.
Perform Real Estate analysis based on the data repository we are continuously growing
Extract insights from our data, and generate predictions using data science techniques
Develop the algorithms which generate the datasets that are used by the whole R&D department
Qualifications
Proven software development ability, building complex and robust systems - at least 4 years
Proven analytical/research ability - data science, cyber-security, intelligence, academic, etc.
Data engineering experience (data pipelines, ML architecture) - advantage
Web scraping experience - advantage
Python/Golang - advantage
ML libraries and tools (pandas, scikit-learn, jupyter notebook) - advantage
Team player, independent, able to work in an unstructured and fast-paced environment."," Proven software development ability, building complex and robust systems - at least 4 years Proven analytical/research ability - data science, cyber-security, intelligence, academic, etc. Data engineering experience  data pipelines, ML architecture  - advantage Web scraping experience - advantage Python/Golang - advantage ML libraries and tools  pandas, scikit-learn, jupyter notebook  - advantage Team player, independent, able to work in an unstructured and fast-paced environment.   Maintain and improve our in-house git-inspired data pipeline orchestration system Write applications that fetch data by various means, including PDF parsing, HTML scraping, and more. Perform Real Estate analysis based on the data repository we are continuously growing Extract insights from our data, and generate predictions using data science techniques Develop the algorithms which generate the datasets that are used by the whole R&D department  ","Proven software development ability, building complex and robust systems - at least 4 years analytical/research ability data science, cyber-security, intelligence, academic, etc. Data engineering experience pipelines, ML architecture advantage Web scraping Python/Golang libraries tools pandas, scikit-learn, jupyter notebook Team player, independent, able to work in an unstructured fast-paced environment. Maintain improve our in-house git-inspired pipeline orchestration system Write applications that fetch by various means, including PDF parsing, HTML scraping, more. Perform Real Estate analysis based on the repository we are continuously growing Extract insights from data, generate predictions using science techniques Develop algorithms which datasets used whole R&D department","Proven software development ability, building complex robust systems - least 4 years analytical/research ability data science, cyber-security, intelligence, academic, etc. Data engineering experience pipelines, ML architecture advantage Web scraping Python/Golang libraries tools pandas, scikit-learn, jupyter notebook Team player, independent, able work unstructured fast-paced environment. Maintain improve in-house git-inspired pipeline orchestration system Write applications fetch various means, including PDF parsing, HTML scraping, more. Perform Real Estate analysis based repository continuously growing Extract insights data, generate predictions using science techniques Develop algorithms datasets used whole R&D department"
245,Data Engineer,Financial Services Advisory Senior - Data Engineer,"New York, NY",New York,NY,"EY is the only professional services firm with a separate business unit (âFSOâ) that is dedicated to the financial services marketplace. Our FSO teams have been at the forefront of every event that has reshaped and redefined the financial services industry. If you have a passion for rallying together to solve the most complex challenges in the financial services industry, come join our dynamic FSO team!

We help our clients transform. Together we are changing the way they interact with their Customers, Employees and Partners alike. We are modernizing their applications, creating new ones and driving business efficiency across their ever-changing platform and channel infrastructures. Everything we do centers around this simple story.

The opportunity

EYâs FSO-Technology practice is hiring talented data engineers with a passion for designing and implementing leading edge data architectures. Our clients in banking and capital markets, wealth and asset management, and insurance are increasingly turning to cloud-native services. Our clients also are owners and stewards of significant amounts of data and want to use AI/ML insights to realize new business capabilities, reduce costs, and better serve their customers. If you want to bring transformative, cloud-enabled solutions to market by leveraging deep learning and other AI methods, consider joining our team of entrepreneurial and collaborative professionals.
In FSO-T, our work goes end-to-end, and therefore we value systems thinkers. We believe industry has a real challenge when it comes to keeping up with the infuriating pace of change and innovation. In FSO-T we are carrying out the important work that enables our clients to differentiate how they go about delivering technology and helping them with actual execution.

Your Key Responsibilities

As a data engineer you will collaborate with a spectrum of EY professionals to deliver solutions utilizing leading big data platforms and cloud technologies (e.g., Azure, Databricks, GCP, Cloudera, etc.). In any normal day, youâll be expected to:

Understand business and technical requirements
Assess the merits of different technology solutions (e.g., cloud native vs. cloud agnostic) to make recommendations
Study and transform data science prototypes into production-ready systems with large volume data requirements
Select appropriate datasets and data representation methods
Design and implement solutions for data aggregation, improve data foundational procedures, integrate new data management technologies and software into the existing system and build data collection pipelines
Conduct data discovery activities, performing root cause analysis, and make recommendations for the remediation of data quality issues
Run (already existing) machine learning tests and experiments, retraining models when necessary
Develop back-end components to improve responsiveness and overall performance
Expose endpoints to provision model outputs in application frameworks (e.g., Spring Boot for Java, Django or Flask for python.
Perform root cause analysis on data processes and pipelines to answer specific business questions, solve issues, and identify opportunities for improvement.
Make AI/ML insights available to the business and IT through data provisioning and channel integration
Keep abreast of developments in the field; review solutions to ensure they are consistent with best practices in AI/ML architecture and engineering.
Be a team player using an agile delivery methodology while consistently delivering quality client services
Skills and attributes for success

Strong skills in cloud, data pipelining, data modeling and productionizing AI/ML models.
Ability to multitask and work in a fast-paced, collaborative team environment
Ability to travel in accordance with client and other job requirements
Excellent written and oral communication skills; writing, publishing and conference-level presentation skills a plus
To qualify for the role you must have

Bachelorâs degree in Computer Science, Mathematics or similar field; Masterâs degree a plus
A minimum of 5 yearsâ experience with at least 3+ years in hands-on development
Proven experience as an AI/ML and cloud data engineer, architect or similar role over with minimum of 1 completed projects.
Sound working knowledge of:
Java, Python, and/or Scala
Kafka (or similar), NiFi, Hadoop, and Spark
Big data querying tools such as Pig or Hive or Impala
NoSQL databases such as MongoDB, Cassandra, and Neo4
Data ingestion, data cleaning, ETL, traditional warehousing and data marts, and data & insights provisioning
Hands on experience with GCP, AWS, or Azure.
Understanding of data structures, data modeling, and software architecture
Familiar with at least one Machine Learning library/framework (scikit-learn, Mahoot, MLib, H2o, etc.).
Ideally, youâll also have

GCP, AWS, or Azure Certifications
Proficiency in a deep learning framework is plus (Tensorflow, Keras, Theano, MXNet, etc.)
Knowledge of data science methods and statistics a plus
What we look for

Weâre interested in highly motivated talented individuals with a strong willingness to think outside of the box. You can expect plenty of autonomy in this role, so youâll need the motivation to take initiative and seek out opportunities to improve our current relationships and expand our business in the evolving market. If youâre serious about consulting and ready to take on some of our clientsâ most complex issues, this role is for you.

What working at EY offers

We offer a competitive compensation package where youâll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, both pension and 401(k) plans, a minimum of 15 days of vacation plus ten observed holidays and three paid personal days, and a range of programs and benefits designed to support your physical, financial and social well-being. Plus we offer

Support, coaching and feedback from some of the most engaging colleagues around
Opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way thatâs right for you
About EY

As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.

Join us in building a better working world.

Apply now.

EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.","  Strong skills in cloud, data pipelining, data modeling and productionizing AI/ML models. Ability to multitask and work in a fast-paced, collaborative team environment Ability to travel in accordance with client and other job requirements Excellent written and oral communication skills; writing, publishing and conference-level presentation skills a plus  Understand business and technical requirements Assess the merits of different technology solutions  e.g., cloud native vs. cloud agnostic  to make recommendations Study and transform data science prototypes into production-ready systems with large volume data requirements Select appropriate datasets and data representation methods Design and implement solutions for data aggregation, improve data foundational procedures, integrate new data management technologies and software into the existing system and build data collection pipelines Conduct data discovery activities, performing root cause analysis, and make recommendations for the remediation of data quality issues Run  already existing  machine learning tests and experiments, retraining models when necessary Develop back-end components to improve responsiveness and overall performance Expose endpoints to provision model outputs in application frameworks  e.g., Spring Boot for Java, Django or Flask for python. Perform root cause analysis on data processes and pipelines to answer specific business questions, solve issues, and identify opportunities for improvement. Make AI/ML insights available to the business and IT through data provisioning and channel integration Keep abreast of developments in the field; review solutions to ensure they are consistent with best practices in AI/ML architecture and engineering. Be a team player using an agile delivery methodology while consistently delivering quality client services  ","Strong skills in cloud, data pipelining, modeling and productionizing AI/ML models. Ability to multitask work a fast-paced, collaborative team environment travel accordance with client other job requirements Excellent written oral communication skills; writing, publishing conference-level presentation plus Understand business technical Assess the merits of different technology solutions e.g., cloud native vs. agnostic make recommendations Study transform science prototypes into production-ready systems large volume Select appropriate datasets representation methods Design implement for aggregation, improve foundational procedures, integrate new management technologies software existing system build collection pipelines Conduct discovery activities, performing root cause analysis, remediation quality issues Run already machine learning tests experiments, retraining models when necessary Develop back-end components responsiveness overall performance Expose endpoints provision model outputs application frameworks Spring Boot Java, Django or Flask python. Perform analysis on processes answer specific questions, solve issues, identify opportunities improvement. Make insights available IT through provisioning channel integration Keep abreast developments field; review ensure they are consistent best practices architecture engineering. Be player using an agile delivery methodology while consistently delivering services","Strong skills cloud, data pipelining, modeling productionizing AI/ML models. Ability multitask work fast-paced, collaborative team environment travel accordance client job requirements Excellent written oral communication skills; writing, publishing conference-level presentation plus Understand business technical Assess merits different technology solutions e.g., cloud native vs. agnostic make recommendations Study transform science prototypes production-ready systems large volume Select appropriate datasets representation methods Design implement aggregation, improve foundational procedures, integrate new management technologies software existing system build collection pipelines Conduct discovery activities, performing root cause analysis, remediation quality issues Run already machine learning tests experiments, retraining models necessary Develop back-end components responsiveness overall performance Expose endpoints provision model outputs application frameworks Spring Boot Java, Django Flask python. Perform analysis processes answer specific questions, solve issues, identify opportunities improvement. Make insights available IT provisioning channel integration Keep abreast developments field; review ensure consistent best practices architecture engineering. Be player using agile delivery methodology consistently delivering services"
246,Data Engineer,AI Data Scientist / Data Engineer - Experienced Associate,"New York, NY",New York,NY,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data and Analytics Technologies
Management Level
Associate
Job Description & Summary
PwC Labs is focused on standardizing, automating, delivering tools and processes and exploring emerging technologies that drive efficiency and enable our people to reimagine the possible. Process improvement, transformation, effective use of innovative technology and data & analytics, and leveraging alternative delivery solutions are key areas of focus to drive additional value for our firm.


The AI Lab focuses on implementing solutions that impact efficiency and effectiveness of our technology functions. Process improvement, transformation, effective use of technology and data & analytics, and leveraging alternative delivery are key areas to drive value and continue to be recognized as the leading professional services firm. AI Lab is focused on identifying and prioritizing emerging technologies to get the most out of our investments.
To really stand out and make us ï¬t for the future in a constantly changing world, each and every one of us at PwC needs to be an authentic and inclusive leader, at all grades/levels and in all lines of service. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.


As an Associate, youâll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:


Invite and provide evidence-based feedback in a timely and constructive manner.
Share and collaborate effectively with others.
Work with existing processes/systems whilst making constructive suggestions for improvements.
Validate data and analysis for accuracy and relevance.
Follow risk management and compliance procedures.
Keep up-to-date with technical developments for business area.
- Communicate confidently in a clear, concise and articulate manner - verbally and in written form.

Seek opportunities to learn about other cultures and other parts of the business across the Network of PwC firms.
Uphold the firmâs code of ethics and business conduct.
Our team is capability centric, focusing on AI and machine learning techniques that are broadly applicable across all industries. We work with a variety of data mediums including text, audio, imagery, sensory, and structured data. Our work involves the use of supervised/unsupervised machine learning algorithms, traditional statistical models, deep neural networks, terabyte scale data, and simulation modelling. Our work is having a tremendous impact on how PwC & our clients do business.
Job Requirements and Preferences :


Basic Qualifications :


Minimum Degree Required :

Bachelor Degree


Minimum Years of Experience :

1 year(s)


Preferred Qualifications :


Preferred Fields of Study :

Computer and Information Science, Computer Engineering, Computer and Information Science & Accounting, Economics, Economics and Finance, Economics and Finance & Technology, Engineering, Mathematics, Mathematical Statistics, Statistics


Preferred Knowledge/Skills :
Demonstrates some knowledge and/or a proven record of success in the following areas:
Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.
Demonstrates some abilities and/or a proven record of success learning and applying new skills quickly, including the following areas and technologies:
Programming: Python, R, Java, JavaScript, C++, Unix;
Hardware: sensors, robotics, GPU enabled machine learning, FPGAs, Raspberry Pis, etc.;
Data Storage Technologies: SQL, NoSQL, Hadoop, cloud-based databases such as GCP BigQuery, and different storage formats (e.g. Parquet, etc.);
Data Processing Tools: Python (Numpy, Pandas, etc.), Spark, cloud-based solutions such as GCP DataFlow;
Machine Learning Libraries: Python (scikit-learn, genism, etc.), TensorFlow, Keras, PyTorch, Spark MLlib;
Visualization: Python (Matplotlib, Seaborn, bokeh, etc.), JavaScript (d3); and,
Productionization and containerization technologies: GitHub, Flask, Docker, Kubernetes.
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required:
Degrees/Field of Study preferred:
Certifications (if blank, certifications not specified)
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Up to 40%
Available for Work Visa Sponsorship?
Yes
Government Clearance Required?
No
Job Posting End Date","Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding of machine learning algorithms  e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.  and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.  Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding of machine learning algorithms  e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.  and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus.    Exploring new analytical technologies and evaluating their technical and commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients; Testing and rejecting hypotheses around data processing and machine learning model building; Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task; Building machine learning pipelines that ingest, clean data, and make predictions; Staying abreast of new AI research from leading labs by reading papers and experimenting with code; Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients; Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability; Applying machine learning techniques for addressing a variety of problems  e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding of machine learning algorithms  e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.  and when it is appropriate to use each technique; Building machine learning models and systems, interpreting their output, and communicating the results; Moving models from development to production is a plus; and, Conducting research in a lab and publishing work is a plus. ","Exploring new analytical technologies and evaluating their technical commercial viability quickly; Working in 4-week sprint cycles to develop proof-of-concepts prototype models that can be demoed explained data scientists, internal stakeholders, clients; Testing rejecting hypotheses around processing machine learning model building; Demonstrating ability experiment, fail quickly, recognize when you need assistance vs. conclude a technology is not suitable for the task; Building pipelines ingest, clean data, make predictions; Staying abreast of AI research from leading labs by reading papers experimenting with code; Developing innovative solutions perspectives on published academic journals/arXiv shared continuously learn quickly evaluate viability; Applying techniques addressing variety problems e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding algorithms k-nearest neighbors, random forests, ensemble methods, deep neural networks, it appropriate use each technique; systems, interpreting output, communicating results; Moving development production plus; and, Conducting lab publishing work plus.","Exploring new analytical technologies evaluating technical commercial viability quickly; Working 4-week sprint cycles develop proof-of-concepts prototype models demoed explained data scientists, internal stakeholders, clients; Testing rejecting hypotheses around processing machine learning model building; Demonstrating ability experiment, fail quickly, recognize need assistance vs. conclude technology suitable task; Building pipelines ingest, clean data, make predictions; Staying abreast AI research leading labs reading papers experimenting code; Developing innovative solutions perspectives published academic journals/arXiv shared continuously learn quickly evaluate viability; Applying techniques addressing variety problems e.g. consumer segmentation, revenue forecasting, image classification, etc. ; Understanding algorithms k-nearest neighbors, random forests, ensemble methods, deep neural networks, appropriate use technique; systems, interpreting output, communicating results; Moving development production plus; and, Conducting lab publishing work plus."
247,Data Engineer,Software Data Engineer/Data Science,"New York, NY 10020",New York,NY,"(We are not sponsoring for this role or in the future)
At Fareportal, we create technology that is driving innovation in the travel industry - one of the world's fastest-growing sectors. Our employees are the core of our organization and together we're revolutionizing the way people book travel.
Our portfolio of brands including CheapOair and OneTravel receive over 100 million visitors annually and drive over $4 billion in annual revenue.
We are looking for a Software Data engineer to join our team. You will be handling hundreds of millions of events per day, responsible for building out the data tooling and processes to support the creation of machine learning models and data science insights that will drive our business.
The ideal candidate will participate in the design and implementation of the entire data pipeline, from capturing and storing data to streaming and processing the data and making it available to the organization.
We are passionate about making data-driven decisions and you will have the opportunity to shape the team's direction and create large impact.
Our team loves Python and Scala (and is not afraid of Functional Programming) and we strongly encourage DevOps approaches.
Responsibilities:
Create and maintain data pipeline architectures for providing a real time and batch processing platform for all models to run on
Create and maintain APIs for our machine learning models
Coordinating the movement of data between data sources in cloud environments (streaming and batch)
Assemble large, complex data sets that meet functional / non-functional business requirements
Our ideal candidate:
Who You Are
You are smart and love to build systems that are well tested as well as flexible
You like being around smart people who will challenge you on a daily basis.
You love to ramp up on new technologies to build awesome things with us!
Requirements
4+ years' experience developing, maintaining, and testing APIs & infrastructure for data generation.
Experience with big data processing: Flink, Spark, Kafka, etc.
Experience with different databases, such as Redis, Elasticsearch, Postgres or Cassandra.
Strong understanding of one of: Python, Java, or Scala
Experience with CI/CD infrastructure and a strong supporter of unit / integration testing
RxUsEWY7gZ","   Create and maintain data pipeline architectures for providing a real time and batch processing platform for all models to run on Create and maintain APIs for our machine learning models Coordinating the movement of data between data sources in cloud environments  streaming and batch  Assemble large, complex data sets that meet functional / non-functional business requirements   4+ years' experience developing, maintaining, and testing APIs & infrastructure for data generation. Experience with big data processing  Flink, Spark, Kafka, etc. Experience with different databases, such as Redis, Elasticsearch, Postgres or Cassandra. Strong understanding of one of  Python, Java, or Scala Experience with CI/CD infrastructure and a strong supporter of unit / integration testing","Create and maintain data pipeline architectures for providing a real time batch processing platform all models to run on APIs our machine learning Coordinating the movement of between sources in cloud environments streaming Assemble large, complex sets that meet functional / non-functional business requirements 4+ years' experience developing, maintaining, testing & infrastructure generation. Experience with big Flink, Spark, Kafka, etc. different databases, such as Redis, Elasticsearch, Postgres or Cassandra. Strong understanding one Python, Java, Scala CI/CD strong supporter unit integration","Create maintain data pipeline architectures providing real time batch processing platform models run APIs machine learning Coordinating movement sources cloud environments streaming Assemble large, complex sets meet functional / non-functional business requirements 4+ years' experience developing, maintaining, testing & infrastructure generation. Experience big Flink, Spark, Kafka, etc. different databases, Redis, Elasticsearch, Postgres Cassandra. Strong understanding one Python, Java, Scala CI/CD strong supporter unit integration"
248,Data Engineer,Software Data Engineer,"New York, NY 10004",New York,NY,"---------------------

1010data values:
---------------------

Integrity: Doing the right things for the right reasons

Agility: Adapting and thriving in a dynamic environment

Teamwork: Combining our strengths to do amazing things

Passion: Channeling enthusiasm to drive excellence

Creativity: Unleashing curiosity to defy the norm

---------------

About the role:
---------------

As a Software Data Engineer at 1010data, you will be responsible for designing, maintaining, and optimizing large-scale automated ELT processes. Working actively with data scientists and analysts specializing in enterprise data warehousing, you will leverage industry-standard data orchestration tools as well as in-house proprietary scheduling and automation tools to create efficient and reliable ELT jobs which support 1010data's product offerings and data warehousing needs for our customers. As we incorporate more cloud technologies into our processes, you will be at the forefront of exploring and defining best practices, and helping us transition our products to be more scalable.

As part of the onboarding process, you will learn about 1010data's proprietary technology stack. Our query engine, query language, database, and data storage layer were all developed and fine-tuned in-house over the lifetime of the company. ELT processes heavily rely on these components, whether they are written in Python and Airflow , K, or our proprietary data orchestration tools. You will be formally trained in the latter as a new 1010data employee. The concepts should be familiar to anyone with exposure to database techniques like normalization/indexing/partitioning, MapReduce, columnar database architecture and distributed systems.

----------------------

What you will take on:
----------------------


Taking end-to-end ownership of data products and custom solutions for our clients
Coordinating with the systems, core, data science, and analytics teams to build and maintain data products and custom solutions for our clients
Designing and writing automated scripts to preprocess terabytes of data from our partners/clients
Designing and writing new enterprise-scale ELT/ETL workflows from scratch in Python using Airflow, Docker, Kubernetes, AWS, etc.
Modifying/redesigning legacy ELT/ETL processes to leverage cutting-edge open source and proprietary technologies
Ensuring quality, reliability and uptime for critical automated processes
Migrating our products and processes into the cloud while drastically reducing our in-house data center footprint

----------------------

What you already have:
----------------------

Required Skills:

At least 1-2 years of professional experience programming in Python
Exposure to ETL/ELT pipeline automation
Exposure to basic database concepts

Preferred Skills:

Good understanding of Data Engineering, NoSQL databases and database design, distributed systems and/or information retrieval
Knowledge of Apache Airflow
Familiarity with functional/vector programming
DBA experience
Ability to plan and collect requirements for projects, and interact with the analyst and data science teams

Education:

STEM Bachelor's required, graduate degree is a big plus

---------------

About 1010data:
---------------

1010data travels at the speed of thought to make Big Data discovery easy; we power sub-second responses to analyses run on billions of rows of data. 1010data is defining the way the world interacts with data. Come be a part of it. Come do powerful things with data.

An essential tool to more than 850 of the world's top retail, manufacturing, telecom, government and financial services enterprises including The New York Stock Exchange, Dollar General, P&G, and RiteAid; the 1010data platform is a highly differentiated product that is becoming the industry standard for Big Data Discovery and Data Sharing.

With more than 30 trillion rows of data in our private cloud, 1010data is designed to scale to the largest volumes of granular data, the most disparate and varied data sets, and the most complex advanced analytics. All while delivering lightning-quick system performance.

1010data is an equal opportunity employer. We embrace humans of every background, appearance, race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, and disability status.","  At least 1-2 years of professional experience programming in Python Exposure to ETL/ELT pipeline automation Exposure to basic database concepts    STEM Bachelor's required, graduate degree is a big plus  ","At least 1-2 years of professional experience programming in Python Exposure to ETL/ELT pipeline automation basic database concepts STEM Bachelor's required, graduate degree is a big plus","At least 1-2 years professional experience programming Python Exposure ETL/ELT pipeline automation basic database concepts STEM Bachelor's required, graduate degree big plus"
249,Data Engineer,Innovations - Data Engineer,"New York, NY 10154",New York,NY,"Job Description:
Employer:

Blackstone
Firm Overview:

Blackstone is one of the worldâs leading investment firms. We seek to create positive economic impact and long-term value for our investors, the companies we invest in, and the communities in which we work. We do this by using extraordinary people and flexible capital to help companies solve problems. Our asset management businesses, with $512 billion in assets under management, include investment vehicles focused on private equity, real estate, public debt and equity, non-investment grade credit, real assets and secondary funds, all on a global basis. Further information is available at www.blackstone.com. Follow Blackstone on Twitter @Blackstone.
Business Unit:

Innovations
Business Unit Overview:

Blackstone Innovations (BXi) is the technology team at the core of each of Blackstoneâs businesses and new growth initiatives. Serving both internal and external clients, we work to build the next generation of systems that manage risk, create efficiency and improve transparency within the firm and across our broad community of investors and portfolio companies.

BXi is nimble and entrepreneurial â our open, iterative design processes and rapid pace of development mean that everyone on the team has the opportunity to make an impact from day one. We are problem solvers who can take projects from idea to implementation. We believe in active mentoring and developing excellence. We collaborate to find the best answers for our customers and for Blackstone. We are critical to the firm maintaining its competitive edge.
Job Title:

Data Engineer
Job Description:

Blackstone is looking to hire a passionate data engineer to be an integral member of the Data Science and Engineering Team within Innovations, the firm's technology team. We are a new and growing team using data science and data engineering to provide a competitive advantage to our investment professionals and management teams, allowing Blackstone to be a more efficient investor and owner of assets.

As a data engineer, you'll design, implement, and extend core systems that enable data science and data visualization at one of the world's leading investment firms. These systems include data lake, data warehouse, and data pipelines, as well as platform tools that help data scientists and data analysts throughout the firm and Blackstone's portfolio companies. The team also owns Blackstone's broad and growing self-service data visualization stack, including the design, development, and curation of golden-copy analytics data sources.

Our data infrastructure is nascent, growing, and constantly improving. We primarily use Python and SQL, AWS cloud services, Snowflake data warehouse, and Airflow for workflow orchestration. We visualize data with Tableau. Our development environment leverages Vagrant and Docker. We practice infrastructure-as-code with Terraform, and we build continuous integration and deployment pipelines with Gitlab and TeamCity.
Key Responsibilities:
Full-stack design, development, and operation of core data stack including data lake, data warehouse, and data pipelines
Build data flows for data acquisition, aggregation, and modeling, using both batch and streaming paradigms
Consolidate/join datasets to create easily consumable, consistent, holistic information
Design and implement machine learning models and prediction APIs, and ensure their operational performance over time
Empower data scientists and data analysts to be as self-sufficient as possible by building core systems and developing reusable library code
Support and optimize desktop and cloud environments for data scientists and data analysts
Ensure efficiency, quality, resiliency of data science core systems
Work with senior technical staff throughout Blackstone's portfolio companies to develop data flows
Qualifications:
Undergraduate or graduate degree in a technical or scientific field, such as Computer Science, Engineering, Mathematics, or similar
2-5 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role
Analytically-minded and detail-oriented: you actually like staring at data, looking for patterns and outliers, establishing data models, and rigorously answering questions
Expertise in data engineering languages such as Python, Java, Scala, SQL.
Data modeling and data governance experience; you've designed and implemented a data mart, a data warehouse, or the back-end database of an application
Experience building ETL and data pipelines, especially via code-oriented systems like Spark, Airflow, Luigi, or similar, and with varied data formats
Cloud-oriented but comfortable with on-premises infrastructure
Experience operating in a secure networking environment (e.g. behind a corporate proxy) is a plus
Creative problem-solving skills, especially in situations where ""nobody has tried this before""
Excellent technical documentation and writing skills: you know Markdown syntax cold, and have published API documentation or similar
You're not satisfied with ""close enough"" solutions, and you design long-term solutions that are robust over time
You have a bias towards automation: ""one-time scripts"" eat away at your soul a little bit each time you write one
Proficiency in statistics and machine learning is a nice-to-have, and interest in learning these is even better!
Familiarity with visualizing data with Tableau and similar tools
Great customer service and technical troubleshooting skills
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, disability, sexual orientation, national origin or any other category protected by law.
If you need a reasonable accommodation to complete your application, please contact Human Resources at 212-583-5000 (US), +44 (0)20 7451 4000 (EMEA) or +852 3656 8600 (APAC).
The Blackstone Group and its affiliates provide equal employment opportunity to all qualified employees and applicants for employment regardless of race, color, creed, religion, sex, pregnancy, national origin, ancestry, citizenship status, age, marital or partnership status, sexual orientation, gender identity or expression, disability, genetic predisposition, veteran or military status, status as a victim of domestic violence, a sex offense or stalking, or any other classification prohibited by applicable law.
To submit your application please complete the form below. Fields marked with a red asterisk * are required in order to enter into a possible employment contract (although some can be answered "" prefer not to say ""). Failure to provide this information may compromise the follow-up of your application. When you have finished click Submit at the bottom of this form.","Undergraduate or graduate degree in a technical or scientific field, such as Computer Science, Engineering, Mathematics, or similar 2-5 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role Analytically-minded and detail-oriented  you actually like staring at data, looking for patterns and outliers, establishing data models, and rigorously answering questions Expertise in data engineering languages such as Python, Java, Scala, SQL. Data modeling and data governance experience; you've designed and implemented a data mart, a data warehouse, or the back-end database of an application Experience building ETL and data pipelines, especially via code-oriented systems like Spark, Airflow, Luigi, or similar, and with varied data formats Cloud-oriented but comfortable with on-premises infrastructure Experience operating in a secure networking environment  e.g. behind a corporate proxy  is a plus Creative problem-solving skills, especially in situations where ""nobody has tried this before"" Excellent technical documentation and writing skills  you know Markdown syntax cold, and have published API documentation or similar You're not satisfied with ""close enough"" solutions, and you design long-term solutions that are robust over time You have a bias towards automation  ""one-time scripts"" eat away at your soul a little bit each time you write one Proficiency in statistics and machine learning is a nice-to-have, and interest in learning these is even better! Familiarity with visualizing data with Tableau and similar tools Great customer service and technical troubleshooting skills   Full-stack design, development, and operation of core data stack including data lake, data warehouse, and data pipelines Build data flows for data acquisition, aggregation, and modeling, using both batch and streaming paradigms Consolidate/join datasets to create easily consumable, consistent, holistic information Design and implement machine learning models and prediction APIs, and ensure their operational performance over time Empower data scientists and data analysts to be as self-sufficient as possible by building core systems and developing reusable library code Support and optimize desktop and cloud environments for data scientists and data analysts Ensure efficiency, quality, resiliency of data science core systems Work with senior technical staff throughout Blackstone's portfolio companies to develop data flows   ","Undergraduate or graduate degree in a technical scientific field, such as Computer Science, Engineering, Mathematics, similar 2-5 years professional experience data engineer, software analyst, scientist, related role Analytically-minded and detail-oriented you actually like staring at data, looking for patterns outliers, establishing models, rigorously answering questions Expertise engineering languages Python, Java, Scala, SQL. Data modeling governance experience; you've designed implemented mart, warehouse, the back-end database of an application Experience building ETL pipelines, especially via code-oriented systems Spark, Airflow, Luigi, similar, with varied formats Cloud-oriented but comfortable on-premises infrastructure operating secure networking environment e.g. behind corporate proxy is plus Creative problem-solving skills, situations where ""nobody has tried this before"" Excellent documentation writing skills know Markdown syntax cold, have published API You're not satisfied ""close enough"" solutions, design long-term solutions that are robust over time You bias towards automation ""one-time scripts"" eat away your soul little bit each write one Proficiency statistics machine learning nice-to-have, interest these even better! Familiarity visualizing Tableau tools Great customer service troubleshooting Full-stack design, development, operation core stack including lake, pipelines Build flows acquisition, aggregation, modeling, using both batch streaming paradigms Consolidate/join datasets to create easily consumable, consistent, holistic information Design implement models prediction APIs, ensure their operational performance Empower scientists analysts be self-sufficient possible by developing reusable library code Support optimize desktop cloud environments Ensure efficiency, quality, resiliency science Work senior staff throughout Blackstone's portfolio companies develop","Undergraduate graduate degree technical scientific field, Computer Science, Engineering, Mathematics, similar 2-5 years professional experience data engineer, software analyst, scientist, related role Analytically-minded detail-oriented actually like staring data, looking patterns outliers, establishing models, rigorously answering questions Expertise engineering languages Python, Java, Scala, SQL. Data modeling governance experience; designed implemented mart, warehouse, back-end database application Experience building ETL pipelines, especially via code-oriented systems Spark, Airflow, Luigi, similar, varied formats Cloud-oriented comfortable on-premises infrastructure operating secure networking environment e.g. behind corporate proxy plus Creative problem-solving skills, situations ""nobody tried before"" Excellent documentation writing skills know Markdown syntax cold, published API You're satisfied ""close enough"" solutions, design long-term solutions robust time You bias towards automation ""one-time scripts"" eat away soul little bit write one Proficiency statistics machine learning nice-to-have, interest even better! Familiarity visualizing Tableau tools Great customer service troubleshooting Full-stack design, development, operation core stack including lake, pipelines Build flows acquisition, aggregation, modeling, using batch streaming paradigms Consolidate/join datasets create easily consumable, consistent, holistic information Design implement models prediction APIs, ensure operational performance Empower scientists analysts self-sufficient possible developing reusable library code Support optimize desktop cloud environments Ensure efficiency, quality, resiliency science Work senior staff throughout Blackstone's portfolio companies develop"
250,Data Engineer,Senior Data Engineer / Researcher,"New York, NY",New York,NY,"Responsibilities:
Identifying, ingesting, and enriching a wide range of structured and unstructured big data into datasets for analysis;
Operating and extending the data infrastructure platform to deliver production-grade data curation and analysis services;
Thinking and acting as data integrity managers - amplifying data quality and completeness with a process-driven approach and measurement dashboards;
Owning end-to-end data workflows and developing deep domain expertise on the underlying actors and behaviors manifested through data;
Communicating data-driven analysis and insights in the form of âgolden triangleâ investment insights that supports our clients investment process.
Requirements:
Masterâs degree or PHD in Mathematics, Finance, Computer Science, Engineering or related fields.
5+ years of experience working with large structured and unstructured datasets.
Expertise in Python and data analysis tools and languages (PyData, R, Julia, Matlab, Tableau).
Expertise in SQL and relational databases.
Deep intellectual curiosity and passion for data.
Excellent problem solving, communication, and analytical skills.
Eagerness to work in an evolving and fast-paced environment.
While financial industry experience is a plus, we are open-minded in our search for critical thinkers who are passionate about technology and data.
Proven track record of working with and managing junior researchers and developers

If you are interested in any of these opportunities, please use the form below to contact our recruiting team or email us at:
careers@altdg.com","   Identifying, ingesting, and enriching a wide range of structured and unstructured big data into datasets for analysis; Operating and extending the data infrastructure platform to deliver production-grade data curation and analysis services; Thinking and acting as data integrity managers - amplifying data quality and completeness with a process-driven approach and measurement dashboards; Owning end-to-end data workflows and developing deep domain expertise on the underlying actors and behaviors manifested through data; Communicating data-driven analysis and insights in the form of âgolden triangleâ investment insights that supports our clients investment process.   Masterâs degree or PHD in Mathematics, Finance, Computer Science, Engineering or related fields. 5+ years of experience working with large structured and unstructured datasets. Expertise in Python and data analysis tools and languages  PyData, R, Julia, Matlab, Tableau . Expertise in SQL and relational databases. Deep intellectual curiosity and passion for data. Excellent problem solving, communication, and analytical skills. Eagerness to work in an evolving and fast-paced environment. While financial industry experience is a plus, we are open-minded in our search for critical thinkers who are passionate about technology and data. Proven track record of working with and managing junior researchers and developers","Identifying, ingesting, and enriching a wide range of structured unstructured big data into datasets for analysis; Operating extending the infrastructure platform to deliver production-grade curation analysis services; Thinking acting as integrity managers - amplifying quality completeness with process-driven approach measurement dashboards; Owning end-to-end workflows developing deep domain expertise on underlying actors behaviors manifested through data; Communicating data-driven insights in form âgolden triangleâ investment that supports our clients process. Masterâs degree or PHD Mathematics, Finance, Computer Science, Engineering related fields. 5+ years experience working large datasets. Expertise Python tools languages PyData, R, Julia, Matlab, Tableau . SQL relational databases. Deep intellectual curiosity passion data. Excellent problem solving, communication, analytical skills. Eagerness work an evolving fast-paced environment. While financial industry is plus, we are open-minded search critical thinkers who passionate about technology Proven track record managing junior researchers developers","Identifying, ingesting, enriching wide range structured unstructured big data datasets analysis; Operating extending infrastructure platform deliver production-grade curation analysis services; Thinking acting integrity managers - amplifying quality completeness process-driven approach measurement dashboards; Owning end-to-end workflows developing deep domain expertise underlying actors behaviors manifested data; Communicating data-driven insights form âgolden triangleâ investment supports clients process. Masterâs degree PHD Mathematics, Finance, Computer Science, Engineering related fields. 5+ years experience working large datasets. Expertise Python tools languages PyData, R, Julia, Matlab, Tableau . SQL relational databases. Deep intellectual curiosity passion data. Excellent problem solving, communication, analytical skills. Eagerness work evolving fast-paced environment. While financial industry plus, open-minded search critical thinkers passionate technology Proven track record managing junior researchers developers"
251,Data Engineer,Sr Data Engineer,"New York, NY 10017",New York,NY,"Caserta is a best-in-class Data Analytics consulting and implementation firm known for its bold solutions to the toughest data challenges. Leading organizations including world-class Financial Services, Healthcare, Media, Ad-tech and Universities, turn to Caserta for the right answer to effect change and realize business goals. We specialize in transformative strategic consulting in the areas of business intelligence, data intelligence and artificial intelligence. Renowned for our advanced technical implementation of data architecture, data engineering and data science, Caserta provides meticulous customer service and is dedicated to solving our clients' toughest data challenges with the right answers.
Description:
As a Sr. Data Engineer at Caserta, you will work in teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core cloud data warehouse tools like Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in data and analytics.
Responsibilities:
Lead a team to develop Cloud enabled Data and Analytics solutionsDrive the development of cloud-based and hybrid data warehouses & business intelligence platformsBuild Data Pipelines to ingest structured and Unstructured Data.Gain hands-on experience with new data platforms and programming languages
Qualifications:
10+ years of experience working in Data Engineering or Data WarehousingHands-on experience with leading commercial Cloud platforms, including AWS, Azure, or GoogleExperience leading data warehousing, data ingestion, and data profiling activitiesAdvanced SQL & Python skillsStrong aptitude for learning new technologies and analytics techniquesHighly self-motivated and able to work independently as well as in a team environmentUnderstanding of agile project approaches and methodologiesExperience working with Business Stakeholders to elicit business requirementsExperience building and migrating complex ETL pipelinesFamiliarity with or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)Bachelor's degree in Business Analytics, Computer Science or a closely related field required
CKiDsoZCwF","10+ years of experience working in Data Engineering or Data WarehousingHands-on experience with leading commercial Cloud platforms, including AWS, Azure, or GoogleExperience leading data warehousing, data ingestion, and data profiling activitiesAdvanced SQL & Python skillsStrong aptitude for learning new technologies and analytics techniquesHighly self-motivated and able to work independently as well as in a team environmentUnderstanding of agile project approaches and methodologiesExperience working with Business Stakeholders to elicit business requirementsExperience building and migrating complex ETL pipelinesFamiliarity with or strong desire to learn quantitative analysis techniques  e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression Bachelor's degree in Business Analytics, Computer Science or a closely related field required  Lead a team to develop Cloud enabled Data and Analytics solutionsDrive the development of cloud-based and hybrid data warehouses & business intelligence platformsBuild Data Pipelines to ingest structured and Unstructured Data.Gain hands-on experience with new data platforms and programming languages  ","10+ years of experience working in Data Engineering or WarehousingHands-on with leading commercial Cloud platforms, including AWS, Azure, GoogleExperience data warehousing, ingestion, and profiling activitiesAdvanced SQL & Python skillsStrong aptitude for learning new technologies analytics techniquesHighly self-motivated able to work independently as well a team environmentUnderstanding agile project approaches methodologiesExperience Business Stakeholders elicit business requirementsExperience building migrating complex ETL pipelinesFamiliarity strong desire learn quantitative analysis techniques e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression Bachelor's degree Analytics, Computer Science closely related field required Lead develop enabled Analytics solutionsDrive the development cloud-based hybrid warehouses intelligence platformsBuild Pipelines ingest structured Unstructured Data.Gain hands-on platforms programming languages","10+ years experience working Data Engineering WarehousingHands-on leading commercial Cloud platforms, including AWS, Azure, GoogleExperience data warehousing, ingestion, profiling activitiesAdvanced SQL & Python skillsStrong aptitude learning new technologies analytics techniquesHighly self-motivated able work independently well team environmentUnderstanding agile project approaches methodologiesExperience Business Stakeholders elicit business requirementsExperience building migrating complex ETL pipelinesFamiliarity strong desire learn quantitative analysis techniques e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression Bachelor's degree Analytics, Computer Science closely related field required Lead develop enabled Analytics solutionsDrive development cloud-based hybrid warehouses intelligence platformsBuild Pipelines ingest structured Unstructured Data.Gain hands-on platforms programming languages"
252,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Munich Re is one of the world's leading reinsurance companies. We are working on topics today that will concern the whole society tomorrow, whether that be climate change, major construction projects, gene technology or space travel.

Munich Engine is a strategic innovation initiative of Munich Re which operates in two locations, New York City and Munich. Our goal is to rethink underwriting and create an advanced next generation risk assessment engine. Munich Engine combines traditional actuarial methods with new machine learning capabilities.

Munich Engine is designed and built based on modern software engineering paradigms, has just successfully finished the piloting phase and now starts to scale. We believe that we are building on the future of insurance.

We are looking for a Data Engineer, dedicated to product excellence, experienced in cloud and container implementations, and well adept in handling large data sets, who will impress us with architectural skills as well as with prototyping and problem-solving speed. Speed and agility in developing and delivering data driven solutions will be key to the success of this unit. The successful candidate will join the Munich Engine team located in NY while also having access to all resources of the global IT Infrastructure.

We expect from you a proactive approach to achieving goals, collaborative workstyle and a high level of self-motivation to achieving our ambitious goals of Munich Engine.

Your Job:

Work with cutting-edge technologies(Azure, Spark, Kubernetes, etc.) to build a data platform for a high profile project to uncover new insights
code
Design, build and maintain high-performance, fault-tolerant, and scalable data pipelines to ensure highly accurate and reliable business reporting
Partner with business leaders to understand and synthesize needs into end-to-end technical solution
Partner with data scientists to design and implement advanced statistical models and machine learning pipelines
Qualifications
Requirements:

Bachelor's degree in Computer Science, Information Systems, or related field
5+ years of experience in data engineering or data infrastructure role
3+ years of experience with Spark, Hadoop, Airflow, Kafka, etc
Proficient in data modeling and system design skills
Advanced experience of SQL and Python (data analysis libraries like pandas, numpy, scikit-learn, etc)
Proficient experience with cloud such as Azure, GCP or AWS (Azure is target platform)
Proficient experience in building and maintaining data processing pipelines
Good understanding of SQL/NoSQL databases such as Cosmos DB, Postgres, MongoDB, MySQL etc.
Experience with version control(git), CI/CD (Microsoft Azure Dev Ops, Jenkins)
Experience with supporting Data Science teams on feature engineering, model training and deployment tasks
Comfortable in Windows/Linux environment
Strong business communication skills
Strong drive to constantly learn and keep up to speed with the new technologies
Ability to understand complex business priorities and translate them into clearly defined technical/data specifications for implementation
Ability to deal with ambiguity and work with rapidly changing business data
Preferred

Extensive experience with Microsoft Azure
Familiarity with tools like Informatica is a plus
Experience with containers (Docker, Kubernetes)
Familiarity with web frameworks such as Flask
Extensive experience of Scala
Company NameMunich Re America
Requisition Number
3185BR
CountryUnited States of America
Employment Type
Full Time"," Bachelor's degree in Computer Science, Information Systems, or related field 5+ years of experience in data engineering or data infrastructure role 3+ years of experience with Spark, Hadoop, Airflow, Kafka, etc Proficient in data modeling and system design skills Advanced experience of SQL and Python  data analysis libraries like pandas, numpy, scikit-learn, etc  Proficient experience with cloud such as Azure, GCP or AWS  Azure is target platform  Proficient experience in building and maintaining data processing pipelines Good understanding of SQL/NoSQL databases such as Cosmos DB, Postgres, MongoDB, MySQL etc. Experience with version control git , CI/CD  Microsoft Azure Dev Ops, Jenkins  Experience with supporting Data Science teams on feature engineering, model training and deployment tasks Comfortable in Windows/Linux environment Strong business communication skills Strong drive to constantly learn and keep up to speed with the new technologies Ability to understand complex business priorities and translate them into clearly defined technical/data specifications for implementation Ability to deal with ambiguity and work with rapidly changing business data    ","Bachelor's degree in Computer Science, Information Systems, or related field 5+ years of experience data engineering infrastructure role 3+ with Spark, Hadoop, Airflow, Kafka, etc Proficient modeling and system design skills Advanced SQL Python analysis libraries like pandas, numpy, scikit-learn, cloud such as Azure, GCP AWS Azure is target platform building maintaining processing pipelines Good understanding SQL/NoSQL databases Cosmos DB, Postgres, MongoDB, MySQL etc. Experience version control git , CI/CD Microsoft Dev Ops, Jenkins supporting Data Science teams on feature engineering, model training deployment tasks Comfortable Windows/Linux environment Strong business communication drive to constantly learn keep up speed the new technologies Ability understand complex priorities translate them into clearly defined technical/data specifications for implementation deal ambiguity work rapidly changing","Bachelor's degree Computer Science, Information Systems, related field 5+ years experience data engineering infrastructure role 3+ Spark, Hadoop, Airflow, Kafka, etc Proficient modeling system design skills Advanced SQL Python analysis libraries like pandas, numpy, scikit-learn, cloud Azure, GCP AWS Azure target platform building maintaining processing pipelines Good understanding SQL/NoSQL databases Cosmos DB, Postgres, MongoDB, MySQL etc. Experience version control git , CI/CD Microsoft Dev Ops, Jenkins supporting Data Science teams feature engineering, model training deployment tasks Comfortable Windows/Linux environment Strong business communication drive constantly learn keep speed new technologies Ability understand complex priorities translate clearly defined technical/data specifications implementation deal ambiguity work rapidly changing"
253,Data Engineer,"Data Engineer Spring 2020 Internships â New York, NY","New York, NY",New York,NY,"Unlike a traditional internship, the Decision Sciences Engineering team embeds future Data Engineers in ongoing projects within NBCUniversal. Future Data Engineers become active members of existing teams building data science products, acquiring specific responsibilities, goals and timelines which are interdependent with the goals and timelines of the teams they are embedded in. Upon completing the internship, future Data Engineers will have gained experience in real-world projects in a typical data science environment, using state of the art data engineering tools, and broad exposure to the media industry.
Qualifications/Requirements
In pursuit of a Masterâs or PhD in Computer Science, Computer Engineering, Software Engineering, or other relevant quantitative fieldsWorking knowledge of at least two programming languages. Examples: R, Python, Julia, Java, or ScalaWorking knowledge of cloud computing technologies such as AWS, Azure, or GCPFamiliarity with Spark, Hive and/or HadoopCumulative GPA of 3.0 or aboveMust be 18 years of age or olderMust be authorized to work in the United States without visa sponsorship by NBCUniversalNeeds to be able to work on-site in New York, NYInternships at NBCUniversal are paid and do not require course creditDeadline: Interested applicants are encouraged to apply by November 1st, 2019Spring Program Dates: January â May 2020
Desired Characteristics
Prior experience with ML and/or statistical modellingAdvanced working knowledge of statistics and its applications for inferential data analysisFamiliarity with Spark, Hive and/or HadoopExperience with data visualization techniques and concepts
Sub-BusinessCampus Programs Interns
Career Level
Co-op/Intern
CityNew York
State/Province
New York
CountryUnited States
About Us
The NBCUniversal Internship Program is an experience like no other. We offer diversity of opportunities, with unique internships across our iconic portfolio of brands. Through unparalleled access to the best in the business, hands-on training & one-of-a-kind networking events, our interns have the chance to influence change. Our interns are ambitious, innovative and savvy; they shape the way we do things. Here you can contribute as content creators, problem solvers & innovators. Here you can learn the power and possibilities of media and technology. Here you can go far.
Notices
NBCUniversalâs policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.","In pursuit of a Masterâs or PhD in Computer Science, Computer Engineering, Software Engineering, or other relevant quantitative fieldsWorking knowledge of at least two programming languages. Examples  R, Python, Julia, Java, or ScalaWorking knowledge of cloud computing technologies such as AWS, Azure, or GCPFamiliarity with Spark, Hive and/or HadoopCumulative GPA of 3.0 or aboveMust be 18 years of age or olderMust be authorized to work in the United States without visa sponsorship by NBCUniversalNeeds to be able to work on-site in New York, NYInternships at NBCUniversal are paid and do not require course creditDeadline  Interested applicants are encouraged to apply by November 1st, 2019Spring Program Dates  January â May 2020    In pursuit of a Masterâs or PhD in Computer Science, Computer Engineering, Software Engineering, or other relevant quantitative fieldsWorking knowledge of at least two programming languages. Examples  R, Python, Julia, Java, or ScalaWorking knowledge of cloud computing technologies such as AWS, Azure, or GCPFamiliarity with Spark, Hive and/or HadoopCumulative GPA of 3.0 or aboveMust be 18 years of age or olderMust be authorized to work in the United States without visa sponsorship by NBCUniversalNeeds to be able to work on-site in New York, NYInternships at NBCUniversal are paid and do not require course creditDeadline  Interested applicants are encouraged to apply by November 1st, 2019Spring Program Dates  January â May 2020","In pursuit of a Masterâs or PhD in Computer Science, Engineering, Software other relevant quantitative fieldsWorking knowledge at least two programming languages. Examples R, Python, Julia, Java, ScalaWorking cloud computing technologies such as AWS, Azure, GCPFamiliarity with Spark, Hive and/or HadoopCumulative GPA 3.0 aboveMust be 18 years age olderMust authorized to work the United States without visa sponsorship by NBCUniversalNeeds able on-site New York, NYInternships NBCUniversal are paid and do not require course creditDeadline Interested applicants encouraged apply November 1st, 2019Spring Program Dates January â May 2020","In pursuit Masterâs PhD Computer Science, Engineering, Software relevant quantitative fieldsWorking knowledge least two programming languages. Examples R, Python, Julia, Java, ScalaWorking cloud computing technologies AWS, Azure, GCPFamiliarity Spark, Hive and/or HadoopCumulative GPA 3.0 aboveMust 18 years age olderMust authorized work United States without visa sponsorship NBCUniversalNeeds able on-site New York, NYInternships NBCUniversal paid require course creditDeadline Interested applicants encouraged apply November 1st, 2019Spring Program Dates January â May 2020"
254,Data Engineer,Senior Streaming Data Engineer (Flink & Kafka),"New York, NY 10261",New York,NY,"The Senior Streaming Data Engineer is a senior level position responsible for establishing and implementing new or revised application systems and programs in coordination with the Technology team. The overall objective of this role is to lead applications systems analysis and programming activities.

Responsibilities:
Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements
Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards
Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint
Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation
Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals
Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions
Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary
Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.

Qualifications:
6-10 years of relevant experience in Apps Development or systems analysis role
Extensive experience system analysis and in programming of software applications
Experience in managing and implementing successful projects
Subject Matter Expert (SME) in at least one area of Applications Development
Ability to adjust priorities quickly as circumstances dictate
Demonstrated leadership and project management skills
Consistently demonstrates clear and concise written and verbal communication

Education:
Bachelorâs degree/University degree or equivalent experience
Masterâs degree preferred
This job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required.
Technical Requirements:
Hands-on production experience with distributed stream processing frameworks: Kafka / Spark Streaming / Storm
Experience with deployment platform such as Kubernetes, YARN, Mesos
Production experience of building a robust, fault-tolerant data pipeline that cleans, transforms, and aggregates unorganized and messy data into databases or data sources
Solid design/development background
Experience with micro-services and distributed architecture
Experience with relational databases (SQL Server, Oracle, DB2, or Sybase etc.) and non-relational databases (e.g. Cassandra and MongoDb)
Practical experience in performance tuning and optimization, bottleneck problem analysis
Excellent communication skills to be able to interface with trading and sales functions
Working knowledge of Java and/or Python
-
Grade :All Job Level - All Job FunctionsAll Job Level - All Job Functions - US
-
Time Type :Full time
-
Citi is an equal opportunity and affirmative action employer.
Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.
Citigroup Inc. and its subsidiaries (""Citiâ) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity CLICK HERE.
To view the ""EEO is the Law"" poster CLICK HERE. To view the EEO is the Law Supplement CLICK HERE.
To view the EEO Policy Statement CLICK HERE.
To view the Pay Transparency Posting CLICK HERE."," 6-10 years of relevant experience in Apps Development or systems analysis role Extensive experience system analysis and in programming of software applications Experience in managing and implementing successful projects Subject Matter Expert  SME  in at least one area of Applications Development Ability to adjust priorities quickly as circumstances dictate Demonstrated leadership and project management skills Consistently demonstrates clear and concise written and verbal communication   Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.  Bachelorâs degree/University degree or equivalent experience Masterâs degree preferred ","6-10 years of relevant experience in Apps Development or systems analysis role Extensive system and programming software applications Experience managing implementing successful projects Subject Matter Expert SME at least one area Applications Ability to adjust priorities quickly as circumstances dictate Demonstrated leadership project management skills Consistently demonstrates clear concise written verbal communication Partner with multiple teams ensure appropriate integration functions meet goals well identify define necessary enhancements deploy new products process improvements Resolve variety high impact problems/projects through in-depth evaluation complex business processes, industry standards Provide expertise advanced knowledge application design adheres the overall architecture blueprint Utilize flow develop for coding, testing, debugging, implementation Develop comprehensive how areas business, such infrastructure, integrate accomplish interpretive thinking issues innovative solutions Serve advisor coach mid-level developers analysts, allocating work Appropriately assess risk when decisions are made, demonstrating particular consideration firm's reputation safeguarding Citigroup, its clients assets, by driving compliance applicable laws, rules regulations, adhering Policy, applying sound ethical judgment regarding personal behavior, conduct practices, escalating, reporting control transparency. Bachelorâs degree/University degree equivalent Masterâs preferred","6-10 years relevant experience Apps Development systems analysis role Extensive system programming software applications Experience managing implementing successful projects Subject Matter Expert SME least one area Applications Ability adjust priorities quickly circumstances dictate Demonstrated leadership project management skills Consistently demonstrates clear concise written verbal communication Partner multiple teams ensure appropriate integration functions meet goals well identify define necessary enhancements deploy new products process improvements Resolve variety high impact problems/projects in-depth evaluation complex business processes, industry standards Provide expertise advanced knowledge application design adheres overall architecture blueprint Utilize flow develop coding, testing, debugging, implementation Develop comprehensive areas business, infrastructure, integrate accomplish interpretive thinking issues innovative solutions Serve advisor coach mid-level developers analysts, allocating work Appropriately assess risk decisions made, demonstrating particular consideration firm's reputation safeguarding Citigroup, clients assets, driving compliance applicable laws, rules regulations, adhering Policy, applying sound ethical judgment regarding personal behavior, conduct practices, escalating, reporting control transparency. Bachelorâs degree/University degree equivalent Masterâs preferred"
255,Data Engineer,Data Engineer,"New York, NY",New York,NY,"The Opportunity:
As a Data Engineer on our Business Intelligence team, you will play a key role in designing and driving our data integration pipelines, ETL/ELT, and analytic strategies across our enterprise. In this role you will be bringing modern data technologies and practices to enable our talented team of researchers, data scientists and analysts to enhance the business with better insights and relevant data products. You will partner with all data teams at Shutterstock to understand requirements and collaborate on providing solutions at scale.

Responsibilities:

Determine optimal solutions for integrating data from a variety of sources into a common data warehouse
Implement, maintain and monitor batch & stream data pipelines with best practice quality controls
Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise
Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations
Collaborate with data infrastructure team to deploy necessary infra capabilities
Day to day operational support of data infrastructure, and services

Requirements:

5+ Years of related work experience
Solid command of Python, Java, and/or Scala
Experience in stream processing technology (Kafka, Spark, Storm, Samza, Flink, etc)
In-depth knowledge of SQL, data modeling and data warehousing concepts
Distributed and low latency (streaming) application architecture
Familiarity with API design
CI/CD systems experience (Jenkins, Github, etc)
Experience adhering to robust audit standards
BS or MS degree in Computer Science or Engineering related experience

Preferred Qualifications:

Knowledge of emerging data integration technologies
Spark Streaming, Kafka Streams, Kafka Connect, etc
Data Science and Modeling pipeline experience
Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools
Strong expertise/background with Linux
Familiarity with MS Sql Server, SSIS, SSAS

About Shutterstock, Inc.

Shutterstock, Inc. (NYSE: SSTK ( https://studio-5.financialcontent.com/prnews?Page=Quote&Ticker=SSTK )), directly and through its group subsidiaries, is a leading global provider ofhigh-quality licensed photographs ( https://www.shutterstock.com/ ),vectors ( https://www.shutterstock.com/vectors ),illustrations ( https://www.shutterstock.com/category/illustrations-clip-art ),videos ( https://www.shutterstock.com/video/ ) andmusic ( https://www.shutterstock.com/music/ ) to businesses, marketing agencies and media organizations around the world. Working with its growing community of over 750,000 contributors, Shutterstock adds hundreds of thousands of images each week, and currently has more than 260 million images and more than 14 million video clips available.

Headquartered in New York City, Shutterstock has offices around the world and customers in more than 150 countries. The company also ownsBigstock ( https://www.bigstockphoto.com/ ), a value-oriented stock media offering; Shutterstock Custom,a custom content creation platform ( https://www.shutterstock.com/custom ), Offset, ahigh-end image collection ( https://offset.com/ ); PremiumBeat a curatedroyalty-free music ( https://www.premiumbeat.com/ ) library; and Shutterstock Editorial, a premier source ofeditorial images ( https://www.shutterstock.com/editorial ) for the world's media.

For more information, please visitwww.shutterstock.com ( https://www.shutterstock.com/ ) and follow Shutterstock onTwitter ( https://twitter.com/shutterstock ) and onFacebook ( https://facebook.com/shutterstock ).

Equal Opportunity Employer, M/F/D/V"," Knowledge of emerging data integration technologies Spark Streaming, Kafka Streams, Kafka Connect, etc Data Science and Modeling pipeline experience Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools Strong expertise/background with Linux Familiarity with MS Sql Server, SSIS, SSAS    Determine optimal solutions for integrating data from a variety of sources into a common data warehouse Implement, maintain and monitor batch & stream data pipelines with best practice quality controls Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations Collaborate with data infrastructure team to deploy necessary infra capabilities Day to day operational support of data infrastructure, and services    5+ Years of related work experience Solid command of Python, Java, and/or Scala Experience in stream processing technology  Kafka, Spark, Storm, Samza, Flink, etc  In-depth knowledge of SQL, data modeling and data warehousing concepts Distributed and low latency  streaming  application architecture Familiarity with API design CI/CD systems experience  Jenkins, Github, etc  Experience adhering to robust audit standards BS or MS degree in Computer Science or Engineering related experience ","Knowledge of emerging data integration technologies Spark Streaming, Kafka Streams, Connect, etc Data Science and Modeling pipeline experience Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools Strong expertise/background Linux MS Sql Server, SSIS, SSAS Determine optimal solutions for integrating from a variety sources into common warehouse Implement, maintain monitor batch & stream pipelines best practice quality controls Evaluate relevant new mature needs, gaps, opportunities arise Work closely collaboratively in an Agile environment our analysts, engineers product teams to analyze issues find insights covering business operations Collaborate infrastructure team deploy necessary infra capabilities Day day operational support infrastructure, services 5+ Years related work Solid command Python, Java, and/or Scala Experience processing technology Kafka, Spark, Storm, Samza, Flink, In-depth knowledge SQL, modeling warehousing concepts Distributed low latency streaming application architecture API design CI/CD systems Jenkins, Github, adhering robust audit standards BS degree Computer Engineering","Knowledge emerging data integration technologies Spark Streaming, Kafka Streams, Connect, etc Data Science Modeling pipeline experience Familiarity machine learning frameworks H2O, scikit-learn similar tools Strong expertise/background Linux MS Sql Server, SSIS, SSAS Determine optimal solutions integrating variety sources common warehouse Implement, maintain monitor batch & stream pipelines best practice quality controls Evaluate relevant new mature needs, gaps, opportunities arise Work closely collaboratively Agile environment analysts, engineers product teams analyze issues find insights covering business operations Collaborate infrastructure team deploy necessary infra capabilities Day day operational support infrastructure, services 5+ Years related work Solid command Python, Java, and/or Scala Experience processing technology Kafka, Spark, Storm, Samza, Flink, In-depth knowledge SQL, modeling warehousing concepts Distributed low latency streaming application architecture API design CI/CD systems Jenkins, Github, adhering robust audit standards BS degree Computer Engineering"
256,Data Engineer,"Senior Engineer, Data","New York, NY 10001",New York,NY,"Location: New York, NY

Position Summary:

The Senior Data Engineer is responsible for building and deploying streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably. As a Senior Data Engineer, you will lead collaboration with product teams, data analysts and data scientists to design and build data-forward solutions. In this highly visible technical lead position, you will be responsible for providing Data Engineering leadership and support to ingest and integrate large volumes of disparate data from a variety of sources. This involves rapid innovation in large scale data pipeline design and development to ensure critical data sets are made available to our users and predictive models in a timely manner. We are looking for someone with strong hands on experience in all layers of the full stack involving data. The Senior Data Engineer plays a significant role in Agile planning, providing advice and guidance, and monitoring emerging technologies. This is not a junior programmer position and requires extensive hands on coding and design experience.

Duties and Responsibilities:

Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably.
Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions.
Gather and process all types of data including raw, structured, semi-structured, and unstructured data.
Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics.
Build and maintain dimensional data warehouses in support of business intelligence tools.
Develop data catalogs and data validations to ensure clarity and correctness of key business metrics.
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result.
Derive an overall strategy of data management, within an established information architecture (including both structured and unstructured data), that supports the development and secure operation of existing and new information and digital services.
Plan effective data storage, security, sharing and publishing within the organization.
Ensure data quality and implement tools and frameworks for automating the identification of data quality issues.
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Mentor and lead data engineers providing technical guidance and oversight.
Provide ongoing support, monitoring, and maintenance of deployed products.
Drive and maintain a culture of quality, innovation and experimentation.
Supervisory Responsibilities:

This is an individual contributor role without direct reports, however as a Senior level role we expect this candidate to coach, mentor, and help develop junior developers and engineers to inspire, motivate, grow, and help structure a high performance team.
Minimum Qualifications:

Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs.
5+ years professional experience working with data extract/manipulation logic.
5+ years professional experience with object-oriented programming, functional programming, and data design.
7+ years experience with Development, Engineering, R&D or Information Technology.
3+ years working with a public cloud big data ecosystem (certification in AWS a plus).
3+ years working with MPP databases, distributed databases, and/or Hadoop.
Requirements and General Skills:
Passion for data engineering, able to excite and lead by example and mentoring others.
Hungry and eager to learn new systems and technologies.
Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.
Ability to deliver exceptional results through iterative improvement rather than initial perfection.
Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including: business users, technical staff, senior level colleagues, vendors, and partners.
An extensive track record that demonstrates effectiveness in driving business results through data and analytics.
The ability to develop and articulate a compelling vision and generate necessary consensus.
A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.
A proven ability to influence decision making across large organizations.
A proven ability to hire, develop, and effectively lead deeply technical resources.
Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.
Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.
Create an environment where people from diverse cultures and backgrounds work together effectively.
Technical Skills:

Experience deploying and running AWS-based data solutions and familiar with tools such as Cloud Formation, IAM, Athena, and Kinesis.
Experience engineering big-data solutions using technologies like EMR, S3, Spark and an in-depth understanding of data partitioning and sharding techniques.
Experience loading and querying both on premise and cloud-hosted databases such as Teradata and Redshift.
Building streaming data pipelines using Kafka, Spark, or Flink.
Familiarity with binary data serialization formats such as Parquet, Avro, and Thrift.
Experience deploying data notebook and analytic environments such as Jupyter and Databricks.
Knowledge of the Python data ecosystem using pandas and numpy.
Experience building and deploying ML pipelines: training models, feature development, regression testing.
Experience with graph-based data workflows using Apache Airflow.
Expertise writing distributed, high-volume services in Python, Java or Scala.
Expertise with high volume heterogeneous data, preferably with distributed systems.
Knowledge of data modeling, data access, and data storage techniques.
Appreciation of agile software processes, data-driven development, reliability, and responsible experimentation.
Familiar with metadata management, data lineage, and principles of data governance.
professional role in one or more of the following:
Strong and thorough knowledge of the following:

ETL/ELT Tools
BI tools
MDM / Reference Data
RDBMS, NoSQL and NewSQL
MS Office Suite
SiriusXM is an equal opportunity employer that does not discriminate on the basis of sex, race, color, age, national origin, religion, creed, physical or mental disability, medical condition, marital status, sexual orientation, gender identity or expression, citizenship, pregnancy, military or veteran status or any other status protected by applicable law.

The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice."," Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs. 5+ years professional experience working with data extract/manipulation logic. 5+ years professional experience with object-oriented programming, functional programming, and data design. 7+ years experience with Development, Engineering, R&D or Information Technology. 3+ years working with a public cloud big data ecosystem  certification in AWS a plus . 3+ years working with MPP databases, distributed databases, and/or Hadoop.  Passion for data engineering, able to excite and lead by example and mentoring others. Hungry and eager to learn new systems and technologies. Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next. Ability to deliver exceptional results through iterative improvement rather than initial perfection. Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including  business users, technical staff, senior level colleagues, vendors, and partners. An extensive track record that demonstrates effectiveness in driving business results through data and analytics. The ability to develop and articulate a compelling vision and generate necessary consensus. A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions. A proven ability to influence decision making across large organizations. A proven ability to hire, develop, and effectively lead deeply technical resources. Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals. Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals. Create an environment where people from diverse cultures and backgrounds work together effectively.   Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably. Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions. Gather and process all types of data including raw, structured, semi-structured, and unstructured data. Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics. Build and maintain dimensional data warehouses in support of business intelligence tools. Develop data catalogs and data validations to ensure clarity and correctness of key business metrics. Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result. Derive an overall strategy of data management, within an established information architecture  including both structured and unstructured data , that supports the development and secure operation of existing and new information and digital services. Plan effective data storage, security, sharing and publishing within the organization. Ensure data quality and implement tools and frameworks for automating the identification of data quality issues. Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings. Mentor and lead data engineers providing technical guidance and oversight. Provide ongoing support, monitoring, and maintenance of deployed products. Drive and maintain a culture of quality, innovation and experimentation.   Passion for data engineering, able to excite and lead by example and mentoring others. Hungry and eager to learn new systems and technologies. Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next. Ability to deliver exceptional results through iterative improvement rather than initial perfection. Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including  business users, technical staff, senior level colleagues, vendors, and partners. An extensive track record that demonstrates effectiveness in driving business results through data and analytics. The ability to develop and articulate a compelling vision and generate necessary consensus. A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions. A proven ability to influence decision making across large organizations. A proven ability to hire, develop, and effectively lead deeply technical resources. Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals. Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals. Create an environment where people from diverse cultures and backgrounds work together effectively. ","Advanced degree in relevant field of study strongly desirable, particularly computer science or engineering level programs. 5+ years professional experience working with data extract/manipulation logic. object-oriented programming, functional and design. 7+ Development, Engineering, R&D Information Technology. 3+ a public cloud big ecosystem certification AWS plus . MPP databases, distributed and/or Hadoop. Passion for engineering, able to excite lead by example mentoring others. Hungry eager learn new systems technologies. Self-directed enjoys the challenge freedom deciding what is most impactful thing work on next. Ability deliver exceptional results through iterative improvement rather than initial perfection. Excellent communication presentation skills ability interact appropriately all levels organization, including business users, technical staff, senior colleagues, vendors, partners. An extensive track record that demonstrates effectiveness driving analytics. The develop articulate compelling vision generate necessary consensus. A successful history translating objectives problems into analytic problems, solutions actionable solutions. proven influence decision making across large organizations. hire, develop, effectively deeply resources. Demonstrate foster sense urgency, strong commitment, accountability while sound decisions achieving goals. Articulate, inspire, engage commitment plan action aligned organizational mission Create an environment where people from diverse cultures backgrounds together effectively. Build deploy streaming batch pipelines capable processing storing petabytes quickly reliably. Collaborate product teams, analysts scientists design build data-forward Gather process types raw, structured, semi-structured, unstructured data. Integrate variety providers ranging marketing, web analytics, consumer devices IoT Telematics. maintain dimensional warehouses support intelligence tools. Develop catalogs validations ensure clarity correctness key metrics. Design, code, test, correct document programs scripts using agreed standards tools achieve well-engineered result. Derive overall strategy management, within established information architecture both structured , supports development secure operation existing digital services. Plan effective storage, security, sharing publishing organization. Ensure quality implement frameworks automating identification issues. internal external validation providing feedback customized changes feeds mappings. Mentor engineers guidance oversight. Provide ongoing support, monitoring, maintenance deployed products. Drive culture quality, innovation experimentation.","Advanced degree relevant field study strongly desirable, particularly computer science engineering level programs. 5+ years professional experience working data extract/manipulation logic. object-oriented programming, functional design. 7+ Development, Engineering, R&D Information Technology. 3+ public cloud big ecosystem certification AWS plus . MPP databases, distributed and/or Hadoop. Passion engineering, able excite lead example mentoring others. Hungry eager learn new systems technologies. Self-directed enjoys challenge freedom deciding impactful thing work next. Ability deliver exceptional results iterative improvement rather initial perfection. Excellent communication presentation skills ability interact appropriately levels organization, including business users, technical staff, senior colleagues, vendors, partners. An extensive track record demonstrates effectiveness driving analytics. The develop articulate compelling vision generate necessary consensus. A successful history translating objectives problems analytic problems, solutions actionable solutions. proven influence decision making across large organizations. hire, develop, effectively deeply resources. Demonstrate foster sense urgency, strong commitment, accountability sound decisions achieving goals. Articulate, inspire, engage commitment plan action aligned organizational mission Create environment people diverse cultures backgrounds together effectively. Build deploy streaming batch pipelines capable processing storing petabytes quickly reliably. Collaborate product teams, analysts scientists design build data-forward Gather process types raw, structured, semi-structured, unstructured data. Integrate variety providers ranging marketing, web analytics, consumer devices IoT Telematics. maintain dimensional warehouses support intelligence tools. Develop catalogs validations ensure clarity correctness key metrics. Design, code, test, correct document programs scripts using agreed standards tools achieve well-engineered result. Derive overall strategy management, within established information architecture structured , supports development secure operation existing digital services. Plan effective storage, security, sharing publishing organization. Ensure quality implement frameworks automating identification issues. internal external validation providing feedback customized changes feeds mappings. Mentor engineers guidance oversight. Provide ongoing support, monitoring, maintenance deployed products. Drive culture quality, innovation experimentation."
257,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Dashlane is a password manager and online security app for everyone who lives, works, and plays on the internet. With a simple, intuitive design and patented security technology, Dashlane keeps passwords, personal data, and payment info at users' fingertips, so they can stop guessing passwords and wasting time filling out forms. Dashlane has helped over 11 million users in 180 countries manage and secure their digital identities and has enabled over $17 billion in e-commerce transactions. Our team in New York, Paris and Lisbon is united by our passion for password security and the belief that our success is built on the diverse backgrounds of every member.

You will be based in New York.

Dashlane is looking for a highly talented data engineer to join the Data team. Optimizing our data pipelines and data warehouses will be an essential part to helping our company scale in the coming years. You have several years of experience and a proven track record in building, deploying and keeping such applications up 24x7. You will work on a daily basis with your teammates in New York City.

At Dashlane you will:

Work on the development and maintenance of messaging services, BI tools, data warehouses
Implement custom ETL/ELT processes in distributed computing environments
Work on improving in-house tracking systems for all of our applications
Design and implement data pipelines capable of modeling data from many sources and store it in such a way that users can self-serve
Work on server applications and APIs that are used by our Data Team
Handle the challenges that come with managing terabytes of data
Develop automated reporting for API and system health (process, memory, response time)

Requirements:

3+ years experience in software development
3+ years experience designing SQL tables, choosing indexes, tuning queries and understanding the intricacies required to optimize a table in different environments

We're also looking for:

Having experience in architecting, implementing and testing data processing pipelines (e.g. Spark, Beam, ...) and data mining / data science algorithms either on-premise or on a cloud environment
Having experience in administrating and ingesting data into standard data warehouses (e.g. Amazon Redshift, Microsoft SQL Server, Google BigQuery or Snowflake)
Having strong experience in improving performance of queries, data jobs and scaling systems for exponential growth in data
Being able to communicate and understand complex technical issues in English

Nice-To-Haves:

Have prior knowledge of Python and Node.js
Having experience with data lakes and expertise with designing and maintaining a BI solution
Enjoy writing clean code that is easy to maintain and understand
Have a security background

A true international company, founded in Paris and currently split between Paris, Lisbon and New York, we thrive off diverse perspectives. We recognize that diversity has different aspects: gender, sexual orientation, ability, ethnic origin, social, age, lifestyle, and more. We're committed to finding diverse talent and fostering a culture where everyone is heard and feels a sense of belonging.","     3+ years experience in software development 3+ years experience designing SQL tables, choosing indexes, tuning queries and understanding the intricacies required to optimize a table in different environments ","3+ years experience in software development designing SQL tables, choosing indexes, tuning queries and understanding the intricacies required to optimize a table different environments","3+ years experience software development designing SQL tables, choosing indexes, tuning queries understanding intricacies required optimize table different environments"
258,Data Engineer,DATA ENGINEER - competative compensation,"New York, NY 10036",New York,NY,"One of the global leading innovative biotech companies.
We are seeking a thoughtful, hands-on Data Engineer to join our client's Data Architecture team. Data Architecture is a broad team that develops and operates the data platform used by developers throughout the Roivant family. The team implements all data ingestion, storage, and analytics tooling and provides other ad hoc database needs.


Key Responsibilities


You will join a small team partnering with product owners and developers at Roivant and Vants to provide end-to-end data solutions for technology tools and products.
You will follow best coding practices to build and optimize tools for data ingestion and storage, including components of client's Data Lake/Warehouse platform.
You will automate and maintain data processing pipelines, implement modern ETL infrastructure, and continuously improve the efficiency of our platform.
You will serve as a subject matter expert on big data analytics projects that provide insights for business and technical stakeholders.




Requirements


BA/BS degree with strong academic performance, preferably in a quantitative field
4+ years experience with Python, database development, Git, Linux and AWS (S3, EC2, SNS, Lambda, SQS)
Experience with Spark, terraform, Docker, big data and/or healthcare data preferred
Knowledge of Scrum and desire to work in an incredibly fast-moving, agile environment
Team player with strong communication skills and the ability to work with minimal supervision
Quick and scrappy learner who adapts well to a fast-moving environment and gets things done; experience in high-growth or startup environments a plus","  You will join a small team partnering with product owners and developers at Roivant and Vants to provide end-to-end data solutions for technology tools and products. You will follow best coding practices to build and optimize tools for data ingestion and storage, including components of client's Data Lake/Warehouse platform. You will automate and maintain data processing pipelines, implement modern ETL infrastructure, and continuously improve the efficiency of our platform. You will serve as a subject matter expert on big data analytics projects that provide insights for business and technical stakeholders.   BA/BS degree with strong academic performance, preferably in a quantitative field 4+ years experience with Python, database development, Git, Linux and AWS  S3, EC2, SNS, Lambda, SQS  Experience with Spark, terraform, Docker, big data and/or healthcare data preferred Knowledge of Scrum and desire to work in an incredibly fast-moving, agile environment Team player with strong communication skills and the ability to work with minimal supervision Quick and scrappy learner who adapts well to a fast-moving environment and gets things done; experience in high-growth or startup environments a plus","You will join a small team partnering with product owners and developers at Roivant Vants to provide end-to-end data solutions for technology tools products. follow best coding practices build optimize ingestion storage, including components of client's Data Lake/Warehouse platform. automate maintain processing pipelines, implement modern ETL infrastructure, continuously improve the efficiency our serve as subject matter expert on big analytics projects that insights business technical stakeholders. BA/BS degree strong academic performance, preferably in quantitative field 4+ years experience Python, database development, Git, Linux AWS S3, EC2, SNS, Lambda, SQS Experience Spark, terraform, Docker, and/or healthcare preferred Knowledge Scrum desire work an incredibly fast-moving, agile environment Team player communication skills ability minimal supervision Quick scrappy learner who adapts well fast-moving gets things done; high-growth or startup environments plus","You join small team partnering product owners developers Roivant Vants provide end-to-end data solutions technology tools products. follow best coding practices build optimize ingestion storage, including components client's Data Lake/Warehouse platform. automate maintain processing pipelines, implement modern ETL infrastructure, continuously improve efficiency serve subject matter expert big analytics projects insights business technical stakeholders. BA/BS degree strong academic performance, preferably quantitative field 4+ years experience Python, database development, Git, Linux AWS S3, EC2, SNS, Lambda, SQS Experience Spark, terraform, Docker, and/or healthcare preferred Knowledge Scrum desire work incredibly fast-moving, agile environment Team player communication skills ability minimal supervision Quick scrappy learner adapts well fast-moving gets things done; high-growth startup environments plus"
259,Data Engineer,Federal - Big Data Engineer,"New York, NY 10011",New York,NY,"Organization: Accenture Federal Services

Location: New York, NY

Accenture Federal Services, a wholly owned subsidiary of Accenture LLP, is a U.S. company with offices in Arlington, Virginia. Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations. Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.

We believe that great outcomes are everything. Itâs what drives us to turn bold ideas into breakthrough solutions. By combining digital technologies with what works across the worldâs leading businesses, we use agile approaches to help clients solve their toughest problems fastâthe first time. So, you can deliver what matters most.

Count on us to help you embrace new ways of working, building for change and put customers at the core. A wholly owned subsidiary of Accenture, we bring over 30 years of experience serving the federal government, including every cabinet-level department. Our 7,200 dedicated colleagues and change makers work with our clients at the heart of the nationâs priorities in defense, intel, public safety, health and civilian to help you make a difference for the people you employ, serve and protect.

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

Accenture Federal Services is looking for Big Data Engineers to lead the way in tackling the most difficult engineering challenges of data systems. The Big Data Engineer will manage the discovery lab's data environment, data governance, and ETL. This individual uses Accenture Insights Platform and manages the data stores including S3 and NOSQL.

Basic Qualifications:
Bachelorâs Degree in related field and 3 years of experience in JavaScript, SQL and/or Python
U.S. Citizenship
Ability to work in fast paced prototyping environment (Average project length is 6-12 weeks, multiple projects assigned simultaneously)
Preferred Skills:
JavaScript
GitOps (Github, Gitlab)
Pandas
Python
Angular
React
Tensorflow / Keras / Pytorch / Fastai
Flask / Django
Jupyter Notebooks / Anaconda
Spark
Databricks
Docker
Kubernetes
SQL
Docker Compose
Docker Swarm
Vue
Jenkins
Gatsby

An active security clearance or the ability to obtain one may be required for this role.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."," Bachelorâs Degree in related field and 3 years of experience in JavaScript, SQL and/or Python U.S. Citizenship Ability to work in fast paced prototyping environment  Average project length is 6-12 weeks, multiple projects assigned simultaneously  JavaScript GitOps  Github, Gitlab  Pandas Python Angular React Tensorflow / Keras / Pytorch / Fastai Flask / Django Jupyter Notebooks / Anaconda Spark Databricks Docker Kubernetes SQL Docker Compose Docker Swarm Vue Jenkins Gatsby   ","Bachelorâs Degree in related field and 3 years of experience JavaScript, SQL and/or Python U.S. Citizenship Ability to work fast paced prototyping environment Average project length is 6-12 weeks, multiple projects assigned simultaneously JavaScript GitOps Github, Gitlab Pandas Angular React Tensorflow / Keras Pytorch Fastai Flask Django Jupyter Notebooks Anaconda Spark Databricks Docker Kubernetes Compose Swarm Vue Jenkins Gatsby","Bachelorâs Degree related field 3 years experience JavaScript, SQL and/or Python U.S. Citizenship Ability work fast paced prototyping environment Average project length 6-12 weeks, multiple projects assigned simultaneously JavaScript GitOps Github, Gitlab Pandas Angular React Tensorflow / Keras Pytorch Fastai Flask Django Jupyter Notebooks Anaconda Spark Databricks Docker Kubernetes Compose Swarm Vue Jenkins Gatsby"
260,Data Engineer,Senior Data Engineer,"New York, NY",New York,NY,"The Opportunity:
As a Data Engineer on our Data Infrastructure team, you will play a key role in designing and driving our data integration pipelines, ETL/ELT, and analytic strategies across our enterprise. In this role you will be bringing modern data technologies and practices to enable our talented team of researchers, data scientists and analysts to enhance the business with better insights and relevant data products. You will partner with all data teams at Shutterstock to understand requirements and collaborate on providing solutions at scale.

Responsibilities:

Determine optimal solutions for integrating data from a variety of sources into a common data warehouse
Implement, maintain and monitor batch & stream data pipelines with best practice quality controls
Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise
Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations
Collaborate with our other data teams, as they are customers
Day to day operational support of data infrastructure, and services

Requirements:

Solid command Java or Scala, and Python
Experience in stream processing technology (Kafka, Spark, Storm, Samza, Flink, etc)
Knowledge of SQL, data modeling and data warehousing concepts
Distributed and low latency (streaming) application architecture
Familiarity with API design
CI/CD systems experience (Jenkins, Github, etc)
Experience adhering to robust audit standards
BS or MS degree in Computer Science or Engineering related experience

Bonus Skills:

Knowledge of emerging data integration technologies
Spark Streaming, Kafka Streams, Kafka Connect, etc
Exposure to Kubernetes and Helm
Data Science and Modeling pipeline experience
Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools
Strong expertise/background with Linux
Familiarity with MS Sql Server, SSIS, SSAS

About Shutterstock, Inc.

Shutterstock, Inc. (NYSE: SSTK ( https://studio-5.financialcontent.com/prnews?Page=Quote&Ticker=SSTK )), directly and through its group subsidiaries, is a leading global provider ofhigh-quality licensed photographs ( https://www.shutterstock.com/ ),vectors ( https://www.shutterstock.com/vectors ),illustrations ( https://www.shutterstock.com/category/illustrations-clip-art ),videos ( https://www.shutterstock.com/video/ ) andmusic ( https://www.shutterstock.com/music/ ) to businesses, marketing agencies and media organizations around the world. Working with its growing community of over 750,000 contributors, Shutterstock adds hundreds of thousands of images each week, and currently has more than 260 million images and more than 14 million video clips available.

Headquartered in New York City, Shutterstock has offices around the world and customers in more than 150 countries. The company also ownsBigstock ( https://www.bigstockphoto.com/ ), a value-oriented stock media offering; Shutterstock Custom,a custom content creation platform ( https://www.shutterstock.com/custom ), Offset, ahigh-end image collection ( https://offset.com/ ); PremiumBeat a curatedroyalty-free music ( https://www.premiumbeat.com/ ) library; and Shutterstock Editorial, a premier source ofeditorial images ( https://www.shutterstock.com/editorial ) for the world's media.

For more information, please visitwww.shutterstock.com ( https://www.shutterstock.com/ ) and follow Shutterstock onTwitter ( https://twitter.com/shutterstock ) and onFacebook ( https://facebook.com/shutterstock ).

Equal Opportunity Employer, M/F/D/V","  Knowledge of emerging data integration technologies Spark Streaming, Kafka Streams, Kafka Connect, etc Exposure to Kubernetes and Helm Data Science and Modeling pipeline experience Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools Strong expertise/background with Linux Familiarity with MS Sql Server, SSIS, SSAS   Determine optimal solutions for integrating data from a variety of sources into a common data warehouse Implement, maintain and monitor batch & stream data pipelines with best practice quality controls Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations Collaborate with our other data teams, as they are customers Day to day operational support of data infrastructure, and services    Solid command Java or Scala, and Python Experience in stream processing technology  Kafka, Spark, Storm, Samza, Flink, etc  Knowledge of SQL, data modeling and data warehousing concepts Distributed and low latency  streaming  application architecture Familiarity with API design CI/CD systems experience  Jenkins, Github, etc  Experience adhering to robust audit standards BS or MS degree in Computer Science or Engineering related experience ","Knowledge of emerging data integration technologies Spark Streaming, Kafka Streams, Connect, etc Exposure to Kubernetes and Helm Data Science Modeling pipeline experience Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools Strong expertise/background Linux MS Sql Server, SSIS, SSAS Determine optimal solutions for integrating from a variety sources into common warehouse Implement, maintain monitor batch & stream pipelines best practice quality controls Evaluate relevant new mature needs, gaps, opportunities arise Work closely collaboratively in an Agile environment our analysts, engineers product teams analyze issues find insights covering business operations Collaborate other teams, they are customers Day day operational support infrastructure, services Solid command Java Scala, Python Experience processing technology Kafka, Spark, Storm, Samza, Flink, SQL, modeling warehousing concepts Distributed low latency streaming application architecture API design CI/CD systems Jenkins, Github, adhering robust audit standards BS degree Computer Engineering related","Knowledge emerging data integration technologies Spark Streaming, Kafka Streams, Connect, etc Exposure Kubernetes Helm Data Science Modeling pipeline experience Familiarity machine learning frameworks H2O, scikit-learn similar tools Strong expertise/background Linux MS Sql Server, SSIS, SSAS Determine optimal solutions integrating variety sources common warehouse Implement, maintain monitor batch & stream pipelines best practice quality controls Evaluate relevant new mature needs, gaps, opportunities arise Work closely collaboratively Agile environment analysts, engineers product teams analyze issues find insights covering business operations Collaborate teams, customers Day day operational support infrastructure, services Solid command Java Scala, Python Experience processing technology Kafka, Spark, Storm, Samza, Flink, SQL, modeling warehousing concepts Distributed low latency streaming application architecture API design CI/CD systems Jenkins, Github, adhering robust audit standards BS degree Computer Engineering related"
261,Data Engineer,Senior Python Data Engineer,"New York, NY",New York,NY,"It's fun to work in a company where people truly BELIEVE in what they are doing!


We're committed to bringing passion and customer focus to the business.
Cigna, a leading Health Services company, is looking for exceptional data engineers in our Data & Analytics Engineering organization. The Senior Python Data Engineer is responsible for the delivery of a business need end-to-end starting from understanding the requirements to deploying the software into production. This role requires you to be fluent in some of the critical technologies (specifically Python, ETL and AWS) with proficiency in others and have a hunger to learn on the job and add value to the business. Critical attributes of being a Senior Python Data Engineer, among others, is Ownership & Accountability.
In addition to Delivery, the Senior Python Data Engineer should have an automation first and continuous improvement mindset. He/She should drive the adoption of CI/CD tools and support the improvement of the tools sets/processes.
This position is for a team that works on managing data related to capitalized payments to providers and Priced Only claims. In addition, team is responsible for maintaining applications (like Claim Services & Claim Events) that perform analytics on claims data and that is further used in multiple reporting and trending applications as downstream consumers.
Behaviors of a Senior Python Data Engineer :
Able to articulate clear business objectives aligned to technical specifications and work in an iterative, agile pattern daily. They have ownership over their work tasks, and embrace interacting with all levels of the team and raise challenges when necessary. We aim to be cutting-edge engineers â not institutionalized developers.
Key Characteristics:
Minimize ""meetings"" to get requirements and have direct business interactions
Write referenceable & modular code
Design and architect the solution independently
Be fluent in particular areas and have proficiency in many areas
Have a passion to learn
Take ownership and accountability
Understands when to automate and when not to
Have a desire to simplify
Be entrepreneurial / business minded
Have a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have business impact
Take risks and champion new ideas
Qualifications:
At least 5 years of software development experience deploying enterprise applications related to Data Management/Data Analytics & reporting.
2+ years being part of Agile teams â Scrum or Kanban
2+ years of programming experience in Python.
5+ years of Database experience â SQL, Teradata, Oracle
2+ years of experience with big data technologies - Hadoop, Hive, Spark (PySpark or Spark Scala)
Experience with Git/SVN or similar configuration management tool
Experience with ETL tools like Informatica is a plus
Experience with Reporting tools like Tableau/Looker is a plus.
Excellent troubleshooting skills
Strong communication skills
Fluent in BDD and TDD development methodologies
Work in an agile CI/CD environment (Jenkins experience a plus)
Prior experience with Health care domains is a plus
Qualified applicants will be considered without regard to race, color, age, disability, sex (including pregnancy), childbirth or related medical conditions including but not limited to lactation, sexual orientation, gender identity or expression, veteran or military status, religion, national origin, ancestry, marital or familial status, genetic information, status with regard to public assistance, citizenship status or any other characteristic protected by applicable equal employment opportunity laws.
If you require an accommodation based on your physical or mental disability please email: SeeYourself@cigna.com. Do not email SeeYourself@cigna.com for an update on your application or to provide your resume as you will not receive a response.","At least 5 years of software development experience deploying enterprise applications related to Data Management/Data Analytics & reporting. 2+ years being part of Agile teams â Scrum or Kanban 2+ years of programming experience in Python. 5+ years of Database experience â SQL, Teradata, Oracle 2+ years of experience with big data technologies - Hadoop, Hive, Spark  PySpark or Spark Scala  Experience with Git/SVN or similar configuration management tool Experience with ETL tools like Informatica is a plus Experience with Reporting tools like Tableau/Looker is a plus. Excellent troubleshooting skills Strong communication skills Fluent in BDD and TDD development methodologies Work in an agile CI/CD environment  Jenkins experience a plus  Prior experience with Health care domains is a plus     ","At least 5 years of software development experience deploying enterprise applications related to Data Management/Data Analytics & reporting. 2+ being part Agile teams â Scrum or Kanban programming in Python. 5+ Database SQL, Teradata, Oracle with big data technologies - Hadoop, Hive, Spark PySpark Scala Experience Git/SVN similar configuration management tool ETL tools like Informatica is a plus Reporting Tableau/Looker plus. Excellent troubleshooting skills Strong communication Fluent BDD and TDD methodologies Work an agile CI/CD environment Jenkins Prior Health care domains","At least 5 years software development experience deploying enterprise applications related Data Management/Data Analytics & reporting. 2+ part Agile teams â Scrum Kanban programming Python. 5+ Database SQL, Teradata, Oracle big data technologies - Hadoop, Hive, Spark PySpark Scala Experience Git/SVN similar configuration management tool ETL tools like Informatica plus Reporting Tableau/Looker plus. Excellent troubleshooting skills Strong communication Fluent BDD TDD methodologies Work agile CI/CD environment Jenkins Prior Health care domains"
262,Data Engineer,"Senior Data Engineer, Peacock, Direct-to-Consumer","New York, NY",New York,NY,"Position Overview:

NBCUniversal, the global media company that brought you some of the worldâs most iconic television and film franchises, including: The Tonight Show, Saturday Night Live, Keeping Up With The Kardashians, The Real Housewives, Mr. Robot, The Voice, This Is Us, The Fast & The Furious, Jurassic Park, Minions, and more - is launching an all-new direct-to-consumer streaming service. It will seamlessly bring together the breadth and depth of NBCUâs broadcast and cable television series, movie titles, premier sporting events, and renowned news reporting... all in one destinationâ¦ all in one app.

Introducing Peacock, NBCUniversal's new streaming service that combines timeless shows and movies, exclusive originals, kids programming and current hits, with timely news, sports and pop culture. All together. All in one app.

In preparation for our launch in 2020, we are building a world-class team of smart, hungry and fearless professionals who are energized by the possibility of working at the epicenter of content, technology and culture. Join us if you would like to be a part of this exciting initiative.

As part of the Direct-to-Consumer Decision Sciences team, the Senior Data Engineer will be responsible for creating a connected data ecosystem that unleashes the power of our streaming data. We gather data from across all customer/prospect journeys in near real-time, to allow fast feedback loops across territories; combined with our strategic data platform, this data ecosystem is at the core of being able to make intelligent customer and business decisions.

In this role, the Senior Data Engineer will lead and be responsible for the development and maintenance of an optimized and highly available data pipelines that facilitate deeper analysis and reporting by the business, as well as support ongoing operations related to the Direct to Consumer data ecosystem.

Design, build, test, scale and maintain data pipelines from a variety of source systems and streams (Internal, third party, cloud based, etc.), according to business and technical requirements.
Deliver observable, reliable and secure software, embracing âyou build it you run itâ mentality, and focus on automation and GitOps.
Continually work on improving the codebase and have active participation in all aspects of the team, including agile ceremonies.
Take an active role in story definition, assisting business stakeholders with acceptance criteria.
Work with Principal Engineers and Architects to share and contribute to the broader technical vision.
Develop and champion best practices, striving towards excellence and raising the bar within the department.
Develop solutions combining data blending, profiling, mining, statistical analysis, and machine learning, to better define and curate models, test hypothesis, and deliver key insights
Operationalize data processing systems (dev ops)
Qualifications/Requirements
Experience of near Real Time & Batch Data Pipeline development in a similar Big Data Engineering roleProgramming skills in one or more of the following: Java, Scala, R, Python, SQL and experience in writing reusable/efficient code to automate analysis and data processesExperience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devicesExperience implementing scalable, distributed, and highly available systems using Google CloudHands on programming experience of the following (or similar) technologies: Apache Beam, Scio, Apache Spark, and Snowflake.Experience in progressive data application development, working in large scale/distributed SQL, NoSQL, and/or Hadoop environment.Build and maintain dimensional data warehouses in support of BI toolsDevelop data catalogs and data cleanliness to ensure clarity and correctness of key business metricsExperience building streaming data pipelines using Kafka, Spark or FlinkData modelling experience (operationalizing data science models/products) a plusBachelorsâ degree with a specialization in Computer Science, Engineering, Physics, other quantitative field or equivalent industry experience.
Desired Characteristics
Masterâs Degree with a specialization in Computer Science, Engineering, Physics or another quantitative field a plus.Experience with graph-based data workflows using Apache AirflowExperience building and deploying ML pipelines: training models, feature development, regression testingStrong Test-Driven Development background, with understanding of levels of testing required to continuously deliver value to production.Experience with large-scale video assetsAbility to work effectively across functions, disciplines, and levelsTeam-oriented and collaborative approach with a demonstrated aptitude, enthusiasm and willingness to learn new methods, tools, practices and skillsExhibit a bias for getting the job doneAbility to recognize discordant views and take part in constructive dialogue to resolve themPride and ownership in your work and confident representation of your team to other parts of NBCUniversal
Sub-BusinessDirect-to-Consumer
Career Level
Experienced
CityNew York
State/Province
New York
CountryUnited States
About Us
At NBCUniversal, we believe in the talent of our people. Itâs our passion and commitment to excellence that drives NBCUâs vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. Itâs what makes us uniquely NBCU. Here you can create the extraordinary. Join us.","Experience of near Real Time & Batch Data Pipeline development in a similar Big Data Engineering roleProgramming skills in one or more of the following  Java, Scala, R, Python, SQL and experience in writing reusable/efficient code to automate analysis and data processesExperience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devicesExperience implementing scalable, distributed, and highly available systems using Google CloudHands on programming experience of the following  or similar  technologies  Apache Beam, Scio, Apache Spark, and Snowflake.Experience in progressive data application development, working in large scale/distributed SQL, NoSQL, and/or Hadoop environment.Build and maintain dimensional data warehouses in support of BI toolsDevelop data catalogs and data cleanliness to ensure clarity and correctness of key business metricsExperience building streaming data pipelines using Kafka, Spark or FlinkData modelling experience  operationalizing data science models/products  a plusBachelorsâ degree with a specialization in Computer Science, Engineering, Physics, other quantitative field or equivalent industry experience.    Experience of near Real Time & Batch Data Pipeline development in a similar Big Data Engineering roleProgramming skills in one or more of the following  Java, Scala, R, Python, SQL and experience in writing reusable/efficient code to automate analysis and data processesExperience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devicesExperience implementing scalable, distributed, and highly available systems using Google CloudHands on programming experience of the following  or similar  technologies  Apache Beam, Scio, Apache Spark, and Snowflake.Experience in progressive data application development, working in large scale/distributed SQL, NoSQL, and/or Hadoop environment.Build and maintain dimensional data warehouses in support of BI toolsDevelop data catalogs and data cleanliness to ensure clarity and correctness of key business metricsExperience building streaming data pipelines using Kafka, Spark or FlinkData modelling experience  operationalizing data science models/products  a plusBachelorsâ degree with a specialization in Computer Science, Engineering, Physics, other quantitative field or equivalent industry experience.","Experience of near Real Time & Batch Data Pipeline development in a similar Big Engineering roleProgramming skills one or more the following Java, Scala, R, Python, SQL and experience writing reusable/efficient code to automate analysis data processesExperience processing structured unstructured into form suitable for reporting with integration variety metric providers ranging from advertising, web analytics, consumer devicesExperience implementing scalable, distributed, highly available systems using Google CloudHands on programming technologies Apache Beam, Scio, Spark, Snowflake.Experience progressive application development, working large scale/distributed SQL, NoSQL, and/or Hadoop environment.Build maintain dimensional warehouses support BI toolsDevelop catalogs cleanliness ensure clarity correctness key business metricsExperience building streaming pipelines Kafka, Spark FlinkData modelling operationalizing science models/products plusBachelorsâ degree specialization Computer Science, Engineering, Physics, other quantitative field equivalent industry experience.","Experience near Real Time & Batch Data Pipeline development similar Big Engineering roleProgramming skills one following Java, Scala, R, Python, SQL experience writing reusable/efficient code automate analysis data processesExperience processing structured unstructured form suitable reporting integration variety metric providers ranging advertising, web analytics, consumer devicesExperience implementing scalable, distributed, highly available systems using Google CloudHands programming technologies Apache Beam, Scio, Spark, Snowflake.Experience progressive application development, working large scale/distributed SQL, NoSQL, and/or Hadoop environment.Build maintain dimensional warehouses support BI toolsDevelop catalogs cleanliness ensure clarity correctness key business metricsExperience building streaming pipelines Kafka, Spark FlinkData modelling operationalizing science models/products plusBachelorsâ degree specialization Computer Science, Engineering, Physics, quantitative field equivalent industry experience."
263,Data Engineer,Data Engineer,"New York, NY 10010",New York,NY,"Overview
Sony Music Entertainment is a global recorded music company with a roster of current artists that includes a broad array of both local artists and international superstars, as well as a vast catalog that comprises some of the most important recordings in history. Sony Music Entertainment is a wholly owned subsidiary of Sony Corporation of America.

Sony Music is committed to providing equal employment opportunity for all persons regardless of age, disability, national origin, race, color, religion, sex, sexual orientation, gender, gender identity or expression, pregnancy, veteran or military status, marital and civil partnership/union status, alienage or citizenship status, creed, genetic information or any other status protected by applicable federal, state, or local law.

The Data Engineer will report to the Head of A&R Research. This role will explore and build products using the latest and greatest in Data Analytics, Cognitive Services, Machine Learning and more. This role is based at Sony Musicâs offices in New York
Responsibilities
Create and maintain systems to load and transform very large data sets from digital media retailers (iTunes, Spotify, YouTube, etc) as well as social media sources.
Work with a cross-functional team to create data-driven insights and reports for business stakeholders.
Work with other members of the team to create customer-facing analytics tools and visualizations.
Process millions of rows of data daily to provide analytics to our end users.
Take advantage of our continuous integration and deployment.
Participate in technical design and peer review for new projects.
Qualifications
Experience using Snowflake and Google Cloud Platform preferred.
Experience with AWS ecosystem. Some preferred services are Redshift, RDS, S3, and SWF.
Proven experience with ETL frameworks (Airflow, Luigi, or our own open sourced garcon ).
Expertise with at least one distributed data stores (Redshift, Cassandra, Snowflake).
Familiarity with noSQL technologies (mongoDB, DynamoDB).
Proficient in scripting language of choice. Python is strongly preferred, PHP a plus.
Highly proficient in writing SQL for a relational datastore (MySQL, PostgreSQL).
Knowledge of technologies that can deal with Big Data is a Big Plus (Kafka, Spark, Hive, Hadoop/MapReduce).
Ability to write automated tests (unit, functional, and integration) to ensure code works as expected.
Desire to collaborate with other engineers through peer code reviews.
Deep understanding of data structures and schema design.
Detail-oriented, proactive problem solving skills.","Experience using Snowflake and Google Cloud Platform preferred. Experience with AWS ecosystem. Some preferred services are Redshift, RDS, S3, and SWF. Proven experience with ETL frameworks  Airflow, Luigi, or our own open sourced garcon  . Expertise with at least one distributed data stores  Redshift, Cassandra, Snowflake . Familiarity with noSQL technologies  mongoDB, DynamoDB . Proficient in scripting language of choice. Python is strongly preferred, PHP a plus. Highly proficient in writing SQL for a relational datastore  MySQL, PostgreSQL . Knowledge of technologies that can deal with Big Data is a Big Plus  Kafka, Spark, Hive, Hadoop/MapReduce . Ability to write automated tests  unit, functional, and integration  to ensure code works as expected. Desire to collaborate with other engineers through peer code reviews. Deep understanding of data structures and schema design. Detail-oriented, proactive problem solving skills.  Create and maintain systems to load and transform very large data sets from digital media retailers  iTunes, Spotify, YouTube, etc  as well as social media sources. Work with a cross-functional team to create data-driven insights and reports for business stakeholders. Work with other members of the team to create customer-facing analytics tools and visualizations. Process millions of rows of data daily to provide analytics to our end users. Take advantage of our continuous integration and deployment. Participate in technical design and peer review for new projects.  ","Experience using Snowflake and Google Cloud Platform preferred. with AWS ecosystem. Some preferred services are Redshift, RDS, S3, SWF. Proven experience ETL frameworks Airflow, Luigi, or our own open sourced garcon . Expertise at least one distributed data stores Cassandra, Familiarity noSQL technologies mongoDB, DynamoDB Proficient in scripting language of choice. Python is strongly preferred, PHP a plus. Highly proficient writing SQL for relational datastore MySQL, PostgreSQL Knowledge that can deal Big Data Plus Kafka, Spark, Hive, Hadoop/MapReduce Ability to write automated tests unit, functional, integration ensure code works as expected. Desire collaborate other engineers through peer reviews. Deep understanding structures schema design. Detail-oriented, proactive problem solving skills. Create maintain systems load transform very large sets from digital media retailers iTunes, Spotify, YouTube, etc well social sources. Work cross-functional team create data-driven insights reports business stakeholders. members the customer-facing analytics tools visualizations. Process millions rows daily provide end users. Take advantage continuous deployment. Participate technical design review new projects.","Experience using Snowflake Google Cloud Platform preferred. AWS ecosystem. Some preferred services Redshift, RDS, S3, SWF. Proven experience ETL frameworks Airflow, Luigi, open sourced garcon . Expertise least one distributed data stores Cassandra, Familiarity noSQL technologies mongoDB, DynamoDB Proficient scripting language choice. Python strongly preferred, PHP plus. Highly proficient writing SQL relational datastore MySQL, PostgreSQL Knowledge deal Big Data Plus Kafka, Spark, Hive, Hadoop/MapReduce Ability write automated tests unit, functional, integration ensure code works expected. Desire collaborate engineers peer reviews. Deep understanding structures schema design. Detail-oriented, proactive problem solving skills. Create maintain systems load transform large sets digital media retailers iTunes, Spotify, YouTube, etc well social sources. Work cross-functional team create data-driven insights reports business stakeholders. members customer-facing analytics tools visualizations. Process millions rows daily provide end users. Take advantage continuous deployment. Participate technical design review new projects."
264,Data Engineer,Google Data Engineer,"New York, NY 10011",New York,NY,"Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet todayâs high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps â Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform. Multi-cloud experience a plus.   Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP DevOps an platform. Multi-cloud a plus. Proven ability to build, manage and foster team-oriented environment work creatively analytically in problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","Minimum 3 years previous Consulting client service delivery experience Google GCP DevOps platform. Multi-cloud plus. Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
265,Data Engineer,Master Data Management Engineer,"New York, NY 10041",New York,NY,"Bruin helps organizations transition their telecom, data and wireless services when business needs change. From online ordering to white glove support, we enable full lifecycle control of your telecom.

Weâre not like other teams. The Telecommunication Expense Management (TEM) space is filled with archaic software and boring slogans. Bruin is blazing a new path and building a team of experts with a passion for creating great software products and outstanding customer relationships.

We have high expectations and a career with Bruin means challenging yourself to always be better.

Role
Bruin is looking for a senior level, master data engineer to help build and enhance capabilities on our web app experience, built using JavaScript based libraries and an MSSQL / SOLR backend. Our teamâs charter is to revolutionize our current UX, while keeping up with the ever-growing user-base. You will be working in a small, focused scrum team obsessed with creating elegant, yet simple user-interaction for complex workflows that will delight our customers. You should be able to seamlessly integrate into agile teams and work closely with all facets of the organization (products, back-end development, QA, UX, and Design) to achieve high quality results. This position will support the application by monitoring and analyzing master data, key data, and master relationship data within the organization.

This high impact, high visibility role is responsible for ensuring all master data is accurate, complete and consistent across the enterprise. The team drives development of policies and procedures in concert with process owners and governance councils, conducts audits, performs analysis, and recommends and implements improvements. The team supports current implementation projects and performs ongoing guidance after through the production environment.

Ensures master data integrity in key systems as well as maintaining the processes to support the data quality.
Identifies areas for data quality improvements and helps to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design strategies.
Ensures quality of master data in key systems, as well as, development and documentation of processes with other functional data owners to support ongoing maintenance and data integrity.
In collaboration with subject matter experts and data stewards, defines and implements data strategy, policies, controls, and programs to ensure the enterprise data is accurate, complete, secure, and reliable.
Provides assistance in resolving data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design strategies collaborating with subject matter experts (SMEs) and data stewards.
Manages, analyzes, and resolves data initiative issues and manages revisions needed to best meet internal and customer requirements while adhering to published data standards.
Assists in data management, governance, and data quality of master data requirements with other functional data owners to ensure functional master data integrity across the operation of financial systems is consistent and meets stated business rules and requirements.
Work closely with the business/IT to ensure alignment of master data rules and the operations of the application meet all requirements.
Essential Job Functions
Defines, designs, and builds dimensional databases to meet business needs.
Assists in the application and implementation procedures of data standards and guidelines on data ownership, coding structures, and data replication to ensure access to and integrity of data sets.
Conducts data cleaning to rid the system of old, unused data, or duplicate data for better management and quicker access.
Researches, coordinates and installs software releases and database integration to ensure
Develops and implements strategies to translate business requirements and models into feasible and acceptable data warehouse designs to ensure that business needs are met.
Provides data consulting in support of business and information technology initiatives to clients to improve client database systems.
Skills/Qualifications
Essential
Excellent attention to detail and accuracy; strong critical self-review skills.
Demonstrates ability to deliver communication which is clear, concise, and relevant to audience through appropriate methods and tools.
Appreciates what constitutes good customer service and displays consistent commitment to delivering.
Exhibits a high degree of professionalism.
Organized methodical application of established data governance standards.
Proactive approach to role and problem solving; solution rather than problem focused.
Able to work collaboratively and communicate effectively with key stakeholders both within and outside of the MDM team to get the job done.
Comfortable pushing back with customers and stakeholders to protect the integrity of the data management process.
Inquisitive and thorough in approach.
Displays a passion for working in master data management.
Self-motivated, flexible, with the ability to deal with high levels of complexity, change and evolving processes, often at short notice.
Desirable
Experience maintaining master data in MS SQL.
Experience with full stack development, preferably in one or more of OO programming languages, like C#, Java, C++
Hands-on, professional experience with consuming and building RESTful web-services.
Experience in working in an agile development environment
Demonstrated mid-level or above proficiency with MS Office Excel, analyzing and manipulating large datasets through formulas and macros.
Experience participating in technology implementation projects.
Proficiency in JIRA, Atlassian, and Git.
Strong analytical abilities with experience extracting data and developing reports.
Strong interest in systems with demonstrable systems mindset and exposure to a large enterprise systems.
Understanding of SDLC and project management methodology.
Experience defining, writing and implementing business processes and data standards in one or more data domains.
CPA, Six Sigma, PMP, or similar.
Education and Experience
Essential
2+ years of business experience
2+ years of analytical experience
Bachelorâs Degree in relevant field of study or within one year of completing Bachelorâs Degree and with 3+ years of relevant work experience.
Desirable
Exposure to the business functions of a telecommunications enterprise from an operations, finance, supply chain, or business analysis perspective.
Masterâs Degree in Computer Science and/or Data Science
Job Type: Full-time/Permanent
Job Location: New York, NY

Bruin participates in the E-Verify Program and visa sponsorship

To apply, send a cover letter and resume/CV to jobs@bruin.com.","Excellent attention to detail and accuracy; strong critical self-review skills. Demonstrates ability to deliver communication which is clear, concise, and relevant to audience through appropriate methods and tools. Appreciates what constitutes good customer service and displays consistent commitment to delivering. Exhibits a high degree of professionalism. Organized methodical application of established data governance standards. Proactive approach to role and problem solving; solution rather than problem focused. Able to work collaboratively and communicate effectively with key stakeholders both within and outside of the MDM team to get the job done. Comfortable pushing back with customers and stakeholders to protect the integrity of the data management process. Inquisitive and thorough in approach. Displays a passion for working in master data management. Self-motivated, flexible, with the ability to deal with high levels of complexity, change and evolving processes, often at short notice.  Excellent attention to detail and accuracy; strong critical self-review skills. Demonstrates ability to deliver communication which is clear, concise, and relevant to audience through appropriate methods and tools. Appreciates what constitutes good customer service and displays consistent commitment to delivering. Exhibits a high degree of professionalism. Organized methodical application of established data governance standards. Proactive approach to role and problem solving; solution rather than problem focused. Able to work collaboratively and communicate effectively with key stakeholders both within and outside of the MDM team to get the job done. Comfortable pushing back with customers and stakeholders to protect the integrity of the data management process. Inquisitive and thorough in approach. Displays a passion for working in master data management. Self-motivated, flexible, with the ability to deal with high levels of complexity, change and evolving processes, often at short notice.   2+ years of business experience 2+ years of analytical experience Bachelorâs Degree in relevant field of study or within one year of completing Bachelorâs Degree and with 3+ years of relevant work experience.  ","Excellent attention to detail and accuracy; strong critical self-review skills. Demonstrates ability deliver communication which is clear, concise, relevant audience through appropriate methods tools. Appreciates what constitutes good customer service displays consistent commitment delivering. Exhibits a high degree of professionalism. Organized methodical application established data governance standards. Proactive approach role problem solving; solution rather than focused. Able work collaboratively communicate effectively with key stakeholders both within outside the MDM team get job done. Comfortable pushing back customers protect integrity management process. Inquisitive thorough in approach. Displays passion for working master management. Self-motivated, flexible, deal levels complexity, change evolving processes, often at short notice. 2+ years business experience analytical Bachelorâs Degree field study or one year completing 3+ experience.","Excellent attention detail accuracy; strong critical self-review skills. Demonstrates ability deliver communication clear, concise, relevant audience appropriate methods tools. Appreciates constitutes good customer service displays consistent commitment delivering. Exhibits high degree professionalism. Organized methodical application established data governance standards. Proactive approach role problem solving; solution rather focused. Able work collaboratively communicate effectively key stakeholders within outside MDM team get job done. Comfortable pushing back customers protect integrity management process. Inquisitive thorough approach. Displays passion working master management. Self-motivated, flexible, deal levels complexity, change evolving processes, often short notice. 2+ years business experience analytical Bachelorâs Degree field study one year completing 3+ experience."
266,Data Engineer,Data Engineer â Royalties,"New York, NY 10011",New York,NY,"We are looking for an experienced software engineer to join our Financial Engineering organisation. Our mission is to build the technical platform that underpins our key company decisions, revenue and royalty calculations, financial performance management and our ability to scale out to new markets and products. As a software engineer for our data products, you will join a team of experienced software engineers and data scientists. Some of the challenges we tackle are:

Design and implement complex logic for calculating royalties from many products and hundreds of tailor-made license contracts
Design large scale data pipelines (billions and billions of data points) that can retrospectively correct metrics over a long period of time without reprocessing huge amounts of data
Design, implement and operate our company core datasets that have extreme requirements on scalability, flexibility, and quality
Evolve and scale our fraud platform by developing rules and machine learning models that detect and continuously adapt to ever-shifting behaviors, swiftly mitigating their effects on the company.
Continuously consume and produce massive amounts of data while optimising for speed, accuracy, and quality
Innovate our data products to create a single coherent platform with sources of truth that serve a plethora of stakeholders from Spotify feature teams to our ad sales organisation
Above all, working as a software engineer in Financial Engineering will challenge your design, quality and problem-solving skills to build robust, highly distributed and scalable data processing systems and pipelines.

What you will do
Apply your expertise in software engineering to design and implement data products that meet extreme requirements on quality and scalability
Work closely with cross-functional teams of data and backend engineers, scientists, user researchers, product managers and designers
Research and Innovate what fuels many of Spotifyâs critical financial systems and product features such as Daily Mixes and Royalty calculations
Gaining technical expertise in building a data platform at scale to solve business, product and technical use cases
Getting hands-on experience with Google Cloud Platform and technology/languages such as BigQuery, Scala, Scio and Docker
Working hand-in-hand with the data science community to understand various user or content trends that influence product changes and customer acquisition strategies
Cross departmental exposure and flexibility to engage with many teams in the company â both in Stockholm and New York
Communicate insights and recommendations to key stakeholders, engineering and product partners
Who you are
Masterâs degree in Computer Science or Electrical Engineering
4+ years of professional software engineering and programming experience (Java, C++, Scala)
3+ years of architecture and design (patterns, reliability, scalability, quality) of complex systems
Advanced coding skills and practices (concurrency, distributed systems, functional principles, performance optimization)
Professional experience working in an agile environment
Strong analytical and problem solving ability
Strong written and verbal communication skills
Experience in operating and maintaining production grade software
Comfortable with tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions
Preferred Skills
Deep knowledge of software engineering best practices
Experience in mentoring and leading junior engineers
Experience in serving as the technical lead for complex software development projects
Experience with large scale distributed data technologies and tools
Strong coding skills for analytics and data engineering (Scala, Java and Python)
Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Googleâs Cloud Platform
T-Shaped. Your primary area is data engineering but you are comfortable working in a second area such as data presentation, backend engineering or front-end development
Experience working in a large scale, global consumer product company, in an engineering or insights role
Understands how to translate business requirements to technical architectures and designs
Comfortable communicating with stakeholders (customers, product managers, C-level management)"," Deep knowledge of software engineering best practices Experience in mentoring and leading junior engineers Experience in serving as the technical lead for complex software development projects Experience with large scale distributed data technologies and tools Strong coding skills for analytics and data engineering  Scala, Java and Python  Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Googleâs Cloud Platform T-Shaped. Your primary area is data engineering but you are comfortable working in a second area such as data presentation, backend engineering or front-end development Experience working in a large scale, global consumer product company, in an engineering or insights role Understands how to translate business requirements to technical architectures and designs Comfortable communicating with stakeholders  customers, product managers, C-level management    ","Deep knowledge of software engineering best practices Experience in mentoring and leading junior engineers serving as the technical lead for complex development projects with large scale distributed data technologies tools Strong coding skills analytics Scala, Java Python performing analysis datasets a cloud based-environment, preferably an understanding Googleâs Cloud Platform T-Shaped. Your primary area is but you are comfortable working second such presentation, backend or front-end scale, global consumer product company, insights role Understands how to translate business requirements architectures designs Comfortable communicating stakeholders customers, managers, C-level management","Deep knowledge software engineering best practices Experience mentoring leading junior engineers serving technical lead complex development projects large scale distributed data technologies tools Strong coding skills analytics Scala, Java Python performing analysis datasets cloud based-environment, preferably understanding Googleâs Cloud Platform T-Shaped. Your primary area comfortable working second presentation, backend front-end scale, global consumer product company, insights role Understands translate business requirements architectures designs Comfortable communicating stakeholders customers, managers, C-level management"
267,Data Engineer,Data Engineer / Sr. Data Engineer,"New York, NY",New York,NY,"Company: AllazoHealth
Location: New York City

AllazoHealth, one of the fastest growing health-tech startups in NYC, combines
behavioral science with data mining and machine learning to help stakeholders across
the healthcare spectrum influence patients to adopt healthier behaviors. Our propriety
AI platform, AllazoEngine TM provides individualized intervention targeting for medication
adherence, disease management, and wellness programs.

Position: Data Engineer / Sr. Data Engineer

We are currently seeking an experienced data engineer with a background of data ware house modeling and ETL development.

Requirements:

2-4 years of experience in advanced SQL
Experience in SSIS, Talend or any other ETL tools.
Experience in performance tuning and reports development.
Demonstrated success in high growth and early-stage environment
Demonstrated GSD ""Get Stuff Done"" attitude and results
Strong influencer and communicator across all levels of the organization
Detail and metric-oriented
Proficient in Microsoft Office and technology

Responsibilities:

Provide recommendations on data team roles and responsibilities as the company continues to grow
Help manage technical planning for all client implementations to ensure the data team is successfully meeting deadline and project deliverables
Identifying areas of opportunities and providing machine learning solutions to enhance AllazoEngine
Build client reporting from performance monitoring to ad hoc requests
Provide planning and continuous development of the database architecture

Benefits:

Very competitive compensation package including cash and equity stock options
Medical/Dental/Vision Benefits
Flat organizational style which empowers everyone in the company to help achieve both company and personal goals
Weekly team outings and high focus on continuously building team rapport and culture as we continue on this incredible growth curve

Company description:
Allazo Health is a high-growth, profitable healthtech start-up founded in 2012 that is leading the development and deployment of revolutionary analytical solutions which is helping patient outcomes and medical costs by influencing patients to adopt healthier behaviors.

If you are passionate about healthcare, are driven to save lives while delivering tremendous value to Fortune 100 customers, and want to make a significant impact within a tremendously fast-growing company, then this is the job for you.

Our customers include some of the largest pharma companies, PBMs, payors, and providers. Backed by extremely sophisticated investors and a strong management team, including a CEO and Founder who built medication adherence programs at one of the industry's leaders, we invite to join us as we revolutionize analytics in the healthcare industry.","   Provide recommendations on data team roles and responsibilities as the company continues to grow Help manage technical planning for all client implementations to ensure the data team is successfully meeting deadline and project deliverables Identifying areas of opportunities and providing machine learning solutions to enhance AllazoEngine Build client reporting from performance monitoring to ad hoc requests Provide planning and continuous development of the database architecture    2-4 years of experience in advanced SQL Experience in SSIS, Talend or any other ETL tools. Experience in performance tuning and reports development. Demonstrated success in high growth and early-stage environment Demonstrated GSD ""Get Stuff Done"" attitude and results Strong influencer and communicator across all levels of the organization Detail and metric-oriented Proficient in Microsoft Office and technology ","Provide recommendations on data team roles and responsibilities as the company continues to grow Help manage technical planning for all client implementations ensure is successfully meeting deadline project deliverables Identifying areas of opportunities providing machine learning solutions enhance AllazoEngine Build reporting from performance monitoring ad hoc requests continuous development database architecture 2-4 years experience in advanced SQL Experience SSIS, Talend or any other ETL tools. tuning reports development. Demonstrated success high growth early-stage environment GSD ""Get Stuff Done"" attitude results Strong influencer communicator across levels organization Detail metric-oriented Proficient Microsoft Office technology","Provide recommendations data team roles responsibilities company continues grow Help manage technical planning client implementations ensure successfully meeting deadline project deliverables Identifying areas opportunities providing machine learning solutions enhance AllazoEngine Build reporting performance monitoring ad hoc requests continuous development database architecture 2-4 years experience advanced SQL Experience SSIS, Talend ETL tools. tuning reports development. Demonstrated success high growth early-stage environment GSD ""Get Stuff Done"" attitude results Strong influencer communicator across levels organization Detail metric-oriented Proficient Microsoft Office technology"
268,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Using neuroscience-based assessments and machine learning algorithms, pymetrics is reinventing the recruiting industry by matching candidates to jobs and companies where they are most likely to succeed. We are leading the charge in an evolving industry, and growing our amazing team to support the mission of using data to unleash one's full potential.

We are looking for a talented Data Engineer to help scale all aspects of our business. You will have the opportunity to work with multiple teams to design and build data driven solutions. Therefore the ideal candidate will be able to wear multiple hats to solve a wide variety of data related challenges while working alongside both technical and non-technical team members.
WHAT YOUâLL DO
Build batch and real-time data pipelines
Collaborate with data scientist and machine learning experts to optimize and scale the predictive modeling process
Design well-structured APIs, services, and architectures for new project objectives
Help improve and expand internal tools for data delivery, access, and analysis
Analyze and visualize data for both internal and client facing use cases
Data quality control
Work with team leads to prioritize business and technology needs
Find and use the right tool for the job and integrate with existing architecture
CURRENT TECHNOLOGIES
Python/Django/Flask
Javascript/Angular/React
Jupyter
AWS/Azure
MySQL/PostgresSQL/Redshift/MongoDB/CosmosDB
RabbitMQ/Celery
Airflow
Docker/Packer/Terraform
Requirements
Highly proficient in Python
Proficient working with data and distributed systems
Experience with Python scientific libraries such as SciPy, Scikit, Pandas, and NumPy
Experience conducting methods using any of the following: machine learning, predictive modeling, statistical inference, experimental design, data mining, and optimization
Experience in Linux/Unix environment and shell scripting
Experience with ETL
Deep understanding of data structures and schema design
Familiarity with both SQL and NoSQL technologies
Experience with AWS and Azure platforms a plus
Knowledgeable about data modeling, data access, and data storage techniques
Critical thinking: ability to track down complex data and engineering issues, evaluate different algorithmic approaches, and analyze data to solve problems
Creativity: conceive of new data driven products, features, and technologies
Benefits
Health Care Plan (Medical, Dental & Vision)
Paid Time Off (Vacation, Sick & Public Holidays)
Family Leave (Maternity, Paternity)
Training & Development
Wellness Resources
Stock Option Plan
Transportation Reimbursement","     Highly proficient in Python Proficient working with data and distributed systems Experience with Python scientific libraries such as SciPy, Scikit, Pandas, and NumPy Experience conducting methods using any of the following  machine learning, predictive modeling, statistical inference, experimental design, data mining, and optimization Experience in Linux/Unix environment and shell scripting Experience with ETL Deep understanding of data structures and schema design Familiarity with both SQL and NoSQL technologies Experience with AWS and Azure platforms a plus Knowledgeable about data modeling, data access, and data storage techniques Critical thinking  ability to track down complex data and engineering issues, evaluate different algorithmic approaches, and analyze data to solve problems Creativity  conceive of new data driven products, features, and technologies ","Highly proficient in Python Proficient working with data and distributed systems Experience scientific libraries such as SciPy, Scikit, Pandas, NumPy conducting methods using any of the following machine learning, predictive modeling, statistical inference, experimental design, mining, optimization Linux/Unix environment shell scripting ETL Deep understanding structures schema design Familiarity both SQL NoSQL technologies AWS Azure platforms a plus Knowledgeable about access, storage techniques Critical thinking ability to track down complex engineering issues, evaluate different algorithmic approaches, analyze solve problems Creativity conceive new driven products, features,","Highly proficient Python Proficient working data distributed systems Experience scientific libraries SciPy, Scikit, Pandas, NumPy conducting methods using following machine learning, predictive modeling, statistical inference, experimental design, mining, optimization Linux/Unix environment shell scripting ETL Deep understanding structures schema design Familiarity SQL NoSQL technologies AWS Azure platforms plus Knowledgeable access, storage techniques Critical thinking ability track complex engineering issues, evaluate different algorithmic approaches, analyze solve problems Creativity conceive new driven products, features,"
269,Data Engineer,"Data Engineer, BI Engineering Team (Python/SQL)","New York, NY",New York,NY,"About the Team:
Vimeo is searching for an experienced Python and SQL software engineer for its Business Intelligence team. This is an opportunity to design and maintain a robust, scalable and sustainable enterprise data platform with other members of the Business Intelligence team. The ideal candidate will enjoy working on a variety of projects at once, with lots of different software services and data sources, all orchestrated with Python code.

Our platform has robust analytics tools that can tell video creators where people are from, where they click, and even track where in a video someone stopped watchingâgiving you insight into creating engaging content in the future.

While working as a peer to both technical and non-technical staff throughout the company, you will drive the improvement process of both our products and our business operations. You will help define data access and discoverability requirements and work to create infrastructure and services that provide access to event streams.

What you'll do:

Work closely with other BI engineers, BI analysts, and business stakeholders to understand and plan technical requirements for BI projects
Provide decision analysis and decision support to business stakeholders using BI and other data
Learn quickly and deeply about Vimeo's software platform, desktop and mobile applications
Contribute software designs, code, tooling, testing, and operational support to a multi-terabyte BI data platform
Collaborative work: iterative development, design and code review sessions
Independent work: author and maintain tools for other developers, plus regular hackweeks to develop for the Vimeo platform

Skills and knowledge you should possess:

3+ years of engineering experience in a fast-paced environment; 1+ years of experience in scalable data architecture, fault-tolerant ETL, and monitoring of data quality in the cloud
Proficiency in SQL
Proficiency in Python
Proficiency with modern source control systems, especially Git
Experience working with non-technical business stakeholders on technical projects

Bonus Points (Nice Skills to Have, but Not Needed):

Airflow, Celery, or other Python-based task processing systems
Cloud-based devops: AWS or Google Cloud Platform
Relational database design
Kafka, Kinesis, PubSub and other durable, scalable messaging systems
Spark
Pandas or R
Redshift, Vertica, Snowflake, CitusDB, or other distributed columnar-store databases
Experience with Mixpanel, Keen.io, Snowplow, or similar event tracking systems
Experience with any distributed map/reduce framework, in Hadoop or otherwise
Basic Linux/Unix system administration skills

About us:
At Vimeo, our mission is to empower video creators to tell exceptional stories and connect with their audiences and communities. Home to more than 90 million members in over 150 countries, Vimeo is the world's largest ad-free open video platform, providing powerful tools to host, share and sell videos in the highest quality possible.

We work hard to enable creators of all kinds to succeed, and to that end, we prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and creativity. We're committed to building a company and a community where people thrive by being themselves and are inspired to do their best work every day.

Vimeo is based in New York City, with additional offices in Europe and India. Vimeo is an operating business of IAC (NASDAQ: IAC). Learn more at www.vimeo.com.","  3+ years of engineering experience in a fast-paced environment; 1+ years of experience in scalable data architecture, fault-tolerant ETL, and monitoring of data quality in the cloud Proficiency in SQL Proficiency in Python Proficiency with modern source control systems, especially Git Experience working with non-technical business stakeholders on technical projects    ","3+ years of engineering experience in a fast-paced environment; 1+ scalable data architecture, fault-tolerant ETL, and monitoring quality the cloud Proficiency SQL Python with modern source control systems, especially Git Experience working non-technical business stakeholders on technical projects","3+ years engineering experience fast-paced environment; 1+ scalable data architecture, fault-tolerant ETL, monitoring quality cloud Proficiency SQL Python modern source control systems, especially Git Experience working non-technical business stakeholders technical projects"
270,Data Engineer,.NET Developer/Data Engineer,"New York, NY 10019",New York,NY,"Company Overview
TrueChoice Solutions is a private, Manhattan based, marketing software company which develops visually interactive, web-based applications for brand-name Fortune 500 companies. Our client base is diverse and includes large automotive companies, consumer electronics manufacturers and health care services companies. Our applications typically appear on major, highly trafficked public websites.
Position Overview
We are looking for a .NET Developer with experience working with business intelligence software to help support and expand our client analytics capabilities. This candidate will collaborate with our data analysts, application delivery teams, and software engineers to automate, scale, and enhance our data engineering, visualizations, and business intelligence competencies.
Responsibilities:
Contribute to the overall design, scope, and roadmap of TrueChoice analytics capabilities
Develop and deliver software solutions to meet the evolving requirements of our business users
Work with business users and analysts to gather requirements and produce detailed functional design specifications
Work with the internal IT team & business users to identify requirements, create prototypes, develop and test solutions
Develop, validate, and maintain the core functionality and capabilities associated with Spotfire visualizations and dashboards
Build and maintain documentation, procedures and guidelines around analytical solution development
Help manage execution of all Spotfire version upgrades
Requirements:
Bachelorâs degree in a technical related field (e.g. Computer Science, Engineering, Math)
3+ years of experience in C# and .NET development
Hands-on experience in database/SQL programming and Oracle
Proven success in contributing to a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Ability to work effectively both independently and as part of an integrated team
Excellent communication skills: written and verbal
Strong team work and interpersonal skills
We would like to see:
 2+ years of experience working with TIBCO Spotfire API & TIBCO Spotfire
Prior experience implementing business intelligence solutions
Experience designing complex reports and TIBCO Spotfire visualizations
Experience with additional programming languages, including Python and JavaScript (Web and Node)
Experience with Linux
Experience with AWS and cloud technologies
All applicants MUST have a permanent legal right to work in the United States.
Qualified candidates should live within a reasonable commuting distance to mid-town Manhattan; TrueChoice Solutions will not cover relocation costs.

How to Apply:
Please send resumes to techjobs@truechoicesolutions.com citing "".NET Developer/Data Engineer"" in the subject line.","   Contribute to the overall design, scope, and roadmap of TrueChoice analytics capabilities Develop and deliver software solutions to meet the evolving requirements of our business users Work with business users and analysts to gather requirements and produce detailed functional design specifications Work with the internal IT team & business users to identify requirements, create prototypes, develop and test solutions Develop, validate, and maintain the core functionality and capabilities associated with Spotfire visualizations and dashboards Build and maintain documentation, procedures and guidelines around analytical solution development Help manage execution of all Spotfire version upgrades   Bachelorâs degree in a technical related field  e.g. Computer Science, Engineering, Math  3+ years of experience in C  and .NET development Hands-on experience in database/SQL programming and Oracle Proven success in contributing to a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Ability to work effectively both independently and as part of an integrated team Excellent communication skills  written and verbal Strong team work and interpersonal skills","Contribute to the overall design, scope, and roadmap of TrueChoice analytics capabilities Develop deliver software solutions meet evolving requirements our business users Work with analysts gather produce detailed functional design specifications internal IT team & identify requirements, create prototypes, develop test Develop, validate, maintain core functionality associated Spotfire visualizations dashboards Build documentation, procedures guidelines around analytical solution development Help manage execution all version upgrades Bachelorâs degree in a technical related field e.g. Computer Science, Engineering, Math 3+ years experience C .NET Hands-on database/SQL programming Oracle Proven success contributing team-oriented environment ability work creatively analytically problem-solving Ability effectively both independently as part an integrated Excellent communication skills written verbal Strong interpersonal","Contribute overall design, scope, roadmap TrueChoice analytics capabilities Develop deliver software solutions meet evolving requirements business users Work analysts gather produce detailed functional design specifications internal IT team & identify requirements, create prototypes, develop test Develop, validate, maintain core functionality associated Spotfire visualizations dashboards Build documentation, procedures guidelines around analytical solution development Help manage execution version upgrades Bachelorâs degree technical related field e.g. Computer Science, Engineering, Math 3+ years experience C .NET Hands-on database/SQL programming Oracle Proven success contributing team-oriented environment ability work creatively analytically problem-solving Ability effectively independently part integrated Excellent communication skills written verbal Strong interpersonal"
271,Data Engineer,SR Data Engineer,"New York, NY 10019",New York,NY,"Job Description

Publicis Spine is looking for talented Sr. Data Engineer for an exciting opportunity on the data engineering team. The successful candidate will be involved with designing workflows for data and analytics tools that are a big part of the road-map for 2019 while managing data and infrastructure to efficiently query data in the billions. Candidates will be considered based on their ability to design large distributed technical solutions, architect, manage, monitor and optimize data pipeline projects resulting in actionable data and data pipelines which support the larger organization.

Core Responsibilities:

In this role, you will be expected to drive Publicis Spine's mission to grow our clients' businesses through the transformative application of data. Your key priorities will include but are not limited to:
Architect, Design and Maintain Data Pipelines through the lifecycle of the product.Optimize and Monitor existing data pipelines using AWS infrastructure.WritePython/Scala applications for data processing and job scheduling.Understand and Manage massive data-stores.Integrate products from data projects into APIs built in Ruby/RailsExpose large data setsEnjoy being challenged and solve complex problems on a daily basisDesign efficient and robust ETL workflowsManage real time streaming application and data flowInvestigate, procure and ramp up to new technologiesAbility to work in teams and collaborate with others to clarify requirementsBuild analytics tools that utilize the data pipelines to provide meaningful insights into data

Qualifications

null

Additional Information

All your information will be kept confidential according to EEO guidelines.","  Architect, Design and Maintain Data Pipelines through the lifecycle of the product.Optimize and Monitor existing data pipelines using AWS infrastructure.WritePython/Scala applications for data processing and job scheduling.Understand and Manage massive data-stores.Integrate products from data projects into APIs built in Ruby/RailsExpose large data setsEnjoy being challenged and solve complex problems on a daily basisDesign efficient and robust ETL workflowsManage real time streaming application and data flowInvestigate, procure and ramp up to new technologiesAbility to work in teams and collaborate with others to clarify requirementsBuild analytics tools that utilize the data pipelines to provide meaningful insights into data  ","Architect, Design and Maintain Data Pipelines through the lifecycle of product.Optimize Monitor existing data pipelines using AWS infrastructure.WritePython/Scala applications for processing job scheduling.Understand Manage massive data-stores.Integrate products from projects into APIs built in Ruby/RailsExpose large setsEnjoy being challenged solve complex problems on a daily basisDesign efficient robust ETL workflowsManage real time streaming application flowInvestigate, procure ramp up to new technologiesAbility work teams collaborate with others clarify requirementsBuild analytics tools that utilize provide meaningful insights","Architect, Design Maintain Data Pipelines lifecycle product.Optimize Monitor existing data pipelines using AWS infrastructure.WritePython/Scala applications processing job scheduling.Understand Manage massive data-stores.Integrate products projects APIs built Ruby/RailsExpose large setsEnjoy challenged solve complex problems daily basisDesign efficient robust ETL workflowsManage real time streaming application flowInvestigate, procure ramp new technologiesAbility work teams collaborate others clarify requirementsBuild analytics tools utilize provide meaningful insights"
272,Data Engineer,Data Engineer & Quality resource,"New York, NY",New York,NY,"Responsibilities
Interact with external / internal data providers to understand the nature of data.
Gather information on general data delivery schedule for all sources.
Build databases and schemas for data warehousing and analysis.
Develop ETLs to move data into the warehouse and analysis layers.
Develop logical checks in ETL process to check for duplicates, obvious data errors etc.
Judge whether an ETL needs to be a batch process or stream process and use appropriate libraries.
Schedule and maintain ETLs for different sources.
Structure / Re-Structure various schema objects and periodically check on query execution plans to maintain optimal performance.
Work with data scientists to ensure the data is formatted in a way that is optimal for their model.
Create and maintain schemas to store MMM results.
Serve as a go to person for ad hoc queries, schemas, views etc.

Technical Skills
Familiar with big data processing and concepts like MapReduce, spark RDDs etc.
Knowledge of cloud storage platforms on AWS, Azure etc.
Java, Python and Scala are required skill sets.
Good command over SQL, ETL Best practices and data warehousing concepts.
Familiarity with ELT.
Familiarity with various modes of file storage like Parquet, ORC, Avro etc.
Prior experience in working with Redshift/Snowflake a plus.

BHwwUR7NP8","  Familiar with big data processing and concepts like MapReduce, spark RDDs etc. Knowledge of cloud storage platforms on AWS, Azure etc. Java, Python and Scala are required skill sets. Good command over SQL, ETL Best practices and data warehousing concepts. Familiarity with ELT. Familiarity with various modes of file storage like Parquet, ORC, Avro etc. Prior experience in working with Redshift/Snowflake a plus.  Interact with external / internal data providers to understand the nature of data. Gather information on general data delivery schedule for all sources. Build databases and schemas for data warehousing and analysis. Develop ETLs to move data into the warehouse and analysis layers. Develop logical checks in ETL process to check for duplicates, obvious data errors etc. Judge whether an ETL needs to be a batch process or stream process and use appropriate libraries. Schedule and maintain ETLs for different sources. Structure / Re-Structure various schema objects and periodically check on query execution plans to maintain optimal performance. Work with data scientists to ensure the data is formatted in a way that is optimal for their model. Create and maintain schemas to store MMM results. Serve as a go to person for ad hoc queries, schemas, views etc.  ","Familiar with big data processing and concepts like MapReduce, spark RDDs etc. Knowledge of cloud storage platforms on AWS, Azure Java, Python Scala are required skill sets. Good command over SQL, ETL Best practices warehousing concepts. Familiarity ELT. various modes file Parquet, ORC, Avro Prior experience in working Redshift/Snowflake a plus. Interact external / internal providers to understand the nature data. Gather information general delivery schedule for all sources. Build databases schemas analysis. Develop ETLs move into warehouse analysis layers. logical checks process check duplicates, obvious errors Judge whether an needs be batch or stream use appropriate libraries. Schedule maintain different Structure Re-Structure schema objects periodically query execution plans optimal performance. Work scientists ensure is formatted way that their model. Create store MMM results. Serve as go person ad hoc queries, schemas, views","Familiar big data processing concepts like MapReduce, spark RDDs etc. Knowledge cloud storage platforms AWS, Azure Java, Python Scala required skill sets. Good command SQL, ETL Best practices warehousing concepts. Familiarity ELT. various modes file Parquet, ORC, Avro Prior experience working Redshift/Snowflake plus. Interact external / internal providers understand nature data. Gather information general delivery schedule sources. Build databases schemas analysis. Develop ETLs move warehouse analysis layers. logical checks process check duplicates, obvious errors Judge whether needs batch stream use appropriate libraries. Schedule maintain different Structure Re-Structure schema objects periodically query execution plans optimal performance. Work scientists ensure formatted way model. Create store MMM results. Serve go person ad hoc queries, schemas, views"
273,Data Engineer,Data Engineer,"New York, NY 10018",New York,NY,"CertiK is a world leading cybersecurity firm that focuses on smart contract analysis and blockchain auditing. CertiK engineers strive to develop the next-generation technologies to improve the reliability, stability, and scalability of large-scale computer systems.
CertiK is looking for talented Data Engineers to join our growing engineering team. Ideal candidates should have experience in designing/implementing big data infrastructures, building ETL pipelines and delivering BI/monitor/alert tools and applications.
Requirements:
A BS/MS degree in Computer Science or relevant field or equivalent professional experience with solid computer science fundamentals in object-oriented design, data structure, algorithms and database systems (SQL and NoSQL)
Experience building and optimizing âbig dataâ data pipelines, architectures and data sets; Strong analytic skills related to working with unstructured datasets
Experience with big data tools: Spark, Kafka; Experience with AWS cloud services: EC2, EMR, RDS, Redshift; Experience with stream-processing systems like Spark-Streaming
Proficient in coding languages such as Python, JavaScript, Scala and Shell scripts
Nice to have:
Passionate about Blockchain and on-chain/off-chain analysis, leverage data science skillsets to solve real-world problems
Experience in deploying, designing, and integrating with the AWS cloud environments to run big data queries
Hands-on experience on building the end-to-end Business Intelligence tools, i.e. able to mine the raw data into valuable datasets and present with data visualizations
Capable of analyzing Blockchain transactional data (Bitcoin and Ethereum), fulfill the business requirements on data traceability and solve the pain points of cryptocurrency AML/Compliance and KYT (Know Your Transaction)","     A BS/MS degree in Computer Science or relevant field or equivalent professional experience with solid computer science fundamentals in object-oriented design, data structure, algorithms and database systems  SQL and NoSQL  Experience building and optimizing âbig dataâ data pipelines, architectures and data sets; Strong analytic skills related to working with unstructured datasets Experience with big data tools  Spark, Kafka; Experience with AWS cloud services  EC2, EMR, RDS, Redshift; Experience with stream-processing systems like Spark-Streaming Proficient in coding languages such as Python, JavaScript, Scala and Shell scripts","A BS/MS degree in Computer Science or relevant field equivalent professional experience with solid computer science fundamentals object-oriented design, data structure, algorithms and database systems SQL NoSQL Experience building optimizing âbig dataâ pipelines, architectures sets; Strong analytic skills related to working unstructured datasets big tools Spark, Kafka; AWS cloud services EC2, EMR, RDS, Redshift; stream-processing like Spark-Streaming Proficient coding languages such as Python, JavaScript, Scala Shell scripts","A BS/MS degree Computer Science relevant field equivalent professional experience solid computer science fundamentals object-oriented design, data structure, algorithms database systems SQL NoSQL Experience building optimizing âbig dataâ pipelines, architectures sets; Strong analytic skills related working unstructured datasets big tools Spark, Kafka; AWS cloud services EC2, EMR, RDS, Redshift; stream-processing like Spark-Streaming Proficient coding languages Python, JavaScript, Scala Shell scripts"
274,Data Engineer,Senior Data Engineer,"New York, NY",New York,NY,"Summary:
BNY Mellon Investment Management is one of the world's leading investment management organizations and one of the top U.S. wealth managers. Our business encompasses BNY Mellon's affiliated investment management firms, wealth management organization, and global distribution companies. Our goal is to build and deliver investment and wealth management strategies and solutions to meet our clientsâ needs.
Drawing on deep expertise, we collaborate with our clients to tailor our best ideas and resources to meet their specific requirements. Through our global network we have developed a significant understanding of local requirements. We pride ourselves on providing dedicated service through our teams.
With extensive experience in anticipating and responding to the investment and financial needs of the world's governments, pension plan sponsors, corporations, foundations, endowments planned giving programs, advisors, intermediaries, individuals and families, and family offices, BNY Mellon Investment Management can help our clients reach their goals.
The Role:
Reporting to the Head of Investment Management (IM) Data Solutions, the Senior Data Engineer will lead, design and architect solutions to complex data integration, process re-engineering, data science, and automation problems. This is a unique opportunity to join the IM Data Solutions team, a small, multi-disciplinary team that is transforming the way information is created, used and communicated within BNY Mellon IM. The group works directly with BNY Mellonâs IM firms on strategic projects, creating valuable tools and insights for decision makers across portfolio management, sales, marketing, and other key areas of the business. The Data Solutions team capitalizes on opportunities across business lines using the best tools and practices, like machine learning, econometrics, agile development (scrum/kanban), and modern collaborative data platforms.
Responsibilities:
Lead data engineering and/or data science projects to support IM firms
Use domain expertise to understand business requirements and design the right solution
Build or implement data pipelines, databases, visualizations, and other data tools (hands-on)
Contribute to team in a wide range of technical areas by instituting new practices and staying abreast of the latest technical developments
Take initiative to lead/contribute to overall team efforts in software development, data engineering, data science, and technical consulting
Work closely with other data engineers and data scientists to improve processes and enable faster insight-generation from complex datasets
Qualifications

BA/BS Degree (advanced degrees and/or CFAs are great too)
10+ years general business experience, especially in financial services, asset management, and/or management consulting or similar environments
6+ years of data engineering, data architecture, data science, and/or software engineering experience
Excel proficiency
SQL proficiency
Python expertise
Experience with machine learning algorithms and techniques
Experience with modern data pipelines and/or data operating platforms (e.g. Dataiku, Alteryx, Spark)
Able to lead ad-hoc and structured product teams within an agile framework
Excellent interpersonal skills necessary to accomplish goals through others, including employees, peers, and other function/business areas of the company


BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer.
Minorities/Females/Individuals With Disabilities/Protected Veterans.

Primary Location: United States-New York-
Internal Jobcode: 70124
Job: Asset Management
Organization: IM Infrastructure-HR14826
Requisition Number: 1914660","BA/BS Degree  advanced degrees and/or CFAs are great too  10+ years general business experience, especially in financial services, asset management, and/or management consulting or similar environments 6+ years of data engineering, data architecture, data science, and/or software engineering experience Excel proficiency SQL proficiency Python expertise Experience with machine learning algorithms and techniques Experience with modern data pipelines and/or data operating platforms  e.g. Dataiku, Alteryx, Spark  Able to lead ad-hoc and structured product teams within an agile framework Excellent interpersonal skills necessary to accomplish goals through others, including employees, peers, and other function/business areas of the company  Lead data engineering and/or data science projects to support IM firms Use domain expertise to understand business requirements and design the right solution Build or implement data pipelines, databases, visualizations, and other data tools  hands-on  Contribute to team in a wide range of technical areas by instituting new practices and staying abreast of the latest technical developments Take initiative to lead/contribute to overall team efforts in software development, data engineering, data science, and technical consulting Work closely with other data engineers and data scientists to improve processes and enable faster insight-generation from complex datasets   ","BA/BS Degree advanced degrees and/or CFAs are great too 10+ years general business experience, especially in financial services, asset management, management consulting or similar environments 6+ of data engineering, architecture, science, software engineering experience Excel proficiency SQL Python expertise Experience with machine learning algorithms and techniques modern pipelines operating platforms e.g. Dataiku, Alteryx, Spark Able to lead ad-hoc structured product teams within an agile framework Excellent interpersonal skills necessary accomplish goals through others, including employees, peers, other function/business areas the company Lead science projects support IM firms Use domain understand requirements design right solution Build implement pipelines, databases, visualizations, tools hands-on Contribute team a wide range technical by instituting new practices staying abreast latest developments Take initiative lead/contribute overall efforts development, Work closely engineers scientists improve processes enable faster insight-generation from complex datasets","BA/BS Degree advanced degrees and/or CFAs great 10+ years general business experience, especially financial services, asset management, management consulting similar environments 6+ data engineering, architecture, science, software engineering experience Excel proficiency SQL Python expertise Experience machine learning algorithms techniques modern pipelines operating platforms e.g. Dataiku, Alteryx, Spark Able lead ad-hoc structured product teams within agile framework Excellent interpersonal skills necessary accomplish goals others, including employees, peers, function/business areas company Lead science projects support IM firms Use domain understand requirements design right solution Build implement pipelines, databases, visualizations, tools hands-on Contribute team wide range technical instituting new practices staying abreast latest developments Take initiative lead/contribute overall efforts development, Work closely engineers scientists improve processes enable faster insight-generation complex datasets"
275,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Overview
Data Engineer
At Hospital for Special Surgery our clinicians and scientists collaborate to deliver the most innovative care. Our specialized focus on orthopedics and rheumatology enables us to help patients get back to what they need and love to do reliably and efficiently. Our patients are overwhelmingly satisfied with the care they receive at our facilities. When you join us, you will become part of this legacy of commitment to the most cutting-edge research and coordinated care.

The ideal candidate must be passionate about the field of analytics and well versed in all aspects of an enterprise-wide analytics architecture. We are looking for Data Engineers who have a strong understanding of data pipelines, ready to work hard but most importantly curious. They will work collaboratively across the organization with clinical and operational leaders on a diverse range of projects to find insights that support our mission of providing world class musculoskeletal health.
As a Data Engineer You Will
Design, implement, and automate data flows
Assist in the implementation and administration of the analytics data lake
Work with partners and vendors on data integration projects
Design and create data models for new or existing data sources
Design and implement ETLs
Qualifications
Your Technical Skills and Experience
3+ yearsâ experience with ETL tool SQL Server Integration Services (SSIS) or similar ETL Tool
3+ yearsâ experience with SQL Server, strong expertise in T-SQL required
3+ yearsâ experience with Web Services: RESTful APIs, .NET WCF and SOAP
1+ yearsâ experience with .NET C# or other OO language required
Experience with Windows Server Operating Systems required
Experience with SSAS and building cubes required
Experience with statistics and machine learning tools preferred: (Python, R, Matlab or Spark)
Experience with public cloud preferred: AWS or Azure
Experience with Big Data or NoSQL technology preferred: Hadoop, Spark, AWS EMR, Hive, etc.
Experience in healthcare a plus
About You
You are passionate about data and exploring new technologies
You are a critical thinker who loves solving difficult problems
You are flexible and a natural team player
Education and Certifications
MS/BS in Computer Science or related field"," 3+ yearsâ experience with ETL tool SQL Server Integration Services  SSIS  or similar ETL Tool 3+ yearsâ experience with SQL Server, strong expertise in T-SQL required 3+ yearsâ experience with Web Services  RESTful APIs, .NET WCF and SOAP 1+ yearsâ experience with .NET C  or other OO language required Experience with Windows Server Operating Systems required Experience with SSAS and building cubes required Experience with statistics and machine learning tools preferred   Python, R, Matlab or Spark  Experience with public cloud preferred  AWS or Azure Experience with Big Data or NoSQL technology preferred  Hadoop, Spark, AWS EMR, Hive, etc. Experience in healthcare a plus  3+ yearsâ experience with ETL tool SQL Server Integration Services  SSIS  or similar ETL Tool 3+ yearsâ experience with SQL Server, strong expertise in T-SQL required 3+ yearsâ experience with Web Services  RESTful APIs, .NET WCF and SOAP 1+ yearsâ experience with .NET C  or other OO language required Experience with Windows Server Operating Systems required Experience with SSAS and building cubes required Experience with statistics and machine learning tools preferred   Python, R, Matlab or Spark  Experience with public cloud preferred  AWS or Azure Experience with Big Data or NoSQL technology preferred  Hadoop, Spark, AWS EMR, Hive, etc. Experience in healthcare a plus   MS/BS in Computer Science or related field ","3+ yearsâ experience with ETL tool SQL Server Integration Services SSIS or similar Tool Server, strong expertise in T-SQL required Web RESTful APIs, .NET WCF and SOAP 1+ C other OO language Experience Windows Operating Systems SSAS building cubes statistics machine learning tools preferred Python, R, Matlab Spark public cloud AWS Azure Big Data NoSQL technology Hadoop, Spark, EMR, Hive, etc. healthcare a plus MS/BS Computer Science related field","3+ yearsâ experience ETL tool SQL Server Integration Services SSIS similar Tool Server, strong expertise T-SQL required Web RESTful APIs, .NET WCF SOAP 1+ C OO language Experience Windows Operating Systems SSAS building cubes statistics machine learning tools preferred Python, R, Matlab Spark public cloud AWS Azure Big Data NoSQL technology Hadoop, Spark, EMR, Hive, etc. healthcare plus MS/BS Computer Science related field"
276,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of whatâs possibleâtogether.

Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 6,000 employees. We were named one of Fortuneâs 100 Best Companies to Work For in 2018 and are regularly recognized by our employees as a best place to work. You can find us in 28 cities across the U.S., U.K., and Canada.

Job Title:
Data Engineer

Data is served to hundreds of our clients and their customers. There is a big demand for analytics and data-freshness, so if you are not afraid of complex business problems, and you think traditional tools are slow, and you wonât stop looking for new approaches and technologies, then we have a position for you!

As a Data Engineer, you should have the expertise in the design, creation, management, and business use of large data sets to drive practical insights. You know and love working with analytics tools and can use your technical skills and creative approaches to help clients solve their most critical business challenges. This individual will be an integral part of New Yorkâs Data and Analytics Practice and, in addition to working with clients, collaborate closely with members of the Slalom team who are focused on data visualization, advanced analytics, data science, and data strategy.

Responsibilities:

Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions
Conduct and support white-boarding sessions, workshops, design sessions, and project meetings as needed, playing a key role in client relations
Design and build data extraction, transformation, and loading processes by writing custom data pipelines using either cloud native services (AWS/GCP/Azure) or using open source tools (like Airflow and Python)
Design, implement, and support an Enterprise Data platform (Data Lake, Data Warehouse, etc.) that can provide ad-hoc access to large scale structured, semi-structured, and/or unstructured datasets
Experience with both traditional (i.e. SSIS, Informatica, Talend) and modern (i.e. Dell Boomi) data integration iPaaS technologies
Highly self-motivated to deliver both independently and with strong team collaboration
Ability to creatively take on new challenges and work outside comfort zone
Strong written and oral communications along with presentation and interpersonal skills
Deliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data architecture and engineering space

Qualifications:

5+ years of experience in using SQL and databases in a business environment
5+ years of experience in custom ETL/ELT design, implementation, and maintenance
3+ years of experience with schema design and data modeling
3+ years of experience applying data architecture or engineering to solve real-world business problems
2+ years of experience with building integration and ingestion frameworks leveraging API based tools and platforms (i.e. Dell Boomi)
Manipulating/mining data from database tables (i.e. SQL Server, Redshift, Oracle)
SQL, ETL/ELT optimization, and analytics tools experience (i.e. R, HiveQL)
Prior implementation experience in building both batch and real-time/near real-time data ingestion frameworks using technologies like Kafka, AWS Kinesis etc.

Nice to have:

Implementation experience with various technologies under Hadoop eco-system (HDFS, Hive, HBase, Sqoop, Pig, Presto etc.) and Spark (PySpark preferred)
Experience with designing digital data platforms leveraging clickstream data from Adobe Analytics (Omniture/SiteCatalyst) or Google Analytics
Experience in languages such as Python and Spark
Experience working in Agile Scrum teams
Linux and Windows proficiency
Management consulting experience
Project management experience
Experience working with various verticals (i.e. insurance, retail, healthcare, financial services, technology)

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."," 5+ years of experience in using SQL and databases in a business environment 5+ years of experience in custom ETL/ELT design, implementation, and maintenance 3+ years of experience with schema design and data modeling 3+ years of experience applying data architecture or engineering to solve real-world business problems 2+ years of experience with building integration and ingestion frameworks leveraging API based tools and platforms  i.e. Dell Boomi  Manipulating/mining data from database tables  i.e. SQL Server, Redshift, Oracle  SQL, ETL/ELT optimization, and analytics tools experience  i.e. R, HiveQL  Prior implementation experience in building both batch and real-time/near real-time data ingestion frameworks using technologies like Kafka, AWS Kinesis etc.   Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions Conduct and support white-boarding sessions, workshops, design sessions, and project meetings as needed, playing a key role in client relations Design and build data extraction, transformation, and loading processes by writing custom data pipelines using either cloud native services  AWS/GCP/Azure  or using open source tools  like Airflow and Python  Design, implement, and support an Enterprise Data platform  Data Lake, Data Warehouse, etc.  that can provide ad-hoc access to large scale structured, semi-structured, and/or unstructured datasets Experience with both traditional  i.e. SSIS, Informatica, Talend  and modern  i.e. Dell Boomi  data integration iPaaS technologies Highly self-motivated to deliver both independently and with strong team collaboration Ability to creatively take on new challenges and work outside comfort zone Strong written and oral communications along with presentation and interpersonal skills Deliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data architecture and engineering space  ","5+ years of experience in using SQL and databases a business environment custom ETL/ELT design, implementation, maintenance 3+ with schema design data modeling applying architecture or engineering to solve real-world problems 2+ building integration ingestion frameworks leveraging API based tools platforms i.e. Dell Boomi Manipulating/mining from database tables Server, Redshift, Oracle SQL, optimization, analytics R, HiveQL Prior implementation both batch real-time/near real-time technologies like Kafka, AWS Kinesis etc. Collaborate engineers customers understand needs, capture requirements deliver complete BI solutions Conduct support white-boarding sessions, workshops, project meetings as needed, playing key role client relations Design build extraction, transformation, loading processes by writing pipelines either cloud native services AWS/GCP/Azure open source Airflow Python Design, implement, an Enterprise Data platform Lake, Warehouse, that can provide ad-hoc access large scale structured, semi-structured, and/or unstructured datasets Experience traditional SSIS, Informatica, Talend modern iPaaS Highly self-motivated independently strong team collaboration Ability creatively take on new challenges work outside comfort zone Strong written oral communications along presentation interpersonal skills Deliver transformative clients are aligned industry best practices thought leadership space","5+ years experience using SQL databases business environment custom ETL/ELT design, implementation, maintenance 3+ schema design data modeling applying architecture engineering solve real-world problems 2+ building integration ingestion frameworks leveraging API based tools platforms i.e. Dell Boomi Manipulating/mining database tables Server, Redshift, Oracle SQL, optimization, analytics R, HiveQL Prior implementation batch real-time/near real-time technologies like Kafka, AWS Kinesis etc. Collaborate engineers customers understand needs, capture requirements deliver complete BI solutions Conduct support white-boarding sessions, workshops, project meetings needed, playing key role client relations Design build extraction, transformation, loading processes writing pipelines either cloud native services AWS/GCP/Azure open source Airflow Python Design, implement, Enterprise Data platform Lake, Warehouse, provide ad-hoc access large scale structured, semi-structured, and/or unstructured datasets Experience traditional SSIS, Informatica, Talend modern iPaaS Highly self-motivated independently strong team collaboration Ability creatively take new challenges work outside comfort zone Strong written oral communications along presentation interpersonal skills Deliver transformative clients aligned industry best practices thought leadership space"
277,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Join Andrew Yang and the fight to put Humanity First as he runs for president in 2020!

As one of the most exciting and unique candidates in the Democratic presidential race, entrepreneur Andrew Yang sets out to bridge the divide in American society. In 2011, he founded Venture for America, an entrepreneurial non-profit that helped create 2,500 jobs in cities like Cleveland, Detroit, Baltimore, and Pittsburgh.

Once Andrew realized that new technology, such as artificial intelligence, threatened to eliminate one-third of all American jobs, he knew he had to do something. In The War on Normal People (2018), he explains the mounting crisis and makes the case for implementing a freedom dividend of $1,000 a month for every American adult: no strings attached. Most importantly, Andrew Yang understands that the future of America is not left, not right, but forward.

To learn more about the campaign, go to Yang2020.com.
You can also hear more from Andrew on the Joe Rogan Experience!

Every campaign claims to be data driven. After working for us, you'll realize everyone else is just fooling themselves.

To support this, we need data engineering talent to help build the data pipelines the campaign will rely on. We're integrating multiple data sources, many of which are difficult to work with.

We're looking for data engineers who enjoy automating data flows, know how to deal with messy data, and who don't mind doing a few (dozen) manual uploads if that's what the job takes.

We would prefer to hire people in NY or who can relocate, but we're open to remote work for the right candidate.
Skills:
Python & Pandas
Spark, MapReduce
AWS
SQL, Postgresql
Past ETL experience
Experience with data governance and ownership of data pipelines
Data is the lifeblood of the campaign, and as a data engineer you'll be responsible for keeping it moving. This is true in every campaign, but no other organization recognizes that fact or values their engineering staff to the extent that we do at Yang2020.

Yang2020 is committed to diversity among its staff, and recognizes that its continued success requires the highest commitment to obtaining and retaining a diverse staff that provides the best quality services to supporters and constituents. We strongly encourage diverse candidates to apply. Yang2020 is an Equal Opportunity Employer and does not discriminate on the basis of race, color, religion, sex, age, national origin, genetic information, protected veterans, marital or familial status, sexual orientation, gender identity or expression, disability status, criminal record information (except where permitted under applicable law), or any other category prohibited by local, state or federal law. This policy applies to all aspects of employment, including recruitment, placement, promotion, transfer, demotion, compensation, benefits, social and recreational activities and termination. For more information about equal employment opportunity, please click here for âEEO Is the Law.â For information regarding your Right to Work, click here for details in English and Spanish.","  Python & Pandas Spark, MapReduce AWS SQL, Postgresql Past ETL experience Experience with data governance and ownership of data pipelines   ","Python & Pandas Spark, MapReduce AWS SQL, Postgresql Past ETL experience Experience with data governance and ownership of pipelines","Python & Pandas Spark, MapReduce AWS SQL, Postgresql Past ETL experience Experience data governance ownership pipelines"
278,Data Engineer,"Sr. Data Engineer, Analytics Center of Excellence","New York, NY",New York,NY,"Work with architects, other data engineers and analysts to identify, engage, and integrate data sources for discovery and profiling and, where necessary, define data services that empower business processes
Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions
Operationalize data processing systems (dev ops)
Develop solutions combining data blending, profiling, mining, statistical analysis, and machine learning, to better define and curate models, test hypothesis, and deliver key insights
Participate in development sprints, demos, and retrospectives, as well as release and deployment
Collaborate in establishing and evolving development, testing, and documentation standards, as well as related code reviews
Partner with business analysts, application engineers, data scientists, etc., leveraging the appropriate tools, solutions, and/or processes as part of their data mining, profiling, blending, and analytical activities
Collaborate with NBCUâs technology domains to lead and ensure development and delivery of analytics solutions
Ensure policies and standards are followed across the organization
Deliver services that meet internal KPIs and customer SLAs
The ability to quickly build rapport and gain the respect and cooperation of both technology and business peers
Able to quickly learn new technologies as they become prevalent and widely implemented, and develop oneself
Driven to utilize both proven and unique solutions to address business problems until issue is resolved
Be able to deal with ambiguity and make quality decisions in a dynamic, fast-paced environment
Customer-focused, action-oriented and driven to achieve results in a positive manner, displaying ethical behavior, integrity, and building trust at all times
Strong teamwork, organizational and interpersonal skills; ability to communicate and persuade peers and thrive in a cross-functional matrix environment
Qualifications/Requirements
10+ years of progressive data application development experience, working in large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ years of experience modeling and implementing ETL/ELT on columnar MPP database technologies such as Snowflake, BigQuery and/or RedshiftExperience with streaming architectures (Kafka, Kinesis, Pub/Sub, etc.)Experience working in hybrid cloud/on-premise environments as well as multi-cloudExperience implementing scalable, distributed, and highly available systems using cloud technologies, such as Microsoft Azure, Amazon Web Services, and/or Google CloudExperience building microservices topologies, including operational concerns such as resiliency, observability, discovery and routing, etc.Undergraduate degree in the field of computer science or engineering, or focus on statistical analysis or equivalent experience highly desired
Desired Characteristics
Analytical â You have experience in delivering self-service analytics solutions that promote data discoveryMedia-focused - Strong working knowledge of media including traditional and Direct to ConsumerCommunicator â You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technicalAction-oriented â You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trustTechnologist â You have a love for data and exploring new ways to do old things, as well as the outside the box thinking required to build scalable technology solutions with pattern-based thinking.
Sub-BusinessTechnology
Career Level
Experienced
CityNew York
State/Province
New York
CountryUnited States
About Us
The Analytics Center of Excellence (ACOE) at NBCUniversal is looking for a passionate problem solver whoâs looking to build the next generation of data and analytics platforms. Reporting to the Chief Architect supporting the enterprise portfolio, this role is right for you if youâre a highly experienced data engineer who likes working in a âhands-onâ agile environment and leveraging data to drive innovative solutions. Transcend the static nouns of platforms, data sources, integrations, interfaces, etc., and relish in the verbs of mining, modeling, predicting, innovating, and all-out rockinâ it to unlock the stories yearning to be told.

As part of the global Operations & Technology organization, the ACOE is focused on data and analytics strategies for the future. We support NBCUâs vast portfolio of brands - from broadcast, cable, news, and sports networks to film studios, world-renowned theme parks and a diverse suite of digital properties. We take pride in providing NBCUniversal with data to advise and shape strategic business decisions.
Notices
NBCUniversalâs policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.","10+ years of progressive data application development experience, working in large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ years of experience modeling and implementing ETL/ELT on columnar MPP database technologies such as Snowflake, BigQuery and/or RedshiftExperience with streaming architectures  Kafka, Kinesis, Pub/Sub, etc. Experience working in hybrid cloud/on-premise environments as well as multi-cloudExperience implementing scalable, distributed, and highly available systems using cloud technologies, such as Microsoft Azure, Amazon Web Services, and/or Google CloudExperience building microservices topologies, including operational concerns such as resiliency, observability, discovery and routing, etc.Undergraduate degree in the field of computer science or engineering, or focus on statistical analysis or equivalent experience highly desired    10+ years of progressive data application development experience, working in large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ years of experience modeling and implementing ETL/ELT on columnar MPP database technologies such as Snowflake, BigQuery and/or RedshiftExperience with streaming architectures  Kafka, Kinesis, Pub/Sub, etc. Experience working in hybrid cloud/on-premise environments as well as multi-cloudExperience implementing scalable, distributed, and highly available systems using cloud technologies, such as Microsoft Azure, Amazon Web Services, and/or Google CloudExperience building microservices topologies, including operational concerns such as resiliency, observability, discovery and routing, etc.Undergraduate degree in the field of computer science or engineering, or focus on statistical analysis or equivalent experience highly desired","10+ years of progressive data application development experience, working in large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ experience modeling and implementing ETL/ELT on columnar MPP database technologies such as Snowflake, BigQuery RedshiftExperience with streaming architectures Kafka, Kinesis, Pub/Sub, etc. Experience hybrid cloud/on-premise environments well multi-cloudExperience scalable, distributed, highly available systems using cloud technologies, Microsoft Azure, Amazon Web Services, Google CloudExperience building microservices topologies, including operational concerns resiliency, observability, discovery routing, etc.Undergraduate degree the field computer science or engineering, focus statistical analysis equivalent desired","10+ years progressive data application development experience, working large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ experience modeling implementing ETL/ELT columnar MPP database technologies Snowflake, BigQuery RedshiftExperience streaming architectures Kafka, Kinesis, Pub/Sub, etc. Experience hybrid cloud/on-premise environments well multi-cloudExperience scalable, distributed, highly available systems using cloud technologies, Microsoft Azure, Amazon Web Services, Google CloudExperience building microservices topologies, including operational concerns resiliency, observability, discovery routing, etc.Undergraduate degree field computer science engineering, focus statistical analysis equivalent desired"
279,Data Engineer,"Senior Data Engineer - Hadoop, VP","New York, NY 10261",New York,NY,"The Applications Development Technology Lead Analyst is a senior level position responsible for establishing and implementing new or revised application systems and programs in coordination with the Technology team. The overall objective of this role is to lead applications systems analysis and programming activities.

Responsibilities:
Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements
Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards
Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint
Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation
Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals
Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions
Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary
Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.

Qualifications:
6-10 years of relevant experience in Apps Development or systems analysis role
Extensive experience system analysis and in programming of software applications
Experience in managing and implementing successful projects
Subject Matter Expert (SME) in at least one area of Applications Development
Ability to adjust priorities quickly as circumstances dictate
Demonstrated leadership and project management skills
Consistently demonstrates clear and concise written and verbal communication

Education:
Bachelorâs degree/University degree or equivalent experience
Masterâs degree preferred
This job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required.
Citiâs Treasury and Trade Solutions group is built on the power of our network. Citi Treasury and Trade Solutions (TTS), provides integrated cash management and trade finance services to multinational corporations, financial institutions and public sector organizations across the globe. With a full range of digital and mobile enabled platforms, tools and analytics, TTS continues to lead the way in delivering innovative and tailored solutions to its clients. TTS offers the industryâs most comprehensive suite of treasury and trade solutions including cash management, payments, receivables, liquidity management and investment services, working capital solutions, commercial card programs, trade finance and services.
TTS counts 80% of Global Fortune 500, over 1,100 public sector entities and thousands of financial institutions amongst its diverse client base
Conducts business In 160 countries and jurisdictions, and transacting in over 140 currencies, TTS processes some USD 4.0 trillion of client payments everyday
Collaborates with clients to deliver next-generation financial solutions using TTS Innovation Labs
Application / Team Overview:
The Vanguard TTS Big Data platform is a multi-year critical program within the Data, Innovation and Architecture Technology, part of Treasury & Trade Solutions. The candidate will have great exposure and opportunity to work with the latest technologies such as Hadoop Big Data eco-system, Cloud Technology, Real-time processing and Analytics.
The candidate will work closely with the Big Data development team and interface with other internal teams and learn how a leading organization conducts itself.
Role Description:
Hiring a hands-on Data Engineer who will take the overall responsibility for end to end software development, continuous integration and continuous deployment, meeting a high level of code quality working within established timelines and Engineering Excellence best practices.
Specifically the candidate will work on a cloud-based initiative to enhance our data management capabilities enabling superior analytics, client experience, and regulatory reporting and data science solutions across business lines globally.
The Data Engineer will have a solid understanding of the âbig dataâ infrastructure and can design, build, integrate data from various resources, and manage big data platform. The data engineer may be required to write complex queries and ensure data is easily accessible, works smoothly with an end goal of optimizing performance of Citi's big data ecosystem. The candidate may also be required to run some ETL on top of big datasets and create big data warehouses that can be used for reporting or analysis by data scientists.
The ideal candidate will be a resourceful software professional who can comfortably work in a large development team in a globally distributed, dynamic work environment that fosters diversity, teamwork and collaboration.
Bachelorâs degree (in Science, Computers, Information Technology or Engineering)
At least 3+ years overall IT experience with 3+ yearsâ experience working as a Data Engineer in Data Warehousing, Data Architecture or Data Management. Minimum 3 years of experience working with Hadoop Spark using Scala/Java.
3 years of Cloudera Hadoop 5.x / 6.x
Experience with real time messaging and ingestion including Kafka
Experience with developing frameworks and utility services including logging/monitoring using ELK or similar
Working experience with Big Data (HBase/Impala/Hive) No-SQL databases
Hands on experience with open source software platforms Linux
Experience with Oracle and/or MYSQL database application development.
Agile build and deploy DevOps experience preferred
Cloud experience with Google Cloud (DataProc or BigQuery) or AWS â EMR/Cloudera on EC2 is a plus
Leadership Skills:
Proven ability in working with the development team members and other partners, with minimal supervision
Strong verbal and written communications skills, excellent interpersonal skills with ability to communicate well at all levels
Team Player, self-starter and thorough who is willing to take on any assigned job/responsibilities
Ability to learn new skills quickly with little supervision and ensuring the detail is of high priority
Efficiently and effectively manages work, time, and resources
Ability to work under high-pressure situations and effectively prioritize in a highly dynamic work environment that includes a global focus
Strong problem solving and program execution skills while being process orientated
Ability to understand the big picture â can step back and understand the context of problems before applying analytical skills to address the issues
-
Grade :All Job Level - All Job FunctionsAll Job Level - All Job Functions - US
-
Time Type :Full time
-
Citi is an equal opportunity and affirmative action employer.
Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.
Citigroup Inc. and its subsidiaries (""Citiâ) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity CLICK HERE.
To view the ""EEO is the Law"" poster CLICK HERE. To view the EEO is the Law Supplement CLICK HERE.
To view the EEO Policy Statement CLICK HERE.
To view the Pay Transparency Posting CLICK HERE."," 6-10 years of relevant experience in Apps Development or systems analysis role Extensive experience system analysis and in programming of software applications Experience in managing and implementing successful projects Subject Matter Expert  SME  in at least one area of Applications Development Ability to adjust priorities quickly as circumstances dictate Demonstrated leadership and project management skills Consistently demonstrates clear and concise written and verbal communication  Proven ability in working with the development team members and other partners, with minimal supervision Strong verbal and written communications skills, excellent interpersonal skills with ability to communicate well at all levels Team Player, self-starter and thorough who is willing to take on any assigned job/responsibilities Ability to learn new skills quickly with little supervision and ensuring the detail is of high priority Efficiently and effectively manages work, time, and resources Ability to work under high-pressure situations and effectively prioritize in a highly dynamic work environment that includes a global focus Strong problem solving and program execution skills while being process orientated Ability to understand the big picture â can step back and understand the context of problems before applying analytical skills to address the issues  Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.  Bachelorâs degree/University degree or equivalent experience Masterâs degree preferred ","6-10 years of relevant experience in Apps Development or systems analysis role Extensive system and programming software applications Experience managing implementing successful projects Subject Matter Expert SME at least one area Applications Ability to adjust priorities quickly as circumstances dictate Demonstrated leadership project management skills Consistently demonstrates clear concise written verbal communication Proven ability working with the development team members other partners, minimal supervision Strong communications skills, excellent interpersonal communicate well all levels Team Player, self-starter thorough who is willing take on any assigned job/responsibilities learn new little ensuring detail high priority Efficiently effectively manages work, time, resources work under high-pressure situations prioritize a highly dynamic environment that includes global focus problem solving program execution while being process orientated understand big picture â can step back context problems before applying analytical address issues Partner multiple teams ensure appropriate integration functions meet goals identify define necessary enhancements deploy products improvements Resolve variety impact problems/projects through in-depth evaluation complex business processes, industry standards Provide expertise advanced knowledge application design adheres overall architecture blueprint Utilize flow develop for coding, testing, debugging, implementation Develop comprehensive how areas business, such infrastructure, integrate accomplish interpretive thinking innovative solutions Serve advisor coach mid-level developers analysts, allocating Appropriately assess risk when decisions are made, demonstrating particular consideration firm's reputation safeguarding Citigroup, its clients assets, by driving compliance applicable laws, rules regulations, adhering Policy, sound ethical judgment regarding personal behavior, conduct practices, escalating, reporting control transparency. Bachelorâs degree/University degree equivalent Masterâs preferred","6-10 years relevant experience Apps Development systems analysis role Extensive system programming software applications Experience managing implementing successful projects Subject Matter Expert SME least one area Applications Ability adjust priorities quickly circumstances dictate Demonstrated leadership project management skills Consistently demonstrates clear concise written verbal communication Proven ability working development team members partners, minimal supervision Strong communications skills, excellent interpersonal communicate well levels Team Player, self-starter thorough willing take assigned job/responsibilities learn new little ensuring detail high priority Efficiently effectively manages work, time, resources work high-pressure situations prioritize highly dynamic environment includes global focus problem solving program execution process orientated understand big picture â step back context problems applying analytical address issues Partner multiple teams ensure appropriate integration functions meet goals identify define necessary enhancements deploy products improvements Resolve variety impact problems/projects in-depth evaluation complex business processes, industry standards Provide expertise advanced knowledge application design adheres overall architecture blueprint Utilize flow develop coding, testing, debugging, implementation Develop comprehensive areas business, infrastructure, integrate accomplish interpretive thinking innovative solutions Serve advisor coach mid-level developers analysts, allocating Appropriately assess risk decisions made, demonstrating particular consideration firm's reputation safeguarding Citigroup, clients assets, driving compliance applicable laws, rules regulations, adhering Policy, sound ethical judgment regarding personal behavior, conduct practices, escalating, reporting control transparency. Bachelorâs degree/University degree equivalent Masterâs preferred"
280,Data Engineer,Senior Data Engineer,"New York, NY",New York,NY,"Job Description

Honcker is a car leasing app startup that is set out to disrupt the automotive leasing industry. We are looking for a data engineer to join our growing team. You will work on improving our data infrastructure, while building out our new data warehouse and business intelligence tools. You will get to make decisions, architect solutions and elevate the organization by solving real problems that will allow the business to make key decisions. Excellent growth opportunity to grow with a startup that is moving fast!

Responsibilities:
Build real-time data capture and transformation functionality across all products
Work with other engineers to enhance data models and improve data query efficiency
Create complex data queries to facilitate ad hoc and exploratory analytics
Act as in-house data expert and make recommendations regarding standards quality and timeliness
Build out technology stack for Business Intelligence and Data Warehouse
Clean data: review for data inconsistencies and identify opportunities to improve data collection process
Wrangle/Munge data: transform or map data from one raw data form into another format with the intent of making it more appropriate and valuable for analytics
Develop, construct, test and maintain architectures such as databases and large-scale data processing systems
Design, construct, install, test and maintain highly scalable data management systems
Build high-performance algorithms, prototypes, predictive models and proof of concepts
Research opportunities for data acquisition and new uses for existing data
Employ a variety of languages and tools (e.g. scripting languages) to marry systems together
Recommend ways to improve data reliability, efficiency and quality
Collaborate with analysts and team members on project goals and targeted, relevant data sets
Build or recommend data visualization tools and business intelligence tools such as interactive dashboards and automated reports, to enable leaders to make swift, fact-based decisions

Qualifications:
Bachelorâs degree in computer science, software/computer engineering, applied math, statistics or related field required
4+ years of related experience as a data engineer
4+ SQL-based technologies (e.g. PostgreSQL and MySQL)
2+ NoSQL technologies (e.g. Cassandra and MongoDB)
Experience with Hadoop-based technologies (e.g. MapReduce, Hive and Pig)
Good with scripting languages (Python, Perl, etc.)
Data warehousing solutions
Statistical analysis and modeling is a PLUS
Machine learning and data mining is a PLUS

No sponsorship available at this time."," Bachelorâs degree in computer science, software/computer engineering, applied math, statistics or related field required 4+ years of related experience as a data engineer 4+ SQL-based technologies  e.g. PostgreSQL and MySQL  2+ NoSQL technologies  e.g. Cassandra and MongoDB  Experience with Hadoop-based technologies  e.g. MapReduce, Hive and Pig  Good with scripting languages  Python, Perl, etc.  Data warehousing solutions Statistical analysis and modeling is a PLUS Machine learning and data mining is a PLUS  No sponsorship available at this time.   Build real-time data capture and transformation functionality across all products Work with other engineers to enhance data models and improve data query efficiency Create complex data queries to facilitate ad hoc and exploratory analytics Act as in-house data expert and make recommendations regarding standards quality and timeliness Build out technology stack for Business Intelligence and Data Warehouse Clean data  review for data inconsistencies and identify opportunities to improve data collection process Wrangle/Munge data  transform or map data from one raw data form into another format with the intent of making it more appropriate and valuable for analytics Develop, construct, test and maintain architectures such as databases and large-scale data processing systems Design, construct, install, test and maintain highly scalable data management systems Build high-performance algorithms, prototypes, predictive models and proof of concepts Research opportunities for data acquisition and new uses for existing data Employ a variety of languages and tools  e.g. scripting languages  to marry systems together Recommend ways to improve data reliability, efficiency and quality Collaborate with analysts and team members on project goals and targeted, relevant data sets Build or recommend data visualization tools and business intelligence tools such as interactive dashboards and automated reports, to enable leaders to make swift, fact-based decisions  ","Bachelorâs degree in computer science, software/computer engineering, applied math, statistics or related field required 4+ years of experience as a data engineer SQL-based technologies e.g. PostgreSQL and MySQL 2+ NoSQL Cassandra MongoDB Experience with Hadoop-based MapReduce, Hive Pig Good scripting languages Python, Perl, etc. Data warehousing solutions Statistical analysis modeling is PLUS Machine learning mining No sponsorship available at this time. Build real-time capture transformation functionality across all products Work other engineers to enhance models improve query efficiency Create complex queries facilitate ad hoc exploratory analytics Act in-house expert make recommendations regarding standards quality timeliness out technology stack for Business Intelligence Warehouse Clean review inconsistencies identify opportunities collection process Wrangle/Munge transform map from one raw form into another format the intent making it more appropriate valuable Develop, construct, test maintain architectures such databases large-scale processing systems Design, install, highly scalable management high-performance algorithms, prototypes, predictive proof concepts Research acquisition new uses existing Employ variety tools marry together Recommend ways reliability, Collaborate analysts team members on project goals targeted, relevant sets recommend visualization business intelligence interactive dashboards automated reports, enable leaders swift, fact-based decisions","Bachelorâs degree computer science, software/computer engineering, applied math, statistics related field required 4+ years experience data engineer SQL-based technologies e.g. PostgreSQL MySQL 2+ NoSQL Cassandra MongoDB Experience Hadoop-based MapReduce, Hive Pig Good scripting languages Python, Perl, etc. Data warehousing solutions Statistical analysis modeling PLUS Machine learning mining No sponsorship available time. Build real-time capture transformation functionality across products Work engineers enhance models improve query efficiency Create complex queries facilitate ad hoc exploratory analytics Act in-house expert make recommendations regarding standards quality timeliness technology stack Business Intelligence Warehouse Clean review inconsistencies identify opportunities collection process Wrangle/Munge transform map one raw form another format intent making appropriate valuable Develop, construct, test maintain architectures databases large-scale processing systems Design, install, highly scalable management high-performance algorithms, prototypes, predictive proof concepts Research acquisition new uses existing Employ variety tools marry together Recommend ways reliability, Collaborate analysts team members project goals targeted, relevant sets recommend visualization business intelligence interactive dashboards automated reports, enable leaders swift, fact-based decisions"
281,Data Engineer,Data Engineer,"New York, NY 10022",New York,NY,"Description
Spruce is looking for a Data Engineer to join the Data Science team in NYC. The Data Engineer will develop new digital capability for new title underwriting automation and data science tools in the real estate industry. You will have initiative, innovation and good judgment in a dynamic startup environment.

Responsibilities:

Build efficient codes to extract data and documents from various sources
Build OCR and NLP pipeline to retrieve data from unstructured data
Build standard reports from extracted data per business requirement
Conduct unit testing and document the finalized code set
Assist production implementation in set up infrastructure and automated processes

You will possess strong social and interpersonal skills and be able to seamlessly collaborate with other members of the team and across functions. You will demonstrate attention to detail, organization, and work, including driving projects to completion in accordance with established timelines.
This position will work with the Director, Data Science and will locate in midtown Manhattan.

Qualifications

Bachelorâs degree in quantitative fields is required, computer science degree preferred
Have two years of Java, Python, and SQL database programming experience. The Python experience should include web scraper development
Have one year of natural language processing work or research experience
Unix, OCR, MongoDB, GCP experience are a plus
Knowledge of insurance industry and statistics is a plus
Excellent written and verbal presentation skills
Highly motivated and creative problem solver; able to self-initiate and thrive in a results-oriented environment; positive attitude
Ability to convey sense of urgency and to work productively and cooperatively with team members

",   Build efficient codes to extract data and documents from various sources Build OCR and NLP pipeline to retrieve data from unstructured data Build standard reports from extracted data per business requirement Conduct unit testing and document the finalized code set Assist production implementation in set up infrastructure and automated processes   ,Build efficient codes to extract data and documents from various sources OCR NLP pipeline retrieve unstructured standard reports extracted per business requirement Conduct unit testing document the finalized code set Assist production implementation in up infrastructure automated processes,Build efficient codes extract data documents various sources OCR NLP pipeline retrieve unstructured standard reports extracted per business requirement Conduct unit testing document finalized code set Assist production implementation infrastructure automated processes
282,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Beeswax is looking for a Data Engineer to join our growing team. We were recently recognized on the Inc. 5000 list as #46 in the fastest growing companies and #5 in the top software companies. In 2018, we were also named by Business Insider as the ""fastest growing company in AdTech""

Beeswax is a high scale, high availability digital advertising platform founded by executives from Google and funded by leading VCs including RRE and Foundry Group. We aim to offer the most extensible and transparent advertising system, servicing technology enabled clients and executing and processing billions of events every day.

The Beeswax engineering team is a top-notch group with backgrounds from Google, Amazon, Oracle and other premier technical teams. Because digital advertising operates at an extremely high scale (millions of transactions per second) and low latency, the opportunity to learn about web-scale distributed systems and hard scaling problems is a great advantage of our engineering culture and work.

We are looking for a Data Engineer to build clean pipelines and maintain data products that our customers rely on.

Our products are built on a variety of technologies including C++, Java, Python and LEMP stack (PHP, MySQL, nginx), and while our engineers typically specialize in one area, they need to coordinate and integrate with a wide variety of systems, including homegrown and AWS-native services.

Responsibilities:

Code in a variety of languages, primarily Python, Java and/or C++
Design and implement data pipelines, building scalable and optimized enterprise level data systems
Work cross functionally with Product, Ops and Engineering counterparts
Participation and collaboration from inception to deployment

Requirements:

MS in Computer Science, Math, related technical field or equivalent practical experience
4+ years of general software programming experience in Java, C/C++, Python and SQL
Large systems software design and development experience, with knowledge of Unix/Linux
Knowledge of database technology, schema design, and query optimization techniques
Solid foundation in data structures, algorithms and software design with strong analytical and debugging skills

Preferred Qualifications:

PhD in Computer Science, Mathematics, or related technical field
Familiarity with open source cloud and application platforms, AWS development experience
Experience with big data technologies such as Spark, Hive, Presto and Impala.
Experience working with MPP databases such as Redshift, Snowflake, Vertica and Netezza
Hands-on experience working in SOA and high throughput environments

About You:

You are passionate about learning, mentoring and building a world class team and culture, while constantly empowering others around you
You take a second to step back and look at the big picture before diving in head first
You care about the quality of the data flowing through your code as much as about the quality of the code
Not afraid to take risks, voice opinions or ideas that help build the next generation of data platforms all within a massive distributed system
You're the type to peel back the layers and use non-conventional means to solve the task at hand.

"," PhD in Computer Science, Mathematics, or related technical field Familiarity with open source cloud and application platforms, AWS development experience Experience with big data technologies such as Spark, Hive, Presto and Impala. Experience working with MPP databases such as Redshift, Snowflake, Vertica and Netezza Hands-on experience working in SOA and high throughput environments    Code in a variety of languages, primarily Python, Java and/or C++ Design and implement data pipelines, building scalable and optimized enterprise level data systems Work cross functionally with Product, Ops and Engineering counterparts Participation and collaboration from inception to deployment    MS in Computer Science, Math, related technical field or equivalent practical experience 4+ years of general software programming experience in Java, C/C++, Python and SQL Large systems software design and development experience, with knowledge of Unix/Linux Knowledge of database technology, schema design, and query optimization techniques Solid foundation in data structures, algorithms and software design with strong analytical and debugging skills ","PhD in Computer Science, Mathematics, or related technical field Familiarity with open source cloud and application platforms, AWS development experience Experience big data technologies such as Spark, Hive, Presto Impala. working MPP databases Redshift, Snowflake, Vertica Netezza Hands-on SOA high throughput environments Code a variety of languages, primarily Python, Java and/or C++ Design implement pipelines, building scalable optimized enterprise level systems Work cross functionally Product, Ops Engineering counterparts Participation collaboration from inception to deployment MS Math, equivalent practical 4+ years general software programming Java, C/C++, Python SQL Large design experience, knowledge Unix/Linux Knowledge database technology, schema design, query optimization techniques Solid foundation structures, algorithms strong analytical debugging skills","PhD Computer Science, Mathematics, related technical field Familiarity open source cloud application platforms, AWS development experience Experience big data technologies Spark, Hive, Presto Impala. working MPP databases Redshift, Snowflake, Vertica Netezza Hands-on SOA high throughput environments Code variety languages, primarily Python, Java and/or C++ Design implement pipelines, building scalable optimized enterprise level systems Work cross functionally Product, Ops Engineering counterparts Participation collaboration inception deployment MS Math, equivalent practical 4+ years general software programming Java, C/C++, Python SQL Large design experience, knowledge Unix/Linux Knowledge database technology, schema design, query optimization techniques Solid foundation structures, algorithms strong analytical debugging skills"
283,Data Engineer,Finance Data Engineer - Finance Platforms & Data,"Jersey City, NJ 07302",Jersey City,NJ,"MORE ABOUT THIS JOB
In Finance Engineering, youâll find an exciting confluence of computer science, finance and mathematics being used to solve for what our shareholders would like from us â a high return for the right risk taken.
Our Data, Platform, Infrastructure and Risk engineers work with multiple Finance businesses to drive consistency, efficiency and reuse across Finance & Risk Engineering solutions via the delivery of common services, tools, frameworks, and practices.
With increasing complexity and volumes in Finance, we continuously need to scale our data. Cutting across all areas of Finance, our Data Engineering team is designing our common datastores in the Data Lake. We are a dynamic team of talented junior and senior developers, technical architects, and functional analysts who work in concert to deliver high profile projects using OO technologies.
RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
Work in a dynamic, fast-paced environment that provides exposure to all areas of FinanceBuild strong relationships with business partnersUnderstand business needs, facilitating and developing process workflow, data requirements, and specifications required to support implementationDevelop technical specifications, high level/detailed design, testing strategies, and implementation plans from business requirementsManage end-to-end systems development cycle from requirements analysis, coding, testing, UAT and maintenance

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
Bachelors degree in Computer Science, Mathematics, Electrical Engineering or related technical disciplineExperience in software development, including a clear understanding of data structures, algorithms, software design and core programming conceptsComfortable multi-tasking, managing multiple stakeholders and working as part of a teamExcellent communication skills including experience speaking to technical and business audiences and working globallyExpertise in Java development & Relational DatabasesCan apply an entrepreneurial approach and passion to problem solving and product developmentStrong problem solving and analytical skills

Preferred Qualifications
Strong programming experience in at least one compiled language (e.g. C, C++, Java)In-depth knowledge of relational and columnar SQL databases, including database designExperience with continuous delivery and deploymentProficient at working with large and complex code basesComfortable working in highly dynamic and rapid development environment (Agile development experience)Technologies: Web/RESTful service development: HTML 5, JavaScript/AngularJS, JSONTechnologies: Linux and shell scripting, TDD (JUnit), build tools (Maven/Gradle/Ant), Scala, Spark, Tableau
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

ÃÂ© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.","Strong programming experience in at least one compiled language  e.g. C, C++, Java In-depth knowledge of relational and columnar SQL databases, including database designExperience with continuous delivery and deploymentProficient at working with large and complex code basesComfortable working in highly dynamic and rapid development environment  Agile development experience Technologies  Web/RESTful service development  HTML 5, JavaScript/AngularJS, JSONTechnologies  Linux and shell scripting, TDD  JUnit , build tools  Maven/Gradle/Ant , Scala, Spark, Tableau    ","Strong programming experience in at least one compiled language e.g. C, C++, Java In-depth knowledge of relational and columnar SQL databases, including database designExperience with continuous delivery deploymentProficient working large complex code basesComfortable highly dynamic rapid development environment Agile Technologies Web/RESTful service HTML 5, JavaScript/AngularJS, JSONTechnologies Linux shell scripting, TDD JUnit , build tools Maven/Gradle/Ant Scala, Spark, Tableau","Strong programming experience least one compiled language e.g. C, C++, Java In-depth knowledge relational columnar SQL databases, including database designExperience continuous delivery deploymentProficient working large complex code basesComfortable highly dynamic rapid development environment Agile Technologies Web/RESTful service HTML 5, JavaScript/AngularJS, JSONTechnologies Linux shell scripting, TDD JUnit , build tools Maven/Gradle/Ant Scala, Spark, Tableau"
284,Data Engineer,Senior Data Engineer,"New York, NY",New York,NY,"Team: Engineering
From collecting and processing terabytes of data a day to improving and building new data assets, tools, and pipelines, we build data driven teams and foster a data-informed culture. Our ideal candidate thrives in a fast-paced consulting environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business problems and, above all else, is passionate about data and analytics. A successful candidate knows and loves working with technical tools, is comfortable accessing and working with data from multiple sources, and partners with the client to identify strategic opportunities and deliver results.
As a Senior Data Engineer at SFL Scientific you will enable data-driven decision making by collecting, transforming, and visualizing data. You will design, build, maintain, and troubleshoot data processing systems with a particular emphasis on the security, reliability, fault- tolerance, scalability, fidelity, and efficiency of such systems. The Senior Data Engineer also analyzes data to gain insight into business outcomes, builds statistical models to support decision-making, and creates machine learning models to automate and simplify key business processes. You will be responsible for designing and implementing solutions using third-party technologies (e.g., different cloud providers) and SFL solutions.

Responsibilities:
Provide technical design leadership with the responsibility to ensure the efficient use of resources, the selection of appropriate technology, and use of appropriate design methodologies
Work closely with business/product stakeholders in understanding requirements and translating them to engineering requirements
Support and enhance data architecture, data instrumentation, define database schema, create ETL pipelining, generate reports/insights, a guide algorithm design
Define and evangelize data warehouse fundamentals and best practices
Work across the organization in optimizing data capture (parameters, metadata, etc.)
Work in DevOps to bring up new data systems and supporting existing data services
Evaluate SaaS solutions (BI, pipelining, etc.) and make build/buy recommendations
General Requirements:
Bachelorâs degree in Computer Science, Engineering, Math, Physics, or equivalent work experience
5+ years of software development experience
Proficient in SQL, NoSQL databases, and GNU Linux
Experience building secure, concurrent, distributed server applications
Experience in data science, analytics, or big data solutions [Hadoop, Spark, AWS, Python, etc.]
Experience with Scala, MongoDB, Cassandra, PostgreSQL, Docker, Kubernetes
Ability to work collaboratively with a distributed team or remotely with clients
Other Skills/Experience:
Enable machine learning and data analysis
Model data and metadata to support dashboards, ad-hoc, and pre-built reporting
Adopt best practices in reporting and analysis: data integrity, analysis, validation, and documentation
Ability to engage clients and lead relevant data discussions
Preferred Qualifications:
AWS Certified DevOps Engineer; AWS Certified Solutions Architect; AWS Certified Big Data
Azure or Google Certified Professional Data Engineer
Master's or Ph.D degree

You must be authorized to work in the US. We support flexible work hours and paid professional development.
Please send cover letter, CV or resume to careers@sflscientific.com or apply below."," AWS Certified DevOps Engineer; AWS Certified Solutions Architect; AWS Certified Big Data Azure or Google Certified Professional Data Engineer Master's or Ph.D degree   Enable machine learning and data analysis Model data and metadata to support dashboards, ad-hoc, and pre-built reporting Adopt best practices in reporting and analysis  data integrity, analysis, validation, and documentation Ability to engage clients and lead relevant data discussions  Provide technical design leadership with the responsibility to ensure the efficient use of resources, the selection of appropriate technology, and use of appropriate design methodologies Work closely with business/product stakeholders in understanding requirements and translating them to engineering requirements Support and enhance data architecture, data instrumentation, define database schema, create ETL pipelining, generate reports/insights, a guide algorithm design Define and evangelize data warehouse fundamentals and best practices Work across the organization in optimizing data capture  parameters, metadata, etc.  Work in DevOps to bring up new data systems and supporting existing data services Evaluate SaaS solutions  BI, pipelining, etc.  and make build/buy recommendations   Bachelorâs degree in Computer Science, Engineering, Math, Physics, or equivalent work experience 5+ years of software development experience Proficient in SQL, NoSQL databases, and GNU Linux Experience building secure, concurrent, distributed server applications Experience in data science, analytics, or big data solutions [Hadoop, Spark, AWS, Python, etc.] Experience with Scala, MongoDB, Cassandra, PostgreSQL, Docker, Kubernetes Ability to work collaboratively with a distributed team or remotely with clients","AWS Certified DevOps Engineer; Solutions Architect; Big Data Azure or Google Professional Engineer Master's Ph.D degree Enable machine learning and data analysis Model metadata to support dashboards, ad-hoc, pre-built reporting Adopt best practices in integrity, analysis, validation, documentation Ability engage clients lead relevant discussions Provide technical design leadership with the responsibility ensure efficient use of resources, selection appropriate technology, methodologies Work closely business/product stakeholders understanding requirements translating them engineering Support enhance architecture, instrumentation, define database schema, create ETL pipelining, generate reports/insights, a guide algorithm Define evangelize warehouse fundamentals across organization optimizing capture parameters, metadata, etc. bring up new systems supporting existing services Evaluate SaaS solutions BI, make build/buy recommendations Bachelorâs Computer Science, Engineering, Math, Physics, equivalent work experience 5+ years software development Proficient SQL, NoSQL databases, GNU Linux Experience building secure, concurrent, distributed server applications science, analytics, big [Hadoop, Spark, AWS, Python, etc.] Scala, MongoDB, Cassandra, PostgreSQL, Docker, Kubernetes collaboratively team remotely","AWS Certified DevOps Engineer; Solutions Architect; Big Data Azure Google Professional Engineer Master's Ph.D degree Enable machine learning data analysis Model metadata support dashboards, ad-hoc, pre-built reporting Adopt best practices integrity, analysis, validation, documentation Ability engage clients lead relevant discussions Provide technical design leadership responsibility ensure efficient use resources, selection appropriate technology, methodologies Work closely business/product stakeholders understanding requirements translating engineering Support enhance architecture, instrumentation, define database schema, create ETL pipelining, generate reports/insights, guide algorithm Define evangelize warehouse fundamentals across organization optimizing capture parameters, metadata, etc. bring new systems supporting existing services Evaluate SaaS solutions BI, make build/buy recommendations Bachelorâs Computer Science, Engineering, Math, Physics, equivalent work experience 5+ years software development Proficient SQL, NoSQL databases, GNU Linux Experience building secure, concurrent, distributed server applications science, analytics, big [Hadoop, Spark, AWS, Python, etc.] Scala, MongoDB, Cassandra, PostgreSQL, Docker, Kubernetes collaboratively team remotely"
285,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Data Engineer - (19001847)
Description

Position Summary
Our Data Engineer will be responsible for assisting with the implementation and maintenance of the enterprise wide data management solution for certified analytics and reporting rectangles (CARRs) and data assets. This includes the analysis, design, development, testing, implementation, and initial maintenance of the solution and acting as interface with SME/Product owners and enterprise data analytics team, by taking a holistic approach for delivering reusable CARRs and data assets enabling data solutions.
Your Responsibilities
 Support the design, build and execution of post source system extraction and data lake ingestion and business transformation, CARR creation framework and development processes in production.
 Enhance analytic environments and platform required for structured, semi-structured and unstructured data.
 Develop data quality metrics that identify gaps and ensure compliance with enterprise wide standards.
 Provide technical support for projects and team members, along with SMEs and product owners.
 Build data pipelines to feed descriptive and predictive analytics use cases, KPI and enterprise wide reporting.
 Interface with architects, product managers/SMEs and product analysts to understand data needs and support the implementation of the business rules into transformation.
 Document the data blending process along with the specifications and workflow/data lineage.
 Maintain/add to existing data dictionaries and work with UI team for creating the profiling platform and data profiling for already created CARRs.
 Execute project-based data governance processes.
 Work with our team in India
Reporting Relationships
As our Data Engineer, you will report to our Data Scientist who reports to our Vice President, Predictive Analytics.


Qualifications

Your Qualifications
 BS or MS in management information systems, computer science, or a related field.
 4-6 years of experience in data management.
 Programming capability in Python, SQL/NoSQL and Unix/shell scripting, as well as in data engineering, databases (e.g., SQL, MongoDB), platform architecture and ETL concepts.
 Amazon Web Services experience.
 Excellent communication and collaboration skills to work across multiple groups within the organization.
 Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operation.
 Experience working with offshore colleagues
Location
 This position can be located in our newest offices in Hudson Yards in Manhattan, in our Holmdel NJ offices (the historic former Bell Labs facility) or in our Gold LEED facility in Bethlehem PA with work from home flexibility.
Travel
 Less than 10% travel between NYC, Holmdel NJ, Bethlehem PA.
Benefits
 Medical, dental, vision, and prescription plans
 Competitive compensation package
 401k with company match
 Attractive Paid Time Off (PTO)
 Life and disability insurance
 Tuition assistance program
 Wellness discounts
Philanthropic Opportunities
 Social responsibility is part of our mission. It stems from our corporate values of putting people first, doing the right thing, and holding ourselves to high standards.
 As a company and as employees, we are engaged in a variety of initiatives such as volunteering within our local communities, educational alliances with colleges, focusing on sustainability, and promoting diversity and inclusion.
Guardian has been helping people protect their futures and secure their lives for more than 150 years.
Every day, we serve approximately 27 million people through a range of insurance and financial products. We help people and their families pursue financial security and well-being in life, health, and wealth. We help companies take care of their employees. And we help people recover and thrive after unexpected loss.
From our founding in 1860, when a community of immigrants joined together to insure and protect their businesses and families, doing the right thing for our policyholders and customers has guided everything we do. Our dedication to customers has helped us remain one of the most highly rated in client satisfaction and financial strength.
And as one of the largest mutual insurance companies, we know what matters most: putting the needs of our customers first.
Because everyone deserves a Guardian.
Learn more about Guardian at www.GuardianLife.com.
Â© Copyright 2019 The Guardian Life Insurance Company of America, New York, NY Guardian2019

Primary Location: United States-New York-New York
Other Locations: United States-New Jersey-Holmdel, United States-New York, United States-Pennsylvania, United States-Pennsylvania-Bethlehem, United States-New Jersey
Job: Information Technology
Schedule: Full-time
Shift: Day Job
Job Type: Standard
Travel: Yes, 10 % of the Time
Job Posting: Sep 23, 2019, 9:44:21 AM"," BS or MS in management information systems, computer science, or a related field.   Support the design, build and execution of post source system extraction and data lake ingestion and business transformation, CARR creation framework and development processes in production.  ","BS or MS in management information systems, computer science, a related field. Support the design, build and execution of post source system extraction data lake ingestion business transformation, CARR creation framework development processes production.","BS MS management information systems, computer science, related field. Support design, build execution post source system extraction data lake ingestion business transformation, CARR creation framework development processes production."
286,Data Engineer,Big Data Engineer,"New York, NY",New York,NY,"Responsibilities:
Implementation including loading from disparate data sets, preprocessing using Hive and Pig.
Manage the technical communication between the team and client
Work with big data team to deliver cutting edge solutions
Qualification:
2-5 years of demonstrable experience designing technological solutions to complex data problems, developing & testing modular, reusable, efficient and scalable code to implement those solutions.
Ideally, this would include work on the following technologies:
Expert-level proficiency in at-least one of R, C++ or Python (preferred). Scala knowledge a strong advantage.
Strong understanding and experience in distributed computing frameworks, particularly Apache Hadoop 2.0 (YARN; MR & HDFS) and associated technologies - one or more of Hive, Sqoop, Avro, Flume, Oozie, Zookeeper, etc..
Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage.
Operating knowledge of cloud computing platforms (AWS, especially EMR, EC2, S3, SWF services and the AWS CLI)
Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasks
Ability to work in a team in an agile setting, familiarity with JIRA and clear understanding of how Git works
In addition, the ideal candidate would have great problem-solving skills, and the ability & confidence to hack their way out of tight corners.
Education:
B.E/B.Tech in Computer Science or related technical degree

xNc0S6Y38a","  Implementation including loading from disparate data sets, preprocessing using Hive and Pig. Manage the technical communication between the team and client Work with big data team to deliver cutting edge solutions   ","Implementation including loading from disparate data sets, preprocessing using Hive and Pig. Manage the technical communication between team client Work with big to deliver cutting edge solutions","Implementation including loading disparate data sets, preprocessing using Hive Pig. Manage technical communication team client Work big deliver cutting edge solutions"
287,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ","Expertise in at least one of the following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing more languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting other customer-facing role","Expertise least one following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role"
288,Data Engineer,Sr. Data Engineer - Power BI,"New York, NY",New York,NY,"Weâre adding to our diverse team of experts and are looking to hire those who are committed to building a culture that enables the creation of innovative solutions for our business units and clients.

The Insurance Operations
Munich Re US is launching new insurance operations described as Munich Re Specialty Insurance that will unite the strengths and expertise of Munich Reâs specialty commercial teams in North America under a new management structure. This will allow for the development of an overarching sales strategy through which the specialty risk appetite and product offerings can be profitably optimized and expanded.

As a member of Munich Re's US operations, we offer the financial strength and stability that comes with being part of the world's preeminent insurance and reinsurance brand. Our risk experts work together to assemble the right mix of products and services to help our clients stay competitive â from traditional reinsurance coverages, to niche and specialty reinsurance and insurance products.

The Opportunity
Future focused and always one step ahead

The chosen candidate will independently provide data and business analytics support by working closely with Analytics, Underwriting, Actuarial, IT, Operations and Claims teams. This support includes expert business knowledge on the extraction, interpretation, architecting, modeling, transforming, and preparation of both quantitative and qualitative data as well as the creation and maintenance of actuarial and analytics databases, process automation and developing end-user tools to support data-driven decision making.

Major accountabilities include:
Supervise the maintenance and enhancement work of key strategic dashboards
Perform extraction, cleaning, transforming, manipulation and loading of various data files formats, databases and cloud/web sources.
Develop complex Power BI design and DAX code
Work with databases containing data from multiple sources, at various granular levels, to deliver data in good quality for use in analytics and visualization projects.
Make data architecture design decision for data dashboard/report and assist to develop data dashboard with visualization capabilities.
Lead complex data requirement gathering phases of dashboard and analytics projects.
Work with business and IT to define demands on the infrastructure of the MR Data Lakes on the basis of Hadoop, Hive, Power BI Workspace and SAS on an ongoing basis.
Work with IT and other stakeholders to prepare, maintain and quality assure internal/external data sources.
Qualifications
Bachelorâs Degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience
5+ years of sound experience with data modelling, data intake and data-curation procedures as well as outstanding SQL skills, incl. advanced concepts such as distributed queries and spatial queries.
Strong hands on experience with Power BI dashboard design, development and WS modeling.
Knowledge of modern web-based procedures for data visualization
Ability to think and work in a Group-wide network, strongly service orientated, team player and highly motivated.
Strong interpersonal skills.
Proficient oral and written communication skills.
Ability to schedule and prioritize workload demands, including multi-tasking.
Experience in results monitoring and rate/product development.
4+ years of hands-on HQL, SQL, Power BI, SAS (a plus).
Insurance/reinsurance/actuarial experience highly preferred.
Experience in the commercial insurance industry (especially property and casualty) is preferred but not necessary.
2+ years of Data architecture experience with regards to duplicative or redundant metadata, data structures or processes.
Proven experience in software development projects with Java Spring, Docker, Git, Maven/Gradle and Jenkins, proven agile project management and requirements engineering skills (SCRUM, Design Thinking).
Proven experience in Data Visualization tools ( such as Power BI, SAS VA, etc.)
Experience leading projects preferred.
Company NameMunich Re America
Requisition Number
3203BR
CountryUnited States of America
Employment Type
Full Time","Bachelorâs Degree  MS preferred  in Computer Science, Statistics, Math or equivalent combination of education and experience 5+ years of sound experience with data modelling, data intake and data-curation procedures as well as outstanding SQL skills, incl. advanced concepts such as distributed queries and spatial queries. Strong hands on experience with Power BI dashboard design, development and WS modeling. Knowledge of modern web-based procedures for data visualization Ability to think and work in a Group-wide network, strongly service orientated, team player and highly motivated. Strong interpersonal skills. Proficient oral and written communication skills. Ability to schedule and prioritize workload demands, including multi-tasking. Experience in results monitoring and rate/product development. 4+ years of hands-on HQL, SQL, Power BI, SAS  a plus . Insurance/reinsurance/actuarial experience highly preferred. Experience in the commercial insurance industry  especially property and casualty  is preferred but not necessary. 2+ years of Data architecture experience with regards to duplicative or redundant metadata, data structures or processes. Proven experience in software development projects with Java Spring, Docker, Git, Maven/Gradle and Jenkins, proven agile project management and requirements engineering skills  SCRUM, Design Thinking . Proven experience in Data Visualization tools   such as Power BI, SAS VA, etc.  Experience leading projects preferred.    ","Bachelorâs Degree MS preferred in Computer Science, Statistics, Math or equivalent combination of education and experience 5+ years sound with data modelling, intake data-curation procedures as well outstanding SQL skills, incl. advanced concepts such distributed queries spatial queries. Strong hands on Power BI dashboard design, development WS modeling. Knowledge modern web-based for visualization Ability to think work a Group-wide network, strongly service orientated, team player highly motivated. interpersonal skills. Proficient oral written communication schedule prioritize workload demands, including multi-tasking. Experience results monitoring rate/product development. 4+ hands-on HQL, SQL, BI, SAS plus . Insurance/reinsurance/actuarial preferred. the commercial insurance industry especially property casualty is but not necessary. 2+ Data architecture regards duplicative redundant metadata, structures processes. Proven software projects Java Spring, Docker, Git, Maven/Gradle Jenkins, proven agile project management requirements engineering skills SCRUM, Design Thinking Visualization tools VA, etc. leading","Bachelorâs Degree MS preferred Computer Science, Statistics, Math equivalent combination education experience 5+ years sound data modelling, intake data-curation procedures well outstanding SQL skills, incl. advanced concepts distributed queries spatial queries. Strong hands Power BI dashboard design, development WS modeling. Knowledge modern web-based visualization Ability think work Group-wide network, strongly service orientated, team player highly motivated. interpersonal skills. Proficient oral written communication schedule prioritize workload demands, including multi-tasking. Experience results monitoring rate/product development. 4+ hands-on HQL, SQL, BI, SAS plus . Insurance/reinsurance/actuarial preferred. commercial insurance industry especially property casualty necessary. 2+ Data architecture regards duplicative redundant metadata, structures processes. Proven software projects Java Spring, Docker, Git, Maven/Gradle Jenkins, proven agile project management requirements engineering skills SCRUM, Design Thinking Visualization tools VA, etc. leading"
289,Data Engineer,Senior Data Engineer,"New York, NY",New York,NY,"At Urbint, our mission is to make communities more resilient. We do this by pairing external data with artificial intelligence to identify areas of high risk and prevent catastrophic loss for utilities and infrastructure operators across the country. We are a team of close-knit engineers, entrepreneurs, and data geeks who obsess over problem-solving, new technologies, and making a positive impact in our communities.
At Urbint, you will collaborate within a cross functional team of other software engineers, product designers, machine learning engineers and product managers to architect, create and maintain custom data pipelines, data lake(s) and a data warehouse to feed our customer facing applications and internal experimentation and modeling teams. Our data sources range from large, high security enterprise databases and storage services hosted by our customers to disparate open data from government organizations to proprietary third party aggregators and auditability and security are very important to their results. You will be working to solve direct customer needs and finding common abstractions to apply and guide the future of information at data driven organization.
Requirements
8+ years of software development experience focused on web technologies including significant production work with Python
3+ years of experience designing, building and maintaining enterprise data pipelines and/or warehouses
High level knowledge of machine learning algorithms and how ML models are built and deployed
Demonstrable knowledge of big data databases such as columnar data stores (e.g. Cassandra or BigTable) or Hadoop as well as SQL (MySQL, MSSQL or PostgreSQL) and ability to select the right tool for the job
Experience with queued work management and message processing (e.g., Kafka, RabbitMQ)
Experience working closely with product and account support personnel to help prioritize the best solutions to the largest problems.
Reliable organization and communication skills and follow through on verbal and written commitments.
Persistent approach to problem-solving and ability to see solutions through to completion even in the face of complexities or unknowns. A proactive mindset that drives you to pursue solutions rather than waiting for the answers to come to you.
Attention to detail in work and ability to identify ambiguities in specifications.
Exceptional written and verbal communication skills, especially when communicating trade-offs between technical decisions to non-technical colleagues.
Flexibility to work and maintain focus in an evolving environment. Ability to let go of previous projects and move on to new ones or to dig deeper into existing projects and grow them depending on the business needs.
Benefits
What We Offer:
Mission Driven - Some companies use AI to serve better digital ads and trade stocks, we seek to make our communities more resilient.
Top Compensation - Competitive compensation package.
Best in Class Medical Coverage - 100% benefits and premiums paid.
Prime NoHo Location - Our office sits in the heart of NYCâs historic NoHo district and is just minutes away from the BDFM, 6, and RW subway lines.
Health Perks - Gym reimbursement and Citibike membership.
Strong Culture - collaborative office focused on teamwork, humility, and hustle.
Catered lunch on Thursdays, plus a kitchen filled with snacks and drinks.
We're an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","     8+ years of software development experience focused on web technologies including significant production work with Python 3+ years of experience designing, building and maintaining enterprise data pipelines and/or warehouses High level knowledge of machine learning algorithms and how ML models are built and deployed Demonstrable knowledge of big data databases such as columnar data stores  e.g. Cassandra or BigTable  or Hadoop as well as SQL  MySQL, MSSQL or PostgreSQL  and ability to select the right tool for the job Experience with queued work management and message processing  e.g., Kafka, RabbitMQ  Experience working closely with product and account support personnel to help prioritize the best solutions to the largest problems. Reliable organization and communication skills and follow through on verbal and written commitments. Persistent approach to problem-solving and ability to see solutions through to completion even in the face of complexities or unknowns. A proactive mindset that drives you to pursue solutions rather than waiting for the answers to come to you. Attention to detail in work and ability to identify ambiguities in specifications. Exceptional written and verbal communication skills, especially when communicating trade-offs between technical decisions to non-technical colleagues. Flexibility to work and maintain focus in an evolving environment. Ability to let go of previous projects and move on to new ones or to dig deeper into existing projects and grow them depending on the business needs. ","8+ years of software development experience focused on web technologies including significant production work with Python 3+ designing, building and maintaining enterprise data pipelines and/or warehouses High level knowledge machine learning algorithms how ML models are built deployed Demonstrable big databases such as columnar stores e.g. Cassandra or BigTable Hadoop well SQL MySQL, MSSQL PostgreSQL ability to select the right tool for job Experience queued management message processing e.g., Kafka, RabbitMQ working closely product account support personnel help prioritize best solutions largest problems. Reliable organization communication skills follow through verbal written commitments. Persistent approach problem-solving see completion even in face complexities unknowns. A proactive mindset that drives you pursue rather than waiting answers come you. Attention detail identify ambiguities specifications. Exceptional skills, especially when communicating trade-offs between technical decisions non-technical colleagues. Flexibility maintain focus an evolving environment. Ability let go previous projects move new ones dig deeper into existing grow them depending business needs.","8+ years software development experience focused web technologies including significant production work Python 3+ designing, building maintaining enterprise data pipelines and/or warehouses High level knowledge machine learning algorithms ML models built deployed Demonstrable big databases columnar stores e.g. Cassandra BigTable Hadoop well SQL MySQL, MSSQL PostgreSQL ability select right tool job Experience queued management message processing e.g., Kafka, RabbitMQ working closely product account support personnel help prioritize best solutions largest problems. Reliable organization communication skills follow verbal written commitments. Persistent approach problem-solving see completion even face complexities unknowns. A proactive mindset drives pursue rather waiting answers come you. Attention detail identify ambiguities specifications. Exceptional skills, especially communicating trade-offs technical decisions non-technical colleagues. Flexibility maintain focus evolving environment. Ability let go previous projects move new ones dig deeper existing grow depending business needs."
290,Data Engineer,Software Engineer/Data Engineer,"New York, NY 10017",New York,NY,"Ideally, the successful candidate will be located near our NYC or College Park, MD office. However, there is the opportunity to work remotely based on role and level.
Software Engineer/Data Engineer
BlueVoyant is seeking a Software Engineer/Data Engineer to help us build a data analytics platform powerful enough to protect some of the world's biggest networks, and nimble enough to adapt to a quickly evolving product vision. We are solving interesting, exciting, and important problems with smart people.
Qualifications for the Software Engineer/Data Engineer:
Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.
Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.
Experience writing and optimizing streaming and batch analytics.
Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.
BS/BA in Computer Science, Engineering, or relevant field experience.
What you will do as a Software Engineer/Data Engineer:
Work closely with analysts to transform threat analytics into production-level code.
Actively contribute to application architecture and product vision.
Participate in requirements gathering and transformation from prototype to product design.
Participate in daily development stand-up meetings and regular sprint planning and product demo meetings.
Help us stay current on the latest data processing tools and trends.
Ideal candidates will:
Thrive in our small, fast-paced, product-driven environment
Collaborate with teams from across the organization
Deliver features and fixes on tight schedules and under pressure
Present ideas in business-friendly and user-friendly language
Create systems that are maintainable, flexible and scalable
Define and follow a disciplined development and engineering workflow
Demonstrate ownership of tasks with escalation as needed
Be a subject matter expert in one or more of the technologies employed
Relentlessly push for successful customer outcomes
Possess a strong interest or background in cyber security
General responsibilities include:
Participate in all stages of an agile software development lifecycle, including product ideation, requirements gathering, architecture, design, implementation, testing, documentation, and support
Refine our software development methodology based on agile/lean practices with continuous feedback and well-defined metrics to drive improvement
Maintain up-to-date knowledge of technology standards, industry trends, emerging technologies, and software development best practices
Ensure technical issues are quickly resolved and help implement strategies and solutions to reduce the likelihood of reoccurrence
Identify competitive offerings and opportunities for innovation including assessments of risk/reward to the company.
About BlueVoyant
BlueVoyant is a global cybersecurity firm that provides Advanced Threat Intelligence, for large companies and a comprehensive Managed Security Service and Professional Services for small businesses, powered by one of the largest commercially available cyber threat databases in the world.
By working with BlueVoyant, companies can gain unique and far-reaching visibility into malicious activity on their networks, in the dark web and across the internet, as well as real-time, automatable remediation services. Through our unique real-time external threat monitoring, predictive human and machine-sourced intelligence, and proactive managed security and incident response, BlueVoyant offers the private sector exceptional cyber defense capabilities.
Co-founded by CEO Jim Rosenthal, former Chief Operating Officer at Morgan Stanley, and Executive Chairman Tom Glocer, former Chief Executive Officer at Thomson Reuters, BlueVoyant has attracted a management team that comes from the world's preeminent intelligence, law enforcement, and private sector organizations. Other leaders include:
Jim Penrose, COO, former EVP at Darktrace with 17 years at the NSA in key leadership roles.
Gad Goldstein, Head of BlueVoyant Europe and Chairman of Israel, former division head (Major General equivalent) in the Israel Security Agency, Shin Bet.
Robert Hannigan, Chairman of BlueVoyant International, former Director of GCHQ.
Austin Berglas, Global Head of Professional Services, former head of the FBI's New York Cyber Branch.
David Etue, Global Head of MSS, former VP of Managed Services at Rapid7.
Milan Patel, Chief Client Officer, former CTO of the FBI Cyber Division.
Ron Feler, Global Head of Threat Intelligence and Operations, former Deputy Commander of Unit 8200, the cybersecurity division of the Israel Defense Forces.
Eldad Chai, CPO, former SVP of Products at Imperva.
Jim Bieda, Senior Advisor, former NSA Deputy CTO.
Bill Crumm, Senior Advisor, former NSA SIGINT Director and former Cybersecurity Head, Morgan Stanley.
Dan Ennis, Senior Advisor, former Head of Threat Intelligence at the NSA
All employees must be authorized to work in the United States or Israel. BlueVoyant provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, BlueVoyant complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
Come work with us!
BlueVoyant is hiring software developers, infrastructure engineers, data science experts, and technologists of all types to build next generation predictive threat intelligence and advanced security monitoring solutions.
Projects currently in development include:
An Internet-scale (multi-PB, > 500k TPS) repository made up of unstructured, structured, and semi-structured data sources used for real time alerting, threat analysis, and research and development internally.
A powerful, enterprise-scale suite of products used to provide managed security services and Security Operations Center (SOC) functionality to small and medium sized enterprises around the globe.
A unique internal platform to support the data research needs of our analysts and SOC so they can quickly and effectively identify new threat actors and techniques.
fZzYqhYXzr"," Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations. Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured  Postgres, MySQL  and unstructured data sources  Elastic, Kafka, and the Hadoop ecosystem  as implemented at Internet-scale. Experience writing and optimizing streaming and batch analytics. Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment. BS/BA in Computer Science, Engineering, or relevant field experience.    ","Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala delivery background middleware, backend implementations. Familiarity large-scale, big data, streaming data technologies, as well exposure to variety structured Postgres, MySQL unstructured sources Elastic, Kafka, the Hadoop ecosystem implemented at Internet-scale. Experience writing optimizing batch analytics. Agile frameworks, secure software design, test-driven development, modern, container-delivered code deployment cloud-based DevOps environment. BS/BA Computer Science, Engineering, or relevant field experience.","Strong hands-on programming skills, expertise multiple implementation languages/frameworks including subset Python, Java, Scala delivery background middleware, backend implementations. Familiarity large-scale, big data, streaming data technologies, well exposure variety structured Postgres, MySQL unstructured sources Elastic, Kafka, Hadoop ecosystem implemented Internet-scale. Experience writing optimizing batch analytics. Agile frameworks, secure software design, test-driven development, modern, container-delivered code deployment cloud-based DevOps environment. BS/BA Computer Science, Engineering, relevant field experience."
291,Data Engineer,Senior Data Engineer,"New York, NY",New York,NY,"About Us

The SecurityScorecard ratings platform helps enterprises across the globe manage the cyber security posture of their vendors. Our SaaS products have created a new category of enterprise software and our culture has helped us be recognized as one of the 10 hottest SaaS startups in NY for two years in a row. Our investors include both Sequoia and Google Ventures. We are scaling quickly but are ever mindful of our people and products as we grow.

Position Summary

The Senior Data Analytics Engineer will build meaningful analytics that inform companies of security risk. You will be working closely with our Data Science team, implementing algorithms and managing the analytic pipeline. We have over 1 PB of data, so the ideal candidate will have experience processing and querying large amounts of data.

This role requires senior level experience with Spark, SQL and Scala. Our interview process will include live coding using these technologies.

Responsibilities


Manage the analytic pipeline using Spark, Hadoop, etc
Leverage cutting-edge technologies to support new and existing and services and processes.
Quickly and efficiently design and implement in an agile environment
Work with other team members to implement consistent architecture
Drive projects through all stages of development
Actively share knowledge and responsibility with other team members and teams
Improve the effective output of the engineering team by managing quality, and identifying inconsistencies.

Skills and Experience:

Bachelor's degree (CS, EE or Math preferred) or equivalent work experience as well as interest in a fast paced, complex environment.
5+ years of experience Scala preferred in a commercial environment
Expert in Spark, experience with the Hadoop ecosystem and similar frameworks
Expert in SQL
Familiarity with various tools such as AWS and Docker and an instinct for automation
Strong understanding of Software Architecture principles and patterns.
Experience working with 3rd party software and libraries, including open source
Experience with Postgres

Traits:

Quick-thinker who takes ownership and pride in their work
A commitment and drive for excellence and continual improvement
A strong sense of adventure, excitement and enthusiasm.
Excellent systems analytical, problem solving and interpersonal skills

Interview Process:

Initial Conversation with a SecurityScorecard Talent team to learn more about your experience and career objectives
Technical Interview with 1- 2 data engineers. This will include live coding in SQL, Spark, Scala.
Coding Exercise - take home exercise
Final Interview: Meet 1-2 engineering leaders

SecurityScorecard embraces diversity. We believe that our team is strengthened through hiring and retaining employees with diverse backgrounds, skillsets, ideas, and perspectives. We make hiring decisions based upon merit and do not discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status.","  Bachelor's degree  CS, EE or Math preferred  or equivalent work experience as well as interest in a fast paced, complex environment. 5+ years of experience Scala preferred in a commercial environment Expert in Spark, experience with the Hadoop ecosystem and similar frameworks Expert in SQL Familiarity with various tools such as AWS and Docker and an instinct for automation Strong understanding of Software Architecture principles and patterns. Experience working with 3rd party software and libraries, including open source Experience with Postgres    ","Bachelor's degree CS, EE or Math preferred equivalent work experience as well interest in a fast paced, complex environment. 5+ years of Scala commercial environment Expert Spark, with the Hadoop ecosystem and similar frameworks SQL Familiarity various tools such AWS Docker an instinct for automation Strong understanding Software Architecture principles patterns. Experience working 3rd party software libraries, including open source Postgres","Bachelor's degree CS, EE Math preferred equivalent work experience well interest fast paced, complex environment. 5+ years Scala commercial environment Expert Spark, Hadoop ecosystem similar frameworks SQL Familiarity various tools AWS Docker instinct automation Strong understanding Software Architecture principles patterns. Experience working 3rd party software libraries, including open source Postgres"
292,Data Engineer,Principal Data Engineer,"New York, NY 10003",New York,NY,"At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference.
Synopsis:
The ideal candidate is an experienced data pipeline builder and data wrangler who is experienced at optimizing data platforms and building them from the ground up. The Principal Data Engineer will support developers, architects, analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout projects.

The candidate must be self-directed and comfortable supporting the needs of multiple teams, systems, and products simultaneously. The right candidate will be excited by the prospect of optimizing and re-designing NMâs data architecture to support our next generation of products and data initiatives.

Responsibilities:
Understand and align data architecture strategy to the business and technology strategy
Partner with architecture and development teams to evolve software products to exceed the needs and expectations of the consumer
Deliver technology products that yield immediate business value
Collaborate as part of a high-performing team and strengthen the NML tech community as a whole
Understand development costs and resourcing, compliance, security, and risk

Skills:
Eat, sleep, breathe data/databases (SQL (MySQL, etc.), NoSQL/BigData(Cassandra, etc.)
Better delivery than Dominos (architectural expertise, CI/CD experience, PjM skills)
More renown architecturally than Frank Lloyd Wright
Knows clouds (AWS) better than the sky
Plays well with others

What youâll do every day:
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, NoSQL, and AWS technologies.
Create and maintain optimal data pipeline architectures
Assemble large, complex data sets that meet functional / non-functional business requirements.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Assure NMLâs data compliance policies are strictly enforced throughout NML IPS data platforms.
Create data pipelines for the analytics and data scientist team membersâ and assist them in building and optimizing our products into an innovative industry leader.
Be a thought leader on strategies and best practices with respect to data architecture, data integration, and data quality
Keep abreast of industry trends & standards
Communicate effectively with technical & non-technical resources
Train and mentor teammates and other (x-dept) Data Engineers

What kind of person should you be?
You pride yourself on your demonstrated secure data architecture and engineering skills
You have incredible and proven problem-solving, and analytical skills
You have the ability to quickly learn unfamiliar platforms and frameworks
You can handle the most stressful situations calmly
You often suggest ideas that can improve operations
You're self-motivated
Your ability to debug & triage complex problems is outstanding
You have the ability to assess maintainability & long-term impacts of design choices
Work well independently or as part of a team
Yourself. Integrity is paramount

What skills do you need?
Bachelorâs degree in Computer Science or Engineering, or equivalent experience
8+ years of work experience with data architecture/engineering experience
8+ years of demonstrated expertise with relational and non-relational database technology
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience building and optimizing data pipelines, architectures, and data sets. âBig Dataâ experience preferred
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Familiar with stream-processing systems: Storm, Spark-Streaming, etc.
Familiar with data pipeline and workflow management tools
Professional CSP Experience (AWS preferred) with services such as RDS, Glue, etc
OSWAP-minded, experienced with static and dynamic code analytical tools (Blackduck, etc)
Excellent written and verbal communications skills

Grow your career with a best-in-class company that puts our clientâs interests at the center of all we do. Get started now!
We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.

Req ID: 25650
Position Type: Regular Full Time
Education Experience: Bachelor's Required
Employment Experience: 9+ years
Licenses/Certifications:
FLSA Status: Exempt
Posting Date: 07/15/2019","  Eat, sleep, breathe data/databases  SQL  MySQL, etc. , NoSQL/BigData Cassandra, etc.  Better delivery than Dominos  architectural expertise, CI/CD experience, PjM skills  More renown architecturally than Frank Lloyd Wright Knows clouds  AWS  better than the sky Plays well with others  Understand and align data architecture strategy to the business and technology strategy Partner with architecture and development teams to evolve software products to exceed the needs and expectations of the consumer Deliver technology products that yield immediate business value Collaborate as part of a high-performing team and strengthen the NML tech community as a whole Understand development costs and resourcing, compliance, security, and risk  ","Eat, sleep, breathe data/databases SQL MySQL, etc. , NoSQL/BigData Cassandra, Better delivery than Dominos architectural expertise, CI/CD experience, PjM skills More renown architecturally Frank Lloyd Wright Knows clouds AWS better the sky Plays well with others Understand and align data architecture strategy to business technology Partner development teams evolve software products exceed needs expectations of consumer Deliver that yield immediate value Collaborate as part a high-performing team strengthen NML tech community whole costs resourcing, compliance, security, risk","Eat, sleep, breathe data/databases SQL MySQL, etc. , NoSQL/BigData Cassandra, Better delivery Dominos architectural expertise, CI/CD experience, PjM skills More renown architecturally Frank Lloyd Wright Knows clouds AWS better sky Plays well others Understand align data architecture strategy business technology Partner development teams evolve software products exceed needs expectations consumer Deliver yield immediate value Collaborate part high-performing team strengthen NML tech community whole costs resourcing, compliance, security, risk"
293,Data Engineer,Data Engineer,"New York, NY",New York,NY,"When was the last time you were planning a business trip and really tried to save your company money? If your company allowed you to stay in a fancy hotel, would you ever volunteer to stay at an Airbnb or at a friend's house? How about flying coach instead of business class? The vast majority of employees optimize for comfort and convenience, spending at the high end of their company policy limits, because, well, why not? So how can a company get its employees to care about expenses without implementing draconian policies, creating friction and frustrating employees? How can a company motivate its employees to save?

The answer is Rocketrip. We're a NYC-based startup that rewards business travelers for cost-sensitive behavior. It's a win-win: companies save, while employees cash in with real rewards.

The Role:
---------

We are seeking a Data Engineer who can operate within our engineering organization and help take our data strategy to the next level. The Data Engineer will be able to design, code and provide architecture solutions for the team, including but not limited to ETL, data warehousing, and data integration. The right candidate for this role is someone who is passionate about technology and interacting with product owners, thrives in ambiguity, and is focused on delivering exceptional results with great teamwork skills in a scrappy, startup environment. The candidate will have the opportunity to influence and interact with fellow engineers beyond their team.

Responsibilities:
-----------------


Design and development of ETL and data pipeline solutions for complex business problems to load Data Warehouse
Data Stewardship - own or support the data definitions and lineage across our organization.
Create a data integration plan and build data integrations between systems.
Figure out the best way to share information and build the tech needed to execute.
Mentoring - help teach other team members about data architecture and also be a consultant for developers who need help with data.

Requirements:
-------------


At least 3 years of relevant experience.
Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments like RDS, MySQL, Python, Pyspark, Airtable, Talend.
Experience developing, deploying, and testing in AWS

Here at Rocketrip, we...
------------------------


Are in growth mode where all work has impact.
Offer great benefits, including medical, dental and optical.
Give all employees free membership to One Medical.
Provide access to a 401k plan and offer matching.
Believe it's important to rejuvenate and offer a ""take what you need"" vacation policy.
Encourage employees to spend the holidays exploring, relaxing, or with loved ones by closing our offices during the last week of December
Regularly huddle up as a company to share goals, learnings and celebrate!
Have a dog-friendly office.
Provide access to gym membership and Citibike discounts.

Founded in 2013 and headquartered in New York City, Rocketrip is aiming to revolutionize business travel by introducing the motivation to save. We're a group of tech innovators who looked at the current state of business travel, became frustrated by the antiquated employee and employer experiences, and decided to do something big about it. Our team is focused on utilizing technology, design and data to align employee and company interests.

Rocketrip is backed by a renowned set of investors and advisors, including Google Ventures, Bessemer Venture Partners, Canaan Partners, Genacast Ventures, and Y Combinator.","   Design and development of ETL and data pipeline solutions for complex business problems to load Data Warehouse Data Stewardship - own or support the data definitions and lineage across our organization. Create a data integration plan and build data integrations between systems. Figure out the best way to share information and build the tech needed to execute. Mentoring - help teach other team members about data architecture and also be a consultant for developers who need help with data.    At least 3 years of relevant experience. Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments like RDS, MySQL, Python, Pyspark, Airtable, Talend. Experience developing, deploying, and testing in AWS ","Design and development of ETL data pipeline solutions for complex business problems to load Data Warehouse Stewardship - own or support the definitions lineage across our organization. Create a integration plan build integrations between systems. Figure out best way share information tech needed execute. Mentoring help teach other team members about architecture also be consultant developers who need with data. At least 3 years relevant experience. Experience working warehouses, including warehouse technical architectures, infrastructure components, ETL/ELT reporting/analytic tools environments like RDS, MySQL, Python, Pyspark, Airtable, Talend. developing, deploying, testing in AWS","Design development ETL data pipeline solutions complex business problems load Data Warehouse Stewardship - support definitions lineage across organization. Create integration plan build integrations systems. Figure best way share information tech needed execute. Mentoring help teach team members architecture also consultant developers need data. At least 3 years relevant experience. Experience working warehouses, including warehouse technical architectures, infrastructure components, ETL/ELT reporting/analytic tools environments like RDS, MySQL, Python, Pyspark, Airtable, Talend. developing, deploying, testing AWS"
294,Data Engineer,"Senior Data Engineer, Refinitiv Labs - NYC","New York, NY",New York,NY,"Refinitiv Labs provides a unique opportunity to work in a global company with a huge amount of high quality data. You will be:
Work collaboratively with team members and customers to identify opportunities and build next generation solutions for our customers.
Building agile data pipelines to support prototypes and MVPs
Working with large, diverse data sets using big data and public cloud technologies
Defining and implementing data engineering best practice across all Lab locations
Creating modern data infrastructure to support research, data-science and machine-learning projects.
Researching new data engineering approaches to handle alternative data
Developing and broadening your skills through mentoring and collaboration with experienced professionals

As a member of our NYC team you will be part of our global network of data and research scientists, engineers, UX/UI designers and other like-minded colleagues in our sister labs in San Francisco, London, and Singapore.
Data is the infrastructure that our Lab runs on. Refinitiv has the worlds largest repository of curated data. From petabytes of tick-by-tick market data to the worlds largest collection of financial documents, our content fuels the financial markets allowing investors to make decisions and financial institutions to reduce risk and even fight financial crime.
Requirements
Collaborate with the Data Science & Engineering teams to share best practice for building data pipelines across all locations of Refinitiv Labs
Provide expertise on development of data models and data wrangling
Create and maintain optimal data pipeline architecture
Manage internal stakeholders to obtain access and meet requirements for data security.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS, GCP, Azure and open source technologies.
Build analytics tools that monitor the data pipelines to ensure resiliency and cost efficiency.
Create data tools for analytics and data scientist team members to easily access, experiment and productionise our innovation and research projects.
Collaborate within the labs, with other teams within the company, and with our customers and partners.

Qualifications for Data Engineer
Bachelor degree required in Computer Science, Technology, or similar field.
3 - 5 years of work experience.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience building and optimizing data pipelines, architectures and data sets.
Experience creating data profiling, cleansing and data management services
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable big data stores.
Strong project management and organizational skills.
Interest in Machine Learning & Analytics Operations

Desirable Technical Requirements
Experience with relational SQL, NoSQL, and graph databases, including DynamoDB, Redis, Postgres, Neo4J and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, Gremlin etc. and experience building MVPs.
Experience with big data tools: Spark, Kafka, Hadoop.
Experience/working with ETL patterns and tools.
Experience with deployment and containerization, e.g., Docker and Kubernetes.
The Financial and Risk Business of Thomson Reuters is now Refinitiv. Refinitiv equips the financial community with access to an open platform that uncovers opportunity and catalyzes change. With a dynamic combination of data, insights, technology, and news from Reuters, our customers can access solutions for every challenge, including a breadth of applications, tools, and contentâall supported by human expertise. At Refinitiv, we facilitate the connections that propel people and organizations to find new possibilities to move forward.
As a global business, we rely on diversity of culture and thought to deliver on our goals. Therefore we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under country or local law. Refinitiv is proud to be an Equal Employment Opportunity/Affirmative Action Employer providing a drug-free workplace.

Intrigued by a challenge as large and fascinating as the world itself? Come join us.

Locations
New York-New York-United States of America"," Bachelor degree required in Computer Science, Technology, or similar field. 3 - 5 years of work experience. Experience supporting and working with cross-functional teams in a dynamic environment. Experience building and optimizing data pipelines, architectures and data sets. Experience creating data profiling, cleansing and data management services Build processes supporting data transformation, data structures, metadata, dependency and workload management. Working knowledge of message queuing, stream processing, and highly scalable big data stores. Strong project management and organizational skills. Interest in Machine Learning & Analytics Operations     Experience with relational SQL, NoSQL, and graph databases, including DynamoDB, Redis, Postgres, Neo4J and Cassandra. Experience with AWS cloud services  EC2, EMR, RDS, Redshift, Athena Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, Gremlin etc. and experience building MVPs. Experience with big data tools  Spark, Kafka, Hadoop. Experience/working with ETL patterns and tools. Experience with deployment and containerization, e.g., Docker and Kubernetes.","Bachelor degree required in Computer Science, Technology, or similar field. 3 - 5 years of work experience. Experience supporting and working with cross-functional teams a dynamic environment. building optimizing data pipelines, architectures sets. creating profiling, cleansing management services Build processes transformation, structures, metadata, dependency workload management. Working knowledge message queuing, stream processing, highly scalable big stores. Strong project organizational skills. Interest Machine Learning & Analytics Operations relational SQL, NoSQL, graph databases, including DynamoDB, Redis, Postgres, Neo4J Cassandra. AWS cloud EC2, EMR, RDS, Redshift, Athena object-oriented/object function scripting languages Python, Java, C++, Scala, Gremlin etc. experience MVPs. tools Spark, Kafka, Hadoop. Experience/working ETL patterns tools. deployment containerization, e.g., Docker Kubernetes.","Bachelor degree required Computer Science, Technology, similar field. 3 - 5 years work experience. Experience supporting working cross-functional teams dynamic environment. building optimizing data pipelines, architectures sets. creating profiling, cleansing management services Build processes transformation, structures, metadata, dependency workload management. Working knowledge message queuing, stream processing, highly scalable big stores. Strong project organizational skills. Interest Machine Learning & Analytics Operations relational SQL, NoSQL, graph databases, including DynamoDB, Redis, Postgres, Neo4J Cassandra. AWS cloud EC2, EMR, RDS, Redshift, Athena object-oriented/object function scripting languages Python, Java, C++, Scala, Gremlin etc. experience MVPs. tools Spark, Kafka, Hadoop. Experience/working ETL patterns tools. deployment containerization, e.g., Docker Kubernetes."
295,Data Engineer,"Engineer, Data","New York, NY 10001",New York,NY,"Location: New York, NY

Position Summary:

The Data Engineer is responsible for building and deploying streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably. You will be responsible for ingesting and integrating large volumes of disparate data from a variety of sources, including but not limited to subscriber & listener data, customer journey data, vehicle data, video and 2nd/3rd party data. This will involve rapid innovation in large scale data pipeline design and development to ensure critical data sets are made available to our users and predictive models in a timely manner. We are looking for someone with hands on experience in all layers of the full stack involving data. The Data Engineer plays a significant role as both an enabler and practitioner of the data and analytics driven culture at SiriusXM.

Duties and Responsibilities:

Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably.
Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions.
Gather and process all types of data including raw, structured, semi-structured, and unstructured data.
Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics.
Build and maintain dimensional data warehouses in support of business intelligence tools.
Develop data catalogs and data validations to ensure clarity and correctness of key business metrics.
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result.
Derive an overall strategy of data management, within an established information architecture (including both structured and unstructured data), that supports the development and secure operation of existing and new information and digital services.
Plan effective data storage, security, sharing and publishing within the organization.
Ensure data quality and implement tools and frameworks for automating the identification of data quality issues.
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Provide ongoing support, monitoring, and maintenance of deployed products.
Drive and maintain a culture of quality, innovation and experimentation.
Supervisory Responsibilities:

None
Minimum Qualifications:

Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs.
Minimum 2 years professional experience working with data extract/manipulation logic.
Minimum 2 years professional experience with object-oriented programming, functional programming, and data design.
1-3 years working with a public cloud big data ecosystem (certification in AWS a plus).
1-3 years working with MPP databases, distributed databases, and/or Hadoop.
Requirements and General Skills:

Passion for data engineering, able to excite and lead by example.
Hungry and eager to learn new systems and technologies.
Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.
Ability to deliver exceptional results through iterative improvement rather than initial perfection.
Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including: business users, technical staff, senior level colleagues, vendors, and partners.
An extensive track record that demonstrates effectiveness in driving business results through data and analytics.
The ability to develop and articulate a compelling vision and generate necessary consensus.
A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.
A proven ability to influence decision making across large organizations.
A proven ability to hire, develop, and effectively lead deeply technical resources.
Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.
Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.
Create an environment where people from diverse cultures and backgrounds work together effectively.
Technical Skills:

Experience deploying and running AWS-based data solutions and familiar with tools such as Cloud Formation, IAM, Athena, and Kinesis.
Experience engineering big-data solutions using technologies like EMR, S3, Spark and an in-depth understanding of data partitioning and sharding techniques.
Experience loading and querying both on premise and cloud-hosted databases such as Teradata and Redshift.
Building streaming data pipelines using Kafka, Spark, or Flink.
Familiarity with binary data serialization formats such as Parquet, Avro, and Thrift.
Experience deploying data notebook and analytic environments such as Jupyter and Databricks.
Knowledge of the Python data ecosystem using pandas and numpy.
Experience building and deploying ML pipelines: training models, feature development, regression testing.
Experience with graph-based data workflows using Apache Airflow a plus.
Knowledge of data profiling, data modeling, and data pipeline development.
Strong knowledge with high volume heterogeneous data, preferably with distributed systems.
Strong knowledge writing distributed, high-volume services in Python, Java or Scala.
Familiar with metadata management, data lineage, and principles of data governance.
Knowledge of data modeling, data access, and data storage techniques.
Appreciation of agile software processes, data-driven development, reliability, and responsible experimentation.
Minimum 2 years' experience with the following:

professional role in one or more of the following: Development, Engineering, R&D or Information Technology
Strong and thorough knowledge of the following:

ETL/ELT Tools
BI tools
MDM / Reference Data
RDBMS, NoSQL and NewSQL
MS Office Suite
SiriusXM is an equal opportunity employer that does not discriminate on the basis of sex, race, color, age, national origin, religion, creed, physical or mental disability, medical condition, marital status, sexual orientation, gender identity or expression, citizenship, pregnancy, military or veteran status or any other status protected by applicable law.

The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice."," Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs. Minimum 2 years professional experience working with data extract/manipulation logic. Minimum 2 years professional experience with object-oriented programming, functional programming, and data design. 1-3 years working with a public cloud big data ecosystem  certification in AWS a plus . 1-3 years working with MPP databases, distributed databases, and/or Hadoop.   Passion for data engineering, able to excite and lead by example. Hungry and eager to learn new systems and technologies. Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next. Ability to deliver exceptional results through iterative improvement rather than initial perfection. Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including  business users, technical staff, senior level colleagues, vendors, and partners. An extensive track record that demonstrates effectiveness in driving business results through data and analytics. The ability to develop and articulate a compelling vision and generate necessary consensus. A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions. A proven ability to influence decision making across large organizations. A proven ability to hire, develop, and effectively lead deeply technical resources. Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals. Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals. Create an environment where people from diverse cultures and backgrounds work together effectively.   Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably. Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions. Gather and process all types of data including raw, structured, semi-structured, and unstructured data. Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics. Build and maintain dimensional data warehouses in support of business intelligence tools. Develop data catalogs and data validations to ensure clarity and correctness of key business metrics. Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result. Derive an overall strategy of data management, within an established information architecture  including both structured and unstructured data , that supports the development and secure operation of existing and new information and digital services. Plan effective data storage, security, sharing and publishing within the organization. Ensure data quality and implement tools and frameworks for automating the identification of data quality issues. Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings. Provide ongoing support, monitoring, and maintenance of deployed products. Drive and maintain a culture of quality, innovation and experimentation.    Passion for data engineering, able to excite and lead by example. Hungry and eager to learn new systems and technologies. Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next. Ability to deliver exceptional results through iterative improvement rather than initial perfection. Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including  business users, technical staff, senior level colleagues, vendors, and partners. An extensive track record that demonstrates effectiveness in driving business results through data and analytics. The ability to develop and articulate a compelling vision and generate necessary consensus. A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions. A proven ability to influence decision making across large organizations. A proven ability to hire, develop, and effectively lead deeply technical resources. Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals. Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals. Create an environment where people from diverse cultures and backgrounds work together effectively. ","Advanced degree in relevant field of study strongly desirable, particularly computer science or engineering level programs. Minimum 2 years professional experience working with data extract/manipulation logic. object-oriented programming, functional and design. 1-3 a public cloud big ecosystem certification AWS plus . MPP databases, distributed and/or Hadoop. Passion for engineering, able to excite lead by example. Hungry eager learn new systems technologies. Self-directed enjoys the challenge freedom deciding what is most impactful thing work on next. Ability deliver exceptional results through iterative improvement rather than initial perfection. Excellent communication presentation skills ability interact appropriately all levels organization, including business users, technical staff, senior colleagues, vendors, partners. An extensive track record that demonstrates effectiveness driving analytics. The develop articulate compelling vision generate necessary consensus. A successful history translating objectives problems into analytic problems, solutions actionable solutions. proven influence decision making across large organizations. hire, develop, effectively deeply resources. Demonstrate foster sense urgency, strong commitment, accountability while sound decisions achieving goals. Articulate, inspire, engage commitment plan action aligned organizational mission Create an environment where people from diverse cultures backgrounds together effectively. Build deploy streaming batch pipelines capable processing storing petabytes quickly reliably. Collaborate product teams, analysts scientists design build data-forward Gather process types raw, structured, semi-structured, unstructured data. Integrate variety providers ranging marketing, web analytics, consumer devices IoT Telematics. maintain dimensional warehouses support intelligence tools. Develop catalogs validations ensure clarity correctness key metrics. Design, code, test, correct document programs scripts using agreed standards tools achieve well-engineered result. Derive overall strategy management, within established information architecture both structured , supports development secure operation existing digital services. Plan effective storage, security, sharing publishing organization. Ensure quality implement frameworks automating identification issues. internal external validation providing feedback customized changes feeds mappings. Provide ongoing support, monitoring, maintenance deployed products. Drive culture quality, innovation experimentation.","Advanced degree relevant field study strongly desirable, particularly computer science engineering level programs. Minimum 2 years professional experience working data extract/manipulation logic. object-oriented programming, functional design. 1-3 public cloud big ecosystem certification AWS plus . MPP databases, distributed and/or Hadoop. Passion engineering, able excite lead example. Hungry eager learn new systems technologies. Self-directed enjoys challenge freedom deciding impactful thing work next. Ability deliver exceptional results iterative improvement rather initial perfection. Excellent communication presentation skills ability interact appropriately levels organization, including business users, technical staff, senior colleagues, vendors, partners. An extensive track record demonstrates effectiveness driving analytics. The develop articulate compelling vision generate necessary consensus. A successful history translating objectives problems analytic problems, solutions actionable solutions. proven influence decision making across large organizations. hire, develop, effectively deeply resources. Demonstrate foster sense urgency, strong commitment, accountability sound decisions achieving goals. Articulate, inspire, engage commitment plan action aligned organizational mission Create environment people diverse cultures backgrounds together effectively. Build deploy streaming batch pipelines capable processing storing petabytes quickly reliably. Collaborate product teams, analysts scientists design build data-forward Gather process types raw, structured, semi-structured, unstructured data. Integrate variety providers ranging marketing, web analytics, consumer devices IoT Telematics. maintain dimensional warehouses support intelligence tools. Develop catalogs validations ensure clarity correctness key metrics. Design, code, test, correct document programs scripts using agreed standards tools achieve well-engineered result. Derive overall strategy management, within established information architecture structured , supports development secure operation existing digital services. Plan effective storage, security, sharing publishing organization. Ensure quality implement frameworks automating identification issues. internal external validation providing feedback customized changes feeds mappings. Provide ongoing support, monitoring, maintenance deployed products. Drive culture quality, innovation experimentation."
296,Data Engineer,Data Engineer,"New York, NY 10003",New York,NY,"Medidata: Conquering Diseases Together

WHAT WE'RE LOOKING FOR
Medidataâs Data and Analytics Group is seeking an engineer that is passionate about working with clinical data. Weâre looking for someone who is excited by data and the value that can be found in it, and who has the skills and drive to deliver value from it. You will be a key contributor in growing and expanding our clinical data platform by making our data assets usable and accessible to platform consumers.
We work with a variety of cutting edge as well as industry standard techniques and tools. We are looking for candidates that can both support existing technologies and apply new techniques to solve the next generation of data issues that we will encounter as we extend and enlarge data platform.
WHAT YOUâLL DO
In this role, you will be responsible for data acquisition from source systems, transformation, standardization, and delivery to enterprise repositories and systems. You will work with product teams to understand business requirements. Using your understanding of clinical data, you will define the acquisition, transformation and delivery needs, will lead efforts to understand and design for production data workloads and shape, and will work with our architecture team to deliver solutions that are aligned with enterprise architecture plans.
You will deliver solutions that work within a DevOps delivery pipeline using infrastructure designs that are scalable, resilient and highly performant. You will develop mapping and testing artifacts that enable data movement solutions to be verified as meeting functional, non-functional and business requirements. Your mission will be to deliver efficient and error free data movement systems and processes.

WHO YOU ARE
Required Skills:
You hold at least a bachelorâs degree in Computer Science or a related discipline
You have at least 3 yearsâ experience working with large and complex data sets
You have at least 6 years experience working with clinical data (e.g. clinical trial data, lab data, EMR data, etc.)
You are a proficient Java developer
You are a proficient SQL developer
You have experience writing ETL/ELT code
You have experience with data profiling tools and concepts
Nice to Have:
Experience with build frameworks such as Maven, Gradle or Ant
Agile experience
Experience working with Message Bus technologies
Any combination of these AWS Technologies:
EC2 w/ EBS
Step Functions
Aurora
Redshift
Kinesis
EMR
ECS/ECR
Prior experience working with ETL systems (e.g. Pentaho, Talend, Informatica, etc.)
Oracle PL/SQL experience
Medidata Solutions, Inc. is an Equal Opportunity Employer. Medidata Solutions provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or status as a veteran. Medidata Solutions complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.

#LI-AS1","  You hold at least a bachelorâs degree in Computer Science or a related discipline You have at least 3 yearsâ experience working with large and complex data sets You have at least 6 years experience working with clinical data  e.g. clinical trial data, lab data, EMR data, etc.  You are a proficient Java developer You are a proficient SQL developer You have experience writing ETL/ELT code You have experience with data profiling tools and concepts   ","You hold at least a bachelorâs degree in Computer Science or related discipline have 3 yearsâ experience working with large and complex data sets 6 years clinical e.g. trial data, lab EMR etc. are proficient Java developer SQL writing ETL/ELT code profiling tools concepts","You hold least bachelorâs degree Computer Science related discipline 3 yearsâ experience working large complex data sets 6 years clinical e.g. trial data, lab EMR etc. proficient Java developer SQL writing ETL/ELT code profiling tools concepts"
297,Data Engineer,AWS Data Engineer,"New York, NY 10011",New York,NY,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
298,Data Engineer,Data Engineer,"New York, NY 10017",New York,NY,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By creating a single source of truth, Reonomy products empower individuals, teams and companies to share information, unlock insights and discover new opportunities.

Headquartered in New York, Reonomy has raised ~$70 million from top investors, including Sapphire Ventures, Bain Capital, Softbank and Primary Ventures. Our clients represent the biggest names in CRE, including Newmark Knight Frank, Cushman & Wakefield, Tishman Speyer and WeWork.

If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning!

ABOUT THE ROLE:

As a Data Engineer at Reonomy, you will tackle hard challenges everyday! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE SaaS solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, ElasticSearch and Docker.
Responsibilities include:
Collaborating with the Engineering team to design, build and improve Reonomyâs complex data layer
Creating data systems that ensure quality and consistency on our data platform
Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data
Playing a major role in the future architecture of our rapidly expanding backend platform
Writing high quality code, participating actively in code reviews, and consistently helping to ship software
ABOUT YOU:
6+ years of experience in a Data Engineering capacity
Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets
Proven ability leveraging database technologies to solve non-trivial, large-scale problems
Advanced/Expert knowledge in SQL and data analysis
Experience programming in both typed (Scala, Java, etc..) and non-type (Python, Ruby, etc..) languages on production projects
Experience building modern, data-driven, web applications with emphasis on strong software design methodologies
A serious passion for data
History of excellence and responsibility in previous engineering positions
BENEFITS:
Competitive salary
Company stock options
100% coverage on medical, vision and dental health plans
Unlimited Vacation
401k plan and commuter benefits
Office perks: catered lunches 3x/week, catered breakfast 2x/week, unlimited snacks, team happy hours, Free Citi Bike membership, fitness discounts, Free Spotify membership, dedicated wellness rooms in the office!
We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means.","   Collaborating with the Engineering team to design, build and improve Reonomyâs complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software  ","Collaborating with the Engineering team to design, build and improve Reonomyâs complex data layer Creating systems that ensure quality consistency on our platform Solving real challenges around creating import, cleanse, structure, display huge volumes of Playing a major role in future architecture rapidly expanding backend Writing high code, participating actively code reviews, consistently helping ship software","Collaborating Engineering team design, build improve Reonomyâs complex data layer Creating systems ensure quality consistency platform Solving real challenges around creating import, cleanse, structure, display huge volumes Playing major role future architecture rapidly expanding backend Writing high code, participating actively code reviews, consistently helping ship software"
299,Data Engineer,Data Engineer - Hux,"New York, NY 10112",New York,NY,"Hux Data Engineer
Locations: New York, NY â Greensboro, NC - Chicago, IL â Raleigh, Durham, Chapel-Hill, NC - Denver, CO
What is Hux? Hux is the Human Experience Platform by Deloitte Digital.
In todayâs world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.
Customers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.
Hux by Deloitte Digital gives companies the ability to build and leverage the connections â between people, systems, data and technologies â so they can deliver personalized, contextual experiences to customers at scale.

Work youâll do
As a Hux Data Engineer, youâll design, implement, and maintain a full suite of real-time and batch jobs that fuels our cutting edge AI to provide real-time marketing intelligence to our existing clients.
Youâll develop, test and deliver production grade code to help our clients solve their marketing challenges using cutting-edge big-data tools. Youâll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall Platform.
As you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, youâll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.
Your responsibilities will include:
Design, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.
Bring together large, complex and sparse data sets to meet functional and non-functional business requirements.
Design and implements data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.
Integrate new data management technologies and software engineering tools into existing structures.
Help build high-performance algorithms, prototypes, predictive models and proof of concepts.
Use a variety of languages, tools and frameworks to marry data and systems together.
Recommend ways to improve data reliability, efficiency and quality.
Collaborate with Data Scientists, DevOps and Project Managers on meeting project goals.
Tackle challenges and solve complex problems on a daily basis.
Qualifications
Required:
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.
Preferred:
Producing high-quality code in Python.
Passionate about testing, and with extensive experience in Agile teams using SCRUM you consider automated build and test to be the norm.
Proven ability to communicate in both verbal and writing in a high performance, collaborative environment.
Follows data development best practices, and enjoy helping others learn to do the same.
An independent thinker who considers the operating context of what he/she is developing.
Believes that the best data pipelines run unattended for weeks and months on end.
Familiar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.
Helpful, but not required:
Knowledge in:
Experience with large consumer data sets used in performance marketing is a major advantage.
Familiarity with machine learning libraries is a plus.
Well-versed in (or contributes to) data-centric open source projects.
Reads Hacker News, blogs, or stays on top of emerging tools in some other way
Data visualization
Industry-specific marketing data
Technologies of Interest:
Languages/Libraries â Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.
Cloud â AWS, Azure, Google
The team
Advertising, Marketing & Commerce
Our Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clientsâ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clientsâ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.

We serve our clients through the following types of work:Cross-channel customer engagement strategy, design and development(web, mobile, social, physical)eCommerce strategy, implementation and operationsMarketing Content and digital asset management solutionsMarketing Technology and Advertising Technology solutionsMarketing analytics implementation and operationsAdvertising campaign ideation, development and executionAcquisition and engagement campaign ideation, development and executionAgile based, design-thinking, user-centric, empirical projects that accelerate results

How youâll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe thereâs always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitteâs culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitteâs impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youâre applying to. Check out recruiting tips from Deloitte professionals.
kwhux"," 4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment. 2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores. 1+ years of experience on distributed, high throughput and low latency architecture. 1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale. A successful track-record of manipulating, processing and extracting value from large disconnected datasets.    ","4+ years of experience in software development, a substantial part which was gained high-throughput, decision-automation related environment. 2+ working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores. 1+ on distributed, high throughput low latency architecture. deploying or managing pipelines for supporting data-science-driven decisioning at scale. A successful track-record manipulating, processing extracting value from large disconnected datasets.","4+ years experience software development, substantial part gained high-throughput, decision-automation related environment. 2+ working big data using technologies like Spark, Kafka, Flink, Hadoop, NoSQL datastores. 1+ distributed, high throughput low latency architecture. deploying managing pipelines supporting data-science-driven decisioning scale. A successful track-record manipulating, processing extracting value large disconnected datasets."
300,Data Engineer,"Data Engineer, Data Acquisition","Hoboken, NJ 07030",Hoboken,NJ,"Position Description
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the worldâs leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services â including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering
Minimum Qualifications
Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment.Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake.Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges.Mitigate Risks in our data infrastructure by developing the best in class tools and processes.Implement controls, policies, processes and best practices in the Data Engineering space.Evangelize an extremely high standard of code quality, system reliability, and performance.Help us improve our database deployment and change management process.Provide reliable and efficient Data services as part of the global data team.Work closely with the team on development best practices and standards.Be a mentor.
Who you are:
You have prior experience with leading data engineering efforts across a variety of data systemsYou have deep understanding of commercial data sources and understand database concepts and terminologyYou have a demonstrated track record of handling multiple complex sourcing projects and delivering results in the data engineering areaYou have strong SQL experience and the ability to work on multiple aspects of a data projects including ETL, tools integrations, data results and APIs.You are a team player, with the courage to drive change through disruption while maintaining a respect for the team
Requirements:
Very Strong engineering skills. Should have an analytical approach and have good programming skills.Provide business insights, while leveraging internal tools and systems, databases and industry dataMinimum of 5+ yearsâ experience. Experience in retail business will be a plus.Excellent written and verbal communication skills for varied audiences on engineering subject matterAbility to document requirements, data lineage, subject matter in both business and technical terminology.Guide and learn from other team members.Demonstrated ability to transform business requirements to code, specific analytical reports and toolsThis role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering teamExperience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.) and platforms such as HDP, Cloudera etc.Strong Hadoop scripting skills to process petabytes of dataExperience in Unix/Linux shell scripting or similar programming/scripting knowledgeReal time data ingestion (Kafka)Experience in ETL/ processes with exposure to one or more tools such as Nifi, Talend, Informatica, SSIS etc.
Additional Preferred Qualifications

Company Summary
The Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the worldâs largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.
Position Summary
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the worldâs leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services â including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering.","Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment.Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake.Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges.Mitigate Risks in our data infrastructure by developing the best in class tools and processes.Implement controls, policies, processes and best practices in the Data Engineering space.Evangelize an extremely high standard of code quality, system reliability, and performance.Help us improve our database deployment and change management process.Provide reliable and efficient Data services as part of the global data team.Work closely with the team on development best practices and standards.Be a mentor.    ","Play a pivotal design and hands on implementation role in improving the Data infrastructure project-oriented work environment.Influence cross functional architecture sprint planningGather process raw data at scale from internal external sources expose mechanisms for large parallel processingDesign, implement manage near real-time ingestion pipeline into warehouse Hadoop lake.Process unstructured form suitable analysis then empower state-of-the-art analysts, scientists, APIsSolve complex SQL Big Performance challenges.Mitigate Risks our by developing best class tools processes.Implement controls, policies, processes practices Engineering space.Evangelize an extremely high standard of code quality, system reliability, performance.Help us improve database deployment change management process.Provide reliable efficient services as part global team.Work closely with team development standards.Be mentor.","Play pivotal design hands implementation role improving Data infrastructure project-oriented work environment.Influence cross functional architecture sprint planningGather process raw data scale internal external sources expose mechanisms large parallel processingDesign, implement manage near real-time ingestion pipeline warehouse Hadoop lake.Process unstructured form suitable analysis empower state-of-the-art analysts, scientists, APIsSolve complex SQL Big Performance challenges.Mitigate Risks developing best class tools processes.Implement controls, policies, processes practices Engineering space.Evangelize extremely high standard code quality, system reliability, performance.Help us improve database deployment change management process.Provide reliable efficient services part global team.Work closely team development standards.Be mentor."
301,Data Engineer,Data Engineer,"Manhattan, NY",Manhattan,NY,"VillageCare â Redefining Wellness
112 Charles St., New York, NY
VillageCareMax
Job Title: Data Engineer

Roles & Responsibilities:
The VillageCare Data Engineer will be responsible for helping to build ETL processes for Village Careâs Data Warehouse in an AWS cloud environment. The Data Engineer will build ETL pipelines, get analysis tools working properly, and stand up core data processing components in Village Careâs cloud-based data processing environment.
Building python-based ETL jobs
Light web application development of purpose-built internal tools
Technical guidance in support of our project management team when defining the scope of data integration projects
Contribute to designs of new components in modeling and data pipelines
Remain current on emerging open source data processing projects and tools
Must have experience with AWS services, Redshift data bases and HIPAA compliant architecture models.
Qualifications:
Three or more years of experience working with *nix-based, open source data processing tools
Fluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets
Two or more years of experience developing production ETL applications
Four or more years of experience in software development
Three or more years of experience with SQL
Curious, informed and opinionated about data processing technologies
Experience with structured and unstructured data storage and modeling
Deep understanding of database and filesystem storage/access
Experience with various data engineering architecture patterns
Interest in Data Science and Data Analysis
Preferred Education and Experience
BS or MS in Computer Science
Experience writing production Python
Implementation experience with Airflow, Python, Spark etc.
Experience with healthcare data formats (x12 EDI, HL7, etc)
Experience implementing stream processing pipelines (Spark, etc)
MapReduce/Hadoop ecosystem experience (Hive, HDFS/S3, Presto)
Experience with source control technology like Git
Experience with testing frameworks (unit and end-to-end)
Familiar with the usage of Continuous Integration/Continuous Deployment frameworks in AWS like Jenkins, CircleCI, Code deploy and code commit
Understanding of Docker implementation in AWS
Understanding of AWS serverless services like Lambda/API gateway
Good to have: AWS elastic beanstalk, AWS cloud formation

Integrity
You are a team member who serves as a positive example and reflection of why others trust the intentions of VillageCare by:
Being honest and trustworthy
Meeting your commitments and obligations
Acknowledging your role in actions or events with unsatisfactory outcomes
Customer Focus/Cultural Awareness
You are a team member who understands the importance of strong customer service internally and externally and you demonstrate this by identifying customer needs and expectations, and responding to them in a timely and effective manner. You are consistently customer focused by:
Demonstrating an awareness of the needs of individuals through recognizing multiple levels of connections
Anticipates and prevents delays or other things that can adversely affect the customer.
Keeping customers informed about the status of pending actions and inquires
Flexibility/Agility
You are a team member who adjusts quickly and effectively to changing conditions and demands. You understand that change is a necessary and an inevitable aspect of organizational life as well as an opportunity to learn new things. As such, you are flexible and agile by:
Maintaining a positive view of potentially stressful situations
Accepting and adapting to organizational or departmental changes
Viewing change as opportunities for VillageCare to grow in a direction that better serves our clients and our employees
Result Oriented/Innovative Thinking
You are a team member who consistently looks for new and innovative approaches that will improve efficiency in your role. You champion new ideas and build upon existing processes by:
Using data/fact-based information to make decisions relevant your role
Understands that obstacles will occur and refuses to use them as an excuse for not achieving results
BEVital
You are a team member that consistently supports VillageCareâs larger organizational culture by displaying a commitment to the three cultural drivers that make VillageCare and our employees vital to the healthcare space by:
Exceeding expectations in both internal and external customer service areas
Using data and key information to inform decisions pertinent to your role (where applicable)
Utilizing relationships, tools and positivity to enhance organizational performance through communication and collaborative team work
VillageCare is committed to superior outcomes in quality health care. Do you share a common commitment to* patient care, customer service and passion *for individualsâ well-being ?
Apply now!
VillageCare:
With over 25,000 people served in 2017, VillageCareâs mission is to promote healing, better health and well-being to the fullest extent possible.
VillageCare began in 1977 as a project by community volunteers to rescue and reorganize a for-profit nursing home slated for closure. It has become a much larger organization that provides post-acute care, community-based services and managed long-term care. As a result of this history, VillageCare has become a valued resource for the people we serve, their caregivers and other provider organizations with which we partner.
VillageCare is committed to the tenets of diversity and workforce that are strengthened by the inclusion of and respect for our differences. We offer our employees a highly competitive compensation and benefits package, a 403(b) retirement plan, and much more.
VillageCare is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation.
* EOE Minorities/Women/Disabled/Veterans*","Three or more years of experience working with  nix-based, open source data processing tools Fluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets Two or more years of experience developing production ETL applications Four or more years of experience in software development Three or more years of experience with SQL Curious, informed and opinionated about data processing technologies Experience with structured and unstructured data storage and modeling Deep understanding of database and filesystem storage/access Experience with various data engineering architecture patterns Interest in Data Science and Data Analysis   Building python-based ETL jobs Light web application development of purpose-built internal tools Technical guidance in support of our project management team when defining the scope of data integration projects Contribute to designs of new components in modeling and data pipelines Remain current on emerging open source data processing projects and tools Must have experience with AWS services, Redshift data bases and HIPAA compliant architecture models.  BS or MS in Computer Science Experience writing production Python Implementation experience with Airflow, Python, Spark etc. Experience with healthcare data formats  x12 EDI, HL7, etc  Experience implementing stream processing pipelines  Spark, etc  MapReduce/Hadoop ecosystem experience  Hive, HDFS/S3, Presto  Experience with source control technology like Git Experience with testing frameworks  unit and end-to-end  Familiar with the usage of Continuous Integration/Continuous Deployment frameworks in AWS like Jenkins, CircleCI, Code deploy and code commit Understanding of Docker implementation in AWS Understanding of AWS serverless services like Lambda/API gateway Good to have  AWS elastic beanstalk, AWS cloud formation  ","Three or more years of experience working with nix-based, open source data processing tools Fluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets Two developing production ETL applications Four software development SQL Curious, informed and opinionated about technologies Experience structured unstructured storage modeling Deep understanding database filesystem storage/access various engineering architecture patterns Interest Data Science Analysis Building python-based jobs Light web application purpose-built internal Technical guidance support our project management team when defining the scope integration projects Contribute to designs new components pipelines Remain current on emerging Must have AWS services, Redshift bases HIPAA compliant models. BS MS Computer writing Python Implementation Airflow, Spark etc. healthcare formats x12 EDI, HL7, etc implementing stream MapReduce/Hadoop ecosystem Hive, HDFS/S3, Presto control technology like Git testing frameworks unit end-to-end Familiar usage Continuous Integration/Continuous Deployment Jenkins, CircleCI, Code deploy code commit Understanding Docker implementation serverless services Lambda/API gateway Good elastic beanstalk, cloud formation","Three years experience working nix-based, open source data processing tools Fluent Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets Two developing production ETL applications Four software development SQL Curious, informed opinionated technologies Experience structured unstructured storage modeling Deep understanding database filesystem storage/access various engineering architecture patterns Interest Data Science Analysis Building python-based jobs Light web application purpose-built internal Technical guidance support project management team defining scope integration projects Contribute designs new components pipelines Remain current emerging Must AWS services, Redshift bases HIPAA compliant models. BS MS Computer writing Python Implementation Airflow, Spark etc. healthcare formats x12 EDI, HL7, etc implementing stream MapReduce/Hadoop ecosystem Hive, HDFS/S3, Presto control technology like Git testing frameworks unit end-to-end Familiar usage Continuous Integration/Continuous Deployment Jenkins, CircleCI, Code deploy code commit Understanding Docker implementation serverless services Lambda/API gateway Good elastic beanstalk, cloud formation"
302,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Founded in 2016 with only a handful of individuals, Quantexa purpose was built that through a greater understanding of context, better decisions can be made. 3 years, 6 locations and 180+ employees later we still believe that today. Working within industries such as Finance, Insurance, Energy and Government, we connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.

Our success is driven by the talent of our staff and our commitment to quality. We are looking for Data Engineers to join us in tackling some of the industryâs most challenging problems.
What does a Data Engineer role at Quantexa look like?
In order to be a successful data Engineer at Quantexa, youâll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.
Being Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. Youâll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service.
We want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but donât worry If thatâs not your strongest language or if you havenât used it before, we make sure that every Quantexan goes through our training academy so theyâre comfortable and confident with using our platform.
Requirements
What do I need to have?
Weâre looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
The desire to learn and code in Scala
Experience in working in an Agile environment
Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
A strong coding background in either Java, Python or Scala
Experience of building data processing pipelines for use in production âhands offâ batch systems, including either traditional ETL pipelines and/or analytics pipelines.
Passion and drive to grow within one of the UKâs fastest growing Start-ups
Benefits
Why join Quantexa?

We know that just having an excellent glass door rating isnât enough, so weâve put together a competitive package as a way of saying âthank youâ for all your hard work!
Competitive Salary
Company Bonus
Excellent private healthcare, Dental and Optic coverage, Life assurance, LTD and STD coverage
401k where weâll match up to 5%
Online training customized to your personal preferences
Generous annual leave
Amazing working environment - Ranging from regular social events, free beverages","     Weâre looking for individuals who have proven big data experience, either from an implementation or a data science prospective. The desire to learn and code in Scala Experience in working in an Agile environment Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch. A strong coding background in either Java, Python or Scala ","Weâre looking for individuals who have proven big data experience, either from an implementation or a science prospective. The desire to learn and code in Scala Experience working Agile environment Expert knowledge of at least one technology such as Spark, Hadoop, Elasticsearch. A strong coding background Java, Python","Weâre looking individuals proven big data experience, either implementation science prospective. The desire learn code Scala Experience working Agile environment Expert knowledge least one technology Spark, Hadoop, Elasticsearch. A strong coding background Java, Python"
303,Data Engineer,Data Engineer,"New York, NY",New York,NY,"Who We Are
Fairygodboss is an early-stage start-up based in New York. Our mission is to improve the workplace for women by creating transparency. We do this by creating a safe, anonymous and supportive place for women to leave job reviews and insight into compensation, benefits and culture. Our platform reaches over 3 million users every month and is sponsored by over 80 top employers. We are looking to double our technical team in 2019.
What You Will Be Doing

We are currently rebuilding our platform and looking to bring on an experienced data engineer to:
Develop personalization and recommendation engines used to tailor unique experiences and results on a user-by-user basis for each of our various projects.
Build out and support our Real-time and Batch processing pipelines.
Configure and monitor our event logs and analytical data stores.
Configure and maintain our BI tools and reporting libraries.

Qualifications
The ideal candidate should have experience with real-time user data processing and storage. The candidate will also have experience working with stakeholders to identify important metrics that can then be incorporated into our APIs for both internal and external use.
Required:

3+ years of Software Engineering experience
2+ years experience work with real-time/streaming data
Experience with a RDBMS (e.g. MySQL, PostgreSQL)
Experience with real time data streaming tools (e.g. Apache Kafka, AWS Kinesis)
Experience working with an OLAP or Time Series Databases (e.g. Druid)
Experience with a data processing solution. (e.g. AWS Athena, Apache Spark)

Nice To Have:
Knowledge of JavaScript & Node.js
Experience building APIs
Experience building personalization or recommendation engines
Experience with Machine Learning/Data Science

Other Tools We Use:
AWS
Docker & Container Orchestration Tools
Druid & Kinesis

Compensation:
Highly competitive and commensurate with experience.

Company Description:
Who We Are:

Fairygodboss is an early-stage start-up based in New York. Our mission is to improve the workplace for women by creating transparency. We do this by creating a safe, anonymous and supportive place for women to leave job reviews and tips about employer pay, benefits and culture. Weâre growing rapidly and expanding our team.
If you're interested in learning the ins and outs of running a digital startup and improving the world for women at the same time, this job is for you.","3+ years of Software Engineering experience 2+ years experience work with real-time/streaming data Experience with a RDBMS  e.g. MySQL, PostgreSQL  Experience with real time data streaming tools  e.g. Apache Kafka, AWS Kinesis  Experience working with an OLAP or Time Series Databases  e.g. Druid  Experience with a data processing solution.  e.g. AWS Athena, Apache Spark      ","3+ years of Software Engineering experience 2+ work with real-time/streaming data Experience a RDBMS e.g. MySQL, PostgreSQL real time streaming tools Apache Kafka, AWS Kinesis working an OLAP or Time Series Databases Druid processing solution. Athena, Spark","3+ years Software Engineering experience 2+ work real-time/streaming data Experience RDBMS e.g. MySQL, PostgreSQL real time streaming tools Apache Kafka, AWS Kinesis working OLAP Time Series Databases Druid processing solution. Athena, Spark"
304,Data Engineer,"Principal Data Engineer, AdSmart","New York, NY",New York,NY,"As a member of the Product Engineering Team, the Principal, Data Engineer, AdSmart will be directly responsible for data design, management, and development as part of building out the necessary platform and products for NBCUniversalâs AdSmart . NBCUniversalâs audience management products will enable NBCUniversal to better understand its brandâs audiences such as NBC News, Bravo, The Tonight Show, Saturday Night Live, and USA Network as well as audiences that cross brands. The goal is to ensure we know who is watching what, where and when. In turn enabling NBCUniversalâs sales teams to properly align our audiences with the market advertisements that can benefit them the most.

Youâre a big thinker who can analyze and evangelize a long range opportunity, architect a ground breaking solution, and roll-up your sleeves to get code out the door when needed. You are data driven and analytical. You understand the concept of a value proposition and evaluation criteria, and you know how to align them with low level milestones to get the work done. You can apply domain knowledge from one technical subject, in order to quickly ramp and deliver on a new one. You know how to learn from failure until you succeed, and you are able to articulate and quantify the reasons for your decisions.

You will be part of the AdSmart's Data Engineering team, participating in the data architecture that will drive both current and future data management initiatives within NBCUniversalâs Audience Studio group.

Responsibilities
Serve as a senior data engineer for audience studio data products.Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and SRAT leadershipDevelop and code the data management services that is core to Audience Studio, under the leadership of the VP ArchitectureSupport product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership
Qualifications/Requirements
Bachelorâs degree in Computer Science or related field
5+ years of software development experience, as a developer or manager
Fluency in Scala and/or Java programming languages
Strong OO & FP design patterns, data structure, and algorithm design skills
Extensive experience developing Apache Spark applications
2+ years of experience with both relational database design (SQL), non-relational (NoSQL) databases, big data, real-time technologies
Familiar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka
Experience with software containerization, such as Docker
Experience developing and / or consuming web interfaces (REST API) and associated skills (HTTP, web services)
Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment

Additional Job Requirements:
Interested candidate must submit a resume/CV through www.nbcunicareers.com to be considered
Must be willing to work in New York, NY
Desired Characteristics
Experience as a development manager (with direct authority over development staff)
Experience with Cluster Management and Container Orchestration technologies such as Mesos, Kubernetes, Hadoop/Yarn
Experience with Apache Kafka or similar streaming technologies
Experience with digital advertising technologies.
Able and eager to learn new technologies
Able to easily transition between high-level strategy and day-to-day implementation
Excellent teamwork and collaboration skills
Results oriented, high energy, self-motivated
Sub-BusinessTechnology
Career Level
Experienced
CityNew York
State/Province
New York
CountryUnited States
About Us
At NBCUniversal, we believe in the talent of our people. Itâs our passion and commitment to excellence that drives NBCUâs vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. Itâs what makes us uniquely NBCU. Here you can create the extraordinary. Join us.
Notices
NBCUniversalâs policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.","  Serve as a senior data engineer for audience studio data products.Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and SRAT leadershipDevelop and code the data management services that is core to Audience Studio, under the leadership of the VP ArchitectureSupport product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership  ","Serve as a senior data engineer for audience studio products.Participate in, and execute, 12-36 month product roadmap with input from the delivery team, stakeholders, SRAT leadershipDevelop code management services that is core to Audience Studio, under leadership of VP ArchitectureSupport overall ensure updates are 100% technically correct.Analyze report results adjust engineering strategy accordingly","Serve senior data engineer audience studio products.Participate in, execute, 12-36 month product roadmap input delivery team, stakeholders, SRAT leadershipDevelop code management services core Audience Studio, leadership VP ArchitectureSupport overall ensure updates 100% technically correct.Analyze report results adjust engineering strategy accordingly"
305,Data Engineer,Data Engineer (AWS),"New York, NY",New York,NY,"Founded in 2011 by executives at Equinox, Blink Fitness is a premium quality, value-based fitness brand with more than 90 locations open or in development throughout New York, New Jersey, Pennsylvania and California. Blink puts Mood Above Muscleâ¢, which celebrates the positive feeling you get from exercise, not just the physical benefits.
Blink recently launched a franchising system to complement its fast growing company-owned business model and has secured franchise development agreements in Georgia, Illinois, Massachusetts, Michigan, Virginia and upstate NY. This includes an agreement with Golden State Warriors forward Draymond Green, two-time NBA Champion, All Star, Olympic Gold Medalist and Defensive Player of the Year, who announced a franchise development deal to bring 20 gyms to his home state of Michigan and a portion of Illinois.
Blink is an exciting and dynamic business that is still in the start-up mode. We are a passionate team with a great entrepreneurial spirit and a willingness to roll up our sleeves to get the work done.
While Blink has grown rapidly and has already achieved significant profitability, the business is just getting warmed-up. Its leadership has a lofty vision of opening more than 300 locations in the next five years through a combination of company-owned and franchise development.
For more information visit Blinkâs consumer website - blinkfitness.com â and its franchise website â blinkfranchising.com.
Job Description:
Blink is seeking a Data Engineering professional to join our technology team. This role is a hands-on engineering position responsible for the build and maintenance of our cutting edge cloud based data platform.
Responsibilities:
Design, develop, deploy and manage a reliable and scalable data analysis pipeline, using technologies including Python, S3, and Redshift.
Participate in cross-functional initiatives to develop new capabilities, including hands-on development responsibilities.
Ability to integrate data from a variety of sources, assuring they adhere to data quality and accessibility standards.
Document processes and standard operating procedures.
Evaluate and conduct POCâs with new technologies.

Qualifications:
Bachelorâs Degree Required: Computer Science or Engineering discipline preferred.
3+ years technology experience working in Software Engineering capacity.
3+ years working in Python (other modern languages considered).
1+ years working within Analytic/Data Warehouse/Data Lake environment.
1+ years working with AWS public cloud (certification a plus).
Expertise in SQL: 10 out of 10, SQL Ninja analytic capabilities.
Strong working knowledge of with Linux Shell.
Understanding of Data Warehouse principles, including Dimensional Modeling.
Creative, flexible, and quick to learn.
Experience with Redshift a plus.
Compensation and Benefits:
Competitive Base Salary
Complimentary Blink membership
Comprehensive benefits package
And more"," Bachelorâs Degree Required  Computer Science or Engineering discipline preferred. 3+ years technology experience working in Software Engineering capacity. 3+ years working in Python  other modern languages considered . 1+ years working within Analytic/Data Warehouse/Data Lake environment. 1+ years working with AWS public cloud  certification a plus . Expertise in SQL  10 out of 10, SQL Ninja analytic capabilities. Strong working knowledge of with Linux Shell. Understanding of Data Warehouse principles, including Dimensional Modeling. Creative, flexible, and quick to learn. Experience with Redshift a plus.   Design, develop, deploy and manage a reliable and scalable data analysis pipeline, using technologies including Python, S3, and Redshift. Participate in cross-functional initiatives to develop new capabilities, including hands-on development responsibilities. Ability to integrate data from a variety of sources, assuring they adhere to data quality and accessibility standards. Document processes and standard operating procedures. Evaluate and conduct POCâs with new technologies.    ","Bachelorâs Degree Required Computer Science or Engineering discipline preferred. 3+ years technology experience working in Software capacity. Python other modern languages considered . 1+ within Analytic/Data Warehouse/Data Lake environment. with AWS public cloud certification a plus Expertise SQL 10 out of 10, Ninja analytic capabilities. Strong knowledge Linux Shell. Understanding Data Warehouse principles, including Dimensional Modeling. Creative, flexible, and quick to learn. Experience Redshift plus. Design, develop, deploy manage reliable scalable data analysis pipeline, using technologies Python, S3, Redshift. Participate cross-functional initiatives develop new capabilities, hands-on development responsibilities. Ability integrate from variety sources, assuring they adhere quality accessibility standards. Document processes standard operating procedures. Evaluate conduct POCâs technologies.","Bachelorâs Degree Required Computer Science Engineering discipline preferred. 3+ years technology experience working Software capacity. Python modern languages considered . 1+ within Analytic/Data Warehouse/Data Lake environment. AWS public cloud certification plus Expertise SQL 10 10, Ninja analytic capabilities. Strong knowledge Linux Shell. Understanding Data Warehouse principles, including Dimensional Modeling. Creative, flexible, quick learn. Experience Redshift plus. Design, develop, deploy manage reliable scalable data analysis pipeline, using technologies Python, S3, Redshift. Participate cross-functional initiatives develop new capabilities, hands-on development responsibilities. Ability integrate variety sources, assuring adhere quality accessibility standards. Document processes standard operating procedures. Evaluate conduct POCâs technologies."
306,Data Engineer,Azure Data Engineer,"New York, NY 10011",New York,NY,"Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, âas isâ and âto beâ scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ","At least 5 years of consulting or client service delivery experience on Azure DevOps an platform Proven ability to build, manage and foster a team-oriented environment","At least 5 years consulting client service delivery experience Azure DevOps platform Proven ability build, manage foster team-oriented environment"
307,Data Engineer,"Data Engineer, Measurement Program, YouTube","New York, NY",New York,NY,"Minimum qualifications:

Bachelor's degree in Computer Science, or a related technical field, or equivalent practical experience.
3 years of experience with ETL and SQL.
2 years of experience working with one or more of the following: C/C++, Java, Go, Python, Unix/Linux systems.
Scripting experience in Shell, Perl or Python.

Preferred qualifications:

MBA, Master's degree or PhD.
2 years of work experience in a client-facing role.
Experience in driving highly cross-functional initiatives that range from structured project management to ambiguous thought leadership.
Experience in successfully navigating large organizations in order to complete both individual and collaborative projects.
Excellent data management, quantitative, and qualitative skills.
About the job
YouTube's Technology Solutions Organization is a global organization dedicated to developing and managing the company's largest and most strategic partnerships. We work closely with the YouTube product, engineering, and content teams to address our partners' most pressing and complex technology challenges. As a Partner Technology Manager, you'll lead deployments, optimize implementations, and handle integrations to build strong, successful, long-term partnerships.
The YouTube Measurement Program (YTMP) is responsible for ensuring that YouTube is represented fairly in third-party reporting tools, and that internal data feeding those tools is accurate and consistent. We provide partner technical support, analytical services, and thought leadership to several initiatives shaping the long-term direction of YouTube.
As a Data Engineer for the YouTube Measurement Program, you will be the integration manager responsible for the success of some of our most important third-party measurement partnerships. You will optimize and scale our validation programs, drive cross functional buy-in on roadmaps, and bring thought leadership to an important and highly visible ecosystem.
At YouTube, we believe that everyone deserves to have a voice, and that the world is a better place when we listen, share, and build community through our stories. We work together to give everyone the power to share their story, explore what they love, and connect with one another in the process. Working at the intersection of cutting-edge technology and boundless creativity, we move at the speed of culture with a shared goal to show people the world. We explore new ideas, solve real problems, and have fun â and we do it all together.
Responsibilities
Manage partner technical integration projects and ensure the prompt and proper resolution of technical challenges.
Develop and maintain third-party data validation methodologies, including building and maintaining automated and scalable technical infrastructure.
Guarantee the technical aspects of a partnerâs integration (both new and ongoing) by providing technical guidance and documentation.
Identify, drive, and optimize new third-party reporting opportunities by leveraging YouTube technologies.
Write and maintain lines of code (Python, C++, etc.) to support your own small to medium scale Extract, Transform, Load (ETL) pipelines.
At Google, we donât just accept differenceâwe celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.","   Manage partner technical integration projects and ensure the prompt and proper resolution of technical challenges. Develop and maintain third-party data validation methodologies, including building and maintaining automated and scalable technical infrastructure. Guarantee the technical aspects of a partnerâs integration  both new and ongoing  by providing technical guidance and documentation. Identify, drive, and optimize new third-party reporting opportunities by leveraging YouTube technologies. Write and maintain lines of code  Python, C++, etc.  to support your own small to medium scale Extract, Transform, Load  ETL  pipelines.   ","Manage partner technical integration projects and ensure the prompt proper resolution of challenges. Develop maintain third-party data validation methodologies, including building maintaining automated scalable infrastructure. Guarantee aspects a partnerâs both new ongoing by providing guidance documentation. Identify, drive, optimize reporting opportunities leveraging YouTube technologies. Write lines code Python, C++, etc. to support your own small medium scale Extract, Transform, Load ETL pipelines.","Manage partner technical integration projects ensure prompt proper resolution challenges. Develop maintain third-party data validation methodologies, including building maintaining automated scalable infrastructure. Guarantee aspects partnerâs new ongoing providing guidance documentation. Identify, drive, optimize reporting opportunities leveraging YouTube technologies. Write lines code Python, C++, etc. support small medium scale Extract, Transform, Load ETL pipelines."
308,Data Engineer,Senior Data Engineer,"New York, NY",New York,NY,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Hihg
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Mastery in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or customer-facing role     ","Mastery in at least one of the following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing more languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role","Mastery least one following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role"
309,Data Engineer,Azure Data Engineer,"Houston, TX 77006",Houston,TX,"Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, âas isâ and âto beâ scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ","At least 5 years of consulting or client service delivery experience on Azure DevOps an platform Proven ability to build, manage and foster a team-oriented environment","At least 5 years consulting client service delivery experience Azure DevOps platform Proven ability build, manage foster team-oriented environment"
310,Data Engineer,AWS Data Engineer,"Houston, TX 77006",Houston,TX,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
311,Data Engineer,Data Engineer,"Houston, TX 77072",Houston,TX,"Tailored Brands, Inc.âs purpose is to help our customers love how they look. We accomplish this by providing a personal, convenient, one-of-a-kind shopping experience with compelling products and world-class service. We help fulfill this mission by providing our employees with an engaging and inclusive workplace focused on teamwork, growth and respect.


Tailored Brands currently has an exciting opportunity to join our engineering team consisting of developers from a wide array of backgrounds. Our data team primarily focuses on Java, SQL and Python. Our team is a tight knit, friendly group of engineers that are dedicated to learning from and teaching to each other. Team members regularly contribute to and optimize our engineering practices and processes.


Key Responsibilities:
Enhance and further develop Big Data processing pipelines for data sources containing structured and unstructured data
Data Warehousing with Data modeler experience
Monitor and optimize key infrastructure components such as Databases, EC2 Clusters, and other aspects of the stack
Help promote best practices for Big Data development
Act as a bridge between the infrastructure and application engineering teams
Provide infrastructure support with a focus on cloud based computing
Build and support visualization and exploration capabilities around our Data Sets
Work with the Data Extraction and Data Science engineers on normalization and analytical processes
Work in an Agile manner with business users and data scientists to understand and discover the potential business value of new and existing Data Sets and help productize those discoveries
Help design and implement disaster recovery efforts
Analyze requirements and architecture specifications to create detailed design
Research areas of interest to the team and help facilitate solutions
Design Cloud Architecture, SaaS, PaaS, and SaaS

Required Experience:
Bachelorâs degree in Computer Science or Information Systems required; Masters preferred
3-5 years minimum retail work experience (preferred) with:
Production deployments
Traditional ETL, SQL and RDBMS: Oracle, MSSQL, MySQL, DataStage, and Talend
Object Oriented programming languages development (Java/Python preferred) as well as SQL and Unix Shell scripting
Modern data technical stack experience such as: Hadoop, Hive, Pig, Sqoop, Flume, MapReduce, Spark, Storm, Apex, Kafka, NiFi, HBase, and MongoDB
Knowledge and experience with:
Datalakes design, Teradata and/or Snowflake (preferred)
Nice to have AWS and Azure Cloud
BigData technologies and techniques, NoSQL systems (Hortonworks/Hadoop Ecosystem, Cassandra, Couchbase)


Required Experience, continued:
Exposure working with:
Rest/SOAP clients
Relational and Non-relational Data Modeling
Data Warehousing concepts
Familiarity with:
Git/SVN
UC4 and BMC Control-M
Jboss/JavaEE
Object Oriented Design Patterns
Serialization technologies such as Apache Avro
MicroStrategy and Tableau Data Visualization concepts
#LI-JE1
Work Environment, Physical & Mental Demands
Ability to sit and work at a computer keyboard for extended periods of time
Ability to stoop, kneel, bend at the waist, and reach on a daily basis
Able to lift and move up to 25 pounds occasionally
Must utilize visual acuity, speech and hearing, hand and eye coordination and manual dexterity necessary to operate a computer and office equipment
Hours may exceed 40 hours per week+

Note: To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed are representative of the knowledge, skill, and/or ability required and is not intended to be an exhaustive list of all duties, responsibilities or qualifications associated with this job.



Work Locations: 01099I IT Dept. 6380 Rogerdale Rd Houston 77072
Job: Information Technology (IT)
Organization: Tailored Shared Services
Shift: Day Job","  Enhance and further develop Big Data processing pipelines for data sources containing structured and unstructured data Data Warehousing with Data modeler experience Monitor and optimize key infrastructure components such as Databases, EC2 Clusters, and other aspects of the stack Help promote best practices for Big Data development Act as a bridge between the infrastructure and application engineering teams Provide infrastructure support with a focus on cloud based computing Build and support visualization and exploration capabilities around our Data Sets Work with the Data Extraction and Data Science engineers on normalization and analytical processes Work in an Agile manner with business users and data scientists to understand and discover the potential business value of new and existing Data Sets and help productize those discoveries Help design and implement disaster recovery efforts Analyze requirements and architecture specifications to create detailed design Research areas of interest to the team and help facilitate solutions Design Cloud Architecture, SaaS, PaaS, and SaaS   ","Enhance and further develop Big Data processing pipelines for data sources containing structured unstructured Warehousing with modeler experience Monitor optimize key infrastructure components such as Databases, EC2 Clusters, other aspects of the stack Help promote best practices development Act a bridge between application engineering teams Provide support focus on cloud based computing Build visualization exploration capabilities around our Sets Work Extraction Science engineers normalization analytical processes in an Agile manner business users scientists to understand discover potential value new existing help productize those discoveries design implement disaster recovery efforts Analyze requirements architecture specifications create detailed Research areas interest team facilitate solutions Design Cloud Architecture, SaaS, PaaS, SaaS","Enhance develop Big Data processing pipelines data sources containing structured unstructured Warehousing modeler experience Monitor optimize key infrastructure components Databases, EC2 Clusters, aspects stack Help promote best practices development Act bridge application engineering teams Provide support focus cloud based computing Build visualization exploration capabilities around Sets Work Extraction Science engineers normalization analytical processes Agile manner business users scientists understand discover potential value new existing help productize discoveries design implement disaster recovery efforts Analyze requirements architecture specifications create detailed Research areas interest team facilitate solutions Design Cloud Architecture, SaaS, PaaS, SaaS"
312,Data Engineer,"Data Engineer, Energy Platform - Houston, TX","Houston, TX",Houston,TX,"Job Description
If you want to bring your technical expertise, âchallenge-accepted!â mentality and passion for building amazing products with real-world impact for hundreds of millions of people come join our group! We have been looking for you! We have an amazing team with deep backgrounds across technology and energy.

Shell New Energies

Shell is leading the transition towards a low-carbon future. We aim to cut the net carbon footprint of our energy products in half by 2050. Our New Energies business, set up in 2016, supports this ambition. New Energies is an emerging opportunity, in which we plan to invest on average $1-2 billion a year until 2020 as we look for commercial investments in new and fast-growing segments of the energy industry.

Shell New Energies focuses on two areas: new fuels and power. New Fuels consists of investments in hydrogen, biofuels, and electric vehicle charging. In Power, we are building up positions across the full electricity value chain, including in renewable generation, retail energy, distributed energy resources, power trading and marketing, and grid services. Within these focus areas, we look for ways to connect customers with new business models for mobility and energy services, enabled by digital technologies and decentralization of energy systems. Development of our IoT platform for Shell New Energy will be paramount in achieving these goals.

Energy Platform Team

The Energy Platform team is a nimble, cross-functional, deeply technical and passionate group that embodies the speed and agility of a startup while embracing the scale of one of the largest companies in the world. Achieving a balance between agility and global scale provides unique opportunities, and the Energy Platform Team borrows from best-in-class product development, continuous delivery, and commercialization techniques while adapting them to the unique global context within Shell.

The Energy Platform team is empowered to coordinate and align Shellâs energy management platform objectives, strategies, and execution approaches across the company, as well as to design, deliver and maintain a mission-critical component of Shellâs ability to deliver differentiated products, offerings,and capabilities across its expanding global footprint.

We are looking for a Data Engineer with the ability to bring their expertise and excitement for solving complex problems while building one of the largest IoT platforms in the world. There will be no shortage of opportunities to lead, eat, drink, and be merry with the most dynamic team ever assembled in the energy industry.

Build, on a daily basis, real-time and big data processing pipelines, optimizing for scalability and performance, under multiple datastore concepts (Relational, NoSQL, Graph).
Proficient in building large scale ETL jobs, leveraging big data infrastructure (Hadoop, Spark, Kafka) and modern container orchestration environments (Kubernetes).
Comfortable working in a fast-paced environment building, running, testing and shipping data pipelines to serve ML/AI workloads under a common API.
Willing to work with a cross-functional team of market analysts, data scientists & software developers to translate their data needs into features inside the Energy Platform.
Create data tooling that assists data scientists and analysts in building low latency, scalable and resilient pipelines for machine learning and optimization workloads.
Enthusiast of data quality, lifecycle and provenance management, helping establish a DataOps centric culture within Engineering teams.
Advanced working knowledge of query authoring and tooling for cross source data aggregation: APIs, 3rd party DB, Object Storages, messaging bus.
You have a proven history of working on large scale ETL jobs for data wrangling and cleansing of IoT time-series & telemetry data, as well as IIoT unstructured datasets, focusing on serving machine learning orchestrations.
You have experience with Analytical Expression Compiling languages, for runtime analytics during SQL querying.
Requirements
Legal authorization to work in the US on a full-time basis for anyone other than current employer
Bachelor's degree in a relevant technical discipline.
5+ years of experience with demonstrable proficiency in one or more DB and data pipeline tooling: mySQL, PostgreSQL, OSI Pi historian, TimescaleDB, Streamsets, Apache Drill, Apache Parquet, Dremio â¦
You are passionate about building scalable, high performing, data pipelines, and analytic catalogs.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc
Proficient with Containerized environments and workloads
Excellent analytical, problem-solving, and troubleshooting skills.
Experience architecting, deploying, and supporting production applications.
You care deeply about performance, accessibility and API design.
Great communication skills.
Work locations may include Houston, TX or San Francisco, CA
Company Description
Shell is a global group of energy and petrochemical companies with about 84,000 employees across more than 70 countries. We aim to meet the worldâs growing need for more and cleaner energy solutions in ways that are economically, environmentally and socially responsible. We have expertise in exploration, production, refining and marketing of oil and natural gas, and the manufacturing and marketing of chemicals. As a global energy company operating in a challenging world, we set high standards of performance and ethical behaviors. We are judged by how we act and how we live up to our core values of honesty, integrity and respect for people. Our Business Principles are based on these. They promote trust, openness, teamwork and professionalism, as well as pride in what we do and how we conduct business.Building on our core values, we aspire to sustain a diverse and inclusive culture where everyone feels respected and valued, from our employees to our customers and partners. A diverse workforce and an inclusive work environment are vital to our success, leading to greater innovation and better energy solutions.
Disclaimer
Please note: We occasionally amend or withdraw Shell jobs and reserve the right to do so at any time, including prior to the advertised closing date.

Before applying, you are advised to read our data protection policy. This policy describes the processing that may be associated with your personal data and informs you that your personal data may be transferred to Royal Dutch/Shell Group companies around the world.

The Shell Group and its approved recruitment consultants will never ask you for a fee to process or consider your application for a career with Shell. Anyone who demands such a fee is not an authorised Shell representative and you are strongly advised to refuse any such demand.

Shell participates in E-Verify.

All qualified applicants will receive consideration for employment without regard to race, color, sex, national origin, age, religion, disability, sexual orientation, gender identity, protected veteran status, citizenship, genetic information or other protected status under federal, state or local laws.

Shell is an Equal Opportunity Employer - Minorities/Females/Veterans/Disability.
Employment TypeFull Time
Skillpool
IT Data and Analytics, Information
Work LocationHouston
No. of Positions
1
Job Expires01-Nov-2019","    Legal authorization to work in the US on a full-time basis for anyone other than current employer Bachelor's degree in a relevant technical discipline. 5+ years of experience with demonstrable proficiency in one or more DB and data pipeline tooling  mySQL, PostgreSQL, OSI Pi historian, TimescaleDB, Streamsets, Apache Drill, Apache Parquet, Dremio â¦ You are passionate about building scalable, high performing, data pipelines, and analytic catalogs. Experience with object-oriented/object function scripting languages  Python, Java, C++, Scala, etc Proficient with Containerized environments and workloads Excellent analytical, problem-solving, and troubleshooting skills. Experience architecting, deploying, and supporting production applications. You care deeply about performance, accessibility and API design. Great communication skills. Work locations may include Houston, TX or San Francisco, CA","Legal authorization to work in the US on a full-time basis for anyone other than current employer Bachelor's degree relevant technical discipline. 5+ years of experience with demonstrable proficiency one or more DB and data pipeline tooling mySQL, PostgreSQL, OSI Pi historian, TimescaleDB, Streamsets, Apache Drill, Parquet, Dremio â¦ You are passionate about building scalable, high performing, pipelines, analytic catalogs. Experience object-oriented/object function scripting languages Python, Java, C++, Scala, etc Proficient Containerized environments workloads Excellent analytical, problem-solving, troubleshooting skills. architecting, deploying, supporting production applications. care deeply performance, accessibility API design. Great communication Work locations may include Houston, TX San Francisco, CA","Legal authorization work US full-time basis anyone current employer Bachelor's degree relevant technical discipline. 5+ years experience demonstrable proficiency one DB data pipeline tooling mySQL, PostgreSQL, OSI Pi historian, TimescaleDB, Streamsets, Apache Drill, Parquet, Dremio â¦ You passionate building scalable, high performing, pipelines, analytic catalogs. Experience object-oriented/object function scripting languages Python, Java, C++, Scala, etc Proficient Containerized environments workloads Excellent analytical, problem-solving, troubleshooting skills. architecting, deploying, supporting production applications. care deeply performance, accessibility API design. Great communication Work locations may include Houston, TX San Francisco, CA"
313,Data Engineer,Data Engineer,"Houston, TX",Houston,TX,"Vroom.com is a venture-backed, fast-growing start-up focused on revolutionizing the car buying experience. Our approach is unique in that we recondition pre-owned vehicles to a high standard, sell online, and deliver anywhere across the US. We have experienced tremendous growth in our first 5 years of operation and have become a disruptive force in the automotive industry. Vroom is an exciting, accelerating workplace, and there's no better time to join the team than right now.

We are building a team of experienced Data Engineers to ensure that our data infrastructure supports and helps drive the dramatic growth that we expect! We're looking for an exceptional data engineer to help us organize, test, and operationalize our data. Our business depends on putting the right data in front of the right people at the right time. As a Data Engineer in this dynamic environment, you will be instrumental in pulling data from multiple sources, performing extensive analysis, and applying a variety of data science models to provide our internal customers with recommendations and feedback.
Responsibilities
Establish and maintain best practices for our data infrastructure
Develop next-gen data pipelining and ETL based on open source data pipeline tools and cloud-based ecosystems that can deal with varied data types from disparate sources
Develop and tune data storage and processing systems at scale
Build real-time data processing systems
Build automated systems to continually monitor data quality and integrity
Work closely with stakeholders across Vroom to ensure data is accurate, timely, and useful
Skills
3+ years experience in data/software engineering or related field
Fluency in Python and SQL, experience with Golang, C, C++, Java, or Scala is a plus
Demonstrated experience with distributed computing (Kafka, Storm, Spark, Hadoop, etc.)
Experience with NoSQL databases
Proficiency in using and managing cloud infrastructure, preferably AWS
Linux and Bash competence
Experience integrating with Salesforce a plus
Benefits

This full-time role offers competitive compensation; health, dental, and vision insurance through United Healthcare; a 401k plan; fully company-paid short term disability, long term disability, and life insurance; access to a healthcare concierge service with virtual visits; and 15 annualized days of paid vacation.

But our biggest benefit is being part of a low-ego, high performing team that's transforming the used car market into a modern, online and data-driven industry. We are looking for people who want to be a part of a contemporary startup culture. What gets us out of bed is working with talented people on a mission that matters.

To Apply

If you think you might be who weâre looking for, apply below with your resume and a cover letter telling us why you think youâd be a great addition to the team.","  3+ years experience in data/software engineering or related field Fluency in Python and SQL, experience with Golang, C, C++, Java, or Scala is a plus Demonstrated experience with distributed computing  Kafka, Storm, Spark, Hadoop, etc.  Experience with NoSQL databases Proficiency in using and managing cloud infrastructure, preferably AWS Linux and Bash competence Experience integrating with Salesforce a plus  Establish and maintain best practices for our data infrastructure Develop next-gen data pipelining and ETL based on open source data pipeline tools and cloud-based ecosystems that can deal with varied data types from disparate sources Develop and tune data storage and processing systems at scale Build real-time data processing systems Build automated systems to continually monitor data quality and integrity Work closely with stakeholders across Vroom to ensure data is accurate, timely, and useful  ","3+ years experience in data/software engineering or related field Fluency Python and SQL, with Golang, C, C++, Java, Scala is a plus Demonstrated distributed computing Kafka, Storm, Spark, Hadoop, etc. Experience NoSQL databases Proficiency using managing cloud infrastructure, preferably AWS Linux Bash competence integrating Salesforce Establish maintain best practices for our data infrastructure Develop next-gen pipelining ETL based on open source pipeline tools cloud-based ecosystems that can deal varied types from disparate sources tune storage processing systems at scale Build real-time automated to continually monitor quality integrity Work closely stakeholders across Vroom ensure accurate, timely, useful","3+ years experience data/software engineering related field Fluency Python SQL, Golang, C, C++, Java, Scala plus Demonstrated distributed computing Kafka, Storm, Spark, Hadoop, etc. Experience NoSQL databases Proficiency using managing cloud infrastructure, preferably AWS Linux Bash competence integrating Salesforce Establish maintain best practices data infrastructure Develop next-gen pipelining ETL based open source pipeline tools cloud-based ecosystems deal varied types disparate sources tune storage processing systems scale Build real-time automated continually monitor quality integrity Work closely stakeholders across Vroom ensure accurate, timely, useful"
314,Data Engineer,Manager Azure Data Engineer,"Houston, TX",Houston,TX,"This position is for a Senior Azure Data Engineer within our North American team who will be designing and implementing Azure data projects for our clients. This qualified individual will work closely with clients to ensure that data technologies meet their needs and keep pace with the rapid changes in Publicis Sapientâs operations and policies/procedures.

Responsibilities:
Lead, design, develop, and deliver large-scale Azure data systems, data processing, and data transformation projects.
Execute technical feasibility assessments and project estimates for moving databases and data processing to Azure.
Design and advocate solutions using modern cloud technologies, design principles, integration points, and automation methods.
Mentor and share knowledge with customers as well as provide architecture reviews, discussions, and prototypes.
Participate in overall engagement from strategy, assessment, migration, and implementations.
Work with customers to deploy, manage, and audit best practices for cloud products.
Requirements:
Demonstrated experience designing, implementing, and supporting enterprise-grade technical solutions in the cloud for meeting complex business data requirements.
Experience with Databricks and using Spark for data processing.
Experience with Azure Data Factory â ADF.
Advanced experience with different query languages (i.e. T-SQL, PostgreSQL, PL-SQL).
Experience designing and building data marts, warehouses, customer profile databases, etc.
Experience with data modeling, table design, and mapping business needs to data structures.
Experience with Azure Data Lake, Azure SQL Data Warehouse, and Cosmos DB are a plus.
Experience with Data Management Gateway, Azure Storage Options, Stream Analytics, and Event Hubs is a plus.
Experience with other cloud based big data architectures is a plus.
Experience/Education:
5 to 10 years of professional experience in the information technology industry.
BS in Computer Science or equivalent education/professional experience is required.
Personal Attributes:
Strong and innovative approach to problem solving and finding solutions.
Excellent communicator (written and verbal, formal, and informal).
Flexible and proactive/self-motivated working style with strong personal ownership.
Ability to multi-task and prioritize under pressure.
Ability to work independently with minimal supervision as well as in a team environment.
Flexibility and mobility is required to deliver this role as there may be requirements to spend time onsite with our clients to enable delivery of the first class services we pride ourselves in.","   Lead, design, develop, and deliver large-scale Azure data systems, data processing, and data transformation projects. Execute technical feasibility assessments and project estimates for moving databases and data processing to Azure. Design and advocate solutions using modern cloud technologies, design principles, integration points, and automation methods. Mentor and share knowledge with customers as well as provide architecture reviews, discussions, and prototypes. Participate in overall engagement from strategy, assessment, migration, and implementations. Work with customers to deploy, manage, and audit best practices for cloud products.  5 to 10 years of professional experience in the information technology industry. BS in Computer Science or equivalent education/professional experience is required.  Demonstrated experience designing, implementing, and supporting enterprise-grade technical solutions in the cloud for meeting complex business data requirements. Experience with Databricks and using Spark for data processing. Experience with Azure Data Factory â ADF. Advanced experience with different query languages  i.e. T-SQL, PostgreSQL, PL-SQL . Experience designing and building data marts, warehouses, customer profile databases, etc. Experience with data modeling, table design, and mapping business needs to data structures. Experience with Azure Data Lake, Azure SQL Data Warehouse, and Cosmos DB are a plus. Experience with Data Management Gateway, Azure Storage Options, Stream Analytics, and Event Hubs is a plus. Experience with other cloud based big data architectures is a plus.","Lead, design, develop, and deliver large-scale Azure data systems, processing, transformation projects. Execute technical feasibility assessments project estimates for moving databases processing to Azure. Design advocate solutions using modern cloud technologies, design principles, integration points, automation methods. Mentor share knowledge with customers as well provide architecture reviews, discussions, prototypes. Participate in overall engagement from strategy, assessment, migration, implementations. Work deploy, manage, audit best practices products. 5 10 years of professional experience the information technology industry. BS Computer Science or equivalent education/professional is required. Demonstrated designing, implementing, supporting enterprise-grade meeting complex business requirements. Experience Databricks Spark processing. Data Factory â ADF. Advanced different query languages i.e. T-SQL, PostgreSQL, PL-SQL . designing building marts, warehouses, customer profile databases, etc. modeling, table mapping needs structures. Lake, SQL Warehouse, Cosmos DB are a plus. Management Gateway, Storage Options, Stream Analytics, Event Hubs other based big architectures","Lead, design, develop, deliver large-scale Azure data systems, processing, transformation projects. Execute technical feasibility assessments project estimates moving databases processing Azure. Design advocate solutions using modern cloud technologies, design principles, integration points, automation methods. Mentor share knowledge customers well provide architecture reviews, discussions, prototypes. Participate overall engagement strategy, assessment, migration, implementations. Work deploy, manage, audit best practices products. 5 10 years professional experience information technology industry. BS Computer Science equivalent education/professional required. Demonstrated designing, implementing, supporting enterprise-grade meeting complex business requirements. Experience Databricks Spark processing. Data Factory â ADF. Advanced different query languages i.e. T-SQL, PostgreSQL, PL-SQL . designing building marts, warehouses, customer profile databases, etc. modeling, table mapping needs structures. Lake, SQL Warehouse, Cosmos DB plus. Management Gateway, Storage Options, Stream Analytics, Event Hubs based big architectures"
315,Data Engineer,Google Data Engineer,"Houston, TX 77006",Houston,TX,"Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet todayâs high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps â Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform. Multi-cloud experience a plus.   Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP DevOps an platform. Multi-cloud a plus. Proven ability to build, manage and foster team-oriented environment work creatively analytically in problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","Minimum 3 years previous Consulting client service delivery experience Google GCP DevOps platform. Multi-cloud plus. Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
316,Data Engineer,Data Engineer,"Houston, TX",Houston,TX,"Need for a well-demonstrated Data Engineer who will work with the Data Science team to complete a major data project.
Mode of Interview : Telephonic/F2F
Job Skills:
Experience working on Hadoop platform components
Knowledge of Big Data tools, such as zookeeper, Kafka Streaming.
Shell scripting experience
Experience with integration of data from multiple data sources (NoSQL, Mongo, SQL)
Experience working with Structured/Unstructured data.
Experience creating ETL pipelines
Experience in Docker builds and Git file versioning
Demonstrated ability to quickly learn new tools and paradigms to deploy cutting edge solutions.
Knowledge of programming in Python
Knowledge of MapR
Knowledge of Scala framework
Experience with Spark, Storm or Flink
Minimum Experience: 8 Yrs
Roles & Responsibilities:
Integrate Data from multiple data sources
Create ETL Pipelines
Work under the guidance of Lead to develop based on design/architecture.
Education:
Bachelorâs Degree in Computer Science or equivalent work experience. Masters preferred"," Experience working on Hadoop platform components Knowledge of Big Data tools, such as zookeeper, Kafka Streaming. Shell scripting experience Experience with integration of data from multiple data sources  NoSQL, Mongo, SQL  Experience working with Structured/Unstructured data. Experience creating ETL pipelines Experience in Docker builds and Git file versioning Demonstrated ability to quickly learn new tools and paradigms to deploy cutting edge solutions. Knowledge of programming in Python Knowledge of MapR Knowledge of Scala framework Experience with Spark, Storm or Flink Integrate Data from multiple data sources Create ETL Pipelines Work under the guidance of Lead to develop based on design/architecture.  ","Experience working on Hadoop platform components Knowledge of Big Data tools, such as zookeeper, Kafka Streaming. Shell scripting experience with integration data from multiple sources NoSQL, Mongo, SQL Structured/Unstructured data. creating ETL pipelines in Docker builds and Git file versioning Demonstrated ability to quickly learn new tools paradigms deploy cutting edge solutions. programming Python MapR Scala framework Spark, Storm or Flink Integrate Create Pipelines Work under the guidance Lead develop based design/architecture.","Experience working Hadoop platform components Knowledge Big Data tools, zookeeper, Kafka Streaming. Shell scripting experience integration data multiple sources NoSQL, Mongo, SQL Structured/Unstructured data. creating ETL pipelines Docker builds Git file versioning Demonstrated ability quickly learn new tools paradigms deploy cutting edge solutions. programming Python MapR Scala framework Spark, Storm Flink Integrate Create Pipelines Work guidance Lead develop based design/architecture."
317,Data Engineer,"Data Engineer, Agile Hub - Houston, TX","Houston, TX",Houston,TX,"Job Description
The Data Engineer collects data from source systems, cleans data and adds metadata for context. Builds data pipelines and ensures that there is appropriate automation in the end-to-end process of gathering and loading data for advanced analytical processing. Builds a robust, fault-tolerant data pipeline that cleans, transforms and aggregates unorganized and messy data from multiple sources into a consolidated data model. This enables the deployment of analytical models and helps data scientists develop models and generate insights more quickly. Develops understanding of data availability, content and quality across a domain and uses this to support product owners and data scientists with feasibility assessments for new features and use cases.

Weâre looking to you to bring your strong technical expertise in data engineering to help us find ways of increasing customer value and assist in the setup of a best-practice data science process, including determining the direction of future tooling, and how the company engages overall in data science.

Calling on your expertise in software engineering and writing code, youâll be pivotal to the drive to build products, working within small product teams across multiple projects that rely on data from a range of locations and systems.

Plus, youâll use your strong people skills to easily nurture and influence trustworthy, collaborative relationships with a range of virtual stakeholders, confidently and clearly communicating complex solutions.

Requirements
Must have legal authorization to work in the US on a full-time basis for anyone other than current employer
Bachelor's Degree preferably higher, in mathematics, statistics, computer science, or another relevant discipline
Minimum five (5) years of relevant experience with a strong background in data and software engineering, with experience of writing code.
Strong experience with Python and relevant libraries.
The ability to work across structured, semi-structured, and unstructured data, extracting information and identifying irregularities and linkages across disparate data sets.
Meaningful experience in distributed processing.
Deep understanding of information security principles to ensure compliant handling and management of client data.
Experience in traditional data warehousing / ETL tools.
Experience and interest in cloud infrastructure and containerization.
Preferably some experience programming with Julia.
Experience or interest in building robust and practical data pipelines on top of cloud infrastructure will also be an advantage.
Company Description
Shell is a global group of energy and petrochemical companies with about 84,000 employees across more than 70 countries. We aim to meet the worldâs growing need for more and cleaner energy solutions in ways that are economically, environmentally and socially responsible. We have expertise in exploration, production, refining and marketing of oil and natural gas, and the manufacturing and marketing of chemicals.

As a global energy company operating in a challenging world, we set high standards of performance and ethical behaviors. We are judged by how we act and how we live up to our core values of honesty, integrity and respect for people. Our Business Principles are based on these. They promote trust, openness, teamwork and professionalism, as well as pride in what we do and how we conduct business.

Building on our core values, we aspire to sustain a diverse and inclusive culture where everyone feels respected and valued, from our employees to our customers and partners. A diverse workforce and an inclusive work environment are vital to our success, leading to greater innovation and better energy solutions.
Disclaimer
Please note: We occasionally amend or withdraw Shell jobs and reserve the right to do so at any time, including prior to the advertised closing date.

Before applying, you are advised to read our data protection policy. This policy describes the processing that may be associated with your personal data and informs you that your personal data may be transferred to Royal Dutch/Shell Group companies around the world.

The Shell Group and its approved recruitment consultants will never ask you for a fee to process or consider your application for a career with Shell. Anyone who demands such a fee is not an authorised Shell representative and you are strongly advised to refuse any such demand.

Shell participates in E-Verify.

All qualified applicants will receive consideration for employment without regard to race, color, sex, national origin, age, religion, disability, sexual orientation, gender identity, protected veteran status, citizenship, genetic information or other protected status under federal, state or local laws.

Shell is an Equal Opportunity Employer - Minorities/Females/Veterans/Disability.
Employment TypeFull Time
Skillpool
IT Service Management & Delivery
Work LocationHouston - EP Center Americas
No. of Positions
1
Job Expires01-Nov-2019","    Must have legal authorization to work in the US on a full-time basis for anyone other than current employer Bachelor's Degree preferably higher, in mathematics, statistics, computer science, or another relevant discipline Minimum five  5  years of relevant experience with a strong background in data and software engineering, with experience of writing code. Strong experience with Python and relevant libraries. The ability to work across structured, semi-structured, and unstructured data, extracting information and identifying irregularities and linkages across disparate data sets. Meaningful experience in distributed processing. Deep understanding of information security principles to ensure compliant handling and management of client data. Experience in traditional data warehousing / ETL tools. Experience and interest in cloud infrastructure and containerization. Preferably some experience programming with Julia. Experience or interest in building robust and practical data pipelines on top of cloud infrastructure will also be an advantage.","Must have legal authorization to work in the US on a full-time basis for anyone other than current employer Bachelor's Degree preferably higher, mathematics, statistics, computer science, or another relevant discipline Minimum five 5 years of experience with strong background data and software engineering, writing code. Strong Python libraries. The ability across structured, semi-structured, unstructured data, extracting information identifying irregularities linkages disparate sets. Meaningful distributed processing. Deep understanding security principles ensure compliant handling management client data. Experience traditional warehousing / ETL tools. interest cloud infrastructure containerization. Preferably some programming Julia. building robust practical pipelines top will also be an advantage.","Must legal authorization work US full-time basis anyone current employer Bachelor's Degree preferably higher, mathematics, statistics, computer science, another relevant discipline Minimum five 5 years experience strong background data software engineering, writing code. Strong Python libraries. The ability across structured, semi-structured, unstructured data, extracting information identifying irregularities linkages disparate sets. Meaningful distributed processing. Deep understanding security principles ensure compliant handling management client data. Experience traditional warehousing / ETL tools. interest cloud infrastructure containerization. Preferably programming Julia. building robust practical pipelines top also advantage."
318,Data Engineer,Data Engineer,"Houston, TX 77046",Houston,TX,"Invesco is a leading global asset management firm with more than $888.2 billion* in assets under management. We provide our retail and institutional clients a diverse and comprehensive range of investment capabilities to help people get more out of life. Invesco is publicly traded on the New York Stock Exchange (IVZ) and has about 7,000 employees in over 20 countries.
(*As of December 31, 2018)


About Invesco Technology

At Invesco Technology, we are strategic problem solvers. Our mission is to create world-class technology solutions to enable global operations, lead in the innovative use of data and emerging technologies to redefine the investment experience, and help our clients âget more out of life."" This mission is fueled by our high-performing teams, which thrive on collaboration, operate on shared trust, and leverage diversity of thought to deliver valuable results every day to Invesco, clients, and partners.

We wholeheartedly believe that our success is driven by our people. That is why we invest heavily in our top talent, providing opportunities for continuous learning and professional development. Our employees are encouraged and supported in taking advantage of development opportunities tied to their goals and are recognized for employing new skills to make an impact beyond the scope of their daily roles.

To continue building our high-performing, OneTech Team, we are seeking candidates who champion innovation, operate effectively in an agile environment, challenge the status quo and are empowered to take risks


Job Purpose (Job Summary):
Weâre seeking a Senior Data Engineer to join a fast-paced agile development team using the latest technologies to build portfolio construction and analysis applications. In this position, you will work closely with the Invesco Global Solutions group to envision, design, and deploy scalable technology solutions. Youâll be expected to have basic to intermediate investments knowledge to quickly and accurately collect and visualize investment processes. The environment is demanding, and you will be challenged. We expect that you are fluent in all things data, and you also understand the nuances of financial services and our investment capabilities. Youâll be a part of a dynamic, collaborative team that wants to hear your input because you have a sound foundation in technology and investments. The ideal candidate is passionate about speed, quality, automation, and continuous delivery. Weâre actively cultivating a culture of innovation and excellence, and while not for everyone, this environment will be challenging and rewarding for the right individual who welcomes dynamism and solving complex problems. Our team is sensitive to an ever-evolving technological landscape where thirst for knowledge and learning is mandatory, and the mastery of new skills and best practices is essential.

Key Responsibilities / Duties:
Work on new and innovative portfolio construction and analytics applications along with other experienced developers.
Identify, ingest, and enrich a diverse set of structured and unstructured big data into datasets for analysis.
Operate and extend the data research platform to deliver production-quality data on time for analysis.
Own end-to-end data workflows and develop deep domain expertise to ensure data quality and completeness
Experiment with new technologies and acquire new skills to find creative solutions to the unique challenges we will encounter along the way

Work Experience / Knowledge:
Minimum 2 - 4 years of proven experience developing data analytics and visualization software and workflows
Intermediate experience with Python and libraries like numpy, pandas, scipy, and matplotlib
Intermediate database programming experience with both SQL (e.g. Oracle, SQL Server, PostgreSQL, MySQL) and noSQL (e.g. MongoDB, Parquet) data stores.
Basic to Intermediate experience with data visualization tools (e.g. Plotly, PowerBI, Tableau, Plotly Dash, or RShiny)
Basic to intermediate experience with HTML, CSS, React.js, and other front-end technologies.
Intermediate to advanced experience with Microsoft Excel
Basic to intermediate experience with Linux server administration
Containerized environments (Docker or LXC), git, continuous integration (e.g. Bamboo, Jenkins, Travis-CI, or CircleCI), documentation (e.g. Sphinx), IT security, distributed computing, and parallel computation
Basic to Intermediate understanding of Equity, Fixed Income, and Derivative instruments

Skills / Other Personal Attributes Required:
Comfortable working with ambiguity (e.g. imperfect data, loosely defined concepts, ideas, or goals) and translating these into more tangible outputs
Strong analytical and critical thinking skills
Self-motivated. Capable of working with little or no supervision
Strong written and verbal communication skills
Enjoy challenging and thought-provoking work and have a strong desire to learn and progress
Ability to manage multiple tasks and requests
Must demonstrate a positive, team-focused attitude
Ability to react positively under pressure to meet tight deadlines
Good inter-personal skills combined with willingness to listen
Structured, disciplined approach to work, with attention to detail
Flexible â able to meet changing requirements and priorities
Maintenance of up-to-date knowledge in the appropriate technical areas
Able to work in a global, multicultural environment

Formal Education: (minimum requirement to perform job duties)

Masters in Statistics, Computer Science or other similar advanced degrees from a top tier educational institution preferred
CFA, CPA, CIPM, CAIA, and/or FRM preferred, but not required.

Working Conditions:
Normal office environment with little exposure to noise, dust and temperatures
The ability to lift, carry or otherwise move objects of up to 10 pounds is also necessary
Normally works a regular schedule of hours, however hours may vary depending upon the project or assignment

FLSA (US Only): Nonexempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.

Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment."," Comfortable working with ambiguity  e.g. imperfect data, loosely defined concepts, ideas, or goals  and translating these into more tangible outputs Strong analytical and critical thinking skills Self-motivated. Capable of working with little or no supervision Strong written and verbal communication skills Enjoy challenging and thought-provoking work and have a strong desire to learn and progress Ability to manage multiple tasks and requests Must demonstrate a positive, team-focused attitude Ability to react positively under pressure to meet tight deadlines Good inter-personal skills combined with willingness to listen Structured, disciplined approach to work, with attention to detail Flexible â able to meet changing requirements and priorities Maintenance of up-to-date knowledge in the appropriate technical areas Able to work in a global, multicultural environment Work on new and innovative portfolio construction and analytics applications along with other experienced developers. Identify, ingest, and enrich a diverse set of structured and unstructured big data into datasets for analysis. Operate and extend the data research platform to deliver production-quality data on time for analysis. Own end-to-end data workflows and develop deep domain expertise to ensure data quality and completeness Experiment with new technologies and acquire new skills to find creative solutions to the unique challenges we will encounter along the way  Masters in Statistics, Computer Science or other similar advanced degrees from a top tier educational institution preferred CFA, CPA, CIPM, CAIA, and/or FRM preferred, but not required. ","Comfortable working with ambiguity e.g. imperfect data, loosely defined concepts, ideas, or goals and translating these into more tangible outputs Strong analytical critical thinking skills Self-motivated. Capable of little no supervision written verbal communication Enjoy challenging thought-provoking work have a strong desire to learn progress Ability manage multiple tasks requests Must demonstrate positive, team-focused attitude react positively under pressure meet tight deadlines Good inter-personal combined willingness listen Structured, disciplined approach work, attention detail Flexible â able changing requirements priorities Maintenance up-to-date knowledge in the appropriate technical areas Able global, multicultural environment Work on new innovative portfolio construction analytics applications along other experienced developers. Identify, ingest, enrich diverse set structured unstructured big data datasets for analysis. Operate extend research platform deliver production-quality time Own end-to-end workflows develop deep domain expertise ensure quality completeness Experiment technologies acquire find creative solutions unique challenges we will encounter way Masters Statistics, Computer Science similar advanced degrees from top tier educational institution preferred CFA, CPA, CIPM, CAIA, and/or FRM preferred, but not required.","Comfortable working ambiguity e.g. imperfect data, loosely defined concepts, ideas, goals translating tangible outputs Strong analytical critical thinking skills Self-motivated. Capable little supervision written verbal communication Enjoy challenging thought-provoking work strong desire learn progress Ability manage multiple tasks requests Must demonstrate positive, team-focused attitude react positively pressure meet tight deadlines Good inter-personal combined willingness listen Structured, disciplined approach work, attention detail Flexible â able changing requirements priorities Maintenance up-to-date knowledge appropriate technical areas Able global, multicultural environment Work new innovative portfolio construction analytics applications along experienced developers. Identify, ingest, enrich diverse set structured unstructured big data datasets analysis. Operate extend research platform deliver production-quality time Own end-to-end workflows develop deep domain expertise ensure quality completeness Experiment technologies acquire find creative solutions unique challenges encounter way Masters Statistics, Computer Science similar advanced degrees top tier educational institution preferred CFA, CPA, CIPM, CAIA, and/or FRM preferred, required."
319,Data Engineer,Data Engineer - Spark Data Engineer,"Spring, TX",Spring,TX,"Overview
HP Engineering entails utilizing established engineering disciplines to test and safeguard the manufacturing standards for new and existing HP products. Working with internal stakeholders and outsourced development partners, you will develop and execute solutions to resolve any existing issues, ensuring that our operating processes are cost-effective and uphold the highest quality.
Full Time
Level: Middle
Travel: Minimal (if any)
Success profile
What makes a successful Engineer at HP? Check out the top traits weâre looking for and see if you have the right mix.

Communicator
Deadline-oriented
Entrepreneurial
Open-minded
Problem-solver
Team Player
Rewards
Medical
Holidays
Flex Time
Life and Disability Insurance
Work/Life Balance
Onsite Gym/ Fitness Center
We are looking for a talented Spark Data Engineer, to join a great team at HP! In this role, you'll bring advanced subject matter knowledge to solve complex business issues, and we'll look to your Spark data engineering subject matter expertise! In this role, you will frequently contribute to the development of new ideas and methods. You will also get to work on complex, interesting problems, where analysis of situations or data requires an in-depth evaluation of multiple factors. We'd love for you to lead and/or provide expertise to functional project teams, and you may also participate in cross-functional initiatives.
We'll rely on your experience and expertise to provide direction and guidance to process improvements, including in helping to establish/advise on policies. You'll have the opportunity to work with a number of external clients, helping to provide them with effective solutions and insights. We'll trust you to utilize significant independent judgment within broadly defined policies and practices, including determining the best method for accomplishing work and achieving objectives. We'd love for you to also use your deep experience to occasionally help mentor and guide less experienced employees. Key skills will by Python, Scala, and Databricks Spark, as applied in a data engineering capacity. If you love data engineering as much as we do, we'd love to learn more about you!
Responsibilities
Mines data using modern tools and programming languages.
Defines and implements models to uncover patterns and predictions creating business value and innovation.
Manages relationships with business partners to evaluate and foster data driven innovation, provide domain-specific expertise in cross-organization projects/initiatives.
Ties insights into effective visualizations communicating business value and innovation potential.
Maintains proficiency within the data science domain by keeping up with technology and trend shifts. Contributes to industry data science domain initiatives.
Leads project team(s) of data science professionals, assuring insights are communicated regularly and effectively, reviewing designs, models and accuracy and data compliance.
Collaborates and communicates with project team regarding project progress and issue resolution.
Communicates and drives data insights/innovation into the business.
Represents the data science team for all phases of larger and more-complex development projects.
Provides guidance, training and mentoring to less experienced staff members.
Strong Scala, Python programming experience is a must
Databricks Spark experience
Knowledge & Skills
Extensive experience using statistics, mathematics, algorithms and programming languages to solve big data challenges.
Fluent in structured and unstructured data, its management, and modern data transformation methodologies.
Ability to define and create complex models to pull valuable insights, predictions and innovation from data.
Effectively and creatively tell stories and create visualizations to describe and communicate data insights.
Strong analytical and problem-solving skills.
Excellent written and verbal communication skills; mastery in English and local language.
Ability to effectively communicate data insights and negotiate options at senior management levels.
Scope & Impact
Collaborates with peers, junior engineers, data scientists and project team.
Typically interacts with high- level Individual Contributors, Managers, Directors and Program Core Teams.
Leads multiple projects requiring data engineering solutions development.
Drives design innovation.
Education & Experience
Bachelor's, Master's or PHD degree in Mathematics, Economics, Physics, Computer Science, or equivalent.
6-10 yearsâ professional experience.
#Li-Post","  Extensive experience using statistics, mathematics, algorithms and programming languages to solve big data challenges. Fluent in structured and unstructured data, its management, and modern data transformation methodologies. Ability to define and create complex models to pull valuable insights, predictions and innovation from data. Effectively and creatively tell stories and create visualizations to describe and communicate data insights. Strong analytical and problem-solving skills. Excellent written and verbal communication skills; mastery in English and local language. Ability to effectively communicate data insights and negotiate options at senior management levels.  Mines data using modern tools and programming languages. Defines and implements models to uncover patterns and predictions creating business value and innovation. Manages relationships with business partners to evaluate and foster data driven innovation, provide domain-specific expertise in cross-organization projects/initiatives. Ties insights into effective visualizations communicating business value and innovation potential. Maintains proficiency within the data science domain by keeping up with technology and trend shifts. Contributes to industry data science domain initiatives. Leads project team s  of data science professionals, assuring insights are communicated regularly and effectively, reviewing designs, models and accuracy and data compliance. Collaborates and communicates with project team regarding project progress and issue resolution. Communicates and drives data insights/innovation into the business. Represents the data science team for all phases of larger and more-complex development projects. Provides guidance, training and mentoring to less experienced staff members. Strong Scala, Python programming experience is a must Databricks Spark experience  Bachelor's, Master's or PHD degree in Mathematics, Economics, Physics, Computer Science, or equivalent. 6-10 yearsâ professional experience. ","Extensive experience using statistics, mathematics, algorithms and programming languages to solve big data challenges. Fluent in structured unstructured data, its management, modern transformation methodologies. Ability define create complex models pull valuable insights, predictions innovation from data. Effectively creatively tell stories visualizations describe communicate insights. Strong analytical problem-solving skills. Excellent written verbal communication skills; mastery English local language. effectively insights negotiate options at senior management levels. Mines tools languages. Defines implements uncover patterns creating business value innovation. Manages relationships with partners evaluate foster driven innovation, provide domain-specific expertise cross-organization projects/initiatives. Ties into effective communicating potential. Maintains proficiency within the science domain by keeping up technology trend shifts. Contributes industry initiatives. Leads project team s of professionals, assuring are communicated regularly effectively, reviewing designs, accuracy compliance. Collaborates communicates regarding progress issue resolution. Communicates drives insights/innovation business. Represents for all phases larger more-complex development projects. Provides guidance, training mentoring less experienced staff members. Scala, Python is a must Databricks Spark Bachelor's, Master's or PHD degree Mathematics, Economics, Physics, Computer Science, equivalent. 6-10 yearsâ professional experience.","Extensive experience using statistics, mathematics, algorithms programming languages solve big data challenges. Fluent structured unstructured data, management, modern transformation methodologies. Ability define create complex models pull valuable insights, predictions innovation data. Effectively creatively tell stories visualizations describe communicate insights. Strong analytical problem-solving skills. Excellent written verbal communication skills; mastery English local language. effectively insights negotiate options senior management levels. Mines tools languages. Defines implements uncover patterns creating business value innovation. Manages relationships partners evaluate foster driven innovation, provide domain-specific expertise cross-organization projects/initiatives. Ties effective communicating potential. Maintains proficiency within science domain keeping technology trend shifts. Contributes industry initiatives. Leads project team professionals, assuring communicated regularly effectively, reviewing designs, accuracy compliance. Collaborates communicates regarding progress issue resolution. Communicates drives insights/innovation business. Represents phases larger more-complex development projects. Provides guidance, training mentoring less experienced staff members. Scala, Python must Databricks Spark Bachelor's, Master's PHD degree Mathematics, Economics, Physics, Computer Science, equivalent. 6-10 yearsâ professional experience."
320,Data Engineer,Data Engineer,"Houston, TX",Houston,TX,"
Houston, TX
Data department

Alice helps small business owners launch and grow. Since launching in May 2017, our goal is to help 6 million owners succeed through our free digital platform,
helloalice.com [http://www.helloalice.com]. Alice serves all entrepreneurs and we wholeheartedly believe that if underserved business owners -- women, people of color, veterans, the LGBTQ+ community, entrepreneurs with disabilities, people in small towns, and immigrants -- are provided better access to resources, they can change the world. Our vision is to be financially successful while making a positive impact on economies and job creation.

Alice is growing a diverse team, and weâre looking for a Data Engineer to help us fulfill our mission of connecting all entrepreneurs to the resources they need to grow and scale their businesses. Weâre leading a movement to connect every founder, regardless of geography, capitalization, prior experience, or cultural constraints, to the experts, tools, knowledge, and communities that will propel their companies forward. Alice is a Series A-backed company with teams in Houston, TX and San Francisco, CA. We are helping hundreds of thousands of owners a week in all fifty states. Led by co-founders Elizabeth Gore and Carolyn Rodz, we are hiring self-starters who can build the plane while flying it.

Engineering is key to scaling our mission, and weâre growing our data engineering team to build new features, improve our AI, and better serve entrepreneurs from around the world. We believe in working fast, but smart, and work toward measurable results. As the first member of the data team you will be able to have a huge impact on the technologies, architecture, and tools we use. Data is key to Aliceâs success and this position will have big visibility throughout the organization.

Required Skills/Experiences:

Thrives in a fast-paced, startup environment, is adaptable and versatile
3+ years of python
Experience in python data libraries (pandas, luigi, dask, etc)
Solid understanding of data structures and algorithms
Building distributed data systems

Desired Skills / Experiences

AWS and Google Cloud management
Ansible, Terraform or other cloud orchestration libraries
RabbitMQ, Kafka or other queuing systems
gRPC or other RPC libraries
Protobufs
Machine learning and statistics libraries
Scraping websites for relevant information

Our company values are important to us. We thought we would share them with you as someone interested in joining our team.

INNOVATE ALWAYS. Seek out unique perspectives, diverse experiences, and disconnected dots. These are the seeds of big ideas and exceeded expectations, made even better through constant collaboration.
EMBRACE FAILURE. Learn from the inevitable failures that result from innovation, and move forward quickly with a pioneering spirit. Champion the doers, celebrate their contributions to the team, and don't shy away from difficult conversations.
DRIVE THE MISSION FORWARD. Everything we do is through the inclusive lens of helping all business owners launch and grow, regardless of who they are or where they come from. Hold yourself to the highest standards of quality and equality in everything you do.
EVERYONE TAKES OUT THE TRASH. No task is too small for the success of our company or our owners. Always be thoughtful and practice extreme kindness toward our team, partners, and owners.
SIMPLIFY AND COMMIT. Maintain a bias toward efficiency, acting quickly and testing often. Recognize the opportunity cost of every decision, commit to deadlines, be on time, and search for simple, smart solutions.

Alice is an Equal Employment Opportunity employer that will consider all qualified applicants, regardless of race, color, religion, gender, sexual orientation, marital status, gender identity or expression, national origin, genetics, age, disability status, protected veteran status, or any other characteristic protected by applicable law.

www.helloalice.com [http://www.helloalice.com/] // Twitter
[https://twitter.com/HelloAlice] // Facebook
[http://www.facebook.com/aliceconnects] // Instagram
[https://www.instagram.com/helloalice_com/] // LinkedIn
[https://www.linkedin.com/company/25067438/]","  Thrives in a fast-paced, startup environment, is adaptable and versatile 3+ years of python Experience in python data libraries  pandas, luigi, dask, etc  Solid understanding of data structures and algorithms Building distributed data systems    ","Thrives in a fast-paced, startup environment, is adaptable and versatile 3+ years of python Experience data libraries pandas, luigi, dask, etc Solid understanding structures algorithms Building distributed systems","Thrives fast-paced, startup environment, adaptable versatile 3+ years python Experience data libraries pandas, luigi, dask, etc Solid understanding structures algorithms Building distributed systems"
321,Data Engineer,Principal Data Engineer & DBA,"Broomfield, CO",Broomfield,CO,"Principal Data Engineer & DBA-19000YP6


Preferred Qualifications

Position Responsibilities

The Oracle Cloud development is one of the biggest initiatives in the history of Oracle. We are introducing revolutionary SAAS, IAAS and PAAS products to the market place, and because many of these products are being built new, we are experiencing tremendous growth in our cloud storage product development team. Our systems move a huge amount of data and we need you to help build both the internal and external infrastructure to support it.

We are looking for an experienced data engineer / Database Administrator to help ETL, manage, and administrate our growing data warehouse. This person will be responsible for all aspects of a DBA role, managing our Cloud based Autonomous Data Warehouse on a daily basis. The person will also be responsible for structuring new DB schemas and working with stakeholders to understand they needs and requirements. Proposals for how to structure the data will need to be created and vetted with upper management. Part of duties includes ETLâing data from various sources into our ADW instance for easy access and analysis. If this opportunity sounds exciting, look no further and join a growing and dynamic team to help develop a data and analytics based infrastructure in the Oracle Cloud organization. Our team has a start-up feel, but with the stability Oracle gives.

DESIRED QUALIFICATIONS:
The ideal candidate will have experience working with large dataset from multiple sources. They should have years of experience as a DBA and familiar with DB tuning. In addition, they should work well in teams, and understand how to create/optimize database schemas. The candidate will be expected to create simple and complex ETL jobs. Specific experience the ideal candidate will demonstrate includes:
Over five years of experience as a DBA
Working knowledge of Oracle Autonomous Data Warehouse environments in Oracleâs OCI (Oracle Cloud Infrastructure)
Experience in tuning and optimizing DB performance
Knowledge of DB security best practices, and back and recovery processes
Experience in developing data architectures
Years of experience in ETL (Extract, Load, and Transform) from various data sources
Knowledge of Big Data and Time-series databases
Data Cleansing expertise
Working knowledge of data analysis / data science

ADDITIONAL SKILLS SOUGHT:
The successful candidate is expected to demonstrate the following additional requirements:

MS in Computer Science or Data Engineering
5+ years of experience working in as a DBA & Data Engineer
Strong knowledge of data structures, algorithms
Highly skilled in Python and SQL
Experience with OCI, AWS, and/or other IaaS environments.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.


Detailed Description and Job Requirements

Design, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.

As a member of the software engineering division, you will assist in defining and developing software for tasks associated with the developing, debugging or designing of software applications or operating systems. Provide technical leadership to other software developers. Specify, design and implement modest changes to existing software architecture to meet changing needs.

Duties and tasks are varied and complex needing independent judgment. Fully competent in own area of expertise. May have project lead role and or supervise lower level personnel. BS or MS degree or equivalent experience relevant to functional area. 4 years of software engineering or related experience.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law."," Over five years of experience as a DBA Working knowledge of Oracle Autonomous Data Warehouse environments in Oracleâs OCI  Oracle Cloud Infrastructure  Experience in tuning and optimizing DB performance Knowledge of DB security best practices, and back and recovery processes Experience in developing data architectures Years of experience in ETL  Extract, Load, and Transform  from various data sources Knowledge of Big Data and Time-series databases Data Cleansing expertise Working knowledge of data analysis / data science   Over five years of experience as a DBA Working knowledge of Oracle Autonomous Data Warehouse environments in Oracleâs OCI  Oracle Cloud Infrastructure  Experience in tuning and optimizing DB performance Knowledge of DB security best practices, and back and recovery processes Experience in developing data architectures Years of experience in ETL  Extract, Load, and Transform  from various data sources Knowledge of Big Data and Time-series databases Data Cleansing expertise Working knowledge of data analysis / data science  ","Over five years of experience as a DBA Working knowledge Oracle Autonomous Data Warehouse environments in Oracleâs OCI Cloud Infrastructure Experience tuning and optimizing DB performance Knowledge security best practices, back recovery processes developing data architectures Years ETL Extract, Load, Transform from various sources Big Time-series databases Cleansing expertise analysis / science","Over five years experience DBA Working knowledge Oracle Autonomous Data Warehouse environments Oracleâs OCI Cloud Infrastructure Experience tuning optimizing DB performance Knowledge security best practices, back recovery processes developing data architectures Years ETL Extract, Load, Transform various sources Big Time-series databases Cleansing expertise analysis / science"
322,Data Engineer,Data Engineer,"Centennial, CO 80112",Centennial,CO,"By joining the Bio-Techne team you will have an impact on future cutting-edge research. Bio-Techne, and all of its brands, provides tools for researchers in Life Sciences and Clinical Diagnostics.
Position Summary
We are looking for a Data Engineer to join our growing team. As a Data Engineer you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The core function will be driving the creation of master data repositories for critical business data. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Key Responsibilities
Create and maintain master data solutions for critical business data
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure or similar cloud âbig dataâ technologies.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications
Qualifications
3+ years working with SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing and stream processing
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 3+ years of experience in a Data Engineer or Database Developer role
Experience with big data tools: Azure SQL Server Analysis Services, Azure Data Warehouse
Experience with relational SQL and NoSQL databases
Experience with SQL Server Enterprise and Microsoft Master Data Services would be a plus
Experience with data pipeline and workflow management tools: Azure Service Bus, Azure Data Pipelines, Dell Boomi or similar technologies
Experience with Azure cloud services
Experience with object-oriented/object function scripting languages: C#, C++, JavaScript, etc.","3+ years working with SQL and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases. Experience building and optimizing data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing and stream processing Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. We are looking for a candidate with 3+ years of experience in a Data Engineer or Database Developer role Experience with big data tools  Azure SQL Server Analysis Services, Azure Data Warehouse Experience with relational SQL and NoSQL databases Experience with SQL Server Enterprise and Microsoft Master Data Services would be a plus Experience with data pipeline and workflow management tools  Azure Service Bus, Azure Data Pipelines, Dell Boomi or similar technologies Experience with Azure cloud services Experience with object-oriented/object function scripting languages  C , C++, JavaScript, etc.  Create and maintain master data solutions for critical business data Create and maintain optimal data pipeline architecture Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure or similar cloud âbig dataâ technologies. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Work with data and analytics experts to strive for greater functionality in our data systems.   ","3+ years working with SQL and experience relational databases, query authoring as well familiarity a variety of databases. Experience building optimizing data pipelines, architectures sets. performing root cause analysis on internal external processes to answer specific business questions identify opportunities for improvement. Strong analytic skills related unstructured datasets. Build supporting transformation, structures, metadata, dependency workload management. A successful history manipulating, processing extracting value from large disconnected Working knowledge message queuing stream project management organizational skills. cross-functional teams in dynamic environment. We are looking candidate Data Engineer or Database Developer role big tools Azure Server Analysis Services, Warehouse NoSQL databases Enterprise Microsoft Master Services would be plus pipeline workflow Service Bus, Pipelines, Dell Boomi similar technologies cloud services object-oriented/object function scripting languages C , C++, JavaScript, etc. Create maintain master solutions critical optimal architecture Assemble large, complex sets that meet functional / non-functional requirements. Identify, design, implement process improvements automating manual processes, delivery, re-designing infrastructure greater scalability, the required extraction, loading wide sources using âbig dataâ technologies. Work stakeholders including Executive, Product, Design assist data-related technical issues support their needs. analytics experts strive functionality our systems.","3+ years working SQL experience relational databases, query authoring well familiarity variety databases. Experience building optimizing data pipelines, architectures sets. performing root cause analysis internal external processes answer specific business questions identify opportunities improvement. Strong analytic skills related unstructured datasets. Build supporting transformation, structures, metadata, dependency workload management. A successful history manipulating, processing extracting value large disconnected Working knowledge message queuing stream project management organizational skills. cross-functional teams dynamic environment. We looking candidate Data Engineer Database Developer role big tools Azure Server Analysis Services, Warehouse NoSQL databases Enterprise Microsoft Master Services would plus pipeline workflow Service Bus, Pipelines, Dell Boomi similar technologies cloud services object-oriented/object function scripting languages C , C++, JavaScript, etc. Create maintain master solutions critical optimal architecture Assemble large, complex sets meet functional / non-functional requirements. Identify, design, implement process improvements automating manual processes, delivery, re-designing infrastructure greater scalability, required extraction, loading wide sources using âbig dataâ technologies. Work stakeholders including Executive, Product, Design assist data-related technical issues support needs. analytics experts strive functionality systems."
323,Data Engineer,Supply Chain Data Engineer,"Englewood, CO",Englewood,CO,"Dignity Health is one of the nation's largest health care systems. As of June 30, 2017, Dignity Health operated more than 400 care centers, including hospitals, urgent and occupational care, imaging and surgery centers, home health, and primary care clinics in 22 states, through its network of more than 9,000 physicians and more than 60,000 employees. Headquartered in San Francisco, CA, Dignity Health is dedicated to providing compassionate, high-quality, and affordable patient-centered care with special attention to those who are poor and underserved. In its fiscal year ended June 30, 2017, Dignity Health provided $2.6 billion in charitable care and services. More information on Dignity Health is available at www.dignityhealth.org .

Responsibilities
Job Summary:

The supply chain data engineer builds and maintains the data pipelines that make data accessible to supply chain staff, our partners, and the larger organization. This includes identifying and deploying best-in-class tools and infrastructure to improve the reliability and efficiency of those pipelines, as well as writing ELT code, tests, and documentation to democratize access to data. The data engineer will work with analytics, business, and IT stakeholders to identify, plan, and execute projects to radically improve the availability, quality, and scope of our data.

Core Duties:
Design, build, test, and maintain data pipelines integrating data from source systems into a cohesive data warehouse and other repositories for reporting and analysis.
Expand and maintain source control, build scripts, and other elements supporting increasingly continuous deployment.
Institute a testing framework for data pipelines and schemas and write tests.
Coordinate the automated transfer of data between local and remote systems.
Work with data science and analytics team to deploy machine learning models.
Implement tools to monitor data quality, availability, and pipeline performance.
Work with IT to plan and execute further improvements to Supply Chain's data platform as needed, potentially including cloud (e.g, Azure or AWS), containerization (e.g., Docker), or other (e.g., Spark) components.
Non-Essential Job Responsibilities:
Other duties as assigned by management
~LI-DH
tb91119

Qualifications
Minimum Qualifications:

Education - Sys/Div/Mkt/Local Manager - Bachelor's Degree and minimum of 3 years leadership experience OR minimum of 5 years leadership experience in the discipline OR Master's Degree and no experience. Bachelor's Degree in Information Systems, Business, Engineering or closely related field
Experience -
5+ years of experience in data engineering and/or devops
1 years of production experience with the SQL Server ecosystem
2 years of production experience with cloud or distributed platforms and architectures such as Azure, AWS, Spark, and/or Docker
Skills -
Python or Scala; advanced SQL
Knowledge of and experience with DevOps concepts, tools, and architectures
Comfort with Linux and Windows server environments, web APIs
Experience with Airflow or similar orchestration tools. SSIS experience helpful.

Preferred Qualifications:

Familiarity with dbt, R, Tableau, or related tools helpful
Relevant experience in a healthcare environment and computer systems preferred","Education - Sys/Div/Mkt/Local Manager - Bachelor's Degree and minimum of 3 years leadership experience OR minimum of 5 years leadership experience in the discipline OR Master's Degree and no experience. Bachelor's Degree in Information Systems, Business, Engineering or closely related field Experience - 5+ years of experience in data engineering and/or devops 1 years of production experience with the SQL Server ecosystem 2 years of production experience with cloud or distributed platforms and architectures such as Azure, AWS, Spark, and/or Docker Skills - Python or Scala; advanced SQL Knowledge of and experience with DevOps concepts, tools, and architectures Comfort with Linux and Windows server environments, web APIs Experience with Airflow or similar orchestration tools. SSIS experience helpful.   Design, build, test, and maintain data pipelines integrating data from source systems into a cohesive data warehouse and other repositories for reporting and analysis. Expand and maintain source control, build scripts, and other elements supporting increasingly continuous deployment. Institute a testing framework for data pipelines and schemas and write tests. Coordinate the automated transfer of data between local and remote systems. Work with data science and analytics team to deploy machine learning models. Implement tools to monitor data quality, availability, and pipeline performance. Work with IT to plan and execute further improvements to Supply Chain's data platform as needed, potentially including cloud  e.g, Azure or AWS , containerization  e.g., Docker , or other  e.g., Spark  components.   ","Education - Sys/Div/Mkt/Local Manager Bachelor's Degree and minimum of 3 years leadership experience OR 5 in the discipline Master's no experience. Information Systems, Business, Engineering or closely related field Experience 5+ data engineering and/or devops 1 production with SQL Server ecosystem 2 cloud distributed platforms architectures such as Azure, AWS, Spark, Docker Skills Python Scala; advanced Knowledge DevOps concepts, tools, Comfort Linux Windows server environments, web APIs Airflow similar orchestration tools. SSIS helpful. Design, build, test, maintain pipelines integrating from source systems into a cohesive warehouse other repositories for reporting analysis. Expand control, build scripts, elements supporting increasingly continuous deployment. Institute testing framework schemas write tests. Coordinate automated transfer between local remote systems. Work science analytics team to deploy machine learning models. Implement tools monitor quality, availability, pipeline performance. IT plan execute further improvements Supply Chain's platform needed, potentially including e.g, Azure AWS , containerization e.g., Spark components.","Education - Sys/Div/Mkt/Local Manager Bachelor's Degree minimum 3 years leadership experience OR 5 discipline Master's experience. Information Systems, Business, Engineering closely related field Experience 5+ data engineering and/or devops 1 production SQL Server ecosystem 2 cloud distributed platforms architectures Azure, AWS, Spark, Docker Skills Python Scala; advanced Knowledge DevOps concepts, tools, Comfort Linux Windows server environments, web APIs Airflow similar orchestration tools. SSIS helpful. Design, build, test, maintain pipelines integrating source systems cohesive warehouse repositories reporting analysis. Expand control, build scripts, elements supporting increasingly continuous deployment. Institute testing framework schemas write tests. Coordinate automated transfer local remote systems. Work science analytics team deploy machine learning models. Implement tools monitor quality, availability, pipeline performance. IT plan execute improvements Supply Chain's platform needed, potentially including e.g, Azure AWS , containerization e.g., Spark components."
324,Data Engineer,"Consultant, Cloud Data Engineer","Denver, CO 80202",Denver,CO,"Slalom is a modern consulting firm focused on strategy, technology, and business transformation. In 30 markets across the U.S., U.K., and Canada, Slalom's teams have autonomy to move fast and do what's right. They're backed by regional innovation hubs, a global culture of collaboration, and partnerships with the world's top technology providers.
Founded in 2001 and headquartered in Seattle, Slalom has organically grown to over 7,000 employees. Slalom was named one of Fortune's 100 Best Companies to Work For in 2019 and is regularly recognized by employees as a best place to work.

Job Title:
Cloud Data Engineer
As a Cloud Data Engineer in our Data & Analytics practice at Slalom, you will analyze, design, and architect cloud-based solutions to address our clientsâ needs for infrastructure-as-a-service, platform-as-a-service, and software-as-a-service. We are looking for sharp, disciplined, and self-motivated individuals who have a passion for utilizing cloud solutions from Amazon Web Services, Microsoft Azure, and Google Cloud Platform to solve real business problems for our clients.
Responsibilities:
Work as part of a team to design and develop cloud data solutions
Gather technical requirements, assess client capabilities, and analyze findings to provide appropriate cloud solution recommendations and adoption strategy
Define Cloud Data strategies, including designing multi-phased implementation roadmaps
Lead analysis, architecture, design, and development of data warehouse and business intelligence solutions
Be versed in Amazon Web Services, Google Cloud Platform, and/or Microsoft Azure cloud solutions, architecture, related technologies, and their interdependencies
Research, analyze, recommend, and select technical approaches for solving development and integration problems
Learn and adopt new tools/techniques to increase performance, automation, and scalability
Assist business development teams with pre-sales activities and RFPs
Understand how to translate business goals and drivers into a technical solution
Provide technical direction and oversight to cloud implementation teams
Qualifications:
3+ years previous consulting experience preferred
3+ years architecting and implementing Microsoft Azure, Amazon Web Services, and/or Google Cloud Platform infrastructure and topologies
Experience implementing core infrastructure, networking, and cloud-based services for business teams or consumers
Experience implementing Lambda architecture-based data designs
Deep product knowledge and understanding of Azure, AWS, and/or GCP
Experience configuring and tuning virtual private clouds
Practical experience sizing hardware and storage needs
Strong analytical problem solving ability
Good written and verbal communication + presentation skills
Self-starter with the ability to work independently or as part of a project team
Capability to execute performance analysis, troubleshooting, and remediation
Knowledge of High Availability and Disaster Recovery principles, patterns, and usage
Understanding of the cloud ecosystem and leading emerging technologies/interdependencies

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."," 3+ years previous consulting experience preferred 3+ years architecting and implementing Microsoft Azure, Amazon Web Services, and/or Google Cloud Platform infrastructure and topologies Experience implementing core infrastructure, networking, and cloud-based services for business teams or consumers Experience implementing Lambda architecture-based data designs Deep product knowledge and understanding of Azure, AWS, and/or GCP Experience configuring and tuning virtual private clouds Practical experience sizing hardware and storage needs Strong analytical problem solving ability Good written and verbal communication + presentation skills Self-starter with the ability to work independently or as part of a project team Capability to execute performance analysis, troubleshooting, and remediation Knowledge of High Availability and Disaster Recovery principles, patterns, and usage Understanding of the cloud ecosystem and leading emerging technologies/interdependencies   Work as part of a team to design and develop cloud data solutions Gather technical requirements, assess client capabilities, and analyze findings to provide appropriate cloud solution recommendations and adoption strategy Define Cloud Data strategies, including designing multi-phased implementation roadmaps Lead analysis, architecture, design, and development of data warehouse and business intelligence solutions Be versed in Amazon Web Services, Google Cloud Platform, and/or Microsoft Azure cloud solutions, architecture, related technologies, and their interdependencies Research, analyze, recommend, and select technical approaches for solving development and integration problems Learn and adopt new tools/techniques to increase performance, automation, and scalability Assist business development teams with pre-sales activities and RFPs Understand how to translate business goals and drivers into a technical solution Provide technical direction and oversight to cloud implementation teams  ","3+ years previous consulting experience preferred architecting and implementing Microsoft Azure, Amazon Web Services, and/or Google Cloud Platform infrastructure topologies Experience core infrastructure, networking, cloud-based services for business teams or consumers Lambda architecture-based data designs Deep product knowledge understanding of AWS, GCP configuring tuning virtual private clouds Practical sizing hardware storage needs Strong analytical problem solving ability Good written verbal communication + presentation skills Self-starter with the to work independently as part a project team Capability execute performance analysis, troubleshooting, remediation Knowledge High Availability Disaster Recovery principles, patterns, usage Understanding cloud ecosystem leading emerging technologies/interdependencies Work design develop solutions Gather technical requirements, assess client capabilities, analyze findings provide appropriate solution recommendations adoption strategy Define Data strategies, including designing multi-phased implementation roadmaps Lead architecture, design, development warehouse intelligence Be versed in Platform, Azure solutions, related technologies, their interdependencies Research, analyze, recommend, select approaches integration problems Learn adopt new tools/techniques increase performance, automation, scalability Assist pre-sales activities RFPs Understand how translate goals drivers into Provide direction oversight","3+ years previous consulting experience preferred architecting implementing Microsoft Azure, Amazon Web Services, and/or Google Cloud Platform infrastructure topologies Experience core infrastructure, networking, cloud-based services business teams consumers Lambda architecture-based data designs Deep product knowledge understanding AWS, GCP configuring tuning virtual private clouds Practical sizing hardware storage needs Strong analytical problem solving ability Good written verbal communication + presentation skills Self-starter work independently part project team Capability execute performance analysis, troubleshooting, remediation Knowledge High Availability Disaster Recovery principles, patterns, usage Understanding cloud ecosystem leading emerging technologies/interdependencies Work design develop solutions Gather technical requirements, assess client capabilities, analyze findings provide appropriate solution recommendations adoption strategy Define Data strategies, including designing multi-phased implementation roadmaps Lead architecture, design, development warehouse intelligence Be versed Platform, Azure solutions, related technologies, interdependencies Research, analyze, recommend, select approaches integration problems Learn adopt new tools/techniques increase performance, automation, scalability Assist pre-sales activities RFPs Understand translate goals drivers Provide direction oversight"
325,Data Engineer,Senior Big Data Engineer,"Englewood, CO",Englewood,CO,"The Company

Hitachi Vantara combines technology, intellectual property and industry knowledge to deliver data-managing solutions that help enterprises improve their customersâ experiences, develop new revenue streams, and lower the costs of business. Hitachi Vantara elevates your innovation advantage by combining IT, operational technology (OT) and domain expertise. Come join our team and our employee-focused culture and help drive our customersâ data to meaningful customer outcomes.

The Role

As a Sr. Big Data Engineer you will provide engineering knowledge to create and enhance data solutions enabling seamless delivery of data across our enterprise. You will be on the cutting edge of finding and integrating new technology and tools for data centric projects. Additionally, you will provide technical consulting to peer data engineers during design and development of highly complex and critical data projects. Some of these projects will include designing and developing data ingestion and processing/transformation frameworks leveraging open source tools such as NiFi, EMR, Java, Scala, Spark APIs, AWS Glue, etc. Additionally, you will work on real time processing solutions using tools such as Spark Streaming, MQ, Kafka, and AWS Kinesis. You will deploy application code using CI/CD tools and techniques.

Responsibilities:

 Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.
Ability to quickly identify an opportunity and recommend possible technical solutions.Utilize multiple development languages/tools such as Python, SPARK, Hive to build prototypes and evaluate results for effectiveness and feasibility.Operationalize data ingestion and data-analytic tools for enterprise use.Utilize tools available to you across AWS ServicesDevelop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, NIFI, Python, HBase and Hadoop.Custom Data pipeline development (Cloud and locally hosted)Work heavily within AWS and Hadoop ecosystemsProvide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc

Qualifications
Bachelorâs in computer science or related field is required (masters preferred)You have a minimum of 7 years of experience in the design, development, and deployment of large-scale, distributed, and cloud-deployed software services.Expected to be an expert in SQL and RDBMS. Good at modeling data for relational, analytical and big data workloadsYou have a minimum of 4 years of experience in Big Data software development technologies (e.g., Hadoop, Hive, Spark, Kafka) and exposure to resource/cluster management technologies.Minimum of 3 year of experience with AWS (e.g., EC2, S3, EMR, SNS, SQS, Aurora, Redshift).Experience with various software technologies/solutions and understand where to use them.2 years of experience with Data Virtualization like Denodo

We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.

#LI-DNI","Bachelorâs in computer science or related field is required  masters preferred You have a minimum of 7 years of experience in the design, development, and deployment of large-scale, distributed, and cloud-deployed software services.Expected to be an expert in SQL and RDBMS. Good at modeling data for relational, analytical and big data workloadsYou have a minimum of 4 years of experience in Big Data software development technologies  e.g., Hadoop, Hive, Spark, Kafka  and exposure to resource/cluster management technologies.Minimum of 3 year of experience with AWS  e.g., EC2, S3, EMR, SNS, SQS, Aurora, Redshift .Experience with various software technologies/solutions and understand where to use them.2 years of experience with Data Virtualization like Denodo  Ability to quickly identify an opportunity and recommend possible technical solutions.Utilize multiple development languages/tools such as Python, SPARK, Hive to build prototypes and evaluate results for effectiveness and feasibility.Operationalize data ingestion and data-analytic tools for enterprise use.Utilize tools available to you across AWS ServicesDevelop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, NIFI, Python, HBase and Hadoop.Custom Data pipeline development  Cloud and locally hosted Work heavily within AWS and Hadoop ecosystemsProvide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc  ","Bachelorâs in computer science or related field is required masters preferred You have a minimum of 7 years experience the design, development, and deployment large-scale, distributed, cloud-deployed software services.Expected to be an expert SQL RDBMS. Good at modeling data for relational, analytical big workloadsYou 4 Big Data development technologies e.g., Hadoop, Hive, Spark, Kafka exposure resource/cluster management technologies.Minimum 3 year with AWS EC2, S3, EMR, SNS, SQS, Aurora, Redshift .Experience various technologies/solutions understand where use them.2 Virtualization like Denodo Ability quickly identify opportunity recommend possible technical solutions.Utilize multiple languages/tools such as Python, SPARK, Hive build prototypes evaluate results effectiveness feasibility.Operationalize ingestion data-analytic tools enterprise use.Utilize available you across ServicesDevelop real-time stream-analytic solutions leveraging Kafka, Apache NIFI, HBase Hadoop.Custom pipeline Cloud locally hosted Work heavily within Hadoop ecosystemsProvide support deployed applications models by being trusted advisor Scientists other consumers identifying problems guiding issue resolution partner Engineers source providers.Provide subject matter expertise analysis, preparation specifications plans processes.Ensure proper governance policies are followed implementing validating Lineage, Quality checks, classification, etc","Bachelorâs computer science related field required masters preferred You minimum 7 years experience design, development, deployment large-scale, distributed, cloud-deployed software services.Expected expert SQL RDBMS. Good modeling data relational, analytical big workloadsYou 4 Big Data development technologies e.g., Hadoop, Hive, Spark, Kafka exposure resource/cluster management technologies.Minimum 3 year AWS EC2, S3, EMR, SNS, SQS, Aurora, Redshift .Experience various technologies/solutions understand use them.2 Virtualization like Denodo Ability quickly identify opportunity recommend possible technical solutions.Utilize multiple languages/tools Python, SPARK, Hive build prototypes evaluate results effectiveness feasibility.Operationalize ingestion data-analytic tools enterprise use.Utilize available across ServicesDevelop real-time stream-analytic solutions leveraging Kafka, Apache NIFI, HBase Hadoop.Custom pipeline Cloud locally hosted Work heavily within Hadoop ecosystemsProvide support deployed applications models trusted advisor Scientists consumers identifying problems guiding issue resolution partner Engineers source providers.Provide subject matter expertise analysis, preparation specifications plans processes.Ensure proper governance policies followed implementing validating Lineage, Quality checks, classification, etc"
326,Data Engineer,Data Engineer,"Denver, CO",Denver,CO,"Location: 1212 S Broadway, Denver, CO 80210

Category: Full time, Exempt

Salary Range: (DOE)

Reports To: CEO
Colorado Community Managed Care Network (CCMCN) is comprised of Coloradoâs Community Health Centers with over 190 clinic sites (including school based clinics, pharmacies, and mobile units). CCMCN was founded as a non-profit organization in 1994 to respond pro-actively to the advent of mandatory Medicaid managed care, and has evolved to a multi-faceted organization that serves its members in areas where a network solution optimizes collaborative endeavors. Areas of focus include population health, accountable care, shared services, health information technology and clinical quality improvement programming. CCMCNâs vision is that all Coloradoans have access to high quality, integrated, accountable health care.

CCMCN is a one-third founding partner in Colorado Access, a health plan focusing on Medicaid, Medicare and Child Health Plan Plus (CHP+) and CCMCN has been a HRSA-supported Health Center Controlled Network (HCCN) since 1995. CCMCN is governed by a Board of Directors comprised of organizational representatives from each of its health center members as well as representation from Colorado Community Health Network (CCHN). The Board membership includes clinician representatives, elected at-large, and carries out policy and decision-making duties.

Position Description:
The Data Operations department provides data management, integration, and reporting services for multiple external and internal consumers. The Data Engineer will be responsible for all aspects of data management that support CCMCNâs production services. Candidate must have experience in Microsoft SQL database development, data integration, ETL (extract, transform and load) tools and methods, analytics, reporting, and documentation.

Essential Functions:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures and code sets as well as applicable data privacy practices and legal requirements.
Required Skills and Experience:
Experience working in a Microsoft SQL Server environment, especially with SSIS, stored procedures and query development.
Knowledge of data warehousing best practices, concepts and processes.
Strong analytical and problem-solving skills, with demonstrated change management experience.
Demonstrated ability to set and meet project timelines and deliverables.
Effective interpersonal and communications skills with the ability to interact with various levels of personnel.
Must be flexible, organized, self-directed, able to prioritize multiple tasks, and able to manage a full workload.
Experienced in Microsoft Office and Microsoft Operating Systems.
Must be able to work in CCMCN's main office and travel to all required meetings
Fluency in written and spoken English.
Additional Preferred Skills:
Strong business and technical writing abilities.
Knowledge of healthcare data standards such as HL7, CCD, CCR and claims data.
Knowledge of standard healthcare code sets like LOINC, ICD9/10, CPT4 and SNOMED.
Knowledge of IHI triple aim, clinical quality improvement, and primary care operations/workflow.
Ability to stay current on healthcare reporting requirements such as UDS, PQRI, NQF, Meaningful Use and Patient Centered Medical Home.
Ability to attend conferences and workshops for further education to expand and improve management skills.
CCMCN is an equal opportunity employer offering a generous benefit package, generous vacation and holiday schedule, casual work environment, and a flexible work schedule.

TO APPLY:
Please submit a resume and cover letter by email to jobs@ccmcn.com.
PLEASE INCLUDE THE JOB TITLE IN THE SUBJECT OF THE EMAIL.
No phone calls please.","  Experience working in a Microsoft SQL Server environment, especially with SSIS, stored procedures and query development. Knowledge of data warehousing best practices, concepts and processes. Strong analytical and problem-solving skills, with demonstrated change management experience. Demonstrated ability to set and meet project timelines and deliverables. Effective interpersonal and communications skills with the ability to interact with various levels of personnel. Must be flexible, organized, self-directed, able to prioritize multiple tasks, and able to manage a full workload. Experienced in Microsoft Office and Microsoft Operating Systems. Must be able to work in CCMCN's main office and travel to all required meetings Fluency in written and spoken English.   ","Experience working in a Microsoft SQL Server environment, especially with SSIS, stored procedures and query development. Knowledge of data warehousing best practices, concepts processes. Strong analytical problem-solving skills, demonstrated change management experience. Demonstrated ability to set meet project timelines deliverables. Effective interpersonal communications skills the interact various levels personnel. Must be flexible, organized, self-directed, able prioritize multiple tasks, manage full workload. Experienced Office Operating Systems. work CCMCN's main office travel all required meetings Fluency written spoken English.","Experience working Microsoft SQL Server environment, especially SSIS, stored procedures query development. Knowledge data warehousing best practices, concepts processes. Strong analytical problem-solving skills, demonstrated change management experience. Demonstrated ability set meet project timelines deliverables. Effective interpersonal communications skills interact various levels personnel. Must flexible, organized, self-directed, able prioritize multiple tasks, manage full workload. Experienced Office Operating Systems. work CCMCN's main office travel required meetings Fluency written spoken English."
327,Data Engineer,Data Engineer,"Denver, CO 80221",Denver,CO,"Job Description Summary
We are looking for Data/Machine Learning engineers at all levels to help us build a robust and scalable data platform to support AI/ML data pipelines, reporting and data analysis as our business scales. We use cloud native (AWS) cutting-edge technologies like Spark, Kinesis/Kafka Streaming, Graph , infrastructure as code, CI/CD to deliver high-quality data solutions to analysts, data scientists, and partners. Weâre looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment. At Transamerica, we believe achieving a secure future requires both smart financial planning and a healthy lifestyle. Weâre using data science, machine learning, computer vision, natural language processing, and Iot to revolutionize the way our customers save, invest, protect, and retire and to help them develop better wellness habits. As part of the Data Engineering team in our Analytics Execution group, you will work with data scientists and analytics engagement managers to develop innovative data-based solutions that transform the way we do business.
Job Description
Qualifications:
Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns.
Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem
Youâre proficient with Linux operations and development, including basic commands and shell scripting
You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices
Must be fluent in Python, R, and Java
Must have excellent experience command of SQL
Must have good experience and knowledge with Data Modeling concepts.
You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills
Preferred Qualifications:
Must have 3 -5 years of experience building data productionalized pipelines.
Must have strong experience ingesting huge volumes of structured and unstructured data both in streaming and batch ingestions patterns.
2 - 4 years of Cloud development experience with AWS and or Azure stack.
Exposure with and have solid experience with statistical analysis and machine learning libraries
Must have previous experience with NoSQL database implementations
You understand the fundamentals of lambda architectures and serverless. applications
Must be proficient in Tableau
Must be comfortable with leveraging ETL tools, like Informatica.
You are proficient in Scala or Node.js
You have a masterâs degree in a quantitative field
Job Description:
Partner with data scientists, analytics engagement managers, and other data engineers to discover, collect, cleanse, and refine the data needed for analysis and modeling
Analyze large data sets to extract actionable insights and inform experimental design and model development
Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build models using basic statistical and machine learning techniques, partnering with data scientists for education and guidance
Weâre looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment.
Working Conditions
Office environment"," Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns. Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem Youâre proficient with Linux operations and development, including basic commands and shell scripting You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices Must be fluent in Python, R, and Java Must have excellent experience command of SQL Must have good experience and knowledge with Data Modeling concepts. You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills    ","Must have a solid understanding of data engineering, integration, and warehousing concepts patterns. experience with design, build, maintain batch streaming solutions at scale in both on-premises cloud environments, specifically the Hadoop ecosystem Youâre proficient Linux operations development, including basic commands shell scripting You can demonstrate DevOps methodologies continuous integration/continuous delivery practices be fluent Python, R, Java excellent command SQL good knowledge Data Modeling concepts. passion for science machine learning strong desire to develop your analysis modeling skills","Must solid understanding data engineering, integration, warehousing concepts patterns. experience design, build, maintain batch streaming solutions scale on-premises cloud environments, specifically Hadoop ecosystem Youâre proficient Linux operations development, including basic commands shell scripting You demonstrate DevOps methodologies continuous integration/continuous delivery practices fluent Python, R, Java excellent command SQL good knowledge Data Modeling concepts. passion science machine learning strong desire develop analysis modeling skills"
328,Data Engineer,Cloud Based Analytics Data Engineer,"Englewood, CO",Englewood,CO,"Whether itâs unlocking the potential of digital content, powering breakthrough innovations, creating entertainment that enriches lives, or keeping nations secure, Quantum works with customers and partners to make the world a happier, safer and smarter place.
Job Summary and Duties:
We are looking for an experienced candidate to be part of an international team of senior engineers that develop a global data warehouse and information system for scalar tape library devices. This is a business critical role that will enable servicing our hyper scale customers.
Specific duties include but are not limited to:
Design and implement data ETL from logfiles, database dumps, etc. into database target to support managed services for global install base. A typical parsing script will be in Python.
Analyze source data and define target data.
Implement ad hoc and scheduled reporting, with Power BI and other reporting tools
Gather, document, and implement requirements for new reporting.

Job Requirements:
Experience with Relational Database
Experience with query tools like SQL
Experience using a report designer such as Power BI
Ability to work in a team environment.
Strong communication skills.

Desired Skills:
Linux command line experience.
Python experience.
Working knowledge and experience with AWS, S3, RDS/Postgres.
Experience with Php / Laravel framework.
Experience with JIRA.
Experience working in an Agile/SCRUM environment.
Experience with containers, AWS cloud, and Azure cloud.
Quantum is proud to be an equal opportunity and affirmative action employer. Female/Minority/Veteran/Disabled/Sexual Orientation/Gender Identity.","  Linux command line experience. Python experience. Working knowledge and experience with AWS, S3, RDS/Postgres. Experience with Php / Laravel framework. Experience with JIRA. Experience working in an Agile/SCRUM environment.    Experience with Relational Database Experience with query tools like SQL Experience using a report designer such as Power BI Ability to work in a team environment. Strong communication skills.","Linux command line experience. Python Working knowledge and experience with AWS, S3, RDS/Postgres. Experience Php / Laravel framework. JIRA. working in an Agile/SCRUM environment. Relational Database query tools like SQL using a report designer such as Power BI Ability to work team Strong communication skills.","Linux command line experience. Python Working knowledge experience AWS, S3, RDS/Postgres. Experience Php / Laravel framework. JIRA. working Agile/SCRUM environment. Relational Database query tools like SQL using report designer Power BI Ability work team Strong communication skills."
329,Data Engineer,BI Data Engineer,"Boulder, CO",Boulder,CO,"About Charlotte's Webâ¢:
Charlotte's Webâ¢ products are made from our world-renowned hemp genetics grown 100% in the USA. Founded by the Stanley Brothers of Colorado, Charlotteâs Webâ¢ leads the industry in quality, safety, consistency and social responsibility to improve thousands of lives daily through the use of Charlotte's Webâ¢. At Charlotteâs Webâ¢, we are driven by principles that extend far beyond the bottom line. It is our goal to provide products of the highest possible quality, while contributing to the sustainability of the communities we have the privilege of serving.
Position Summary:
The BI Data Engineer will be responsible for managing and compiling data and metrics to help drive decision support for all functional business units.

Essential Duties:
Develop, implement, and maintain SAAS based data warehouse systems, integrations, and data frameworks.
Provide source to target mappings and data model specifications
Working with the Business Intelligence Manager to interface with the business to establish needs and data standards.
Developing and implementing data collection systems and other tactics that optimize data efficiency and accuracy
Acquiring data from primary or secondary data sources and maintaining (through 3rd party) databases
Scoping actionable analysis projects with functional business units
Prioritizing analysis projects on the basis of actionable outputs and potential business impact
Deliver summarized analysis, implications for action, and recommended use of insights to stakeholders
Complete and deliver large-scale analysis projects pertaining to product selection, customer behavior patterns, and business efficiency
Communicate data-driven insights to functional business groups and ensure a level of understanding that is actionable and impacts the business
Collaborate with the Business Intelligence Reporting Analyst to standardize and operationalize deep-dive analyses into relevant reporting dashboards
Convey analysis methodology to functional team
Ensure that all related documentation (business requirements, technical designs, data dictionary, etc.) are created, maintained and available in a central repository.
Coordinate cross-functionally with other departmental managers and Subject Matter Experts (SMEs) in Accounting / Finance, Marketing, Ecommerce, Sales, Supply Chain, Manufacturing, and Cultivation
Support corporate objectives and global growth strategies
Provides recommendations for improvements
Responsible for escalating support issues to third-parties, as needed
Provide regular project status reports
Predict project risk factors and address proactively
Qualifications:
Seeking of 3-5+ years of experience with data analysis, data warehousing, and delivering reporting
Fluent in at least SQL syntax (Read functions)
Experience with Tableau, Power BI, or Looker
Proficiency with MS Excel
Experience with statistical models and methodologies
Experience with eCommerce analysis (CLV, AOV, Retention Rate, etc) a plus
Experience with Python, Javascript, and R are a plus
Experience with Financial Management, Supply Chain, Warehouse Management, and Manufacturing
Experience working with an eCommerce platform
Bachelorâs Degree in Information Technology or Mathematics or Statistics, or related field preferred
Excellent planning and project management skills
Self-starter with a high level of initiative and strong sense of ownership
Strong communication and interpersonal skills
Experience working in the CPG industry preferred
Experience with both B2C and B2B business models preferred

Benefits: We offer best-in-class benefits, including:
Company-Paid Medical, Dental, and Vision
3 Weeks of Paid Vacation Your First Year
401K Match with Automatic Vesting
Up to 9 Weeks Paid Parental Leave
Self-Tailored Wellness Program
Generous Employee Discount

Charlotteâs Webâ¢ provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Charlotteâs Webâ¢ is an At-Will Employer.","Seeking of 3-5+ years of experience with data analysis, data warehousing, and delivering reporting Fluent in at least SQL syntax  Read functions  Experience with Tableau, Power BI, or Looker Proficiency with MS Excel Experience with statistical models and methodologies Experience with eCommerce analysis  CLV, AOV, Retention Rate, etc  a plus Experience with Python, Javascript, and R are a plus Experience with Financial Management, Supply Chain, Warehouse Management, and Manufacturing Experience working with an eCommerce platform Bachelorâs Degree in Information Technology or Mathematics or Statistics, or related field preferred Excellent planning and project management skills Self-starter with a high level of initiative and strong sense of ownership Strong communication and interpersonal skills Experience working in the CPG industry preferred Experience with both B2C and B2B business models preferred     ","Seeking of 3-5+ years experience with data analysis, warehousing, and delivering reporting Fluent in at least SQL syntax Read functions Experience Tableau, Power BI, or Looker Proficiency MS Excel statistical models methodologies eCommerce analysis CLV, AOV, Retention Rate, etc a plus Python, Javascript, R are Financial Management, Supply Chain, Warehouse Manufacturing working an platform Bachelorâs Degree Information Technology Mathematics Statistics, related field preferred Excellent planning project management skills Self-starter high level initiative strong sense ownership Strong communication interpersonal the CPG industry both B2C B2B business","Seeking 3-5+ years experience data analysis, warehousing, delivering reporting Fluent least SQL syntax Read functions Experience Tableau, Power BI, Looker Proficiency MS Excel statistical models methodologies eCommerce analysis CLV, AOV, Retention Rate, etc plus Python, Javascript, R Financial Management, Supply Chain, Warehouse Manufacturing working platform Bachelorâs Degree Information Technology Mathematics Statistics, related field preferred Excellent planning project management skills Self-starter high level initiative strong sense ownership Strong communication interpersonal CPG industry B2C B2B business"
330,Data Engineer,Data Engineer (Up to 25% Profit Sharing Benefit!),"Denver, CO 80225",Denver,CO,"Job Description
BITS, a CACI Company, offers very rewarding and unique benefits, which equates to 50% of compensation on TOP of your base salary! The first part is a tax-qualified profit-sharing retirement plan, to which BITS annually contributes up to 25% of your base salary (not in excess of applicable IRS limits) to your retirement account. The second component consists of BITS' Individual Benefit Account (IBA), which is used for premiums, medical reimbursements, dependent care, education and Paid Time Off (PTO) policy. Both components of the BITS benefit package are paid for by BITS, in addition to your base salary and potential performance bonuses. We believe in a healthy home/work balance and both of our locations offer a wide variety of activities to balance with your work life. Learn more at http://www.caci.com/bit-systems/
We are seeking a passionate Data Engineer. The commitment of our employees to ""Engineering Results"" is the catalyst that has propelled BITS to becoming a leader in software development, R&D, sensor development and signal processing. Our engineering teams are highly adept at solving complex problems with the application of leading-edge technology solutions, empowering decision-makers to make better mission-critical decisions. Our operational team excels at signal collection, processing and analysis. We have operational personnel stationed around the world in support of our customers' missions.
What Youâll Get to Do:
As our Data Engineer you will have the opportunity to:
Work with external data providers and stakeholders to engineer the ingest of and distribution of mission-relevant data feeds into the FADE data ecosystem.
Focus on the operations of COTS and GOTS software to ensure low latency, high data integrity, quality, and availability for over 400 data feeds (and growing).
Work with users to help them troubleshoot and resolve issues with FADE data.
Work in a fast moving environment with many moving parts and must be able to juggle several tasks at once.
Work with a highly qualified team to maintain the integrity of data flow by responding to real-time operational alerts.
Bring a continuous delivery mindset while striving for a 99.9% uptime to the flow of data through the system.
Specific duties include:
Configure and maintain data ingest workflows (ETL) across several production systems
Install, configure, and update a wide array of COTS/GOTS and homegrown software applications
Support and troubleshoot diverse IT infrastructure hardware platforms and protocols
Work with software development and systems administration staff to monitor and troubleshoot production systems
Generate and maintain systems documentation and diagrams
Troubleshoot network issues and establish new connectivity
Monitor and maintain a variety of databases
Youâll Bring These Qualifications:
Clearance: Active TS/SCI with current Poly
Education: Bachelorâs degree or equivalent (Master's preferred)
Experience: A minimum of five years of related work experience
Required Skills:
Minimum 5 years experience with Linux/Unix environments
Strong software scripting skills in Python and other scripting languages (Bash, Perl, etc.)
Strong understanding of RDBMS databases (PostgreSQL, Oracle, MySql, etc.)
Prior experience working with the Git version-control system
Strong systems engineering background
Must be certified to meet DoD 8570 level IAT-II qualifications. Security+ certification or CISSP preferred.
These Qualifications Would be Nice to Have:
Red Hat Enterprise Linux administration experience
Understanding of Amazon Web Services â EC2, RDS, S3
Hadoop, Accumulo and Map Reduce techniques
Familiarity with monitoring tools such as Grafana and Kibana
Understands compiled languages including Java
Familiarity with Software Development Programs
Familiarity with Agile Development methodologies
What We can Offer You:
Weâve been named a Best Place to Work by the Washington Post and one of the Top Workplaces in the Denver, Co by the Denver Post.
Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives.
We offer competitive benefits and learning and development opportunities.
We are mission-oriented and ever vigilant in aligning our solutions with the nationâs highest priorities.
For over 55 years, the principles of CACIâs unique, character-based culture have been the driving force behind our success.
TH-JOBS!BB
HJ-BLITZ-101019!
Job Location
US-Denver-CO-DENVER


CACI employs a diverse range of talent to create an environment that fuels innovation and fosters continuous improvement and success. At CACI, you will have the opportunity to make an immediate impact by providing information solutions and services in support of national security missions and government transformation for Intelligence, Defense, and Federal Civilian customers. CACI is proud to provide dynamic careers for employees worldwide. CACI is an Equal Opportunity Employer - Females/Minorities/Protected Veterans/Individuals with Disabilities."," Clearance  Active TS/SCI with current Poly Education  Bachelorâs degree or equivalent  Master's preferred  Experience  A minimum of five years of related work experience  Minimum 5 years experience with Linux/Unix environments Strong software scripting skills in Python and other scripting languages  Bash, Perl, etc.  Strong understanding of RDBMS databases  PostgreSQL, Oracle, MySql, etc.  Prior experience working with the Git version-control system Strong systems engineering background Must be certified to meet DoD 8570 level IAT-II qualifications. Security+ certification or CISSP preferred.   ","Clearance Active TS/SCI with current Poly Education Bachelorâs degree or equivalent Master's preferred Experience A minimum of five years related work experience Minimum 5 Linux/Unix environments Strong software scripting skills in Python and other languages Bash, Perl, etc. understanding RDBMS databases PostgreSQL, Oracle, MySql, Prior working the Git version-control system systems engineering background Must be certified to meet DoD 8570 level IAT-II qualifications. Security+ certification CISSP preferred.","Clearance Active TS/SCI current Poly Education Bachelorâs degree equivalent Master's preferred Experience A minimum five years related work experience Minimum 5 Linux/Unix environments Strong software scripting skills Python languages Bash, Perl, etc. understanding RDBMS databases PostgreSQL, Oracle, MySql, Prior working Git version-control system systems engineering background Must certified meet DoD 8570 level IAT-II qualifications. Security+ certification CISSP preferred."
331,Data Engineer,Google Data Engineer,"Denver, CO 80203",Denver,CO,"Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet todayâs high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on GCP and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security (Cloud IAM, Data Loss Prevention API, etc)Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutionsMinimum of 3 years of hands-on experience in GCP and Big Data technologies such as Java, Node.js, C##, Python, PySpark, Spark/SparkSQL, Hadoop, Hive, Pig, Oozie and streaming technologies such as Kafka, Stream Ingestion API, Unix shell/Perl scripting etc.
Extensive experience providing practical direction with the GCP Native and Hadoop ecosystem
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Experience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps â Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plus
Understanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus


Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform. Multi-cloud experience a plus.   Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP DevOps an platform. Multi-cloud a plus. Proven ability to build, manage and foster team-oriented environment work creatively analytically in problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","Minimum 3 years previous Consulting client service delivery experience Google GCP DevOps platform. Multi-cloud plus. Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
332,Data Engineer,Data Engineer,"Denver, CO",Denver,CO,"Description

About the role:
Data Engineer works in the Data Engineering team and has primary responsibility for building Enterprise Data Integration solutions by working on enterprise class data integration initiatives. The Data Engineer will be responsible for building solutions which are flexible, performant and scalable. In this role, you are the primary resource on the most complex or escalated issues and may provide direction, guidance and mentoring to team members. You will apply specialized business knowledge, technical skills and creativity to significant deliverables and projects that involve multiple IT departments and business units which have enterprise scope. The Data Engineer should be able to explore newer technology options, if need be, and must have a high sense of ownership over every deliverable.
Responsibilities:
Builds scalable and reliable Data Integration solutions which are flexible, scalable and elastic.Develops low latency Data Integration solutions to provision data near real time for multiple consumers.Collaborates with Data Engineers, Data Architects and Service developers to build optimal and efficient ETL and Database code.Produces dynamic, data driven solutions to support the strategic business goals.Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute to meet business objectives.Help develop an enterprise scale Data WarehouseDesign and develop new systems and tools to enable stakeholders to consume and understand data fasterSupports ETL processing.Provides on-call support of Data Integration processes on a rotating basis and other on-call as required.Produces dynamic, data driven solutions to support the strategic business goals.Performs other duties and responsibilities as assigned.

Qualifications
Minimum of a Bachelorâs degree in Computer Science, MIS or related degree and five (5) years of relevant development or engineering experience or combination of education, training and experience.Expert/Advanced level experience with ETL Tools, ODI preferably.Expert Level experience with Oracle as a Database Platform.Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.Experience or understanding of programming languages like Python, Java, R etc.Experience or understanding of Cloud Data Platforms a plus.Strong understanding of Data Warehousing concepts.Financial Services Industry knowledge is a plus.May occasionally work a non-standard shift including nights and/or weekends and/or have on-call responsibilities.
Licenses/Certifications:
None required","Minimum of a Bachelorâs degree in Computer Science, MIS or related degree and five  5  years of relevant development or engineering experience or combination of education, training and experience.Expert/Advanced level experience with ETL Tools, ODI preferably.Expert Level experience with Oracle as a Database Platform.Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.Experience or understanding of programming languages like Python, Java, R etc.Experience or understanding of Cloud Data Platforms a plus.Strong understanding of Data Warehousing concepts.Financial Services Industry knowledge is a plus.May occasionally work a non-standard shift including nights and/or weekends and/or have on-call responsibilities.  Builds scalable and reliable Data Integration solutions which are flexible, scalable and elastic.Develops low latency Data Integration solutions to provision data near real time for multiple consumers.Collaborates with Data Engineers, Data Architects and Service developers to build optimal and efficient ETL and Database code.Produces dynamic, data driven solutions to support the strategic business goals.Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute to meet business objectives.Help develop an enterprise scale Data WarehouseDesign and develop new systems and tools to enable stakeholders to consume and understand data fasterSupports ETL processing.Provides on-call support of Data Integration processes on a rotating basis and other on-call as required.Produces dynamic, data driven solutions to support the strategic business goals.Performs other duties and responsibilities as assigned.  ","Minimum of a Bachelorâs degree in Computer Science, MIS or related and five 5 years relevant development engineering experience combination education, training experience.Expert/Advanced level with ETL Tools, ODI preferably.Expert Level Oracle as Database Platform.Deep SQL tuning, tuning solutions, physical optimization databases.Experience understanding programming languages like Python, Java, R etc.Experience Cloud Data Platforms plus.Strong Warehousing concepts.Financial Services Industry knowledge is plus.May occasionally work non-standard shift including nights and/or weekends have on-call responsibilities. Builds scalable reliable Integration solutions which are flexible, elastic.Develops low latency to provision data near real time for multiple consumers.Collaborates Engineers, Architects Service developers build optimal efficient code.Produces dynamic, driven support the strategic business goals.Focus on designing, building, launching infrastructure scale compute meet objectives.Help develop an enterprise WarehouseDesign new systems tools enable stakeholders consume understand fasterSupports processing.Provides processes rotating basis other required.Produces goals.Performs duties responsibilities assigned.","Minimum Bachelorâs degree Computer Science, MIS related five 5 years relevant development engineering experience combination education, training experience.Expert/Advanced level ETL Tools, ODI preferably.Expert Level Oracle Database Platform.Deep SQL tuning, tuning solutions, physical optimization databases.Experience understanding programming languages like Python, Java, R etc.Experience Cloud Data Platforms plus.Strong Warehousing concepts.Financial Services Industry knowledge plus.May occasionally work non-standard shift including nights and/or weekends on-call responsibilities. Builds scalable reliable Integration solutions flexible, elastic.Develops low latency provision data near real time multiple consumers.Collaborates Engineers, Architects Service developers build optimal efficient code.Produces dynamic, driven support strategic business goals.Focus designing, building, launching infrastructure scale compute meet objectives.Help develop enterprise WarehouseDesign new systems tools enable stakeholders consume understand fasterSupports processing.Provides processes rotating basis required.Produces goals.Performs duties responsibilities assigned."
333,Data Engineer,Data Engineer,"Denver, CO",Denver,CO,"Qualifications:
Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns.
Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem
Youâre proficient with Linux operations and development, including basic commands and shell scripting
You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices
Must be fluent in Python, R, and Java
Must have excellent experience command of SQL
Must have good experience and knowledge with Data Modeling concepts.
You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills
Preferred Qualifications:
Must have 3 -5 years of experience building data productionalized pipelines.
Must have strong experience ingesting huge volumes of structured and unstructured data both in streaming and batch ingestions patterns.
2 - 4 years of Cloud development experience with AWS and or Azure stack.
Exposure with and have solid experience with statistical analysis and machine learning libraries
Must have previous experience with NoSQL database implementations
You understand the fundamentals of lambda architectures and serverless. applications
Must be proficient in Tableau
Must be comfortable with leveraging ETL tools, like Informatica.
You are proficient in Scala or Node.js
You have a masterâs degree in a quantitative field
Job Description:
Partner with data scientists, analytics engagement managers, and other data engineers to discover, collect, cleanse, and refine the data needed for analysis and modeling
Analyze large data sets to extract actionable insights and inform experimental design and model development
Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build models using basic statistical and machine learning techniques, partnering with data scientists for education and guidance
Weâre looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment.
Working Conditions
Office environment"," Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns. Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem Youâre proficient with Linux operations and development, including basic commands and shell scripting You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices Must be fluent in Python, R, and Java Must have excellent experience command of SQL Must have good experience and knowledge with Data Modeling concepts. You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills    ","Must have a solid understanding of data engineering, integration, and warehousing concepts patterns. experience with design, build, maintain batch streaming solutions at scale in both on-premises cloud environments, specifically the Hadoop ecosystem Youâre proficient Linux operations development, including basic commands shell scripting You can demonstrate DevOps methodologies continuous integration/continuous delivery practices be fluent Python, R, Java excellent command SQL good knowledge Data Modeling concepts. passion for science machine learning strong desire to develop your analysis modeling skills","Must solid understanding data engineering, integration, warehousing concepts patterns. experience design, build, maintain batch streaming solutions scale on-premises cloud environments, specifically Hadoop ecosystem Youâre proficient Linux operations development, including basic commands shell scripting You demonstrate DevOps methodologies continuous integration/continuous delivery practices fluent Python, R, Java excellent command SQL good knowledge Data Modeling concepts. passion science machine learning strong desire develop analysis modeling skills"
334,Data Engineer,Data Engineer,"Denver, CO",Denver,CO,"Qwinix Technologies is seeking a Data Engineer to assist our client in leveraging Big Data frameworks, optimizing data pipeline architecture, and assisting with cloud strategy!

Requirements:
Strong candidates will have recent experience with the following:


Must have a Data warehouse/Big Data background
3+ years experience with Hadoop
3+ years experience with Java
3+ years SQL experience
2+ years experience in AWS services (EMR, Glue, S3, Lambda, ect.)
The ability to build CI/CD pipelines with large data sets and cloud technologies

Bonus Points!


MapReduce framework experience!
Oozie workflows experience!
Hbase experience!

[https://www.linkedin.com/in/bethany-delaney-307180153/]","     Must have a Data warehouse/Big Data background 3+ years experience with Hadoop 3+ years experience with Java 3+ years SQL experience 2+ years experience in AWS services  EMR, Glue, S3, Lambda, ect.  The ability to build CI/CD pipelines with large data sets and cloud technologies ","Must have a Data warehouse/Big background 3+ years experience with Hadoop Java SQL 2+ in AWS services EMR, Glue, S3, Lambda, ect. The ability to build CI/CD pipelines large data sets and cloud technologies","Must Data warehouse/Big background 3+ years experience Hadoop Java SQL 2+ AWS services EMR, Glue, S3, Lambda, ect. The ability build CI/CD pipelines large data sets cloud technologies"
335,Data Engineer,Data Engineer,"Denver, CO 80221",Denver,CO,"Job Description Summary
At Transamerica, we are innovating the next generation of data solutions and capabilities to help our customers achieve a lifetime of financial security. As part of the Data Engineering team in our Enterprise Data Services group, you will apply your engineering skills and passion in developing modern architectures to enable our data-driven digital business.
Job Description
Data Engineers are responsible for the design, architecture and support of the systems, services and applications required for the collection, storage, processing, and analysis of all forms of data in order to enable data-driven decisions and outcomes within the organization.
Responsibilities:
Working collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:
Architect, build and support the operation of our Cloud and On-Premises enterprise data infrastructure and tools
Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Qualifications:
Bachelorâs degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience
2+ years of collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization
2+ years of experience architecting, building and administering big data and real-time streaming analytics architectures in both on premises and cloud environments (AWS, Azure, Google) leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB
1+ years of experience architecting, building and administering large-scale distributed applications
1+ years of experience with Linux operations and development, including basic commands and shell scripting
2+ years of experience with execution of DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environment
Software development experience in least two or more of following languages: Java, Python, Scala, Node.js
Expertise in usage of SQL for data profiling, analysis and extraction
Preferred Qualifications:
Masterâs Degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience
1+ years of experience with advanced analytics and machine learning concepts and technology implementations (Tensorflow, H20)
2+ years of experience with NoSQL implementations (Mongo, Cassandra, HBase)
3+ years of experience in implementing serverless architecture leveraging AWS Lambda or similar technology
1+ years of experience with data visualization tools such as Tableau and PowerBI
Solid understanding of the Hadoop ecosystem (e.g. HDFS, MapReduce, HBase, Pig, Scoop, Spark, Hive)
Solid understanding of the Hadoop ecosystem (e.g. HDFS, MapReduce, HBase, Pig, Scoop, Spark, Hive)
2+ years of experience with data warehousing architecture and implementation, including hands on experience developing ETL (Informatica, SSIS, etc.)
Relevant technology or platform certification (AWS Certified, Microsoft Certified)
Behavioral & Leadership Competencies:
Attention to detail and results oriented, with a strong customer focus
The ability to work within a team environment
Problem-solving and effective technical communication skills
Meet tight deadlines, multi-task, and prioritize workload
Willing to learn and keep pace with the latest advances in the related field (Grasp new technologies rapidly as needed to progress varied initiatives )
Our Culture:
At Transamerica we promote a Future Fit mindset. What is a Future Fit mindset?
Acting as One fosters an environment of positive collaboration
Accountability allows us to own the problem as well as the solution
Agility inspires new ideas, innovation and challenges the status quo
Customer Centricity encourages an above and beyond approach to our customer
Working Environment:
Office environment
Due to the nature of the role, work outside of normal business hours may be required as needed
Occasional travel less than 10%"," Bachelorâs degree in a technical field  e.g. Comp Science, Math, Engineering  or related experience 2+ years of collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization 2+ years of experience architecting, building and administering big data and real-time streaming analytics architectures in both on premises and cloud environments  AWS, Azure, Google  leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB 1+ years of experience architecting, building and administering large-scale distributed applications 1+ years of experience with Linux operations and development, including basic commands and shell scripting 2+ years of experience with execution of DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environment Software development experience in least two or more of following languages  Java, Python, Scala, Node.js Expertise in usage of SQL for data profiling, analysis and extraction   Working collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment  Architect, build and support the operation of our Cloud and On-Premises enterprise data infrastructure and tools Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications Assist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage  ","Bachelorâs degree in a technical field e.g. Comp Science, Math, Engineering or related experience 2+ years of collective data engineering, analysis, warehousing, integration business intelligence, similarly sized organization architecting, building and administering big real-time streaming analytics architectures both on premises cloud environments AWS, Azure, Google leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB 1+ large-scale distributed applications with Linux operations development, including basic commands shell scripting execution DevOps methodologies Continuous Integration/Continuous Delivery within large scale delivery environment Software development least two more following languages Java, Python, Scala, Node.js Expertise usage SQL for profiling, analysis extraction Working collaboratively other engineers, scientists, teams, product owners an agile Architect, build support the operation our Cloud On-Premises enterprise infrastructure tools Design robust, reusable scalable driven solutions pipeline frameworks to automate ingestion, processing structured unstructured batch Build APIs services critical operational processes, analytical models machine learning Assist selection tools, required expand platform capabilities Understand implement best practices management data, master reference metadata, quality lineage","Bachelorâs degree technical field e.g. Comp Science, Math, Engineering related experience 2+ years collective data engineering, analysis, warehousing, integration business intelligence, similarly sized organization architecting, building administering big real-time streaming analytics architectures premises cloud environments AWS, Azure, Google leveraging technologies Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB 1+ large-scale distributed applications Linux operations development, including basic commands shell scripting execution DevOps methodologies Continuous Integration/Continuous Delivery within large scale delivery environment Software development least two following languages Java, Python, Scala, Node.js Expertise usage SQL profiling, analysis extraction Working collaboratively engineers, scientists, teams, product owners agile Architect, build support operation Cloud On-Premises enterprise infrastructure tools Design robust, reusable scalable driven solutions pipeline frameworks automate ingestion, processing structured unstructured batch Build APIs services critical operational processes, analytical models machine learning Assist selection tools, required expand platform capabilities Understand implement best practices management data, master reference metadata, quality lineage"
336,Data Engineer,Data Engineer (Imagine Analytics),"Denver, CO",Denver,CO,"Why Join IMA?
Imagine a Companyâ¦
â¦that is embarking on a journey to rethink how 300-year-old processes can be engineered to be simple, easy and inspired by the best of consumer tech. A company that looks for the best athletes but loves the art of winning as a team. A company that knows relationships are the currency of the business, and that digitizing them, to reflect their importance today â will only make the future stronger and the industry better. A company working to limit risk every day, while risking it all.

Imagine being part of a team, where impact is tangible, your voice is part of the story and the work is industry changing. This is not imaginary, it is Imagine Analytics. Explore the frontier of insurance with us!

Working at Imagine Analytics
We are doing highly impactful work in an established industry with a lot of complexity and a rich array of interesting challenges. Learning about insurance markets and working closely with partners to identify opportunities and create efficiencies is what drives us. And we have the best of both worlds, the stability of 40 years in business with the speed and energy of a startup.

Our team is a small group of experienced designers, engineers and entrepreneurs. We believe that software development is a team sport, and that the best products are built by teams with diverse backgrounds who are empowered with a high degree of autonomy. We strive for rapid iteration and continuous improvement in both our products themselves and our approaches to building them. We work at a sustainable pace and know from experience that building a lasting product organization is a marathon, not a sprint.

We are building a greenfield product on a modern technology stack, which currently includes React/Apollo, Scala/Spark/Python, Elasticsearch, Postgres, and various serverless technologies. We iterate quickly and deploy continuously, while striving to keep the quality of our user experience high and our codebases tidy. And we have a hell of a lot of fun doing it, together.
What Youâll Do
Imagine Analytics (IA) is looking for a Data Engineer to build and maintain technology products for our mid-market insurance data platform. They will work in a highly interdisciplinary fashion with other functions within the team to prioritize, scope, design, build, deliver, and manage features and systems within IAâs product portfolio, operating with a high degree of autonomy and flexibility. The ideal candidate will have prior experience building and managing data systems, as well as working in an early-stage company environment.
Roles & Responsibilities
You will be responsible for architecting, building and maintaining data pipelines, storage systems, analysis flows, and procedures to support IAâs product objectives and business goals.
You should be comfortable working in a fast-paced environment with quick changes and a high degree of uncertainty.
A high level of proficiency in tools and architectures for building modern data infrastructure is a must, as is a desire to learn quickly to adapt to changing technologies and business challenges.
As a member of the engineering team, you will help guide both the technical and usability aspects of the software we build and will be involved all stages of the cycle of iterative product development.
Youâll work with partners and stakeholders to analyze data sources and gather requirements, with designers to define user flows and look and feel, with other engineers to implement functionality, and with customers to provide support and gather feedback.
You will have a high degree of freedom to pick the tools and frameworks that you feel best address the problems at hand and will be responsible for managing and maintaining those technology investments over their entire lifetime.
You Should Have:
Bachelor's degree, or related field or equivalent experience
2+ years of experience building data pipelines and storage systems. Experience with data warehousing and/or machine learning a plus.
Design Centered: A high degree of product sense & strong appreciation for UX
Excitement to explore new technologies and platforms that could add system value

This Job Description is not a complete statement of all duties and responsibilities comprising this position.

The IMA Financial Group, Inc. provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, The IMA Financial Group, Inc. complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.","   You will be responsible for architecting, building and maintaining data pipelines, storage systems, analysis flows, and procedures to support IAâs product objectives and business goals. You should be comfortable working in a fast-paced environment with quick changes and a high degree of uncertainty. A high level of proficiency in tools and architectures for building modern data infrastructure is a must, as is a desire to learn quickly to adapt to changing technologies and business challenges. As a member of the engineering team, you will help guide both the technical and usability aspects of the software we build and will be involved all stages of the cycle of iterative product development. Youâll work with partners and stakeholders to analyze data sources and gather requirements, with designers to define user flows and look and feel, with other engineers to implement functionality, and with customers to provide support and gather feedback. You will have a high degree of freedom to pick the tools and frameworks that you feel best address the problems at hand and will be responsible for managing and maintaining those technology investments over their entire lifetime.  ","You will be responsible for architecting, building and maintaining data pipelines, storage systems, analysis flows, procedures to support IAâs product objectives business goals. should comfortable working in a fast-paced environment with quick changes high degree of uncertainty. A level proficiency tools architectures modern infrastructure is must, as desire learn quickly adapt changing technologies challenges. As member the engineering team, you help guide both technical usability aspects software we build involved all stages cycle iterative development. Youâll work partners stakeholders analyze sources gather requirements, designers define user flows look feel, other engineers implement functionality, customers provide feedback. have freedom pick frameworks that feel best address problems at hand managing those technology investments over their entire lifetime.","You responsible architecting, building maintaining data pipelines, storage systems, analysis flows, procedures support IAâs product objectives business goals. comfortable working fast-paced environment quick changes high degree uncertainty. A level proficiency tools architectures modern infrastructure must, desire learn quickly adapt changing technologies challenges. As member engineering team, help guide technical usability aspects software build involved stages cycle iterative development. Youâll work partners stakeholders analyze sources gather requirements, designers define user flows look feel, engineers implement functionality, customers provide feedback. freedom pick frameworks feel best address problems hand managing technology investments entire lifetime."
337,Data Engineer,US Contractor - Data Engineer (Big Data),"Louisville, CO",Louisville,CO,"We work to solve deep technical problems that improve the world of Healthcare. These problems span a variety of core topics in computer science ranging from databases to distributed systems. We are looking for an Experienced Data Engineer for a 6-month contract (possibility to extend or convert to FTE) to join our new Data Organization.

Principle duties and responsibilities:

Leads backend and ETL development effort for the Data Team.
Designs, develops, and performance-tunes extraction, transformation, and load (ETL) processes using SQL, PySpark or Python source-to-target data mappings
Analyzing Use Case requirements and working with teammates to implement defined functionality
Provide architectural guidance and development/build standards for the team
Promoting collaboration through activities such as design sessions, design reviews, pair programming, etc.,
Required Qualifications & Skills:

Bachelorâs degree in Computer Science, Computer Engineering or Information Technology
8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills
5 years of experience of ETL development in a big data environment
5 years working in an agile development environment.
Experience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS
Strong ownership, urgency, and drive to ship code
Ability to think outside the normal concepts to implement
Ability to communicate technical concepts and designs to cross functional teams and off shore teams with varying degrees of technical experience.
Displays flexibility in adapting to changing conditions.
Strong team player, makes a valuable contribution to team objectives, displays trust and mutual understanding, accepts constructive feedback, and handles confrontation constructively.
Preferred Qualifications & Skills

Application/system architecture experience
Experience with AWS Services RedShift Spectrum, Elastic Search, API Gateway and Lambda"," Bachelorâs degree in Computer Science, Computer Engineering or Information Technology 8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills 5 years of experience of ETL development in a big data environment 5 years working in an agile development environment. Experience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS   Bachelorâs degree in Computer Science, Computer Engineering or Information Technology 8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills 5 years of experience of ETL development in a big data environment 5 years working in an agile development environment. Experience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS    ","Bachelorâs degree in Computer Science, Engineering or Information Technology 8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills 5 ETL development a big environment working an agile environment. Experience developing AWS using S3, EC2, Redshift, Glue, Athena and RDS","Bachelorâs degree Computer Science, Engineering Information Technology 8 years data engineering experience building business intelligence applications exceptional SQL, PL/SQL, and/or Python skills 5 ETL development big environment working agile environment. Experience developing AWS using S3, EC2, Redshift, Glue, Athena RDS"
338,Data Engineer,AWS Data Engineer,"Denver, CO 80203",Denver,CO,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
339,Data Engineer,Associate Data Engineer,"Boulder, CO 80302",Boulder,CO,"Come work for us!
We are looking for dedicated employees to join our team to help our customers have the best experience possible every time they enter a Finish Line Store.
Our employees are key to our success.
Job Summary:
Engineer, Enterprise Data Solutions performs activities related to the data foundation of Finish Line/JD Sports; including development & support of pipelines, ETL jobs and tools, data marts, data lake, and data warehouse in multiple environments. The Engineering position partners with the Product Manager to understand business requirements (user stories), and develops solutions to meet business objectives. The Engineering team (EDS) is responsible for data from production system to BI tool, including movement and transformation. This role reports to the Consulting Engineer.
Key Responsibilities and Tasks:
Development of jobs & pipelines from multiple production data sources into Data Lake environments
Engineers production ready solutions, inclusive of alerting and error handling
Works with Cloud based tools (Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc.) to deliver best-in-class cloud based data solutions
Works collaboratively with DBA team for operational execution and reliability of data solutions, both in Oracle and BigQuery
Assists in maintaining data governance through documentation of data solutions, through ERDs, Confluence documentation, or external tools
Engineer & model curated and keyed Data Warehouse solutions that meet business objectives that perform efficiently and effectively
Works in Agile product management method, managing tasks & objectives (user stories) through JIRA and providing updates to SCRUM master
Partners with Product Manager (PO) to understand business requirements across multiple functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer & Loyalty, Legal, & Data Science
Support current Data Warehouse ETL jobs, respond to tickets and inquiries from business partners when data quality issues occur
Other projects and duties as assigned
Required Education and/or Experience
Bachelorâs degree (B.A.) in Information Systems or other related field from a four-year college or university, or equivalent combination of education and experience. 1-3 years of proven work ability in data analytics, data engineering, and process documentation required. Must have experience with partnering with stakeholders of all levels of the organization to plan and solve problems.
Physical Demands â The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
EEOC Statement:
The Finish Line, Inc. is an Equal Opportunity Employer and is committed to complying with all federal, state, and local EEO laws. The Finish Line, Inc. prohibits discrimination against employees and applicants for employment based on the individual's race or color, religion or creed, national origin, alienage or citizenship status, marital status, sex, pregnancy status, age, military status, disability, or any other protected characteristic or class protected by law. The Finish Line, Inc. provides reasonable accommodation for disabilities in accordance with applicable laws.

Need accessibility assistance to apply?

Applicants who require accessibility assistance to submit an employment application can either call Finish Line at ( 317) 613-6890 or email us at talentacquisition@finishline.com. A member of our Talent Acquisition team will respond as soon as reasonably possible. ( This email address and phone number is only for individuals seeking accommodation when applying for a job. )","  Development of jobs & pipelines from multiple production data sources into Data Lake environments Engineers production ready solutions, inclusive of alerting and error handling Works with Cloud based tools  Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc.  to deliver best-in-class cloud based data solutions Works collaboratively with DBA team for operational execution and reliability of data solutions, both in Oracle and BigQuery Assists in maintaining data governance through documentation of data solutions, through ERDs, Confluence documentation, or external tools Engineer & model curated and keyed Data Warehouse solutions that meet business objectives that perform efficiently and effectively Works in Agile product management method, managing tasks & objectives  user stories  through JIRA and providing updates to SCRUM master Partners with Product Manager  PO  to understand business requirements across multiple functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer & Loyalty, Legal, & Data Science Support current Data Warehouse ETL jobs, respond to tickets and inquiries from business partners when data quality issues occur Other projects and duties as assigned   ","Development of jobs & pipelines from multiple production data sources into Data Lake environments Engineers ready solutions, inclusive alerting and error handling Works with Cloud based tools Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc. to deliver best-in-class cloud solutions collaboratively DBA team for operational execution reliability both in Oracle BigQuery Assists maintaining governance through documentation ERDs, Confluence documentation, or external Engineer model curated keyed Warehouse that meet business objectives perform efficiently effectively Agile product management method, managing tasks user stories JIRA providing updates SCRUM master Partners Product Manager PO understand requirements across functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer Loyalty, Legal, Science Support current ETL jobs, respond tickets inquiries partners when quality issues occur Other projects duties as assigned","Development jobs & pipelines multiple production data sources Data Lake environments Engineers ready solutions, inclusive alerting error handling Works Cloud based tools Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc. deliver best-in-class cloud solutions collaboratively DBA team operational execution reliability Oracle BigQuery Assists maintaining governance documentation ERDs, Confluence documentation, external Engineer model curated keyed Warehouse meet business objectives perform efficiently effectively Agile product management method, managing tasks user stories JIRA providing updates SCRUM master Partners Product Manager PO understand requirements across functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer Loyalty, Legal, Science Support current ETL jobs, respond tickets inquiries partners quality issues occur Other projects duties assigned"
340,Data Engineer,Data Engineer,"Denver, CO",Denver,CO,"Join a high performing and rapidly growing team

Valen Analytics is a rapidly expanding advanced data and predictive analytics company headquartered in downtown Denver. Valenâs state-of-the-art analytics and predictive modeling products are built on Valenâs unique industry-wide consortium data platform, specifically designed for property and casualty insurance carriers.

Valen Analytics is looking for a Data Engineer to expand our growing data processing needs. We are looking for candidates with at least 2 years of experience, who demonstrate a curious analytical mind with ability to understand business objectives, ask insightful questions, and be detail oriented in implementation.

As a Data Engineer, you will work with customers, Valen team members, and 3rd party data providers, to develop, maintain, and enhance our data engineering capabilities in support of our data and predictive analytic offerings to the market.

Responsibilities

This position will be part of an existing team whose primary responsibilities are to identify, acquire, validate, cleanse, and produce data and datasets to be used in advanced analytics and predictive modeling initiatives by our customers and internal teams. This is accomplished by combining data processing experience with software engineering concepts into solutions that are hosted in our cloud-based platform, InsureRight.

This position will leverage the following tools and platforms:

Microsoft SQL Server
PostgreSQL
MongoDB
Microsoft SSIS
BIRST BI
Valen InsureRight platform
This position will make sure of the following skills:

Data extraction, transformation, and cleansing
Data profiling and visualization
Collaborating with data scientists, software engineers, production operations, subject matter experts and customers
Fostering continuous delivery pipelines
Managing and maintaining metadata
This position requires the ability to:

Work in a fast-paced environment as part of a small team
Collaborate with team members in the development and maintenance of our solutions
Identify opportunities to automate data engineering tasks and workflow
Fostering continuous delivery pipelines
Managing and maintaining metadata
Education & Experience

2+ years Data Engineering experience with TSQL, python, map reduce or functional programming
Bachelorâs degree in programming or related technical areas
Developing and supporting an end user production system
Reporting/data warehousing experience
Insurance industry knowledge or experience with insurance data a plus
The Valen Team
Valenâs mission is to help our clients achieve their goals and solve problems by leveraging data to make more informed decisions.

Guide Customer Success: We relentlessly pursue making our customers successful.

Live the Golden Rule: We treat our customers, employees, vendors and shareholders how we expect to be treated as customers, employees, vendors and shareholdersâ¦period.

Be Agile: Valen is a test and learn environment. We organize everything we do around our customerâs success to provide something of value quickly. We learn and then adapt. Then, we learn some more.

Have Fun: We have great attitudes and we have fun. We do not take ourselves too seriously, we celebrate our successes and we enjoy our work. Most of all, we live passionately.

Embrace Simplicity: We endeavor to make everything we provide our customers ridiculously easy.

Expect Ownership: At Valen we take responsibility for our actions and we build trusting relationships by making and meeting our commitments.","   Microsoft SQL Server PostgreSQL MongoDB Microsoft SSIS BIRST BI Valen InsureRight platform  2+ years Data Engineering experience with TSQL, python, map reduce or functional programming Bachelorâs degree in programming or related technical areas Developing and supporting an end user production system Reporting/data warehousing experience Insurance industry knowledge or experience with insurance data a plus ","Microsoft SQL Server PostgreSQL MongoDB SSIS BIRST BI Valen InsureRight platform 2+ years Data Engineering experience with TSQL, python, map reduce or functional programming Bachelorâs degree in related technical areas Developing and supporting an end user production system Reporting/data warehousing Insurance industry knowledge insurance data a plus","Microsoft SQL Server PostgreSQL MongoDB SSIS BIRST BI Valen InsureRight platform 2+ years Data Engineering experience TSQL, python, map reduce functional programming Bachelorâs degree related technical areas Developing supporting end user production system Reporting/data warehousing Insurance industry knowledge insurance data plus"
341,Data Engineer,Lead Data Engineer,"Denver, CO 80202",Denver,CO,"As a Lead Data Engineer on Healthgrades' Facility Data as a Service team, you will develop and maintain database functionality to support corporate products and services, focused on mastering healthcare claims data for consumption across Healthgrades' products. You will collaborate with multi-functional database and product development teams using Agile / Scrum, SQL Server, .Net and Open Source technology.
In this role, you will implement database technologies and development processes to support database development for a Data Platform that is changing the game. On this product, we are currently transitioning our Microsoft technology stacks to other Open Source technology stacks, so experience or interest in learning those tools is a plus. If you are passionate about growing your expertise in these technologies, this will be a great opportunity for you.

What You Will Do:
Oversee day-to-day operational matters, provide training, and supervise performance related to company and individual OKRs
Lead and manage complex development projects including plans, designs, technical leadership, schedules, and resource allocation while also being a hands-on engineer
Build and maintain complex T-SQL statements that perform efficiently against large data sets
Performance tune large Data Warehousing platforms, with a focus on schema bound views, database partitioning, and etl/query optimization
Build and maintain Windows Workflow Engine Packages and Models using Windows Workflow Foundation
Create and maintain automated ETL processes with special focus on data flow, error recovery, and exception handling and reporting
Create and modify data models and implementations as they relate to RDBMS, DW, and BI
Design of specialized data structures for the purpose of data consumption by a public facing website and/or Business Analytics data visualization
Load, process and migrate incoming data feeds and create outgoing data extracts
Create and maintain documentation to support developed applications

What You Will Bring:
Ability to participate in a culture of communication, collaboration and creativity
Previous experience in a lead or management role with direct reports
Strong RDBMS and Microsoft SQL Server 2014/2016 skills
SQL Programming / ETL and data architecture management experience
Experience building and maintaining database structures, ETL processes, stored procedures, audit reports, data extracts, SSIS, SSAS, SSRS, etc. to meet project objectives
Perform Unit Tests and internal QA checks to insure high quality work
Good collaboration and idea sharing in team environment
A Bachelorâs Degree in related field or equivalent experience

Preferred Qualifications:
Data modeling and corporate-level data management experience
Previously worked with Windows Workflow Foundation, Windows Workflow Designer and Windows Presentation Foundation technologies
Experience with Elasticsearch, Aurora, MySQL, Spark, Streamsets, Kafka, Python, and Scala is a plus

Why Healthgrades?
At Healthgrades, we recognize that our people drive our greatest achievements. We are passionate about maintaining a fulfilling, rewarding and high-energy work environment while setting the stage for your continued success.
Meaningful Work â empowering consumers with data to make the right decisions for themselves and their families
Changing the Game - evolving, dynamic culture with career advancement opportunities
Community Builders- participating in local charity organizations and wellness initiatives
Robust Perks â generous PTO, 401k contributions, tuition assistance, entertainment discounts & more!"," Data modeling and corporate-level data management experience Previously worked with Windows Workflow Foundation, Windows Workflow Designer and Windows Presentation Foundation technologies Experience with Elasticsearch, Aurora, MySQL, Spark, Streamsets, Kafka, Python, and Scala is a plus    ","Data modeling and corporate-level data management experience Previously worked with Windows Workflow Foundation, Designer Presentation Foundation technologies Experience Elasticsearch, Aurora, MySQL, Spark, Streamsets, Kafka, Python, Scala is a plus","Data modeling corporate-level data management experience Previously worked Windows Workflow Foundation, Designer Presentation Foundation technologies Experience Elasticsearch, Aurora, MySQL, Spark, Streamsets, Kafka, Python, Scala plus"
342,Data Engineer,Data Engineer - Node,"Denver, CO 80246",Denver,CO,"Data Engineer â Node [Denver, CO]
Become a part of something great at MeetingOne.

MeetingOne is a rapidly growing software development and services company focused on audio and web conferencing technology solutions. Headquartered in Denver, Colorado, MeetingOne is a full-service audio and web conferencing, e-learning, event solution and consulting services provider. Since 1999, MeetingOne has enabled businesses and educational organizations around the world to communicate more effectively, using innovative virtual meeting and event technologies and services. More info at www.meetingone.com.

This position is a great opportunity to own the core technology stack that is vital to our growing company. Learn, develop, refactor and master the heart of our product portfolio that in turn achieves high velocity in driving value to our beloved clients. Demonstrate the skills and knowledge you collected over the years to help lead our development efforts to success.

As the Software Engineer, you will be reporting to the Software Development Manager. Your focus is on owning key layers in our technology. You will have the opportunity to imagine, create and deliver leading edge solutions that meet the needs of a rapidly expanding user base. You will have the ability to influence people and process to achieve this goal.

You will have the opportunity to mentor other developers, sharing your years of experience with tactics and strategies that lead to quality coding. With senior experience, you are a development team advocate. Ensuring the right tools are available and continuous training opportunities are available.

You will work closely with QA and Product to aid in the dependable release of value. You will be responsible for handling development related items escalated with respective core technology stack.

At the end of the day you love working on the most valuable technology the company owns. You strive at every opportunity to leverage bleeding techniques and technology to express your creative side. You enjoy having an impact on co-workers and the user community a-like.

What does success look like?

You exhibit creative problem solving to achieve effective results.
You prioritize action to drive achievements that delight.
You enjoy independence to define, direct, and/or perform critical thinking to resolve complex issues.
Qualifications/ Experience:

Experience with databases SQL and NoSQL
Deep knowledge of Node, JavaScript,
Experience with scaling backend of large-scale applications
Algorithms
Positive attitude
Curious, eager to learn, and colaborate
Pluses:
Experience with audio, video conferencing, WebRTC, SFU, MCU, WebAssembly
Experience with other languages e.g. C++, LUA
Experience with IM/Presence (XMPP, Other,â¦)
Experience with Docker, AWS
Telecommunications knowledge
Education OR equal work experience:

Required: Bachelorâs degree in Computer Science or equivalent work experience
Preferred: Masterâs degree"," Experience with databases SQL and NoSQL Deep knowledge of Node, JavaScript, Experience with scaling backend of large-scale applications Algorithms Positive attitude Curious, eager to learn, and colaborate    Required  Bachelorâs degree in Computer Science or equivalent work experience Preferred  Masterâs degree ","Experience with databases SQL and NoSQL Deep knowledge of Node, JavaScript, scaling backend large-scale applications Algorithms Positive attitude Curious, eager to learn, colaborate Required Bachelorâs degree in Computer Science or equivalent work experience Preferred Masterâs","Experience databases SQL NoSQL Deep knowledge Node, JavaScript, scaling backend large-scale applications Algorithms Positive attitude Curious, eager learn, colaborate Required Bachelorâs degree Computer Science equivalent work experience Preferred Masterâs"
343,Data Engineer,Data Engineer - Hux,"Denver, CO 80203",Denver,CO,"Hux Data Engineer
Locations: New York, NY â Greensboro, NC - Chicago, IL â Raleigh, Durham, Chapel-Hill, NC - Denver, CO
What is Hux? Hux is the Human Experience Platform by Deloitte Digital.
In todayâs world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.
Customers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.
Hux by Deloitte Digital gives companies the ability to build and leverage the connections â between people, systems, data and technologies â so they can deliver personalized, contextual experiences to customers at scale.

Work youâll do
As a Hux Data Engineer, youâll design, implement, and maintain a full suite of real-time and batch jobs that fuels our cutting edge AI to provide real-time marketing intelligence to our existing clients.
Youâll develop, test and deliver production grade code to help our clients solve their marketing challenges using cutting-edge big-data tools. Youâll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall Platform.
As you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, youâll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.
Your responsibilities will include:
Design, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.
Bring together large, complex and sparse data sets to meet functional and non-functional business requirements.
Design and implements data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.
Integrate new data management technologies and software engineering tools into existing structures.
Help build high-performance algorithms, prototypes, predictive models and proof of concepts.
Use a variety of languages, tools and frameworks to marry data and systems together.
Recommend ways to improve data reliability, efficiency and quality.
Collaborate with Data Scientists, DevOps and Project Managers on meeting project goals.
Tackle challenges and solve complex problems on a daily basis.
Qualifications
Required:
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.
Preferred:
Producing high-quality code in Python.
Passionate about testing, and with extensive experience in Agile teams using SCRUM you consider automated build and test to be the norm.
Proven ability to communicate in both verbal and writing in a high performance, collaborative environment.
Follows data development best practices, and enjoy helping others learn to do the same.
An independent thinker who considers the operating context of what he/she is developing.
Believes that the best data pipelines run unattended for weeks and months on end.
Familiar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.
Helpful, but not required:
Knowledge in:
Experience with large consumer data sets used in performance marketing is a major advantage.
Familiarity with machine learning libraries is a plus.
Well-versed in (or contributes to) data-centric open source projects.
Reads Hacker News, blogs, or stays on top of emerging tools in some other way
Data visualization
Industry-specific marketing data
Technologies of Interest:
Languages/Libraries â Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.
Cloud â AWS, Azure, Google
The team
Advertising, Marketing & Commerce
Our Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clientsâ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clientsâ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.

We serve our clients through the following types of work:Cross-channel customer engagement strategy, design and development(web, mobile, social, physical)eCommerce strategy, implementation and operationsMarketing Content and digital asset management solutionsMarketing Technology and Advertising Technology solutionsMarketing analytics implementation and operationsAdvertising campaign ideation, development and executionAcquisition and engagement campaign ideation, development and executionAgile based, design-thinking, user-centric, empirical projects that accelerate results

How youâll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe thereâs always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitteâs culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitteâs impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youâre applying to. Check out recruiting tips from Deloitte professionals.
kwhux"," 4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment. 2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores. 1+ years of experience on distributed, high throughput and low latency architecture. 1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale. A successful track-record of manipulating, processing and extracting value from large disconnected datasets.    ","4+ years of experience in software development, a substantial part which was gained high-throughput, decision-automation related environment. 2+ working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores. 1+ on distributed, high throughput low latency architecture. deploying or managing pipelines for supporting data-science-driven decisioning at scale. A successful track-record manipulating, processing extracting value from large disconnected datasets.","4+ years experience software development, substantial part gained high-throughput, decision-automation related environment. 2+ working big data using technologies like Spark, Kafka, Flink, Hadoop, NoSQL datastores. 1+ distributed, high throughput low latency architecture. deploying managing pipelines supporting data-science-driven decisioning scale. A successful track-record manipulating, processing extracting value large disconnected datasets."
344,Data Engineer,Data Engineer (Mid and Senior),"Denver, CO",Denver,CO,"Data Engineer, Mid to Senior Level
Datalere team members lead by example, focus on customer needs and have a thirst to learn all they can about data analytics. Successful candidates are self-starters and never shy away from challenges.
We need team members that excel when working directly with clients to meet their goals. They understand the client's needs and requirements and build a collaborative environment to ensure a successful project delivery.
Data Engineers analyze and develop on-premises and/or cloud data and ETL solutions to solve the client's challenges. They enjoy the challenges of consulting and thrive to knock the socks off of clients
Please note that this role is vendor agnostic in regards to what ETL tools are used, so having multi vendor experience would be ideal.
Responsibilities:
Hands-on development and serve as technical expert on projects
Develop data solutions leveraging traditional and cloud product offerings from leading vendors
Develop data models to meet client needs
Develop data models to meet client needs, including transactional, third-normal form, dimensional, columnar, distributed and NoSQL
Develop ETL/ELT processes and patterns to efficiently move data
Create data visualizations, dashboards and reports as needed
Develop and scope requirements
Travel as needed (currently less than 5%)
Maintain effective communication with team and customers
Qualifications
2+ years designing and developing data analytics solutions
2+ years with RDBMS such as SQL Server, Oracle, MySQL
2+ years data warehouse, dimensional modeling design and architecture
A passion to learn and improve your skills to deliver the best possible solutions to customers
Experience with cloud based data services offered by Azure, AWS and Google
Experience with data visualization tools such as Power BI and Tableau
Previous consulting experience preferred
Degree in computer science, information technology, engineering or business
Must be authorized to work in the US. We are unable to sponsor H-1B visas at this time.
About Us
At Datalere, we work with our clients to transform their enterprise through the use of modern compute technologies and proven deployment processes providing cost effective durable solutions for the competitive world.
If you are seeking new challenges, interested in staying up to date with the latest releases and can deliver uncompromised service to our customers, then we'd like to hear from you. If you are interested and meet the above qualifications, please submit your resume and cover letter indicating your interest to join our team.","2+ years designing and developing data analytics solutions 2+ years with RDBMS such as SQL Server, Oracle, MySQL 2+ years data warehouse, dimensional modeling design and architecture A passion to learn and improve your skills to deliver the best possible solutions to customers Experience with cloud based data services offered by Azure, AWS and Google Experience with data visualization tools such as Power BI and Tableau Previous consulting experience preferred Degree in computer science, information technology, engineering or business Must be authorized to work in the US. We are unable to sponsor H-1B visas at this time.   Hands-on development and serve as technical expert on projects Develop data solutions leveraging traditional and cloud product offerings from leading vendors Develop data models to meet client needs Develop data models to meet client needs, including transactional, third-normal form, dimensional, columnar, distributed and NoSQL Develop ETL/ELT processes and patterns to efficiently move data Create data visualizations, dashboards and reports as needed Develop and scope requirements Travel as needed  currently less than 5%  Maintain effective communication with team and customers  ","2+ years designing and developing data analytics solutions with RDBMS such as SQL Server, Oracle, MySQL warehouse, dimensional modeling design architecture A passion to learn improve your skills deliver the best possible customers Experience cloud based services offered by Azure, AWS Google visualization tools Power BI Tableau Previous consulting experience preferred Degree in computer science, information technology, engineering or business Must be authorized work US. We are unable sponsor H-1B visas at this time. Hands-on development serve technical expert on projects Develop leveraging traditional product offerings from leading vendors models meet client needs needs, including transactional, third-normal form, dimensional, columnar, distributed NoSQL ETL/ELT processes patterns efficiently move Create visualizations, dashboards reports needed scope requirements Travel currently less than 5% Maintain effective communication team","2+ years designing developing data analytics solutions RDBMS SQL Server, Oracle, MySQL warehouse, dimensional modeling design architecture A passion learn improve skills deliver best possible customers Experience cloud based services offered Azure, AWS Google visualization tools Power BI Tableau Previous consulting experience preferred Degree computer science, information technology, engineering business Must authorized work US. We unable sponsor H-1B visas time. Hands-on development serve technical expert projects Develop leveraging traditional product offerings leading vendors models meet client needs needs, including transactional, third-normal form, dimensional, columnar, distributed NoSQL ETL/ELT processes patterns efficiently move Create visualizations, dashboards reports needed scope requirements Travel currently less 5% Maintain effective communication team"
345,Data Engineer,Sr. Data Engineer,"Denver, CO 80202",Denver,CO,"At Ping Identity, we're changing the way people think about enterprise security technology. With our innovative Identity Defined Security platform, we're helping to build a borderless world where people have total freedom to work wherever and however they want. Without friction. Without fear.

We're headquartered in Denver, Colorado, and we have offices and employees around the globe. And we serve the largest, most demanding enterprises worldwide, including over half of the Fortune 100. Because even in the most complex enterprise environments, security shouldn't be a source of anxiety. It should be one of your greatest competitive advantages.

We call this digital freedom. And it's not just something we provide our customers. It's something that drives our company. People don't come here to join a culture that's build on digital freedom. They come to cultivate it.

The Senior Data Engineer will have the opportunity to play a critical role in the early stages of developing Pingâs Business Intelligence and Data Analytics capabilities. This individual will collaborate with other teams across the organization (e.g., Product Management, Engineering, Sales, and Finance) to gain a quick understanding of Ping products, business process areas, and/or technologies, and build the analytical framework to enable the business to better understand and leverage complex data sets. The Senior Data Engineer will be expected to architect and build core datasets, implement efficient ETL processes, and own the design, development and maintenance of critical metrics, reports, analyses, dashboards, etc. to drive key business decisions. In addition, this individual must be able to adapt and thrive in a fast-paced and changing business and technical environment.

Key Responsibilities:

Architect, build, and support the operation of enterprise data and analytical infrastructure and tools
Design robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in the selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in the management of enterprise data, including master data, reference data, metadata, data quality and lineage
Develop and prepare strategies for Business Intelligence processes for the organization
Manage and customize all ETL processes as per customer requirement and analyze all processes for same
Perform assessment on all reporting requirements and contribute to the development of a long-term strategy for various reporting solutions
Coordinate with data generator and ensure compliance to all enterprise data model according to data standards
Essential Qualifications:

Bachelorâs degree in computer science, math, engineering, or relevant technical field
4+ years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration, and data integration concepts and methodologies
3+ years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments
3+ years of experience with execution of DevOps methodologies and continuous integration/continuous delivery
Object-oriented/object function scripting languages: Python, R, C/C++, Java, Scala, etc.
SQL, relational databases and NoSQL databases
Data integration tools (e.g. Talend, SnapLogic, Informatica) and data warehousing / data lake tools
API based data acquisition and management
MSSQL, PostgreSQL, MySQL, etc. - MemSQL, CrateDB, etc.
Business intelligence tools such as Tableau, PowerBI, Zoomdata, Pentaho, etc.
Data modeling tools such as ERWin, Enterprise Architect, Visio, etc.
Data integration tools such as Boomi, Pentaho, Talend, Informatica, SnapLogic, etc.
Familiarity with cloud-based data engineering (AWS, GCP, or Azure)
Familiarity with data science techniques and frameworks
Creative thinker with strong analytical skills
Ability to work in a team environment
Strong technical communication skills
Ability to prioritize work to meet tight deadlines
Ability to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables
Desired Qualifications:

Experience with advanced analytics and machine learning concepts and technology implementations
Experience in a fast-paced, ever-changing and growing environment"," Bachelorâs degree in computer science, math, engineering, or relevant technical field 4+ years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration, and data integration concepts and methodologies 3+ years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments 3+ years of experience with execution of DevOps methodologies and continuous integration/continuous delivery Object-oriented/object function scripting languages  Python, R, C/C++, Java, Scala, etc. SQL, relational databases and NoSQL databases Data integration tools  e.g. Talend, SnapLogic, Informatica  and data warehousing / data lake tools API based data acquisition and management MSSQL, PostgreSQL, MySQL, etc. - MemSQL, CrateDB, etc. Business intelligence tools such as Tableau, PowerBI, Zoomdata, Pentaho, etc. Data modeling tools such as ERWin, Enterprise Architect, Visio, etc. Data integration tools such as Boomi, Pentaho, Talend, Informatica, SnapLogic, etc. Familiarity with cloud-based data engineering  AWS, GCP, or Azure  Familiarity with data science techniques and frameworks Creative thinker with strong analytical skills Ability to work in a team environment Strong technical communication skills Ability to prioritize work to meet tight deadlines Ability to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables    Architect, build, and support the operation of enterprise data and analytical infrastructure and tools Design robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications Assist in the selection and integration of data related tools, frameworks, and applications required to expand platform capabilities Understand and implement best practices in the management of enterprise data, including master data, reference data, metadata, data quality and lineage Develop and prepare strategies for Business Intelligence processes for the organization Manage and customize all ETL processes as per customer requirement and analyze all processes for same Perform assessment on all reporting requirements and contribute to the development of a long-term strategy for various reporting solutions Coordinate with data generator and ensure compliance to all enterprise data model according to data standards   ","Bachelorâs degree in computer science, math, engineering, or relevant technical field 4+ years of collective experience the application data analytics, warehousing, business intelligence, database administration, and integration concepts methodologies 3+ architecting, building, administering big real-time streaming analytics architectures on-premises cloud environments with execution DevOps continuous integration/continuous delivery Object-oriented/object function scripting languages Python, R, C/C++, Java, Scala, etc. SQL, relational databases NoSQL Data tools e.g. Talend, SnapLogic, Informatica warehousing / lake API based acquisition management MSSQL, PostgreSQL, MySQL, - MemSQL, CrateDB, Business intelligence such as Tableau, PowerBI, Zoomdata, Pentaho, modeling ERWin, Enterprise Architect, Visio, Boomi, Informatica, Familiarity cloud-based engineering AWS, GCP, Azure science techniques frameworks Creative thinker strong analytical skills Ability to work a team environment Strong communication prioritize meet tight deadlines learn keep pace latest technology advances quickly grasp new technologies support contribute project deliverables build, operation enterprise infrastructure Design robust, reusable, scalable driven solutions pipeline automate ingestion, processing both structured unstructured batch Build APIs services critical operational processes, models machine learning applications Assist selection related tools, frameworks, required expand platform capabilities Understand implement best practices data, including master reference metadata, quality lineage Develop prepare strategies for Intelligence processes organization Manage customize all ETL per customer requirement analyze same Perform assessment on reporting requirements development long-term strategy various Coordinate generator ensure compliance model according standards","Bachelorâs degree computer science, math, engineering, relevant technical field 4+ years collective experience application data analytics, warehousing, business intelligence, database administration, integration concepts methodologies 3+ architecting, building, administering big real-time streaming analytics architectures on-premises cloud environments execution DevOps continuous integration/continuous delivery Object-oriented/object function scripting languages Python, R, C/C++, Java, Scala, etc. SQL, relational databases NoSQL Data tools e.g. Talend, SnapLogic, Informatica warehousing / lake API based acquisition management MSSQL, PostgreSQL, MySQL, - MemSQL, CrateDB, Business intelligence Tableau, PowerBI, Zoomdata, Pentaho, modeling ERWin, Enterprise Architect, Visio, Boomi, Informatica, Familiarity cloud-based engineering AWS, GCP, Azure science techniques frameworks Creative thinker strong analytical skills Ability work team environment Strong communication prioritize meet tight deadlines learn keep pace latest technology advances quickly grasp new technologies support contribute project deliverables build, operation enterprise infrastructure Design robust, reusable, scalable driven solutions pipeline automate ingestion, processing structured unstructured batch Build APIs services critical operational processes, models machine learning applications Assist selection related tools, frameworks, required expand platform capabilities Understand implement best practices data, including master reference metadata, quality lineage Develop prepare strategies Intelligence processes organization Manage customize ETL per customer requirement analyze Perform assessment reporting requirements development long-term strategy various Coordinate generator ensure compliance model according standards"
346,Data Engineer,Data Engineer with TS/SCI with Poly,"Aurora, CO 80011",Aurora,CO,"Description
Job Description:
The National Solutions Group at Leidos has an opening for a passionate Data Engineer to provide a variety of software and IT support services to ensure customer satisfaction with production software and systems. The position specifically focuses on the operations of GOTS software to ensure data integrity and availability for a large ETL system, but includes general system troubleshooting and direct customer support. The position is for a mid-senior level engineer with experience in both software development and systems administration. Candidates must be able to work in a fast moving environment with many moving parts and must be able to juggle several tasks at once. In addition, they must be willing to share on-call responsibilities to troubleshoot customer issues during non-business hours.
Primary Roles & Responsibilities:
As a Data Engineer you will have the opportunity to:Support of production data processing and data distribution systemsWork with data providers and customers to ensure data quality and availabilityGather requirements and work with data providers to enable distribution of new data sourcesSupport software deployments and integration of geographically diverse computing systemsProvide direct support to end usersConfigure and maintain data ingest workflows (ETL) across several production systemsInstall, configure, and update a wide array of COTS/GOTS and homegrown software applicationsSupport and troubleshoot diverse IT infrastructure hardware platforms and protocolsWork with software development and systems administration staff to monitor and troubleshoot production systemsGenerate and maintain systems documentation and diagramsTroubleshoot network issues and establish new connectivityMonitor and maintain a variety of databases
Minimum Qualifications:United Statesâ citizen with current TS/SCI, SSBI and polyBachelorâs Degree with at least 4-8 years of applicable experience or Masters degree with 2-6 years of experience or 4 additional years in lieu of degreeStrong grasp of LinuxAutomating tasks by writing quality codeStrong coding skills (Java, Javascript, shell scripting, Perl, Python)Configuration managements tools (Puppet, Chef)Monitoring complex systems (Nagios, ElasticSearch, Grafana)Automation tools (Jenkins, Bamboo)Source-control systems (Git, SVN)Candidate must be certified to meet DoD 8570 level IAT-II qualifications. A Security+ certification is requiredWilling to share on-call responsibilities to include coming into work during non-business hours to troubleshoot customer issues
Preferred Qualifications:Masterâs DegreeRed Hat Enterprise Linux administration experienceUnderstanding of Amazon Web Services â EC2, RDS, S3Hadoop, Accumulo and Map Reduce techniquesUnderstands compiled languages including JAVASoftware versioning control systems â GIT/SVNFamiliarity with Software Development ProgramsFamiliarity with Agile Development methodologiesJava Programming experience
External Referral Bonus:
Ineligible
Potential for Telework:
No
Clearance Level Required:
Top Secret/SCI with Polygraph
Travel:
Yes, 10% of the time
Scheduled Weekly Hours:
40
Shift:
Day
Requisition Category:
Professional
Job Family:
Software Engineering
Leidos is a Fortune 500Â® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.
Pay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.
Leidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.
All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws.","United Statesâ citizen with current TS/SCI, SSBI and polyBachelorâs Degree with at least 4-8 years of applicable experience or Masters degree with 2-6 years of experience or 4 additional years in lieu of degree  Support of production data processing and data distribution systemsWork with data providers and customers to ensure data quality and availabilityGather requirements and work with data providers to enable distribution of new data sourcesSupport software deployments and integration of geographically diverse computing systemsProvide direct support to end usersConfigure and maintain data ingest workflows  ETL  across several production systemsInstall, configure, and update a wide array of COTS/GOTS and homegrown software applicationsSupport and troubleshoot diverse IT infrastructure hardware platforms and protocolsWork with software development and systems administration staff to monitor and troubleshoot production systemsGenerate and maintain systems documentation and diagramsTroubleshoot network issues and establish new connectivityMonitor and maintain a variety of databases  ","United Statesâ citizen with current TS/SCI, SSBI and polyBachelorâs Degree at least 4-8 years of applicable experience or Masters degree 2-6 4 additional in lieu Support production data processing distribution systemsWork providers customers to ensure quality availabilityGather requirements work enable new sourcesSupport software deployments integration geographically diverse computing systemsProvide direct support end usersConfigure maintain ingest workflows ETL across several systemsInstall, configure, update a wide array COTS/GOTS homegrown applicationsSupport troubleshoot IT infrastructure hardware platforms protocolsWork development systems administration staff monitor systemsGenerate documentation diagramsTroubleshoot network issues establish connectivityMonitor variety databases","United Statesâ citizen current TS/SCI, SSBI polyBachelorâs Degree least 4-8 years applicable experience Masters degree 2-6 4 additional lieu Support production data processing distribution systemsWork providers customers ensure quality availabilityGather requirements work enable new sourcesSupport software deployments integration geographically diverse computing systemsProvide direct support end usersConfigure maintain ingest workflows ETL across several systemsInstall, configure, update wide array COTS/GOTS homegrown applicationsSupport troubleshoot IT infrastructure hardware platforms protocolsWork development systems administration staff monitor systemsGenerate documentation diagramsTroubleshoot network issues establish connectivityMonitor variety databases"
347,Data Engineer,DATA ENGINEER,"Denver, CO 80202",Denver,CO,"Decentrix is offering an exciting opportunity to an individual with the right skill set and background to join our elite team and work with the most advanced Media Advertising Enhancement/Optimization Technologies. You would be using your business, technical & development skills in an extremely fast paced environment providing services to forward thinking media corporations. See www.bianalytix.com for more information.
Position Responsibilities:
Design, Develop and Maintain Data Storage and Data Analytics Products, Systems and Solutions for clients in the Media Industry
Develop, Enhance & Configure data transformation and storage solutions in support of our product offerings
Work closely with other internal engineering teams focused on front-end and back-end APIs development and configuration in direct support of various products
Work closely with internal product and project management teams to design and prioritize features and their associated delivery timelines
Diagnose and correct system and product faults, designing & implementing solutions to correct
Technical Proficiencies
Scala Language development with experience in full stack testing
Apache Spark and distributed data processing methodologies
T-SQL / PL SQL Development
Complex data processing methodologies
Massive scale data processing methodologies
Amazon AWS technologies and products (ECS, EC2, EMR, S3, etc.)
Linux
Docker image development & container execution
Advanced knowledge of RDBMS systems and associated storage optimization techniques
Analytic data structure knowledge and experience
Working knowledge of software engineering and applying database methodologies, techniques, and tools
Personal
The experience in the design of data storage and data transformation processes to leverage and analyze complex data relationships
The skills to quickly diagnose and solve technical problems associated to large data solutions and the associated technologies
Interest and ability to work at the technical installation level with major media corporations
Maturity to manage task associated to products and/or services to an installation contract
Excellent communication skills, including good verbal and written abilities","  Design, Develop and Maintain Data Storage and Data Analytics Products, Systems and Solutions for clients in the Media Industry Develop, Enhance & Configure data transformation and storage solutions in support of our product offerings Work closely with other internal engineering teams focused on front-end and back-end APIs development and configuration in direct support of various products Work closely with internal product and project management teams to design and prioritize features and their associated delivery timelines Diagnose and correct system and product faults, designing & implementing solutions to correct   ","Design, Develop and Maintain Data Storage Analytics Products, Systems Solutions for clients in the Media Industry Develop, Enhance & Configure data transformation storage solutions support of our product offerings Work closely with other internal engineering teams focused on front-end back-end APIs development configuration direct various products project management to design prioritize features their associated delivery timelines Diagnose correct system faults, designing implementing","Design, Develop Maintain Data Storage Analytics Products, Systems Solutions clients Media Industry Develop, Enhance & Configure data transformation storage solutions support product offerings Work closely internal engineering teams focused front-end back-end APIs development configuration direct various products project management design prioritize features associated delivery timelines Diagnose correct system faults, designing implementing"
348,Data Engineer,Cloud Data Engineer,"Denver, CO 80209",Denver,CO,"Businessolver delivers market-changing benefits administration technology supported by an intrinsic and unwavering responsiveness to client needs. Our clients trust Businessolver to take care of them and their employees with a configurable and secure SaaS platform and a culture of service, all aimed at total and measurable success and our clients' complete delight.

We work with some of the most recognizable brands in the U.S. We look to our rock-star employees to help these clients maximize the investment in their benefits program, minimize their exposure to risk, engage their employees with our easy-to-use solution and full suite of communication tools, and empower their employees to use their benefits wisely.

At Businessolver you will have opportunities for individual development through our common language: Trust through transparency. Assume positive intent. Be real. Live a growth attitude. Embrace the reverse golden rule.

The Cloud Data Engineer (CDE) will be responsible for architecting, developing, implementing, and operating stable, scalable, low cost solutions to source data from production systems into the data lake (AWS) and data warehouse (Redshift) and into end-user facing applications (AWS Quicksight). The ideal candidate should be able to work with Infrastructure, Data Analysts, and Machine Learning Engineers in a fast-paced environment, understanding the business requirements, and implementing ETL, machine learning and cloud solutions. This role will serve on the Cloud Data Engineering team.

Qualifications:

Degree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development
Proficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc.
Experience with developing on an MPP database Redshift/Teradata/Snowflake
Proficient handling large data sets using SQL and databases in a business and engineering environment
Experience with operations in a Public Cloud Environment (AWS/Azure/GCP)
Experience with ETL and Data Warehouse/Lake processes
Excellent verbal and written communication skills
Strong troubleshooting and problem-solving skills
Thrive in a fast-paced, innovative environment

Preferred Qualifications:

Oracle, Postgres, EMR, Redshift, Linux experience
Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysis
Experience with Agile Methodologies
Experience with complex/large data sets (Big Data)
Experience operating a Data Lake
Experience with Cloud Architecture/Engineering

The Businessolver Wayâ¦

Our team has spent nearly two decades crafting a culture that challenges each employee to perform at the top of their game â and have fun doing it! If you desire to use your skills and experience in an environment where you can make a difference, we want to hear from you! Businessolver employees experience a vibrant work culture with extensive workplace perks including:


Competitive pay, great benefits, and vacation time. We are an equal opportunity employer with competitive benefits including medical, dental, life insurance, disability, 401(k) with company match, among others.
Smart Casual Dress. No need to suit up, but we also have on-site dry cleaning services for those that prefer to dress-up!
Weekly catered meals. Breakfast every other Mondays, lunch Wednesdays, and afternoon appetizers on Fridays encourage collaboration across our teams.
Fully-stocked kitchens. We know it takes fuel to perform, so we provide a kitchen stocked with healthy cereals, fruit, snacks, and beverages to keep you at the top of your game.
Fitnessolver. If you need a boost, visit our on-site fitness facility to clear your head.
Massages. With a ""work hard/play hard"" atmosphere we all need a little stress relief at times.
Charity and community involvement. Participate in a variety of ways to support those around us.
Learning & Development. Continue to learn about the industry through our online and instructor-led classes.
Recognition. Want some swag? Earn tons of it by helping out your co-workers through our employee recognition program.
Culture. Want a culture most dream of? Most companies talk about it, we live it. Come find out for yourself!

Interested? Great, we look forward to reading your application - make sure it includes:


A cover letter that highlights why you think you'd be great for the gig, focusing on how your past work experience has prepared you for this kind of position â or why you think you can rock the job even though you don't have past work experience that's perfectly aligned.
Your resume.

You will receive an auto-reply confirming that we've received your application, and you will hear from us again after we've reviewed your application and decided whether or not to move you forward in our recruiting process.

If you do decide to apply, please know that every complete application will be carefully reviewed. Seriously! We know it is a time commitment to prepare an application. We will respect that effort by thoughtfully reviewing every single complete application and we are truly grateful for your interest.

Thanks for your interest in Businessolver!

Check us out on Twitter ( https://twitter.com/businessolver ), Facebook ( https://www.facebook.com/bsolver ) and LinkedIn ( https://www.linkedin.com/company/232793?trk=tyah&trkInfo=tarId%3A1415406210925%2Ctas%3Abusinessolver%2Cidx%3A2-1-4 ) for a look at our vibrant culture."," Degree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development Proficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc. Experience with developing on an MPP database Redshift/Teradata/Snowflake Proficient handling large data sets using SQL and databases in a business and engineering environment Experience with operations in a Public Cloud Environment  AWS/Azure/GCP  Experience with ETL and Data Warehouse/Lake processes Excellent verbal and written communication skills Strong troubleshooting and problem-solving skills Thrive in a fast-paced, innovative environment     ","Degree in Computer Engineering/Science or related field, with 4+ years of professional experience database/data lake development Proficient processing data on relational databases like Oracle/SQL Server/MySQL/etc. Experience developing an MPP database Redshift/Teradata/Snowflake handling large sets using SQL and a business engineering environment operations Public Cloud Environment AWS/Azure/GCP ETL Data Warehouse/Lake processes Excellent verbal written communication skills Strong troubleshooting problem-solving Thrive fast-paced, innovative","Degree Computer Engineering/Science related field, 4+ years professional experience database/data lake development Proficient processing data relational databases like Oracle/SQL Server/MySQL/etc. Experience developing MPP database Redshift/Teradata/Snowflake handling large sets using SQL business engineering environment operations Public Cloud Environment AWS/Azure/GCP ETL Data Warehouse/Lake processes Excellent verbal written communication skills Strong troubleshooting problem-solving Thrive fast-paced, innovative"
349,Data Engineer,Sr. Data Engineer,"Westminster, CO",Westminster,CO,"Please review the job details below.
The Data Intelligence Team is a central data engineering and analytics team that supports business decisions and company strategy across all parts of MAXAR. We collect data from a variety of interesting sources and build automated data flows that help enable actionable insights for many different teams and products. We are looking for data professionals that are excited to work with cloud-based tools and build high quality data flows. We are a team of people from a variety of data focused backgrounds. We support each other, and we help each other learn and grow. Come collaborate with us to help guide MAXAR and improve our collective understanding of our planet.
Responsibilities
Collect, store, and aggregate data to support the creation of great data products for the business and our customers
Be creative and cooperative in designing and building data pipelines
Use Cloud-based infrastructure and applications (we use AWS and maintain our own Kubernetes clusters)
Continually learn and seek ways to improve our data flows
Collaborate well in a team environment (we use agile)
Validate data and maintain healthy infrastructure and data flows
Required
Bachelorâs Degree in a technical field or equivalent work experience
Experience building and maintaining ETL pipelines
2 years of experience with Python, SQL, AWS (S3, EC2)
Data visualization or reporting experience
Good verbal and written communication skills
Preferred
Any experience withâ¦
Airflow or other ETL scheduling tools
Building or improving APIs
Collecting data from a variety of sources (APIs, Postgres, Oracle, SAP, Salesforce, etc.)
Thoughtfully storing data in databases (especially Postgres) to support data products and reporting
Any experience with or willingness to learnâ¦
Big data pipelines using Spark and streaming tools
Maintaining data pipelines used in a production environment
Working with geospatial data
Additional coding languages, especially JVM languages or Bash
Any of these or similar tools (Kubernetes, Ansible, Jenkins, PostgreSQL, Tableau)
Additional AWS tools (EFS, Lambda, RDS/Aurora, Glue, Athena)
Perks
Diverse team that works and learns together
Great benefits, flexible time off for family and travel, pet insurance discounts
Working to improve products that helps us understand and protect our planet
Well-lit office space with on-site cafeteria and coffee shop
Private frisbee golf course and very nice game and quiet rooms for mental breaks
Quarterly self-guided work time to build what you want or learn new skills and technology
MAXAR Technologies offers a generous compensation package including a competitive salary; choice of medical plan; dental, life, and disability insurance; a 401(K) plan with competitive company match; paid holidays and paid time off.","  Collect, store, and aggregate data to support the creation of great data products for the business and our customers Be creative and cooperative in designing and building data pipelines Use Cloud-based infrastructure and applications  we use AWS and maintain our own Kubernetes clusters  Continually learn and seek ways to improve our data flows Collaborate well in a team environment  we use agile  Validate data and maintain healthy infrastructure and data flows   ","Collect, store, and aggregate data to support the creation of great products for business our customers Be creative cooperative in designing building pipelines Use Cloud-based infrastructure applications we use AWS maintain own Kubernetes clusters Continually learn seek ways improve flows Collaborate well a team environment agile Validate healthy","Collect, store, aggregate data support creation great products business customers Be creative cooperative designing building pipelines Use Cloud-based infrastructure applications use AWS maintain Kubernetes clusters Continually learn seek ways improve flows Collaborate well team environment agile Validate healthy"
350,Data Engineer,Tableau Developer Data Engineer (BHJOB22048_585),"Denver, CO",Denver,CO,"Tableau BI Engineer â ITmPowered Tableau Engineer will design, develop, and deliver high performance Tableau dashboards, workbooks, and visualizations providing Business Intelligence on hundreds of enterprise technology projects. Provide deep Tableau Server knowledge and expertise to the team on how to build and maintain Tableau Server dashboards and reports. Tableau Developer design, develop and implement [â¦]

Tableau BI Engineer â ITmPowered

Tableau Engineer will design, develop, and deliver high performance Tableau dashboards, workbooks, and visualizations providing Business Intelligence on hundreds of enterprise technology projects.
Provide deep Tableau Server knowledge and expertise to the team on how to build and maintain Tableau Server dashboards and reports.
Tableau Developer design, develop and implement BI solutions leveraging Tableau Desktop, Tableau Online, Tableau Prep, and Tableau Server.
Provide Tableau development and support across dozens of systems and data sources. Support existing Tableau server, Tableau reports, and Tableau Dashboard solutions.
Performance tune Tableau dashboards (optimize extracts, limit fields/records, marks, optimize/materialize calculations, query optimization, workbook cleanup).
Engineer BI dashboards for Technology Projects and related Finance information (Burn Rates, Accruals, Milestones, Expenses, budgets, Earned Value, ROI) as well as project KPIs.
Identify BI performance bottlenecks and design optimal solutions at the report / dashboard level (Tableau, Cognos, PowerBI), SQL data munging level (SQL joins, pivots, enrichment, aggregations Oracle, Hadoop, SQL), or ETL / Data movement level (DataStage, Informatica, Sqoop).
Tableau Server Configuration, Administration, Tuning & Performance, Data Connections, APIs
Tableau Desktop & Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience â including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouseâs, Hadoop/Cloudera, and Cloud platforms.
Work with end users to gather BI requirements (use cases, visualization, drill up/down, hierarchies, tables, pivots, outcomes, and data sources, etc.).
Build data models. Prepare, wrangle, and model the Data to derive effective data models supporting performant BI solutions. Understand technical data sources, data structures, data quality and necessary transformations to aggregate, enrich, validate, and publish data.
Source to target mappings and work with ETL Engineers to optimize data flows, and data preparation.
Develop and optimize BI Dashboards, Reports, Data Models, and data flows.
Support existing BI environments Tableau, Cognos, data movement (DataStage, Informatica, sqoop), and backend data repositories (Oracle, SQL Server, Hadoop).
Cloud BI migration â Migrate big data solutions to cloud in PowerBI / Azure â design and delivery.
 Requirements:

3-5 Years BI Experience developing BI dashboards, reporting, visualizations and data models. Proven BI dashboard, report, and visualization design and delivery.
1-2 years Tableau Server experience. Proven Tableau performance tuning reporting and dashboard solutions.
Tableau Server Experience: Configuration, Administration, Tuning & Performance, Data Connections, APIs
Tableau Desktop & Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience â including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouseâs, Hadoop/Cloudera, and Cloud platforms.
Experience with Tableau and Cognos Development.
Experience in data modeling and data prep (discovery, structuring, cleaning, enriching, validation, publishing). Tableau Prep preferred.
Source to Target mapping. Tableau Prep experience a major plus.
SQL Experience (joins, queries, select statements) â Oracle, SQL Server database back ends.
BI performance tuning across front end, ETL, Data back end optimization.
Solid Agile experience on BI projects â Gathering requirements, translating to use cases, BI stories, design/delivery of BI use cases â using SAFe, SCRUM, Agile, Kanban or similar method.
Exposure Python.","     3-5 Years BI Experience developing BI dashboards, reporting, visualizations and data models. Proven BI dashboard, report, and visualization design and delivery. 1-2 years Tableau Server experience. Proven Tableau performance tuning reporting and dashboard solutions. Tableau Server Experience  Configuration, Administration, Tuning & Performance, Data Connections, APIs Tableau Desktop & Tableau Prep Experience  Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience â including data extraction, transformation and load  ETL  from many sources; OLAP, OLTP, Datawarehouseâs, Hadoop/Cloudera, and Cloud platforms. Experience with Tableau and Cognos Development. Experience in data modeling and data prep  discovery, structuring, cleaning, enriching, validation, publishing . Tableau Prep preferred. Source to Target mapping. Tableau Prep experience a major plus. SQL Experience  joins, queries, select statements  â Oracle, SQL Server database back ends. BI performance tuning across front end, ETL, Data back end optimization. Solid Agile experience on BI projects â Gathering requirements, translating to use cases, BI stories, design/delivery of BI use cases â using SAFe, SCRUM, Agile, Kanban or similar method. Exposure Python.","3-5 Years BI Experience developing dashboards, reporting, visualizations and data models. Proven dashboard, report, visualization design delivery. 1-2 years Tableau Server experience. performance tuning reporting dashboard solutions. Configuration, Administration, Tuning & Performance, Data Connections, APIs Desktop Prep Advanced Visualizations, including custom visualizations, Modeling â extraction, transformation load ETL from many sources; OLAP, OLTP, Datawarehouseâs, Hadoop/Cloudera, Cloud platforms. with Cognos Development. in modeling prep discovery, structuring, cleaning, enriching, validation, publishing . preferred. Source to Target mapping. experience a major plus. SQL joins, queries, select statements Oracle, database back ends. across front end, ETL, end optimization. Solid Agile on projects Gathering requirements, translating use cases, stories, design/delivery of cases using SAFe, SCRUM, Agile, Kanban or similar method. Exposure Python.","3-5 Years BI Experience developing dashboards, reporting, visualizations data models. Proven dashboard, report, visualization design delivery. 1-2 years Tableau Server experience. performance tuning reporting dashboard solutions. Configuration, Administration, Tuning & Performance, Data Connections, APIs Desktop Prep Advanced Visualizations, including custom visualizations, Modeling â extraction, transformation load ETL many sources; OLAP, OLTP, Datawarehouseâs, Hadoop/Cloudera, Cloud platforms. Cognos Development. modeling prep discovery, structuring, cleaning, enriching, validation, publishing . preferred. Source Target mapping. experience major plus. SQL joins, queries, select statements Oracle, database back ends. across front end, ETL, end optimization. Solid Agile projects Gathering requirements, translating use cases, stories, design/delivery cases using SAFe, SCRUM, Agile, Kanban similar method. Exposure Python."
351,Data Engineer,Data Engineer,"Broomfield, CO 80021",Broomfield,CO,"Data Engineer

Position Overview
Validity is looking for a talented Data Engineer with 3+ years of experience in implementing modern data architectures. You will work closely with all areas of the business on engineering and analytical initiatives marked by greater complexity and less structure that will yield substantial product enhancements, uncover insights, and inform business decision making and focus. You will be working on one of the biggest opportunities at Validity: A major build-out of our data architecture. Your first projects will include helping to scale our data infrastructure and build out our data warehouse and analytics footprint. You will collaborate closely with Engineers and Product Managers to inform product decision making with data and to identify opportunities to create more value for our customers. This is a high-impact role that will help shape the future of Validity's products and services.
Company Overview
Validity is a leading global provider of data integrity and compliance offerings that thousands of organizations worldwide rely on to trust their data. We're passionate about our people, our customers, our values and our culture!
Join a passionate, driven team committed to bringing better insights and data-driven decisions to our internal and external customers. We're looking for people with a growth mindset and the insight to solve for today while building for the future. You will be working for a company that truly values the power of data.
Essential Position Duties and Responsibilities
3+ years experience with Data Warehouse Systems and working on an ETL system, either a commercial one like Matillion or Fivetran, an open-source one like Airflow, or a custom one you or your company built
Experience with one major cloud analytics database (Snowflake, Redshift, Google Big Query), Snowflake preferred
Strong familiarity with SQL
Python development experience in production
Familiarity with Spark
A strong desire to show ownership of problems you identify and proven ability to empower others to get more done
Familiarity with modern BI and exploration tools, Looker is preferred.
Familiarity with GitHub or other CD/CI tools
Basic AWS experience (S3, EC2) (1-2 years)
Some familiarity with streaming approaches preferred
CS Degree preferred
Some experience preferred with Jenkins, Docker, Kubernetes
Experience/Skills
SQL
Python
Cloud Data Warehouse Systems
ETL
Workflow Tools
Batch Processing
Spark
CD/CI tools

We are looking for someone to work in our Broomfield office but are open to a remote position depending on the situation.
fCVh3Ty9i9","  SQL Python Cloud Data Warehouse Systems ETL Workflow Tools Batch Processing Spark CD/CI tools  3+ years experience with Data Warehouse Systems and working on an ETL system, either a commercial one like Matillion or Fivetran, an open-source one like Airflow, or a custom one you or your company built Experience with one major cloud analytics database  Snowflake, Redshift, Google Big Query , Snowflake preferred Strong familiarity with SQL Python development experience in production Familiarity with Spark A strong desire to show ownership of problems you identify and proven ability to empower others to get more done Familiarity with modern BI and exploration tools, Looker is preferred. Familiarity with GitHub or other CD/CI tools Basic AWS experience  S3, EC2   1-2 years  Some familiarity with streaming approaches preferred CS Degree preferred Some experience preferred with Jenkins, Docker, Kubernetes  ","SQL Python Cloud Data Warehouse Systems ETL Workflow Tools Batch Processing Spark CD/CI tools 3+ years experience with and working on an system, either a commercial one like Matillion or Fivetran, open-source Airflow, custom you your company built Experience major cloud analytics database Snowflake, Redshift, Google Big Query , Snowflake preferred Strong familiarity development in production Familiarity A strong desire to show ownership of problems identify proven ability empower others get more done modern BI exploration tools, Looker is preferred. GitHub other Basic AWS S3, EC2 1-2 Some streaming approaches CS Degree Jenkins, Docker, Kubernetes","SQL Python Cloud Data Warehouse Systems ETL Workflow Tools Batch Processing Spark CD/CI tools 3+ years experience working system, either commercial one like Matillion Fivetran, open-source Airflow, custom company built Experience major cloud analytics database Snowflake, Redshift, Google Big Query , Snowflake preferred Strong familiarity development production Familiarity A strong desire show ownership problems identify proven ability empower others get done modern BI exploration tools, Looker preferred. GitHub Basic AWS S3, EC2 1-2 Some streaming approaches CS Degree Jenkins, Docker, Kubernetes"
352,Data Engineer,Sr Data Engineer,"Bellevue, WA 98004",Bellevue,WA,"A Sr Data Engineer will build, manage, integrate, and optimize reservoirs for data in support of delivering relevant information for business consumption promoting advances in predictive analytics and machine learning. This individual develops, constructs, tests and maintains designs for databases and large-scale data processing systems in support of underlying business, solution, and enterprise architectures. They support and maintain pipelines delivering relevant data sets for business consumption and analysis. This individual works closely with architects to determine which data management systems are appropriate and with the business to determine what data is needed for analysis. This individual works with architects to guide/align data management systems and closely with the business to determine what data is needed for analysis. This individual will wrestle with problems associated with database integration and messy, unstructured data sets toward the ultimate goal of providing clean, usable data to whomever may require it.

Responsibilities

Deliver on large and complex solution data needs by leading requirements and technical specifications that drive resulting data designs. Participate in strategic and innovative data design, requirements, walkthroughs, and reviews. Design and document system integration/configuration as required.
Implement and advance data models and configurations in support of integrating data from source systems and environments to promote Continuous Integration/Continuous Deployment (CI/CD) and CQRS pattern caching.
Research, design, and implement next generation analytics and machine learning platforms.
Build tools, frameworks, APIs, and dashboards to support telemetry and advanced analytics focusing on ways to improve data security, accessibility, reliability, scalability, efficiency and quality.
Lead data governance and guide all data schema/configuration changes.
Required/Preferred Qualifications

Education Required:
B.S. in Computer Science, Mathematics, Software/Computer Engineering, Information Systems or science related field. A Data Professional Certification (e.g., ICCP Certified Data Professional, BI Professional, Big Data Professional, Data Governance Professional, or vendor equivalent) is encouraged.

Minimum Years of Related Work Experience Required:
Minimum of 7-10 years of data design & development experience in relevant technologies/systems required including technical experience implementing and delivering from system architecture, design, integration, implementation, security, and capability roadmap for a data environment.

Skills and Abilities Required:
 Deep experience with RDMS databases (SQL Server) and Data Warehouse (OLAP, Redshift) managing connection-pools, performance tuning and optimizations.
 Awareness/exposure to NoSQL technologies (Key/Value, Columnar, Document, Graph)
 Creative Problem-Solving: Approaches data organization challenges leveraging experience with multiple, diverse technical configurations, technologies and processing environments.
 Effective Collaboration: Carefully listens to business partners, data scientists and architects to ascertain their needs partnering to establishing optimal outcomes.
 Intellectual Curiosity: Awareness and exposure with Data Visualization (e.g., Power BI, Microstrategy, Tableau), big data systems including MapReduce technologies (e.g., Hadoop, Spark), NoSQL technologies (Key/Value, Columnar, Document, Graph), and Monitoring platforms (e.g., Splunk, the Elastic Stack, CloudTrail, CloudWatch).
 Awareness/exposure development and modeling programming languages (e.g., Java, C#, R, Python).

Symetra is a dynamic and growing financial services company with 60 years of experience and customers nationwide. In our daily work delivering retirement, employee benefits, and life insurance products, we're guided by the principles of VALUE, TRANSPARENCY AND SUSTAINABILITY. That means we provide products and services people need at a competitive price, we communicate clearly and honestly so people understand what they're getting, and we build products that stand the test of time. We work hard and do what's right for our customers, communities and employees. Join our team and share in our success as we work toward becoming the next national player in our industry."," Deep experience with RDMS databases  SQL Server  and Data Warehouse  OLAP, Redshift  managing connection-pools, performance tuning and optimizations.   Deep experience with RDMS databases  SQL Server  and Data Warehouse  OLAP, Redshift  managing connection-pools, performance tuning and optimizations.   Deliver on large and complex solution data needs by leading requirements and technical specifications that drive resulting data designs. Participate in strategic and innovative data design, requirements, walkthroughs, and reviews. Design and document system integration/configuration as required. Implement and advance data models and configurations in support of integrating data from source systems and environments to promote Continuous Integration/Continuous Deployment  CI/CD  and CQRS pattern caching. Research, design, and implement next generation analytics and machine learning platforms. Build tools, frameworks, APIs, and dashboards to support telemetry and advanced analytics focusing on ways to improve data security, accessibility, reliability, scalability, efficiency and quality. Lead data governance and guide all data schema/configuration changes.  Deep experience with RDMS databases  SQL Server  and Data Warehouse  OLAP, Redshift  managing connection-pools, performance tuning and optimizations.  ","Deep experience with RDMS databases SQL Server and Data Warehouse OLAP, Redshift managing connection-pools, performance tuning optimizations. Deliver on large complex solution data needs by leading requirements technical specifications that drive resulting designs. Participate in strategic innovative design, requirements, walkthroughs, reviews. Design document system integration/configuration as required. Implement advance models configurations support of integrating from source systems environments to promote Continuous Integration/Continuous Deployment CI/CD CQRS pattern caching. Research, implement next generation analytics machine learning platforms. Build tools, frameworks, APIs, dashboards telemetry advanced focusing ways improve security, accessibility, reliability, scalability, efficiency quality. Lead governance guide all schema/configuration changes.","Deep experience RDMS databases SQL Server Data Warehouse OLAP, Redshift managing connection-pools, performance tuning optimizations. Deliver large complex solution data needs leading requirements technical specifications drive resulting designs. Participate strategic innovative design, requirements, walkthroughs, reviews. Design document system integration/configuration required. Implement advance models configurations support integrating source systems environments promote Continuous Integration/Continuous Deployment CI/CD CQRS pattern caching. Research, implement next generation analytics machine learning platforms. Build tools, frameworks, APIs, dashboards telemetry advanced focusing ways improve security, accessibility, reliability, scalability, efficiency quality. Lead governance guide schema/configuration changes."
353,Data Engineer,Staff Data Engineer / Staff Software Engineer,"Seattle, WA",Seattle,WA,"About Foursquare:
Foursquare is the leading independent location technology platform, powering business solutions and consumer products through a deep understanding of location. Foursquare's business solutions include Pilgrim SDK, Places API, Analytics, Placed powered by Foursquare, and Pinpoint. Together, these products empower brands to analyze trends; measure foot traffic lift; optimize advertising campaigns; and drive deeper engagement via Foursquare's industry-leading developer tools, which have been selected by 150,000 developers including AccuWeather, Apple, Samsung, Microsoft, Snapchat, Tinder, TripAdvisor, Twitter and Uber. Our toolkit also includes our consumer apps Foursquare City Guide and Swarm. Over the past 10 years, we've counted more 13 billion verified signals from people around the world, helping us to keep our dynamic map and models fresh and up-to-date.

About our Engineering Team:
As a member of Foursquare's engineering team, we want you to bring experience building real products from the ground up. We're passionate about tackling tough challenges in the location space and look for others who like to dive deep into code and help solve hard problems. You should be comfortable running with your own ideas and eager to learn new skills on a bleeding edge platform. We use a variety of tools, technologies, and languages to build software (Scala, Python, Thrift, MongoDB, Memcached, JS/jQuery, Kafka, Pants, Hadoop, MR, Spark, Databricks) but experience with equivalent ones will do just fine.

As a senior/staff engineer on the team, you will own critical pieces of our machine learning and analytics platforms. You will build data processing pipelines that process terabytes of data every day, and collaborate with core tech's major teams, product owners across the company and actively build our next-gen products that set us apart in the location intelligence space.

About Core Tech Offline Visitation Team:
Pilgrim SDK, which is our always-on, passive location detection engine. It provides contextual awareness to mobile applications and connected devices to understand where and how users are moving through the real world. Pilgrim Core Visits are generated as a part of The Pilgrim SDK, the team is responsible for generating valuable visitation data that powers downstream products and consumers and internal tools. The team generates all the visits that are used across the entire company, which is the core infrastructure piece of Foursquare's location intelligence and is a high impact team with a big impact on the broader company success.

About the role:
Join us and help bring our feature ideas (and your own!) off the whiteboard and into reality.

As our Core Visits Staff Software Engineer, you will be responsible for the following:


Launch features that enable developers to build rich contextual location experiences in their apps.
Create and execute on technical designs, develop and deliver new Visitation Offline pipelines processing big data at scale.
Establish process and tooling improvements to increase code quality, bring in best practices team can learn from.
Build pipelines in Scala/Spark and/or Py/Spark to run on Databricks.
Mentor and coach junior engineers on the team and demonstrate leadership skills.
Collaborate with product owners and product managers on roadmaps and OKRs.
Work on cross-functional project teams with product managers, designers, Android engineers, and server engineers.

*** The position is available in our Seattle engineering office.

Qualifications:
---------------


Degree in Computer Science, Computer Engineering, or Statistics or commensurate experience
6-8 years of software engineering, Big Data Engineering and/or team lead experience
Proven track record of building and shipping large-scale engineering products
Experience working with large, complex data sets from a variety of sources
Ability to collaborate with a diverse set of engineers, data scientists and product managers
Experience with functional and object-oriented programming, Scala a plus
Experience with Databricks, Spark, AWS and EMR
Effective communication skills (both written and verbal), comfortable presenting to a group
Comfort in a fast-paced startup environment

Foursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and the products we love.

Foursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law."," Degree in Computer Science, Computer Engineering, or Statistics or commensurate experience 6-8 years of software engineering, Big Data Engineering and/or team lead experience Proven track record of building and shipping large-scale engineering products Experience working with large, complex data sets from a variety of sources Ability to collaborate with a diverse set of engineers, data scientists and product managers Experience with functional and object-oriented programming, Scala a plus Experience with Databricks, Spark, AWS and EMR Effective communication skills  both written and verbal , comfortable presenting to a group Comfort in a fast-paced startup environment     ","Degree in Computer Science, Engineering, or Statistics commensurate experience 6-8 years of software engineering, Big Data Engineering and/or team lead Proven track record building and shipping large-scale engineering products Experience working with large, complex data sets from a variety sources Ability to collaborate diverse set engineers, scientists product managers functional object-oriented programming, Scala plus Databricks, Spark, AWS EMR Effective communication skills both written verbal , comfortable presenting group Comfort fast-paced startup environment","Degree Computer Science, Engineering, Statistics commensurate experience 6-8 years software engineering, Big Data Engineering and/or team lead Proven track record building shipping large-scale engineering products Experience working large, complex data sets variety sources Ability collaborate diverse set engineers, scientists product managers functional object-oriented programming, Scala plus Databricks, Spark, AWS EMR Effective communication skills written verbal , comfortable presenting group Comfort fast-paced startup environment"
354,Data Engineer,Senior Data Engineer- Enterprise Data Platform,"Seattle, WA 98101",Seattle,WA,"About the team
Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to create the next-generation tools for intuitive data access?

Zillow Group's Enterprise Applications organization is seeking a savvy Data Engineer to join our team and shape the future of our Enterprise data platform. This data platform serves Analytics and reporting needs of Zillow's Finance, HR and other organizational functions. You will be responsible for building data collection, transformation and processing pipeline automation for Zillowâs Enterprise function data to support Zillowâs rapidly growing and dynamic businesses, and use it to deliver the BI and Insights critical for success of these functions.

Our team enjoys a good challenge and we celebrate our successes together. If you enjoy working in a supportive environment that encourages creativity and promotes ownership and career growth, come and join us.
About the role
The right candidate has experience in building and maintaining data warehouses and BI systems for Enterprise functions and shared services, and is excited at the opportunity to build and optimize our data systems from the ground up.
Key Responsibilities:
Design, implement, and support a platform providing secured access to large datasets.
Collaborate with customers from Finance, HR and other shared service functions understanding their requirements and delivering data solutions they need.
Understand the data sources, develop an ETL strategy with these sources, perform data modeling to meet customersâ data needs.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
Recognize and adopt best practices in reporting and analysis: data integrity, data security, analysis, validation, and documentation.
Monitor, manage and administer the Enterprise Applications Data Warehouse and ensure optimal performance at scale.
Design, implement and manage the access to the datasets based on the Zillowâs data access policies.
Who you are
As a Data Engineer, you should be an expert with data warehousing components (e.g. Data Modeling, ETL and Reporting) and integrations. You should have a deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management of large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. You should have strong analytical Skills.
Qualifications:
BS in CS or similar and 8+ years professional experience, or MS with 5+ years of Professional experience.
Solid Experience in dimensional data modeling, ETL development, and Data Warehousing.
Hands on experience building and supporting Enterprise level Data warehouses by sourcing data from SaaS applications like Workday, Anaplan, Zuora or other ERPs.
Solid experience with data modelling, SQL, writing, debugging and performance tuning.
Experience with any BI reporting tools like Tableau, Domo.
Basic database administration.
System monitoring and alerting, dashboarding experience.
Experience with Snowflake is a plus.
Experience with middleware tools such as Boomi, Workato or Mulesoft.
You know how to work with high volume of rapidly changing data.
Get to know us
Zillow Group houses the largest portfolio of real estate brands on mobile and the web. We are on a mission to rewire the real estate transaction and are building transformational tools and services that make it easier for everyone to find and get into a home they love. We are working to create an on-demand real estate transaction experience for every stage of the home lifecycle - for buyers, sellers, renters and borrowers - and we're well on our way. No matter what job you're in, you will play a critical role in making this vision a reality for millions of people.
At Zillow Group, we're powered by our inclusive work culture, where everyone has the support and resources to do the best work of their careers. Our efforts to streamline the real estate transaction is supported by our passion to empower people and enrich lives around everything home, a deep-rooted culture of innovation, a fundamental commitment to Equity and Belonging, and world-class benefits. But, don't just take our word for it. Read our reviews on Glassdoor and recent recognition from multiple organizations, including: Fortune 100 Best Companies to Work For (#69), Fortune Best Workplaces for Diversity (#38), Fortune Best Workplaces for Parents (#31), Fortune Best Workplaces for Women (#20), Fatherly's Best Workplaces for New Dads (#37), JUST Capital 100 Company (#69), Bloomberg Gender Equality Index constituent.
Zillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know.","   Design, implement, and support a platform providing secured access to large datasets. Collaborate with customers from Finance, HR and other shared service functions understanding their requirements and delivering data solutions they need. Understand the data sources, develop an ETL strategy with these sources, perform data modeling to meet customersâ data needs. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets. Recognize and adopt best practices in reporting and analysis  data integrity, data security, analysis, validation, and documentation. Monitor, manage and administer the Enterprise Applications Data Warehouse and ensure optimal performance at scale. Design, implement and manage the access to the datasets based on the Zillowâs data access policies.  ","Design, implement, and support a platform providing secured access to large datasets. Collaborate with customers from Finance, HR other shared service functions understanding their requirements delivering data solutions they need. Understand the sources, develop an ETL strategy these perform modeling meet customersâ needs. Continually improve ongoing reporting analysis processes, automating or simplifying self-service for Recognize adopt best practices in integrity, security, analysis, validation, documentation. Monitor, manage administer Enterprise Applications Data Warehouse ensure optimal performance at scale. implement datasets based on Zillowâs policies.","Design, implement, support platform providing secured access large datasets. Collaborate customers Finance, HR shared service functions understanding requirements delivering data solutions need. Understand sources, develop ETL strategy perform modeling meet customersâ needs. Continually improve ongoing reporting analysis processes, automating simplifying self-service Recognize adopt best practices integrity, security, analysis, validation, documentation. Monitor, manage administer Enterprise Applications Data Warehouse ensure optimal performance scale. implement datasets based Zillowâs policies."
355,Data Engineer,Big Data Engineer,"Bellevue, WA",Bellevue,WA,"About us

Launched in October 2018, the Likewise app is the fun, social and incredibly useful way for people to discover, curate, and share recommendations on TV shows, movies, books, podcasts, restaurants, travel and more. Best of all, Likewise helps people quickly find recommendations from their friends, family, and other trusted sources.
Imagined and backed by Bill Gatesâ private office, Likewise is a rare early-stage startup that is thinking big, playing to win, and investing to continue its rapid growth trajectory. If you are passionate about what you do, and want to be a core part of creating a household consumer name, then come talk to us about getting in on the fun!
Here's a link to a Geekwire article about us: https://bit.ly/2RuxBlx. And Built In Seattle named us one of Seattleâs 50 Startups to Watch in 2019! https://bit.ly/2VXup3o
Role
With the Likewise app launched, we have a lot of fun and creative work ahead of us in making Likewiseâs AI into the end-all-be-all for determining the best recommendations to consumers across any category â movies, podcasts, books, restaurants, travel, and more! The work you will be doing is the foundation to making Likewise AI real, and it wonât happen without you. Youâll redefine how AI makes recommendations, and in doing so, change peopleâs lives for the better. We expect to grow the team as the company grows, and the right candidate will have the potential to lead that growth.
Objectives
Create a process that handles our disparate internal and external data sources and automatically converts that unstructured data into structured data to be consumed by machine learning and our product, marketing, and executive teams
Build data process pipelines for new and existing data sources
Glean insights and business opportunities from the data, and champion ideas for improvement based on those insights to the product team
Lead the external data sources collection effort, and creatively identify new, relevant data sources that will positively impact our products and users
Work closely with the Data Science team to complete all data needs
Find the handful of outliers in massive data sets and define processes to handle them
Requirements
Qualifications
4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL
Strong Python coding skills
Experience developing and implementing ETL architectures with large, complex data sets
Understanding of database architecture and data lakes
Distributing computing (parallel processing, multi-threading) â Hadoop, MapReduce, Spark
Hands-on experience with web crawling/web scraping required (6+ months)
Experience developing APIs
Experience with Node.js and familiarity with Machine Learning are pluses
Strong quantitative data analysis skills
Beyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech
Curiosity about anomalies in the data and the ability to identify the business opportunities they represent.
Strong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels
Azure experience is a plus
Benefits
Working here
Located in downtown Bellevue, close to restaurants, shopping, parks and transit, we are proud to offer a competitive benefits package including stock options, health care where we pay 100% of employee premiums, 401(k) plan, commuter benefits, flexible paid time off, and a culture thatâs team-based, open, casual and fun. If youâre looking for a rare opportunity to be a part of an innovative, exciting company and become a key member on our team, join us!
We support workplace diversity and do not discriminate on the basis of race, color, religion, gender identity or expression, national origin, age, military service eligibility, veteran status, sexual orientation, marital status, physical or mental disability, or any other protected class."," 4+ years of relevant technical experience, including 2+ years with noSQL databases  MongoDB preferred  as well as experience with SQL Strong Python coding skills Experience developing and implementing ETL architectures with large, complex data sets Understanding of database architecture and data lakes Distributing computing  parallel processing, multi-threading  â Hadoop, MapReduce, Spark Hands-on experience with web crawling/web scraping required  6+ months  Experience developing APIs Experience with Node.js and familiarity with Machine Learning are pluses Strong quantitative data analysis skills Beyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech Curiosity about anomalies in the data and the ability to identify the business opportunities they represent. Strong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels Azure experience is a plus      4+ years of relevant technical experience, including 2+ years with noSQL databases  MongoDB preferred  as well as experience with SQL Strong Python coding skills Experience developing and implementing ETL architectures with large, complex data sets Understanding of database architecture and data lakes Distributing computing  parallel processing, multi-threading  â Hadoop, MapReduce, Spark Hands-on experience with web crawling/web scraping required  6+ months  Experience developing APIs Experience with Node.js and familiarity with Machine Learning are pluses Strong quantitative data analysis skills Beyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech Curiosity about anomalies in the data and the ability to identify the business opportunities they represent. Strong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels Azure experience is a plus ","4+ years of relevant technical experience, including 2+ with noSQL databases MongoDB preferred as well experience SQL Strong Python coding skills Experience developing and implementing ETL architectures large, complex data sets Understanding database architecture lakes Distributing computing parallel processing, multi-threading â Hadoop, MapReduce, Spark Hands-on web crawling/web scraping required 6+ months APIs Node.js familiarity Machine Learning are pluses quantitative analysis Beyond the technical, strong business thinking is required, or interest in consumer apps/consumer tech Curiosity about anomalies ability to identify opportunities they represent. communication excitement around championing your great ideas insights stakeholders at all levels Azure a plus","4+ years relevant technical experience, including 2+ noSQL databases MongoDB preferred well experience SQL Strong Python coding skills Experience developing implementing ETL architectures large, complex data sets Understanding database architecture lakes Distributing computing parallel processing, multi-threading â Hadoop, MapReduce, Spark Hands-on web crawling/web scraping required 6+ months APIs Node.js familiarity Machine Learning pluses quantitative analysis Beyond technical, strong business thinking required, interest consumer apps/consumer tech Curiosity anomalies ability identify opportunities represent. communication excitement around championing great ideas insights stakeholders levels Azure plus"
356,Data Engineer,AWS Data Engineer,"Seattle, WA 98104",Seattle,WA,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
357,Data Engineer,Data Engineer- Python,"Seattle, WA 98104",Seattle,WA,"As a Senior Consultant, you will focus on managing the information supply chain from acquisition to ingestion, storage and the provisioning of data to points of impact by modernizing and enabling new capabilities. Information value is enhanced through enterprise-scale applications that enable visualization, consumption and monetization of both structured and unstructured data. Big data is becoming one of the most important technology trends that has the potential for dramatically changing the way organizations use information to enhance the customer experience and transform their business models.
Work you'll do

Senior Consultants work within an engagement team. Key responsibilities will include:
 Function as integrators between business needs and technology solutions, helping to create technology solutions to meet clientsâ business needs.
 Identifying business requirements, requirements management, functional design, prototyping, process design (including scenario design, flow mapping), testing, training, defining support procedures and supporting implementations.

The Team

Analytics & Cognitive

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.


The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.


Analytics & Cognitive will work with our clients to:
 Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
 Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
 Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements


Qualifications

Required:

 5+ years of experience in core JAVA and SQL
 3+ years of experience in Python& Unix Shell Scripting
 3+ years of experience in building scalable and high performance data pipelines using Apache Hadoop, Map Reduce, Pig & Hive
 Experience with bigdata cross platform compatible file formats like Apache Avro & Apache Parquet
 Experience in Apache Spark is a plus
 1+ years of experience with data lake implementations, core modernizations and data ingestion

 1 or more years of hands on experience designing and implementing data ingestion techniques for real time and batch processes for video, voice, weblog, sensor, machine and social media data into Hadoop ecosystems and HDFS clusters.
 2+ years of experience leading workstreams or small teams
 Willingness for weekly client-based travel, up to 80-100% (Monday â Thursday/Friday)
 Bachelorâs Degree or equivalent professional experience

 Preferred:

AWS Certification, Hadoop Certification or Spark Certification
Experience with Cloud using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
Experience with data integration products like Informatica Power Center Big Data Edition (BDE), IBM BigInsights, Talend etc.
Experience designing and implementing reporting and visualization for unstructured and structured data sets
Experience in designing and implementing scalable, distributed systems leveraging cloud computing technologies like AWS EC2, AWS Elastic Map Reduce and Microsoft Azure
Experience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
Knowledge of data, master data and metadata related standards, processes and technology
Experience working with multi-Terabyte data sets
Experience with Data Integration on traditional and Hadoop environments
Ability to work independently, manage small engagements or parts of large engagements.
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint).
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment.
Eagerness to mentor junior staff.
An advanced degree in the area of specialization is preferred.

How youâll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe thereâs always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.


Benefits

At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitteâs culture

Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.


Corporate citizenship

Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitteâs impact on the world.


Recruiter tips

We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youâre applying to. Check out recruiting tips from Deloitte professionals.

#LI:PTY
#IND:PTY", 5+ years of experience in core JAVA and SQL    ,5+ years of experience in core JAVA and SQL,5+ years experience core JAVA SQL
358,Data Engineer,"Data Engineer, Sales & Marketing","Seattle, WA 98104",Seattle,WA,"Avalara is a fast-growing company providing a suite of sales tax SaaS solutions for thousands of businesses worldwide. Our Sales & Marketing team is seeking a skilled and experienced data engineer to build and integrate scalable data pipelines and structures, enabling advanced analytics to accelerate growth and efficiency. Youâll take an active role in empowering marketers and leadership with information-rich data sources, using cutting-edge tools to consolidate data from across the organization and within our data warehouse.

Responsibilities:
Translate requirements from various internal business teams to deliveroptimal data structures used to drive insightful, actionable reports and dashboards.
Plan and develop complex ETL processes extracting data from source systems and loading cleaned, transformed, and conformed data into our enterprise data warehouse
Ensure data integrity by validating against existing reports and source data
Work cross functionally with our Data Services and Data Science teams to architect and implement optimized data pipelines
Create dimensional models that meet both immediate analytical and reporting needs and also conform to long-term data strategy
Scope projects and provide estimates of work to be performed and timelines
Develop ETL specifications, source to target mappings, and other documentation required for ETL development and maintenance

Required:
5+ years experience in a data or software engineering role focused on data-heavy applications
Strong experience with ETL tools, preferably Talend, Wherescape and SSIS
Track record of optimizing SQL scripts for performance in a data warehouse environment
Proficiency in dimensional modeling that enables analysts to seamlessly generate insights from data structures
Strong communication skills, specifically the ability to translate non-technical stakeholder requirements into data architecture that meets and/or exceeds current needs and long-term analytics goals
Experience working cross-functionally to identify ideal source data, share tools, plan data strategy, and collaborate on ETL pipelines that can serve the needs of multiple teams
Experience in optimizing scripts and dimensional modeling on big data platforms (Snowflake preferred, or Hadoop or Spark)
Experience with a functional/object-oriented scripting language like Python, Java, Scala, etc. for automating jobs
Ability to self-direct to seek information and solve problems
Rigorous attention to detail

Preferred:
Passion for integrating data and the technology behind it (especially for Big Data technologies)
Familiarity with CRM systems, specifically Salesforce
Knowledge of best practices for extracting data from systems such as Adobe Analytics and Eloqua
Experience with AWS cloud services such as EC2
Experience working in visual analysis tools such as Tableau
About Avalara

Avalara helps businesses of all sizes achieve compliance with transaction taxes, including sales and use, VAT, excise, communications, and other tax types. The company delivers comprehensive, automated, cloud-based solutions designed to be fast, accurate, and easy to use. The Avalara Compliance CloudÂ® platform helps customers manage complicated and burdensome tax compliance obligations imposed by state, local, and other taxing authorities throughout the world.

Avalara offers more than 600 pre-built connectors into leading accounting, ERP, ecommerce and other business applications, making the integration of tax and compliance solutions easy for customers. Each year, the company processes billions of indirect tax transactions for customers and users, files more than a million tax returns, and manages millions of tax exemption certificates and other compliance documents.

Headquartered in Seattle, Avalara has offices across the U.S. and overseas in the U.K., Belgium, Brazil, and India. More information at www.avalara.com

Avalara is an Equal Opportunity Employer. All qualified candidates will receive consideration for employment without regard to race, color, creed, religion, age, gender, national orientation, disability, sexual orientation, US Veteran status, or any other factor protected by law.

#LI-POST","   Translate requirements from various internal business teams to deliveroptimal data structures used to drive insightful, actionable reports and dashboards. Plan and develop complex ETL processes extracting data from source systems and loading cleaned, transformed, and conformed data into our enterprise data warehouse Ensure data integrity by validating against existing reports and source data Work cross functionally with our Data Services and Data Science teams to architect and implement optimized data pipelines Create dimensional models that meet both immediate analytical and reporting needs and also conform to long-term data strategy Scope projects and provide estimates of work to be performed and timelines Develop ETL specifications, source to target mappings, and other documentation required for ETL development and maintenance  ","Translate requirements from various internal business teams to deliveroptimal data structures used drive insightful, actionable reports and dashboards. Plan develop complex ETL processes extracting source systems loading cleaned, transformed, conformed into our enterprise warehouse Ensure integrity by validating against existing Work cross functionally with Data Services Science architect implement optimized pipelines Create dimensional models that meet both immediate analytical reporting needs also conform long-term strategy Scope projects provide estimates of work be performed timelines Develop specifications, target mappings, other documentation required for development maintenance","Translate requirements various internal business teams deliveroptimal data structures used drive insightful, actionable reports dashboards. Plan develop complex ETL processes extracting source systems loading cleaned, transformed, conformed enterprise warehouse Ensure integrity validating existing Work cross functionally Data Services Science architect implement optimized pipelines Create dimensional models meet immediate analytical reporting needs also conform long-term strategy Scope projects provide estimates work performed timelines Develop specifications, target mappings, documentation required development maintenance"
359,Data Engineer,Sr. Data Engineer,"Seattle, WA",Seattle,WA,"Beyondsoft Consulting, Inc., is a leading, technical solutions and consulting partner. We combine emerging technologies and proven methodologies to tailor elegant solutions that solve complex challenges and empower our customers to accelerate their business goals. Our services include end-to-end support for cloud, digital, data analytics, multi-language translation, and testing.

Our client is growing their Data Engineering team within a demanding and well recognized enterprise and information technology company. This role will be the core solution of the Strategic Analytics organization, ensuring both the reliability and applicability of the teamâs data products to the organization. This individual will have extensive experience with ETL design, coding, and testing patterns as well as engineering software platforms and large-scale data infrastructures. The Data Engineers will have the capability to architect highly scalable end-to-end pipeline using different open source tools, including building and operationalizing high-performance algorithms. Proven experience with technologies to solve big data problems with expert knowledge in programming languages like Java, Python, Linux, PHP, Hive, Impala, and Spark.
Responsibilities
Responsibilities:

Translate complex functional and technical requirements into detailed design
Hadoop technical development and implementation
Loading from disparate data sets by leveraging various big data technology e.g. Kafka
Pre-processing using Hive, Impala, Spark, and Pig
Design and implement data modeling
Maintain security and data privacy in an environment secured using Kerberos and LDAP
High-speed querying using in-memory technologies such as Spark
Following and contributing best engineering practice for source control, release management, deployment etc
Production support, job scheduling/monitoring, ETL data quality, data freshness reporting
Qualifications
Qualifications:

5-8 years of Python or Java/J2EE development experience
3+ years of demonstrated technical proficiency with Hadoop and big data projects
5-8 years of demonstrated experience and success in data modeling
Fluent in writing shell scripts [bash, korn]
Writing high-performance, reliable and maintainable code
Ability to write MapReduce jobs
Knowledge of database structures, theories, principles, and practices
Understand how to develop code in an environment secured using a local KDC and OpenLDAP
Familiarity with and implementation knowledge of loading data using Sqoop
Knowledge and ability to implement workflow/schedulers within Oozie
Experience working with AWS components [EC2, S3, SNS, SQS]
Analytical and problem-solving skills, applied to Big Data domain
Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark
Aptitude in multi-threading and concurrency concepts
M.S. in Computer Science or Engineering"," 5-8 years of Python or Java/J2EE development experience 3+ years of demonstrated technical proficiency with Hadoop and big data projects 5-8 years of demonstrated experience and success in data modeling Fluent in writing shell scripts [bash, korn] Writing high-performance, reliable and maintainable code Ability to write MapReduce jobs Knowledge of database structures, theories, principles, and practices Understand how to develop code in an environment secured using a local KDC and OpenLDAP Familiarity with and implementation knowledge of loading data using Sqoop Knowledge and ability to implement workflow/schedulers within Oozie Experience working with AWS components [EC2, S3, SNS, SQS] Analytical and problem-solving skills, applied to Big Data domain Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark Aptitude in multi-threading and concurrency concepts M.S. in Computer Science or Engineering   Translate complex functional and technical requirements into detailed design Hadoop technical development and implementation Loading from disparate data sets by leveraging various big data technology e.g. Kafka Pre-processing using Hive, Impala, Spark, and Pig Design and implement data modeling Maintain security and data privacy in an environment secured using Kerberos and LDAP High-speed querying using in-memory technologies such as Spark Following and contributing best engineering practice for source control, release management, deployment etc Production support, job scheduling/monitoring, ETL data quality, data freshness reporting  ","5-8 years of Python or Java/J2EE development experience 3+ demonstrated technical proficiency with Hadoop and big data projects success in modeling Fluent writing shell scripts [bash, korn] Writing high-performance, reliable maintainable code Ability to write MapReduce jobs Knowledge database structures, theories, principles, practices Understand how develop an environment secured using a local KDC OpenLDAP Familiarity implementation knowledge loading Sqoop ability implement workflow/schedulers within Oozie Experience working AWS components [EC2, S3, SNS, SQS] Analytical problem-solving skills, applied Big Data domain Proven understanding hands on Hadoop, Hive, Pig, Impala, Spark Aptitude multi-threading concurrency concepts M.S. Computer Science Engineering Translate complex functional requirements into detailed design Loading from disparate sets by leveraging various technology e.g. Kafka Pre-processing Spark, Pig Design Maintain security privacy Kerberos LDAP High-speed querying in-memory technologies such as Following contributing best engineering practice for source control, release management, deployment etc Production support, job scheduling/monitoring, ETL quality, freshness reporting","5-8 years Python Java/J2EE development experience 3+ demonstrated technical proficiency Hadoop big data projects success modeling Fluent writing shell scripts [bash, korn] Writing high-performance, reliable maintainable code Ability write MapReduce jobs Knowledge database structures, theories, principles, practices Understand develop environment secured using local KDC OpenLDAP Familiarity implementation knowledge loading Sqoop ability implement workflow/schedulers within Oozie Experience working AWS components [EC2, S3, SNS, SQS] Analytical problem-solving skills, applied Big Data domain Proven understanding hands Hadoop, Hive, Pig, Impala, Spark Aptitude multi-threading concurrency concepts M.S. Computer Science Engineering Translate complex functional requirements detailed design Loading disparate sets leveraging various technology e.g. Kafka Pre-processing Spark, Pig Design Maintain security privacy Kerberos LDAP High-speed querying in-memory technologies Following contributing best engineering practice source control, release management, deployment etc Production support, job scheduling/monitoring, ETL quality, freshness reporting"
360,Data Engineer,Cloud Data Engineer,"Seattle, WA",Seattle,WA,"Businessolver delivers market-changing benefits administration technology supported by an intrinsic and unwavering responsiveness to client needs. Our clients trust Businessolver to take care of them and their employees with a configurable and secure SaaS platform and a culture of service, all aimed at total and measurable success and our clients' complete delight.

We work with some of the most recognizable brands in the U.S. We look to our rock-star employees to help these clients maximize the investment in their benefits program, minimize their exposure to risk, engage their employees with our easy-to-use solution and full suite of communication tools, and empower their employees to use their benefits wisely.

At Businessolver you will have opportunities for individual development through our common language: Trust through transparency. Assume positive intent. Be real. Live a growth attitude. Embrace the reverse golden rule.

The Cloud Data Engineer (CDE) will be responsible for architecting, developing, implementing, and operating stable, scalable, low cost solutions to source data from production systems into the data lake (AWS) and data warehouse (Redshift) and into end-user facing applications (AWS Quicksight). The ideal candidate should be able to work with Infrastructure, Data Analysts, and Machine Learning Engineers in a fast-paced environment, understanding the business requirements, and implementing ETL, machine learning and cloud solutions. This role will serve on the Cloud Data Engineering team.

Qualifications:

Degree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development
Proficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc.
Experience with developing on an MPP database Redshift/Teradata/Snowflake
Proficient handling large data sets using SQL and databases in a business and engineering environment
Experience with operations in a Public Cloud Environment (AWS/Azure/GCP)
Experience with ETL and Data Warehouse/Lake processes
Excellent verbal and written communication skills
Strong troubleshooting and problem-solving skills
Thrive in a fast-paced, innovative environment

Preferred Qualifications:

Oracle, Postgres, EMR, Redshift, Linux experience
Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysis
Experience with Agile Methodologies
Experience with complex/large data sets (Big Data)
Experience operating a Data Lake
Experience with Cloud Architecture/Engineering

The Businessolver Wayâ¦

Our team has spent nearly two decades crafting a culture that challenges each employee to perform at the top of their game â and have fun doing it! If you desire to use your skills and experience in an environment where you can make a difference, we want to hear from you! Businessolver employees experience a vibrant work culture with extensive workplace perks including:


Competitive pay, great benefits, and vacation time. We are an equal opportunity employer with competitive benefits including medical, dental, life insurance, disability, 401(k) with company match, among others.
Smart Casual Dress. No need to suit up, but we also have on-site dry cleaning services for those that prefer to dress-up!
Weekly catered meals. Breakfast every other Mondays, lunch Wednesdays, and afternoon appetizers on Fridays encourage collaboration across our teams.
Fully-stocked kitchens. We know it takes fuel to perform, so we provide a kitchen stocked with healthy cereals, fruit, snacks, and beverages to keep you at the top of your game.
Fitnessolver. If you need a boost, visit our on-site fitness facility to clear your head.
Massages. With a ""work hard/play hard"" atmosphere we all need a little stress relief at times.
Charity and community involvement. Participate in a variety of ways to support those around us.
Learning & Development. Continue to learn about the industry through our online and instructor-led classes.
Recognition. Want some swag? Earn tons of it by helping out your co-workers through our employee recognition program.
Culture. Want a culture most dream of? Most companies talk about it, we live it. Come find out for yourself!

Interested? Great, we look forward to reading your application - make sure it includes:


A cover letter that highlights why you think you'd be great for the gig, focusing on how your past work experience has prepared you for this kind of position â or why you think you can rock the job even though you don't have past work experience that's perfectly aligned.
Your resume.

You will receive an auto-reply confirming that we've received your application, and you will hear from us again after we've reviewed your application and decided whether or not to move you forward in our recruiting process.

If you do decide to apply, please know that every complete application will be carefully reviewed. Seriously! We know it is a time commitment to prepare an application. We will respect that effort by thoughtfully reviewing every single complete application and we are truly grateful for your interest.

Thanks for your interest in Businessolver!

Check us out on Twitter ( https://twitter.com/businessolver ), Facebook ( https://www.facebook.com/bsolver ) and LinkedIn ( https://www.linkedin.com/company/232793?trk=tyah&trkInfo=tarId%3A1415406210925%2Ctas%3Abusinessolver%2Cidx%3A2-1-4 ) for a look at our vibrant culture."," Degree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development Proficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc. Experience with developing on an MPP database Redshift/Teradata/Snowflake Proficient handling large data sets using SQL and databases in a business and engineering environment Experience with operations in a Public Cloud Environment  AWS/Azure/GCP  Experience with ETL and Data Warehouse/Lake processes Excellent verbal and written communication skills Strong troubleshooting and problem-solving skills Thrive in a fast-paced, innovative environment     ","Degree in Computer Engineering/Science or related field, with 4+ years of professional experience database/data lake development Proficient processing data on relational databases like Oracle/SQL Server/MySQL/etc. Experience developing an MPP database Redshift/Teradata/Snowflake handling large sets using SQL and a business engineering environment operations Public Cloud Environment AWS/Azure/GCP ETL Data Warehouse/Lake processes Excellent verbal written communication skills Strong troubleshooting problem-solving Thrive fast-paced, innovative","Degree Computer Engineering/Science related field, 4+ years professional experience database/data lake development Proficient processing data relational databases like Oracle/SQL Server/MySQL/etc. Experience developing MPP database Redshift/Teradata/Snowflake handling large sets using SQL business engineering environment operations Public Cloud Environment AWS/Azure/GCP ETL Data Warehouse/Lake processes Excellent verbal written communication skills Strong troubleshooting problem-solving Thrive fast-paced, innovative"
361,Data Engineer,Senior Big Data Engineer (Data Platform),"Seattle, WA 98101",Seattle,WA,"Coupang is one of the largest and fastest growing e-commerce platforms on the planet. We are on a mission to revolutionize everyday lives for our customers, employees and partners. We solve problems no one has solved before to create a world where people ask, ""How did we ever live without Coupang?"" Coupangâ¯is a global company with offices in Beijing, Los Angeles, Seattle, Seoul, Shanghai, and Silicon Valley.

Job Overview:
As a Big Data Engineer of our data platform team, you willâ¯design, develop, test and implement the data infrastructure that supports company-wide Big Data Processing teams. This role will involve working closely with the bigdata user community across the company including Data Scientists, Engineers & Analysts who use our bigdata platform to do interesting and impactful analysis. You will have significant responsibility and influence in shaping the future of big data platform engineering at Coupang.

Qualifications:

BS or advanced degree in Computer Science/Engineering, information systems or related technical field.
Prior experience supporting platforms built using open source technologies like Jupyter, Hadoop, Hive, Presto, Spark etc.
Big Data tech - Hadoop, Hive, Presto, Yarn, HDFS, Spark, Tez etc. Significant experience with other ETL tech (Informatica, SSIS, etc) is very valuable, but expect to work in a ""Big Data"" environment. Experience in Hadoop cluster admin is preferred.
Python for scripting and automation and basic SQL experience is required.
Prior experience in either AWS or Azure or Cloudera technologies, and any MPP/Cloud data warehouse solutions.
Solid understanding of JRE (Java Runtime Environment) and JVM langauges (Java, Scala)
You have the desire and aptitude to learn how the pieces of big data platform work together
Curious, Determined, Talented and Inspired to solve user issues.
Sharp communicator who can explain complex data problems in clear and concise language.
Love freedom and hate being micromanaged. Given context, you're capable of self-direction. And be comfortable outside of your comfort zone.

"," BS or advanced degree in Computer Science/Engineering, information systems or related technical field. Prior experience supporting platforms built using open source technologies like Jupyter, Hadoop, Hive, Presto, Spark etc. Big Data tech - Hadoop, Hive, Presto, Yarn, HDFS, Spark, Tez etc. Significant experience with other ETL tech  Informatica, SSIS, etc  is very valuable, but expect to work in a ""Big Data"" environment. Experience in Hadoop cluster admin is preferred. Python for scripting and automation and basic SQL experience is required. Prior experience in either AWS or Azure or Cloudera technologies, and any MPP/Cloud data warehouse solutions. Solid understanding of JRE  Java Runtime Environment  and JVM langauges  Java, Scala  You have the desire and aptitude to learn how the pieces of big data platform work together Curious, Determined, Talented and Inspired to solve user issues. Sharp communicator who can explain complex data problems in clear and concise language. Love freedom and hate being micromanaged. Given context, you're capable of self-direction. And be comfortable outside of your comfort zone.     ","BS or advanced degree in Computer Science/Engineering, information systems related technical field. Prior experience supporting platforms built using open source technologies like Jupyter, Hadoop, Hive, Presto, Spark etc. Big Data tech - Yarn, HDFS, Spark, Tez Significant with other ETL Informatica, SSIS, etc is very valuable, but expect to work a ""Big Data"" environment. Experience Hadoop cluster admin preferred. Python for scripting and automation basic SQL required. either AWS Azure Cloudera technologies, any MPP/Cloud data warehouse solutions. Solid understanding of JRE Java Runtime Environment JVM langauges Java, Scala You have the desire aptitude learn how pieces big platform together Curious, Determined, Talented Inspired solve user issues. Sharp communicator who can explain complex problems clear concise language. Love freedom hate being micromanaged. Given context, you're capable self-direction. And be comfortable outside your comfort zone.","BS advanced degree Computer Science/Engineering, information systems related technical field. Prior experience supporting platforms built using open source technologies like Jupyter, Hadoop, Hive, Presto, Spark etc. Big Data tech - Yarn, HDFS, Spark, Tez Significant ETL Informatica, SSIS, etc valuable, expect work ""Big Data"" environment. Experience Hadoop cluster admin preferred. Python scripting automation basic SQL required. either AWS Azure Cloudera technologies, MPP/Cloud data warehouse solutions. Solid understanding JRE Java Runtime Environment JVM langauges Java, Scala You desire aptitude learn pieces big platform together Curious, Determined, Talented Inspired solve user issues. Sharp communicator explain complex problems clear concise language. Love freedom hate micromanaged. Given context, capable self-direction. And comfortable outside comfort zone."
362,Data Engineer,Senior Data Engineer,"Seattle, WA 98101",Seattle,WA,"Coupang is one of the largest and fastest growing e-commerce platforms on the planet. We are on a mission to revolutionize everyday lives for our customers, employees and partners. We solve problems no one has solved before to create a world where people ask, ""How did we ever live without Coupang?"" Coupangâ¯is a global company with offices in Beijing, Los Angeles, Seattle, Seoul, Shanghai, and Silicon Valley.

Job Overview:
As our Senior Software (data)Engineer for data platform team, you will be responsible forâ¯design, develop and implement the data infrastructure that elevates data-driven decision- making for the data platform, including not limited to new business initiatives, cost modelling and pricing.

Key Responsibilities:

Architect and drive the build out of Coupang's next generation platform.
Build durable code, with the ability to scale with very large data volumes.
Engage in system architecture decisions.
Apply knowledge and implement big data technologies such as Hadoop, Hive, HBase, Spark and enhance AWS technologies to support Coupang.com's big data requirements in support of its online e-commerce system.
Foster, mentor, and enforce industry best practices in data architecture design, design patterns and coding standard.

Qualifications:

BS or advanced degree in Computer Science, Computer Science/ Engineering, or Electrical Engineering, technology, information systems or related technical field.
5 years of software industry experience or related
4+ years of experience in data integration applications, data processing applications, data warehouse, business intelligence, SQL
3 years of experience in Hadoop, Map reduce, Hive, HBase, Pig or other big data frameworks
3 years of experience either AWS or Azure or Cloudera technologies- Spark, Redshift, EMR, HDInsight or equivalent data platform technologies.
Experienced in architecting, building and maintaining large -scale data infrastructures
Possesses full life cycle knowledge of big data technologies including map reduce, hive and HBase
Experienced with python scripting and other platform agnostic language including Java.
Experienced in data structures, algorithm design and troubleshoot using Java/Python.

Coupang is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex or gender (including pregnancy, gender identity, gender expression, sexual orientation, transgender status), national origin, age, disability, medical condition, HIV/AIDS or Hepatitis C status, marital status, military or veteran status, use of a trained dog guide or service animal, political activities, affiliations, citizenship, or any other characteristic or class protected by the laws or regulations in the locations where we operate. If you need assistance and/or a reasonable accommodation in the application or recruiting process due to a disability, please contact us at usrecruiting@coupang.com."," BS or advanced degree in Computer Science, Computer Science/ Engineering, or Electrical Engineering, technology, information systems or related technical field. 5 years of software industry experience or related 4+ years of experience in data integration applications, data processing applications, data warehouse, business intelligence, SQL 3 years of experience in Hadoop, Map reduce, Hive, HBase, Pig or other big data frameworks 3 years of experience either AWS or Azure or Cloudera technologies- Spark, Redshift, EMR, HDInsight or equivalent data platform technologies. Experienced in architecting, building and maintaining large -scale data infrastructures Possesses full life cycle knowledge of big data technologies including map reduce, hive and HBase Experienced with python scripting and other platform agnostic language including Java. Experienced in data structures, algorithm design and troubleshoot using Java/Python.    Architect and drive the build out of Coupang's next generation platform. Build durable code, with the ability to scale with very large data volumes. Engage in system architecture decisions. Apply knowledge and implement big data technologies such as Hadoop, Hive, HBase, Spark and enhance AWS technologies to support Coupang.com's big data requirements in support of its online e-commerce system. Foster, mentor, and enforce industry best practices in data architecture design, design patterns and coding standard.   ","BS or advanced degree in Computer Science, Science/ Engineering, Electrical technology, information systems related technical field. 5 years of software industry experience 4+ data integration applications, processing warehouse, business intelligence, SQL 3 Hadoop, Map reduce, Hive, HBase, Pig other big frameworks either AWS Azure Cloudera technologies- Spark, Redshift, EMR, HDInsight equivalent platform technologies. Experienced architecting, building and maintaining large -scale infrastructures Possesses full life cycle knowledge technologies including map hive HBase with python scripting agnostic language Java. structures, algorithm design troubleshoot using Java/Python. Architect drive the build out Coupang's next generation platform. Build durable code, ability to scale very volumes. Engage system architecture decisions. Apply implement such as Spark enhance support Coupang.com's requirements its online e-commerce system. Foster, mentor, enforce best practices design, patterns coding standard.","BS advanced degree Computer Science, Science/ Engineering, Electrical technology, information systems related technical field. 5 years software industry experience 4+ data integration applications, processing warehouse, business intelligence, SQL 3 Hadoop, Map reduce, Hive, HBase, Pig big frameworks either AWS Azure Cloudera technologies- Spark, Redshift, EMR, HDInsight equivalent platform technologies. Experienced architecting, building maintaining large -scale infrastructures Possesses full life cycle knowledge technologies including map hive HBase python scripting agnostic language Java. structures, algorithm design troubleshoot using Java/Python. Architect drive build Coupang's next generation platform. Build durable code, ability scale volumes. Engage system architecture decisions. Apply implement Spark enhance support Coupang.com's requirements online e-commerce system. Foster, mentor, enforce best practices design, patterns coding standard."
363,Data Engineer,Senior Data Engineer,"Tacoma, WA",Tacoma,WA,"Description of the Role:
Infoblox is looking for a Senior Data Engineer to augment our growing Cyber Security Software Development Team. This growing team supports the Infoblox mission to thwart cybersecurity threats in our customerâs networks. This is an opportunity to work closely with data scientists and threat analysts to curate the data that makes this mission possible.

Description of an Ideal Candidate:
The ideal candidate is a savvy software engineer with experience in data engineering and a solid background in Spark and Python. Preferably you know that countMinSketch is not a childrenâs game. You are comfortable wearing several hats in a small organization with a wide range of responsibilities and have worked in a cloud environment, such as Amazon EMR. You know that Big Data is both a blessing and a curse; without good data engineering it loses its potential. You are passionate about the nexus between data and computer science-driven to figure out how best to represent and summarize data in a way that informs good decisions and drives new products. When someone says, âmy Spark job failedâ, your first question is âwhatâs the skew?â. Come join our growing Cyber Threat Intelligence team and help us build world-class solutions!

Responsibilities:
Curate very large-scale data from a multitude of sources into appropriate sets for research and development for the data science, threat analysts, and developers across the company
Design, test, and implement storage solutions for various consumers of the data
Design and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methods
Leverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analytics
Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines
Collaborate on design, implementation, and deployment of applications with the rest of software engineering
Support data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage data
Build and maintain tools for automation, deployment, monitoring, and operations
Create test plans, test cases, and run tests with automated tools

Requirements:
5+ years of experience with Python3, and 2+ years experience with Spark. Scala experience is helpful
5+ years of experience in data engineering, data science, and related data-centric fields using large-scale data environments
3+ years of experience in using SQL and working with modern relational databases, including MySQL or PostgreSQL
3+ years of experience with developing ETL pipelines and data manipulation scripts
Proficient in Object Oriented Design and S.O.L.I.D principles.
Strong emphasis on unit testing and code quality
Proficient with AWS products (EMR S3, Lambda, VPC, EC2, API Gateway, etc)

Preferred Experience:
Very strong Python and PySpark experience
Very strong back end development experience
Strong experience with cloud deployments and CI/CD
Experience with virtualization, containers, and orchestration (Docker, Kubernetes, XEN)
Experience with NoSQL Non-Relational databases (AWS DynamoDB)

Education:
MS or BS in Computer Science or a related field, or equivalent work experience required

About Infoblox:
Itâs an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologiesâand having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox teamâour future looks bright, and so will yours. To check out what itâs like to be a Bloxer click here.","   Curate very large-scale data from a multitude of sources into appropriate sets for research and development for the data science, threat analysts, and developers across the company Design, test, and implement storage solutions for various consumers of the data Design and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methods Leverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analytics Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines Collaborate on design, implementation, and deployment of applications with the rest of software engineering Support data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage data Build and maintain tools for automation, deployment, monitoring, and operations Create test plans, test cases, and run tests with automated tools  MS or BS in Computer Science or a related field, or equivalent work experience required  5+ years of experience with Python3, and 2+ years experience with Spark. Scala experience is helpful 5+ years of experience in data engineering, data science, and related data-centric fields using large-scale data environments 3+ years of experience in using SQL and working with modern relational databases, including MySQL or PostgreSQL 3+ years of experience with developing ETL pipelines and data manipulation scripts Proficient in Object Oriented Design and S.O.L.I.D principles. Strong emphasis on unit testing and code quality Proficient with AWS products  EMR S3, Lambda, VPC, EC2, API Gateway, etc ","Curate very large-scale data from a multitude of sources into appropriate sets for research and development the science, threat analysts, developers across company Design, test, implement storage solutions various consumers Design mechanisms to monitor over time changes using summarization, monitoring, statistical methods Leverage computer science algorithms constructs, including probabilistic structures, distill large insight enable future analytics Convert prototypes production engineering through disciplined software practices, Spark optimizations, modern deployment pipelines Collaborate on design, implementation, applications with rest Support scientists analysts in building, debugging deploying that best leverage Build maintain tools automation, deployment, operations Create test plans, cases, run tests automated MS or BS Computer Science related field, equivalent work experience required 5+ years Python3, 2+ Spark. Scala is helpful engineering, data-centric fields environments 3+ SQL working relational databases, MySQL PostgreSQL developing ETL manipulation scripts Proficient Object Oriented S.O.L.I.D principles. Strong emphasis unit testing code quality AWS products EMR S3, Lambda, VPC, EC2, API Gateway, etc","Curate large-scale data multitude sources appropriate sets research development science, threat analysts, developers across company Design, test, implement storage solutions various consumers Design mechanisms monitor time changes using summarization, monitoring, statistical methods Leverage computer science algorithms constructs, including probabilistic structures, distill large insight enable future analytics Convert prototypes production engineering disciplined software practices, Spark optimizations, modern deployment pipelines Collaborate design, implementation, applications rest Support scientists analysts building, debugging deploying best leverage Build maintain tools automation, deployment, operations Create test plans, cases, run tests automated MS BS Computer Science related field, equivalent work experience required 5+ years Python3, 2+ Spark. Scala helpful engineering, data-centric fields environments 3+ SQL working relational databases, MySQL PostgreSQL developing ETL manipulation scripts Proficient Object Oriented S.O.L.I.D principles. Strong emphasis unit testing code quality AWS products EMR S3, Lambda, VPC, EC2, API Gateway, etc"
364,Data Engineer,Data Engineer,"Kent, WA",Kent,WA,"Description:
As part of a small, passionate and accomplished team of experts, you will work with stakeholders and technical product managers to create a world class decision support system. To successfully accomplish this task, you will design and implement data pipelines from scores of source systems, create flexible and powerful data models and pathways to allow reliable and timely information to be securely delivered downstream to systems and people. This position requires a commitment to quality and attention to detail that will directly impact the history of space exploration and will require your dedicated commitment and detailed attention towards safe and repeatable spaceflight.
Responsibilities:
Collaborate with departments and technical product managers to collect, transform and aggregate information that leads to business insights
Build and maintain tools, data pipelines, analytics, reports to highlight technical performance metrics and other key information identified by programs and functional leadership
Work with application developers to collect data from custom applications
Establish processes and tools for monitoring and improving performance and effectivity of new and existing data integrations and pipelines
Perform quality assurance and code reviews to ensure both functional and non-functional requirements are being met
Qualifications:
5+ years of data engineering, ETL and/or data warehouse development
Masterâs Degree in Computer Science (or similar area of study)
Technical expertise and experience both SQL and NOSQL databases
Advanced understanding of a wide array of data models including relational, dimensional, document-based, object oriented, object-relational, and graphical
Advanced experience in database interrogation of SQL and NOSQL databases
Experience implementing High Availability systems requirement
Experience with web based APIs (e.g. REST, SOAP)
Experience with AWS Stack (RDS, Kinesis, Lambda, Redshift, SQS, etc)
Proficiency in scripting languages (e.g. Python, Bash)
Strong analytic skill set and a high degree of proficiency in data mining
Excellent written communication and presentation skills
Must be a U.S. citizen or national, U.S. permanent resident (current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum.
Desired:
Experience with and knowledge of project management principles and practices
Experience in manufacturing processes such as Integrated Supply Chain
Experience with OLAP Cubes or similar BI constructs
Experience with Kafka, Spark and other big data pipeline technologies
Experience with IoT / Smart Factory data collection and aggregation
Blue Origin offers a phenomenal work environment and awesome culture with competitive compensation, benefits, 401K, and relocation.


Blue Origin is an equal opportunity employer . In addition to EEO being the law, it is a policy that is fully consistent with Blue's principles. All qualified applicants will receive consideration for employment without regard to status as a protected veteran or a qualified individual with a disability, or other protected status such as race, religion, color, national origin, sex, sexual orientation, gender identity, genetic information, pregnancy or age. Blue Origin prohibits any form of workplace harassment.","5+ years of data engineering, ETL and/or data warehouse development Masterâs Degree in Computer Science  or similar area of study  Technical expertise and experience both SQL and NOSQL databases Advanced understanding of a wide array of data models including relational, dimensional, document-based, object oriented, object-relational, and graphical Advanced experience in database interrogation of SQL and NOSQL databases Experience implementing High Availability systems requirement Experience with web based APIs  e.g. REST, SOAP  Experience with AWS Stack  RDS, Kinesis, Lambda, Redshift, SQS, etc  Proficiency in scripting languages  e.g. Python, Bash  Strong analytic skill set and a high degree of proficiency in data mining Excellent written communication and presentation skills Must be a U.S. citizen or national, U.S. permanent resident  current Green Card holder , or lawfully admitted into the U.S. as a refugee or granted asylum.   Collaborate with departments and technical product managers to collect, transform and aggregate information that leads to business insights Build and maintain tools, data pipelines, analytics, reports to highlight technical performance metrics and other key information identified by programs and functional leadership Work with application developers to collect data from custom applications Establish processes and tools for monitoring and improving performance and effectivity of new and existing data integrations and pipelines Perform quality assurance and code reviews to ensure both functional and non-functional requirements are being met   ","5+ years of data engineering, ETL and/or warehouse development Masterâs Degree in Computer Science or similar area study Technical expertise and experience both SQL NOSQL databases Advanced understanding a wide array models including relational, dimensional, document-based, object oriented, object-relational, graphical database interrogation Experience implementing High Availability systems requirement with web based APIs e.g. REST, SOAP AWS Stack RDS, Kinesis, Lambda, Redshift, SQS, etc Proficiency scripting languages Python, Bash Strong analytic skill set high degree proficiency mining Excellent written communication presentation skills Must be U.S. citizen national, permanent resident current Green Card holder , lawfully admitted into the as refugee granted asylum. Collaborate departments technical product managers to collect, transform aggregate information that leads business insights Build maintain tools, pipelines, analytics, reports highlight performance metrics other key identified by programs functional leadership Work application developers collect from custom applications Establish processes tools for monitoring improving effectivity new existing integrations pipelines Perform quality assurance code reviews ensure non-functional requirements are being met","5+ years data engineering, ETL and/or warehouse development Masterâs Degree Computer Science similar area study Technical expertise experience SQL NOSQL databases Advanced understanding wide array models including relational, dimensional, document-based, object oriented, object-relational, graphical database interrogation Experience implementing High Availability systems requirement web based APIs e.g. REST, SOAP AWS Stack RDS, Kinesis, Lambda, Redshift, SQS, etc Proficiency scripting languages Python, Bash Strong analytic skill set high degree proficiency mining Excellent written communication presentation skills Must U.S. citizen national, permanent resident current Green Card holder , lawfully admitted refugee granted asylum. Collaborate departments technical product managers collect, transform aggregate information leads business insights Build maintain tools, pipelines, analytics, reports highlight performance metrics key identified programs functional leadership Work application developers collect custom applications Establish processes tools monitoring improving effectivity new existing integrations pipelines Perform quality assurance code reviews ensure non-functional requirements met"
365,Data Engineer,Data Engineer,"Seattle, WA",Seattle,WA,"Description:
Campfire data engineers balance between strategy and execution to deliver best-in-class client service. We don't hire 'report monkeys' - we're looking for bright and curious minds who love to explore data, dig for insights, and mine opportunities for the clients we serve. Our Data Engineers are comfortable operating without oversight to solve problems and deliver value. Client-facing consulting experience is strongly preferred.

Requirements:
Responsibilities


Develop data processes for data modeling, mining, reporting and QA
Ensure architecture will support the business requirements
Employ a variety of languages and tools (e.g. scripting languages) to integrate data from different systems.
Recommend ways to improve data reliability, efficiency and quality.
Employ sophisticated analytics programs, machine learning and statistical methods to prepare data for use in predictive and prescriptive modeling.
Explore and examine data to find hidden patterns.
Analyze potential data quality issues to determine the root cause and create effective solutions.
Optimize processes involving large data sets to improve performance.
Work with stakeholder to understand their business and make recommendations to help solve problems or improve processes.
Deliver high quality projects on time and budget in a fast-paced environment.
Preparing and presenting technical information to non-technical and highly technical audiences.
Working on multiple projects simultaneously.
Training and supporting others as needed.

Minimum Qualifications


Very strong Problem solving / critical thinking skills
Demonstrated experience working directly with and creating data architectures.
Advanced SQL Server programming (e.g. functions, views, stored procedures, parameters) and knowledge of data warehousing best practices.
Creating stored procedures, SSIS packages and using other methods to import/translate/manipulate data.
Working experience with job automation (e.g. SQL server agent, Azure Data flow.. etc.).
Advanced knowledge designing, developing, testing, and supporting SSAS technologies, and dimensional modeling.
Ability to manage projects, lead teams, document findings, and track action items to closure.
Report development experience (Tableau, PowerBI, Excel or other reporting solutions)
Performance Tuning (e.g. indexing, partitioning, optimizing queries)

Preferred Qualifications


Experience using statistical computer languages (R, Python, SLQ, etc.) to manipulate data and draw insights from large data sets.
Experience creating and using advanced machine learning algorithms and statistics and their real-world advantages/drawbacks: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, text mining, social network analysis etc.
Experience with distributed data/ cloud computing tools: Azure, AWS, Map/Reduce, MS Cosmos, Hadoop, Hive, Spark
Experience with DAX and MDX (eg: Ranking, date changing, data transformations, relating data)
MCSE certification preferred
Knowledge of best practices for database hardware & software environments and configurations.
Degree in Computer, Science, Math, Engineering, MIS or related Fields (Or equivalent experience)

","     Develop data processes for data modeling, mining, reporting and QA Ensure architecture will support the business requirements Employ a variety of languages and tools  e.g. scripting languages  to integrate data from different systems. Recommend ways to improve data reliability, efficiency and quality. Employ sophisticated analytics programs, machine learning and statistical methods to prepare data for use in predictive and prescriptive modeling. Explore and examine data to find hidden patterns. Analyze potential data quality issues to determine the root cause and create effective solutions. Optimize processes involving large data sets to improve performance. Work with stakeholder to understand their business and make recommendations to help solve problems or improve processes. Deliver high quality projects on time and budget in a fast-paced environment. Preparing and presenting technical information to non-technical and highly technical audiences. Working on multiple projects simultaneously. Training and supporting others as needed. ","Develop data processes for modeling, mining, reporting and QA Ensure architecture will support the business requirements Employ a variety of languages tools e.g. scripting to integrate from different systems. Recommend ways improve reliability, efficiency quality. sophisticated analytics programs, machine learning statistical methods prepare use in predictive prescriptive modeling. Explore examine find hidden patterns. Analyze potential quality issues determine root cause create effective solutions. Optimize involving large sets performance. Work with stakeholder understand their make recommendations help solve problems or processes. Deliver high projects on time budget fast-paced environment. Preparing presenting technical information non-technical highly audiences. Working multiple simultaneously. Training supporting others as needed.","Develop data processes modeling, mining, reporting QA Ensure architecture support business requirements Employ variety languages tools e.g. scripting integrate different systems. Recommend ways improve reliability, efficiency quality. sophisticated analytics programs, machine learning statistical methods prepare use predictive prescriptive modeling. Explore examine find hidden patterns. Analyze potential quality issues determine root cause create effective solutions. Optimize involving large sets performance. Work stakeholder understand make recommendations help solve problems processes. Deliver high projects time budget fast-paced environment. Preparing presenting technical information non-technical highly audiences. Working multiple simultaneously. Training supporting others needed."
366,Data Engineer,Principal Data Engineer - Temporary,"Renton, WA 98057",Renton,WA,"Description:
Providence St. Joseph Health is calling a Principal Data Engineer to one of our following locations: Renton, WA (preferred), Spokane, WA, Richland, WA, Everett, WA, Olympia, WA, Anchorage, AK, Missoula, MT, Portland, OR, Beaverton, OR, Anaheim, CA, Burbank, CA or Lubbock, TX. This position is 1 year-long in duration with full medical, dental, vision and vacation benefits. We are open to the possibility of working remotely within our 7 state region of operation: AK, WA, OR, MT, CA, NM and TX.
We are seeking a Principal Data Engineer to design and build modern data-centric software applications to support clinical and operational processes across all parts of the health system. These applications leverage cloud computing, big data, mobile, data science, and modern software development methodologies and frameworks. Data Engineers build data pipelines, enrichment processes, provisioning layers, APIs and user interfaces to meet the requirements of key initiatives. The Principal Data Engineer will take point on development of best practices and standards across the engineering team and participate in research and development of new technologies. A Principal Data Engineer should be able to and emphasize mentoring less experienced Data Engineers and training the team, as needed, to develop a robust skillset among the entire team. Strongly encourages and places a priority on collaboration with meticulous source control and documentation. An emphasis on simple solutions to complex problems through the use of modern and emerging methods and tools is critical. This position will works closely with the Product, Platform, and Architecture teams to deliver on joint efforts.
In this position you will have the following responsibilities:
Design, build and deliver quantitative applications that improve operations and generate value
Participate in DevOps, Agile, and continuous integration frameworks
Stay abreast of emerging technologies, open source projects, and best practices in the field
Data warehousing, big data, enterprise search, business intelligence, analytics, modern and mobile applications
Build processes that are fault-tolerant, self-healing, reliable, resilient and secure
Work effectively and in real-time with other developers, product managers, and customers to deliver on collective goals
Actively participate in code reviews, support the overall code base, and support the establishment of standard processes and frameworks. Take a lead role in the development of standard practices and enforce following standard processes.
Qualifications:
Required qualifications for this position include:
Bachelorâs Degree in computer science, engineering, mathematics, MIS or similar field.
10 years in technology roles.
Must have experience with the following technologies:
C#
ASP.net
T-SQL
HTML/CSS
JavaScript
Nodejs
Demonstrated analytical skills
Demonstrated problem solving skills
Promotes information sharing
Ability to work within tight timeframes and meet strict deadlines.
Possesses strong technical Aptitude.
Preferred qualifications for this position include:
Masterâs Degree.
Cloud computing, Linux, Hadoop, MapReduce, Spark, Hbase, Kudu and NoSQL platforms in general; Apache Solr and Lucene
Java, Scala, C#, Python, shell scripting and/or similar languages
Relational database platforms, database design, and SQL
APIs, JSON, REST and other relevant W3C open standards
Modern application development frameworks
Familiarity with commercial or open source ETL tools
About the department you will serve.
Providence Strategic and Management Services provides a variety of functional and system support services for all eight regions of Providence Health & Services from Alaska to California. We are focused on supporting our Mission by delivering a robust foundation of services and sharing of specialized expertise.
We offer a full comprehensive range of benefits - see our website for details
http://www.providenceiscalling.jobs/rewards-benefits/
Our Mission
As expressions of Godâs healing love, witnessed through the ministry of Jesus, we are steadfast in serving all, especially those who are poor and vulnerable.
About Us
Providence Health & Services is a not-for-profit Catholic network of hospitals, care centers, health plans, physicians, clinics, home health care and services guided by a Mission of caring the Sisters of Providence began over 160 years ago. Providence is proud to be an Equal Opportunity Employer. Providence does not discriminate on the basis of race, color, gender, disability, veteran, military status, religion, age, creed, national origin, sexual identity or expression, sexual orientation, marital status, genetic information, or any other basis prohibited by local, state, or federal law.
Schedule: Full-time
Shift: Day
Job Category: Information Technology
Location: Alaska-Anchorage
Other Location(s): Oregon-Portland, Oregon-Beaverton, Montana-Missoula, Washington-Everett, Washington-Renton, Washington-Richland, Washington-Spokane, California-Anaheim
Req ID: 234029","Bachelorâs Degree in computer science, engineering, mathematics, MIS or similar field. 10 years in technology roles. Must have experience with the following technologies  C  ASP.net T-SQL HTML/CSS JavaScript Nodejs Demonstrated analytical skills Demonstrated problem solving skills Promotes information sharing Ability to work within tight timeframes and meet strict deadlines. Possesses strong technical Aptitude.     ","Bachelorâs Degree in computer science, engineering, mathematics, MIS or similar field. 10 years technology roles. Must have experience with the following technologies C ASP.net T-SQL HTML/CSS JavaScript Nodejs Demonstrated analytical skills problem solving Promotes information sharing Ability to work within tight timeframes and meet strict deadlines. Possesses strong technical Aptitude.","Bachelorâs Degree computer science, engineering, mathematics, MIS similar field. 10 years technology roles. Must experience following technologies C ASP.net T-SQL HTML/CSS JavaScript Nodejs Demonstrated analytical skills problem solving Promotes information sharing Ability work within tight timeframes meet strict deadlines. Possesses strong technical Aptitude."
367,Data Engineer,"Sr. Data Engineer - BI, Analyst, Cloud","Seattle, WA 98115",Seattle,WA,"Developer â Big Data
Seattle, WA

About the Role. . .
Very strong cloud analytics development skills are vital to Logic 20/20âs success. In order to continue and accelerate our growth, we are looking for Big Data Developers to add to our Seattle, Washington-based team. Big Data Developers are responsible for delivering client value and ensuring high client satisfaction. They are expected to be adept at recognizing, subscribing, and applying best practices, methodology, tools, and techniques to meet client requirements, timelines, and budgets.
You like to . . .
Develop end-to-end cloud analytics solutions
Lead small teams of developers on BI projects on client projects in the greater Seattle region
Communicate project status to internal and external stakeholders
Translate client user requirements into data flows, data mapping, etc.
Ensure that the design system runs efficiently and accurately
Create and deliver technical and project documentation as required throughout the project lifecycle
Deliver projects and solve problems within small, medium and large organizations utilizing disparate tools and methodologies
Deliver high quality projects on time and budget in adverse environments
Qualifications
8+ years of BI development experience with 4+ of cloud analytics experience
Cloud solution implementation experience with some of the following technologies:
Jenkins
Sqoop
Hadoop including Hive and Pig
Spark and SparkSQL
AWS S3, Redshift, Dinamo DB
Azure Data Lake Store, HDInsight, U-SQL
Python, JSON, Java
Advanced data analysis skills including advanced SQL query capabilities
Deep experience in data modeling, data analysis and relational database design
Demonstrated ability to identify business and technical impacts of user requirements and incorporate them into the project schedule
Ability to work both independently and as part of a team
Ability to work under pressure and to independently handle multiple projects and deadlines
Experience working with large datasets (over 100M rows)
Experience with Talend / Informatica is a plus
Experience building real time or near real time BI solutions for PROD
An undergraduate degree in technology or business is required
Cloud certification (AWS or GCE) is a plus

About Logic20/20. . .
Logic20/20 is one of Seattleâs fastest growing full-service consulting firms. Our core competency is creating simplicity and efficiency in complex solutions. Although we make it look like magic, we succeed by combining methodical and structured approaches with our substantial experience to design elegant solutions for even the most intricate challenges. Our rapid growth is in response to our ability to deliver consistently for our clients, which is directly related to the quality of the people we hire.
The past four years, weâve been in the top 10 âBest Companies to Work Forâ â¦.. why? Our team members are highly self-motivated, comfortable conceiving strategies on the fly, and enjoy working both individually and as part of a team. Our environment is very high-energy and demanding, and individuals with remarkable enthusiasm and a can-do attitude are joining our team. We have lots of fun, focus on our employees and our clients, and work to bring our best to every opportunity."," 8+ years of BI development experience with 4+ of cloud analytics experience Cloud solution implementation experience with some of the following technologies  Jenkins Sqoop Hadoop including Hive and Pig Spark and SparkSQL AWS S3, Redshift, Dinamo DB Azure Data Lake Store, HDInsight, U-SQL Python, JSON, Java    ","8+ years of BI development experience with 4+ cloud analytics Cloud solution implementation some the following technologies Jenkins Sqoop Hadoop including Hive and Pig Spark SparkSQL AWS S3, Redshift, Dinamo DB Azure Data Lake Store, HDInsight, U-SQL Python, JSON, Java","8+ years BI development experience 4+ cloud analytics Cloud solution implementation following technologies Jenkins Sqoop Hadoop including Hive Pig Spark SparkSQL AWS S3, Redshift, Dinamo DB Azure Data Lake Store, HDInsight, U-SQL Python, JSON, Java"
368,Data Engineer,Data Engineer,"Seattle, WA 98104",Seattle,WA,"doxo is disrupting bill pay. We are founded on the idea to create a simple, secure way to pay all your bills from a single account. Weâre a rapidly growing startup looking for a high lead energy Data Engineer with experience building scalable data collection, storage, and analysis systems.

Major Responsibilities
Define the architecture, technologies, and processes used to create a high scale data processing pipeline
Work with engineers to create the tools and infrastructure used to collect, transform, and store data to be used for analytics
Identify and implement analytics tools used by internal stakeholders to engage with data
Ensure proper security and access control to analytics data
Work collaboratively with the team to deploy, support and operate your services and applications.
Skills and Qualifications
3+ years of industry experience
Experience with high scale, distributed, 24x7 systems and applications
5+ years of experience with SQL in both transactional and analytical applications
Experience with Linux platforms
Experience with AWS data technologies
Experience with Ruby, Python, or a similar programming language
Strong analytical and design skills
Capacity for learning quickly in a fast paced environment and handling multiple tasks simultaneously
Strong written and oral communication skills
Education
BS/MS in Computer Science or Engineering","3+ years of industry experience Experience with high scale, distributed, 24x7 systems and applications 5+ years of experience with SQL in both transactional and analytical applications Experience with Linux platforms Experience with AWS data technologies Experience with Ruby, Python, or a similar programming language Strong analytical and design skills Capacity for learning quickly in a fast paced environment and handling multiple tasks simultaneously Strong written and oral communication skills  3+ years of industry experience Experience with high scale, distributed, 24x7 systems and applications 5+ years of experience with SQL in both transactional and analytical applications Experience with Linux platforms Experience with AWS data technologies Experience with Ruby, Python, or a similar programming language Strong analytical and design skills Capacity for learning quickly in a fast paced environment and handling multiple tasks simultaneously Strong written and oral communication skills  Define the architecture, technologies, and processes used to create a high scale data processing pipeline Work with engineers to create the tools and infrastructure used to collect, transform, and store data to be used for analytics Identify and implement analytics tools used by internal stakeholders to engage with data Ensure proper security and access control to analytics data Work collaboratively with the team to deploy, support and operate your services and applications.  BS/MS in Computer Science or Engineering ","3+ years of industry experience Experience with high scale, distributed, 24x7 systems and applications 5+ SQL in both transactional analytical Linux platforms AWS data technologies Ruby, Python, or a similar programming language Strong design skills Capacity for learning quickly fast paced environment handling multiple tasks simultaneously written oral communication Define the architecture, technologies, processes used to create scale processing pipeline Work engineers tools infrastructure collect, transform, store be analytics Identify implement by internal stakeholders engage Ensure proper security access control collaboratively team deploy, support operate your services applications. BS/MS Computer Science Engineering","3+ years industry experience Experience high scale, distributed, 24x7 systems applications 5+ SQL transactional analytical Linux platforms AWS data technologies Ruby, Python, similar programming language Strong design skills Capacity learning quickly fast paced environment handling multiple tasks simultaneously written oral communication Define architecture, technologies, processes used create scale processing pipeline Work engineers tools infrastructure collect, transform, store analytics Identify implement internal stakeholders engage Ensure proper security access control collaboratively team deploy, support operate services applications. BS/MS Computer Science Engineering"
369,Data Engineer,Senior Data Engineer,"Bellevue, WA 98004",Bellevue,WA,"Egencia
Egencia is Expedia Groupâs business travel company. In the same spirit, we are pioneers in the industry. Always solutions oriented, we value human contact. Egencia's teams collaborate globally to provide business travelers with the best possible user experience every single day.
As a Senior Data Engineer in the Business Intelligence team, you are an authority, familiar with all the data warehousing technical components, infrastructure, and their integration. Youâll analyze large amounts of data, discover and solve real world problems. You are responsible for high level design/architecture - with extensive experience with Star Schemas, Data Cubes and Dimensional Models.
We expect our data engineers to build durable data pipelines with the ability to scale elegantly with data volume growth. The code you write must enable our users to get data in a timely manner. You'll work on a variety of tools and systems, most of which are Cloud-based applications or data-platform components (e.g., data pipelines, processing, Visualization, reporting, etc.). In this role, you will be working in Agile development method for faster, quick-wins with the caliber of design, development and operating BI components you develop.
Responsibilities:
Design, architect, implement, and support key datasets that provide structured and timely access to actionable business information with the needs of the end customer always in view
Retrieve and analyze data using SQL, Excel, and other data management systems.
Create ETLs/ELTs to take data from various operational systems and create a unified dimensional or star schema data model for analytics and reporting
Develop a deep understanding of vast data sources (existing on the cloud) and know exactly how, when, and which data to use to solve particular business problems
Competencies:
Experience developing and operating large scale analytical databases/platforms/data warehouses and performing ETL across various operating systems (Microsoft and Linux)
Experience working with Cloud BI Production implementation
Knowledge of Agile, DevOps, CICD frameworks
Experience with multi-terabyte data sets using relational databases (RDBMS) and SQL
Agile/Scrum methodologies to iterate quickly on product changes, developing user stories and working through backlogs.
Experience and Qualification:
7+ years' experience with detailed knowledge of data warehouse technical architectures, ETL/ ELT, reporting/analytic tools, and data security
5+ years of experience in designing data warehouse solutions and integrating technical components
2+ yrs of scripting language (Ruby, python, etc.) experience
1+ yr experience with production BI implementation in Cloud
Exposure in at least one reporting tool like Qlikview, Tableau, Power BI etc.
Capable of investigating, familiarizing and mastering new data sets quickly
Experience with building data platform on cloud (AWS)
Good familiarity with Linux/Unix scripting
Experience with Hadoop, MPP DB platform, other NoSQL (Mongo, Cassandra) technologies is a big plus
Bachelor, or Masters, of Science Degree in Computer Science or related field
Why join us
Expedia Group recognizes our success is dependent on the success of our people. We are the world's travel platform, made up of the most knowledgeable, passionate, and creative people in our business. Our brands recognize the power of travel to break down barriers and make people's lives better â that responsibility inspires us to be the place where exceptional people want to do their best work, and to provide them the tools to do so.
Whether you're applying to work in engineering or customer support, marketing or lodging supply, at Expedia Group we act as one team, working towards a common goal; to bring the world within reach. We relentlessly strive for better, but not at the cost of the customer. We act with humility and optimism, respecting ideas big and small. We value diversity and voices of all volumes. We are a global organization but keep our feet on the ground so we can act fast and stay simple. Our teams also have the chance to give back on a local level and make a difference through our corporate social responsibility program, Expedia Cares.
If you have a hunger to make a difference with one of the most loved consumer brands in the world and to work in the dynamic travel industry, this is the job for you.
Our family of travel brands includes: Brand ExpediaÂ®, Hotels.comÂ®, ExpediaÂ® Partner Solutions, EgenciaÂ®, trivagoÂ®, HomeAwayÂ®, OrbitzÂ®, TravelocityÂ®, WotifÂ®, lastminute.com.auÂ®, ebookersÂ®, CheapTicketsÂ®, HotwireÂ®, Classic VacationsÂ®, ExpediaÂ® Media Solutions, CarRentals.comâ¢, Expedia Local ExpertÂ®, ExpediaÂ® CruiseShipCentersÂ®, SilverRail Technologies, Inc., ALICE and TraveldooÂ®.LI-ES1
Expedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization.","   Design, architect, implement, and support key datasets that provide structured and timely access to actionable business information with the needs of the end customer always in view Retrieve and analyze data using SQL, Excel, and other data management systems. Create ETLs/ELTs to take data from various operational systems and create a unified dimensional or star schema data model for analytics and reporting Develop a deep understanding of vast data sources  existing on the cloud  and know exactly how, when, and which data to use to solve particular business problems  ","Design, architect, implement, and support key datasets that provide structured timely access to actionable business information with the needs of end customer always in view Retrieve analyze data using SQL, Excel, other management systems. Create ETLs/ELTs take from various operational systems create a unified dimensional or star schema model for analytics reporting Develop deep understanding vast sources existing on cloud know exactly how, when, which use solve particular problems","Design, architect, implement, support key datasets provide structured timely access actionable business information needs end customer always view Retrieve analyze data using SQL, Excel, management systems. Create ETLs/ELTs take various operational systems create unified dimensional star schema model analytics reporting Develop deep understanding vast sources existing cloud know exactly how, when, use solve particular problems"
370,Data Engineer,Senior Data Engineer,"Bellevue, WA",Bellevue,WA,"From the very beginning, OfferUp has believed that the right people united by the right mission can redefine the possible.

OfferUp is dedicated to building the simplest and most trustworthy way for people to buy and sell in their communities. Every year, millions of people use OfferUp to buy and sell locally, resulting in billions of dollars of local commerce. As the largest mobile marketplace for local buyers and sellers in the U.S., our iOS and Android app has been in the top five most popular shopping apps lists for more than three years. Join us as we build the marketplace of the future and help more people discover value right where they are.

At a Glance
-----------


85+ Million Downloads
Geekwire App of Year
15%+ of adults in several markets use OfferUp every month (LA, Miami, Phoenix, Seattle, Las Vegas, Riverside, Orlando)

Senior Data Engineer

Here at OfferUp, data is at the core of our business, providing insights into the effectiveness of our products, and enabling technology that powers them. As the Data Engineering team, we build and operate OfferUp's data platform for streaming and batch computation, for data analysis and BI, and to train ML models. If you're passionate about building large scale distributed data processing systems, and you are motivated to make an impact in creating a robust and scalable data platform used by every team, come join us. You will be part of a team that builds the data ingestion, transport, storage, and orchestration layers. You will help shape the vision and architecture of OfferUp's next generation data infrastructure, making it easy for developers to build data-driven products and features.

Building the largest and most responsive mobile marketplace poses unique data challenges that require leveraging the latest developments in data infrastructure. We leverage open source infrastructure where we can, but are ready to build and share solutions if they don't exist yet. You will be building an analytics platform working with cutting-edge technologies like Kafka, Spark, Snowflake, and Airflow.

We regard culture and trust highly, and are looking forward to welcoming your contribution to the team!

Responsibilities:

Build and operate large scale data infrastructure in production.
Design, implement and debug distributed data processing systems.
Thinking through long-term impacts of key design decisions and handling failure scenarios.
Building self-service platforms to power all other OfferUp teams.
Mentor other engineers and help them with their growth.
Drive engineering best practices, set standards and propose larger projects which may require cross-team collaboration.
Contribute at a senior level to the data warehouse design and data preparation by implementing a robust and extensible design
Design and develop applications to process large amounts of critical information in batch and near real-time to power user-facing features.
Influence technical direction for the company, leveraging your prior experiences and helping evaluate emerging technologies and approaches.

Requirements:

5+ years of professional software development experience.
Strong ability in distributed systems for processing large scale data processing.
Ability to communicate technical information effectively to technical and non-technical audiences.
Proficiency in Java, Scala and Python.
Experience leveraging open source data infrastructure projects, such as Apache Spark, Airflow, Kafka, Flink, Samza, Avro, Parquet, Hadoop, Hive, HBase.
Experience building data pipelines and real-time data streams.
Experience building software in AWS or a similar cloud environment is highly desirable.
Experience with AWS services like EMR, Kinesis, Firehose, Lambda, Sagemaker, Athena, Elasticsearch is a big plus.
Computer Science or Engineering degree required, Masters degree preferred.
Must be eligible to work in the United States.

OfferUp is changing the way people buy and sell locally...Come do work that matters. join the team and take the ride of your life!

OfferUp provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, OfferUp complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, transfer, leaves of absence, compensation, and training.

OfferUp expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of OfferUp's employees to perform their job duties may result in discipline up to and including discharge.","   Build and operate large scale data infrastructure in production. Design, implement and debug distributed data processing systems. Thinking through long-term impacts of key design decisions and handling failure scenarios. Building self-service platforms to power all other OfferUp teams. Mentor other engineers and help them with their growth. Drive engineering best practices, set standards and propose larger projects which may require cross-team collaboration. Contribute at a senior level to the data warehouse design and data preparation by implementing a robust and extensible design Design and develop applications to process large amounts of critical information in batch and near real-time to power user-facing features. Influence technical direction for the company, leveraging your prior experiences and helping evaluate emerging technologies and approaches.    5+ years of professional software development experience. Strong ability in distributed systems for processing large scale data processing. Ability to communicate technical information effectively to technical and non-technical audiences. Proficiency in Java, Scala and Python. Experience leveraging open source data infrastructure projects, such as Apache Spark, Airflow, Kafka, Flink, Samza, Avro, Parquet, Hadoop, Hive, HBase. Experience building data pipelines and real-time data streams. Experience building software in AWS or a similar cloud environment is highly desirable. Experience with AWS services like EMR, Kinesis, Firehose, Lambda, Sagemaker, Athena, Elasticsearch is a big plus. Computer Science or Engineering degree required, Masters degree preferred. Must be eligible to work in the United States. ","Build and operate large scale data infrastructure in production. Design, implement debug distributed processing systems. Thinking through long-term impacts of key design decisions handling failure scenarios. Building self-service platforms to power all other OfferUp teams. Mentor engineers help them with their growth. Drive engineering best practices, set standards propose larger projects which may require cross-team collaboration. Contribute at a senior level the warehouse preparation by implementing robust extensible Design develop applications process amounts critical information batch near real-time user-facing features. Influence technical direction for company, leveraging your prior experiences helping evaluate emerging technologies approaches. 5+ years professional software development experience. Strong ability systems processing. Ability communicate effectively non-technical audiences. Proficiency Java, Scala Python. Experience open source projects, such as Apache Spark, Airflow, Kafka, Flink, Samza, Avro, Parquet, Hadoop, Hive, HBase. building pipelines streams. AWS or similar cloud environment is highly desirable. services like EMR, Kinesis, Firehose, Lambda, Sagemaker, Athena, Elasticsearch big plus. Computer Science Engineering degree required, Masters preferred. Must be eligible work United States.","Build operate large scale data infrastructure production. Design, implement debug distributed processing systems. Thinking long-term impacts key design decisions handling failure scenarios. Building self-service platforms power OfferUp teams. Mentor engineers help growth. Drive engineering best practices, set standards propose larger projects may require cross-team collaboration. Contribute senior level warehouse preparation implementing robust extensible Design develop applications process amounts critical information batch near real-time user-facing features. Influence technical direction company, leveraging prior experiences helping evaluate emerging technologies approaches. 5+ years professional software development experience. Strong ability systems processing. Ability communicate effectively non-technical audiences. Proficiency Java, Scala Python. Experience open source projects, Apache Spark, Airflow, Kafka, Flink, Samza, Avro, Parquet, Hadoop, Hive, HBase. building pipelines streams. AWS similar cloud environment highly desirable. services like EMR, Kinesis, Firehose, Lambda, Sagemaker, Athena, Elasticsearch big plus. Computer Science Engineering degree required, Masters preferred. Must eligible work United States."
371,Data Engineer,Senior Data Engineer - Turn 10 Studios,"Redmond, WA",Redmond,WA,"Games. Xbox. Big data. AI/ML. Turn 10 Studios, makers of the award winning, billion-dollar Forza franchise, is searching for a senior engineer to help build our next generation data pipelines. You would be joining a small team of awesome people that move fast, innovate daily, and have fun (we make games!).

Analytics is a crucial part of the business at Turn 10. Understanding user motivations and behaviors enables the team to build fun, engaging racing experiences on PC, Xbox and mobile. The gaming industry is evolving to a GaaS model (Games as a Service) where the most successful studios will learn continuously from data and respond rapidly to customer's needs. This is our challenge. As a leader on the data engineering team, you will be front and center building the platform that will enable the entire studio to quickly learn, adapt and transform our games.
Responsibilities
You'll focus on:

Evolving our big data pipelines to streamline data collection (measure things) and democratize the consumption of data (generate information).
Working with business leaders and game designers to answer the key questions that enable the team to drive franchise growth and create experiences that thrill customers.

Expanding the studios capabilities in AI/ML, building an intelligent cloud for Forza.
Qualifications
We only have a few requirements:

Enthusiasm for cars and/or gaming - Our priority is making amazing racing games. The ideal candidate must have enthusiasm for our products and empathy for our customers.
Enthusaism for cloud data technology - Our pipelines are fully supported by Azure leveraging things like Data Explorer (Kusto), Data Warehouse, Data Factory, Data Lake, SQL and Power BI. The ideal candidate has a passion for cloud technology and a minimum of 5 years' experience.
A drive to develop data insights - Collecting data is the easy part. Helping business leaders and game designers ask the right questions and answering these questions with a relentless attention to details (accuracy) is where the fun begins. The ideal candidate is a meticulous gatekeeper for data and code quality, passionate about generating insights from data, and a strong communicator and collaborator.
Enthusiasm for AI/ML or an interest to learn - We don't do science projects, but we have an aspiration to build AI/ML capabilities that generate customer value.

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work."," Enthusiasm for cars and/or gaming - Our priority is making amazing racing games. The ideal candidate must have enthusiasm for our products and empathy for our customers. Enthusaism for cloud data technology - Our pipelines are fully supported by Azure leveraging things like Data Explorer  Kusto , Data Warehouse, Data Factory, Data Lake, SQL and Power BI. The ideal candidate has a passion for cloud technology and a minimum of 5 years' experience. A drive to develop data insights - Collecting data is the easy part. Helping business leaders and game designers ask the right questions and answering these questions with a relentless attention to details  accuracy  is where the fun begins. The ideal candidate is a meticulous gatekeeper for data and code quality, passionate about generating insights from data, and a strong communicator and collaborator. Enthusiasm for AI/ML or an interest to learn - We don't do science projects, but we have an aspiration to build AI/ML capabilities that generate customer value.   Evolving our big data pipelines to streamline data collection  measure things  and democratize the consumption of data  generate information . Working with business leaders and game designers to answer the key questions that enable the team to drive franchise growth and create experiences that thrill customers.  ","Enthusiasm for cars and/or gaming - Our priority is making amazing racing games. The ideal candidate must have enthusiasm our products and empathy customers. Enthusaism cloud data technology pipelines are fully supported by Azure leveraging things like Data Explorer Kusto , Warehouse, Factory, Lake, SQL Power BI. has a passion minimum of 5 years' experience. A drive to develop insights Collecting the easy part. Helping business leaders game designers ask right questions answering these with relentless attention details accuracy where fun begins. meticulous gatekeeper code quality, passionate about generating from data, strong communicator collaborator. AI/ML or an interest learn We don't do science projects, but we aspiration build capabilities that generate customer value. Evolving big streamline collection measure democratize consumption information . Working answer key enable team franchise growth create experiences thrill","Enthusiasm cars and/or gaming - Our priority making amazing racing games. The ideal candidate must enthusiasm products empathy customers. Enthusaism cloud data technology pipelines fully supported Azure leveraging things like Data Explorer Kusto , Warehouse, Factory, Lake, SQL Power BI. passion minimum 5 years' experience. A drive develop insights Collecting easy part. Helping business leaders game designers ask right questions answering relentless attention details accuracy fun begins. meticulous gatekeeper code quality, passionate generating data, strong communicator collaborator. AI/ML interest learn We science projects, aspiration build capabilities generate customer value. Evolving big streamline collection measure democratize consumption information . Working answer key enable team franchise growth create experiences thrill"
372,Data Engineer,Data Engineer,"Seattle, WA",Seattle,WA,"Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.
One thing that's stayed the same since our founding: our commitment to our customers, partners and employees.
Join us on our journey as we continue to grow into a powerful contender in the field of insurance.
Data Engineer

Weâre looking for a Data Engineer to help us transform our data systems and architecture to support greater variety, volume, and velocity of data and data sources. You might be a good fit if:

You enjoy extracting data from a variety of sources and find ways to connect them and make them suitable for use in software systems and for the development of models and algorithms.
You enjoy interacting with new database systems and learning new data technologies and are interesting in developing your knowledge of new tools and techniques.
You are interested in automating data engineering efforts to minimize human interaction and optimizing data quality.
You have an interest in developing your knowledge of practical data science techniques and technologies in addition to your data engineering knowledge and experience.
This role requires comprehensive data engineering skills and is not a SQL developer role though SQL is a required skill.
Responsibilities:
Weâre looking for an experienced data engineer to help us:
Build and Maintain serverless data ingestion and refresh pipelines in terabyte scale using AWS cloud services â Amazon Glue, Amazon Redshift, Amazon S3, Amazon Athena, DynamoDB, and others
Incorporate new data sources from external vendors using flat files, APIs, web-scraping, and databases.
Maintain and provide support for the existing data pipelines using Python, Glue, Spark, and SQL
Work to develop and enhance the database architecture of the new analytic data environment that includes recommending optimal choices between relational, columnar, and document databases based on requirement
Identify and deploy appropriate file formats for data ingestion into various storage and/or compute services via Glue for multiple use cases
Develop real-time/near real-time data ingestion from web and web service logs from Splunk
Maintain existing processes and develop new methods to match external data sources to Homesite data using exact and fuzzy methods
Implement and use machine learning based data wrangling tools like Trifacta to cleanse and reshape 3rd party data to make suitable for use.
Develop and implement tests to ensure data quality across all integrated data sources.
Serve as internal subject matter expert and coach to train team members in the use of distributed computing frameworks for data analysis and modeling including AWS services and Apache projects
Qualifications:
Masterâs degree in Computer Science, Engineering, or equivalent work experience
Two to four yearsâ experience working with datasets with hundreds of millions of rows using a variety of technologies
Intermediate to expert level programming experience in Python and SQL in Windows and Mac/Linux environment
Intermediate level experience working with distributed computing frameworks, especially Spark","Masterâs degree in Computer Science, Engineering, or equivalent work experience Two to four yearsâ experience working with datasets with hundreds of millions of rows using a variety of technologies Intermediate to expert level programming experience in Python and SQL in Windows and Mac/Linux environment Intermediate level experience working with distributed computing frameworks, especially Spark  Build and Maintain serverless data ingestion and refresh pipelines in terabyte scale using AWS cloud services â Amazon Glue, Amazon Redshift, Amazon S3, Amazon Athena, DynamoDB, and others Incorporate new data sources from external vendors using flat files, APIs, web-scraping, and databases. Maintain and provide support for the existing data pipelines using Python, Glue, Spark, and SQL Work to develop and enhance the database architecture of the new analytic data environment that includes recommending optimal choices between relational, columnar, and document databases based on requirement Identify and deploy appropriate file formats for data ingestion into various storage and/or compute services via Glue for multiple use cases Develop real-time/near real-time data ingestion from web and web service logs from Splunk Maintain existing processes and develop new methods to match external data sources to Homesite data using exact and fuzzy methods Implement and use machine learning based data wrangling tools like Trifacta to cleanse and reshape 3rd party data to make suitable for use. Develop and implement tests to ensure data quality across all integrated data sources. Serve as internal subject matter expert and coach to train team members in the use of distributed computing frameworks for data analysis and modeling including AWS services and Apache projects   ","Masterâs degree in Computer Science, Engineering, or equivalent work experience Two to four yearsâ working with datasets hundreds of millions rows using a variety technologies Intermediate expert level programming Python and SQL Windows Mac/Linux environment distributed computing frameworks, especially Spark Build Maintain serverless data ingestion refresh pipelines terabyte scale AWS cloud services â Amazon Glue, Redshift, S3, Athena, DynamoDB, others Incorporate new sources from external vendors flat files, APIs, web-scraping, databases. provide support for the existing Python, Spark, Work develop enhance database architecture analytic that includes recommending optimal choices between relational, columnar, document databases based on requirement Identify deploy appropriate file formats into various storage and/or compute via Glue multiple use cases Develop real-time/near real-time web service logs Splunk processes methods match Homesite exact fuzzy Implement machine learning wrangling tools like Trifacta cleanse reshape 3rd party make suitable use. implement tests ensure quality across all integrated sources. Serve as internal subject matter coach train team members frameworks analysis modeling including Apache projects","Masterâs degree Computer Science, Engineering, equivalent work experience Two four yearsâ working datasets hundreds millions rows using variety technologies Intermediate expert level programming Python SQL Windows Mac/Linux environment distributed computing frameworks, especially Spark Build Maintain serverless data ingestion refresh pipelines terabyte scale AWS cloud services â Amazon Glue, Redshift, S3, Athena, DynamoDB, others Incorporate new sources external vendors flat files, APIs, web-scraping, databases. provide support existing Python, Spark, Work develop enhance database architecture analytic includes recommending optimal choices relational, columnar, document databases based requirement Identify deploy appropriate file formats various storage and/or compute via Glue multiple use cases Develop real-time/near real-time web service logs Splunk processes methods match Homesite exact fuzzy Implement machine learning wrangling tools like Trifacta cleanse reshape 3rd party make suitable use. implement tests ensure quality across integrated sources. Serve internal subject matter coach train team members frameworks analysis modeling including Apache projects"
373,Data Engineer,Data Engineer In Test,"Seattle, WA",Seattle,WA,"Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.
One thing that's stayed the same since our founding: our commitment to our customers, partners and employees.
Join us on our journey as we continue to grow into a powerful contender in the field of insurance.
This position contributes to developing, implementing, and sustaining manual & automation testing including performance testing processes, practices, and controls in support of application and system requirement throughout the software development and sustainment lifecycles. Provides direction on the development and implementation of test automation and performance testing processes, methods and tools.

The position requires understanding & experience in AWS Data Platform

Experience is required in creating test plans/test cases, executing tests for applications & validating data using Tableau.
Skills Required:
Hands-on Engineer with experience in development/testing software with big data components in AWS Cloud infrastructure
Experience in testing AWS data pipelines using S3, AWS GLUE, Athena, PySpark, AWS Code Pipeline, Jupyter Notebooks, XML/JSON, Redshift, Tableau, etc.
Skills in SQL, Python, pytest, Git, Code deployment & CI/CD practices
Experience scripting, running ETL jobs, troubleshooting errors, analyzing data and performance testing.
Experience working with Agile SDLC frameworks i.e. SCRUM, Kanban, DevOps.
Experience developing or working with commercial or open source automation tools and frameworks
Demonstrate knowledge using version control and defect tracking methods, including an understanding of associated tools
Demonstrated collaboration working with diverse teams including project managers, business analysts, and Engineers related to quality assurance roles and responsibilities
Qualifications:
Bachelor's in Computer Science or related degree
3-5 years of experience in Data Engineering in Test
3+ years of experience with SQL, Python & Tableau
Understanding of key QA metrics and defect management",Bachelor's in Computer Science or related degree  Bachelor's in Computer Science or related degree    ,Bachelor's in Computer Science or related degree,Bachelor's Computer Science related degree
374,Data Engineer,Systems Data Engineer - New Glenn,"Kent, WA",Kent,WA,"Job Description
As part of a small, passionate, and accomplished team of experts, you will fill a flexible role within a fast pace development team. Your initial focus will be to interface with engineering teams and aid them in managing their data sets. You will be part of the software / process development team and will influence the features within the software tools. You will advocate for the customer in setting software development priorities and will primarily be focused on ensuring the underlying data sets are valid, current, and correct. You will work across the New Glenn program with teams of engineers in fluids, mechanical, electrical, and software subsystem teams to integrate and align data sets. Your work will ensure we have robust configuration control of data used to manufacture and operate New Glenn. We are expecting you to bring engineering analytical skills and experience working the interface between hardware and software. Once this foundation is established, your role will transition into support for building reliable health monitoring and prediction capabilities that will truly enable our vehicles to be reusable. This hands-on position requires a commitment to quality and attention to detail commensurate with safe human spaceflight. This is a rare opportunity to directly impact the future of human space exploration.
Responsibilities:
Work with various subject matter experts to build reports and run analysis for various program and tactical related questions
Give voice to the data, talk with people, ensure they understand what it means and it meets their needs
Act as a change agent, help engineering teams adopt new more efficient methods of operating within large related data sets
Determine and articulate balance of priorities to enable incremental delivery of needed functionality
Perform data mining for valued projects
Capture user feedback
Work as a flexible contributor in an agile team including building consensus on designs and participating in code reviews
Participate in Strategic Development of technical performance / launch system health monitoring solutions
Qualifications:
Minimum of a B.S. degree in Electrical Engineering, Systems Engineering, Computer Science, Computer Engineering, Physics, Mathematics, or other major requiring engineering core courses
Minimum of 5+ years of experience with aerospace/ control systems and software
Excellent written communication and presentation skills
Ability to collaborate across teams and balance priorities
The ability to quickly absorb information in an unfamiliar domain
A self-driven nature with the ability to seek out requirements and propose solutions with minimal direction
Technical expertise in data visualization tools (e.g. Tableau, PowerBI)
Strong analytic skill set and a high degree of proficiency in data mining
Must be a U.S. citizen or national, U.S. permanent resident (current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum.
Desired:
Exposure to product lifecycle / configuration management systems, specifically Windchill
Experience in multiple coding languages
Proficiency in scripting languages (e.g. Python)
Experience with collaboration tools such as Confluence and JIRA
Experience with web based APIs (e.g. REST, SOAP)
Experience with micro-service architecture
Proficiency in database interrogation of SQL and NOSQL databases (e.g. Oracle, MySQL, Neo4J, MongoDB)
Blue Origin offers a phenomenal work environment and awesome culture with competitive compensation, benefits, 401K, and relocation.


Blue Origin is an equal opportunity employer . In addition to EEO being the law, it is a policy that is fully consistent with Blue's principles. All qualified applicants will receive consideration for employment without regard to status as a protected veteran or a qualified individual with a disability, or other protected status such as race, religion, color, national origin, sex, sexual orientation, gender identity, genetic information, pregnancy or age. Blue Origin prohibits any form of workplace harassment.","Minimum of a B.S. degree in Electrical Engineering, Systems Engineering, Computer Science, Computer Engineering, Physics, Mathematics, or other major requiring engineering core courses Minimum of 5+ years of experience with aerospace/ control systems and software Excellent written communication and presentation skills Ability to collaborate across teams and balance priorities The ability to quickly absorb information in an unfamiliar domain A self-driven nature with the ability to seek out requirements and propose solutions with minimal direction Technical expertise in data visualization tools  e.g. Tableau, PowerBI  Strong analytic skill set and a high degree of proficiency in data mining Must be a U.S. citizen or national, U.S. permanent resident  current Green Card holder , or lawfully admitted into the U.S. as a refugee or granted asylum.   Work with various subject matter experts to build reports and run analysis for various program and tactical related questions Give voice to the data, talk with people, ensure they understand what it means and it meets their needs Act as a change agent, help engineering teams adopt new more efficient methods of operating within large related data sets Determine and articulate balance of priorities to enable incremental delivery of needed functionality Perform data mining for valued projects Capture user feedback Work as a flexible contributor in an agile team including building consensus on designs and participating in code reviews Participate in Strategic Development of technical performance / launch system health monitoring solutions   ","Minimum of a B.S. degree in Electrical Engineering, Systems Computer Science, Physics, Mathematics, or other major requiring engineering core courses 5+ years experience with aerospace/ control systems and software Excellent written communication presentation skills Ability to collaborate across teams balance priorities The ability quickly absorb information an unfamiliar domain A self-driven nature the seek out requirements propose solutions minimal direction Technical expertise data visualization tools e.g. Tableau, PowerBI Strong analytic skill set high proficiency mining Must be U.S. citizen national, permanent resident current Green Card holder , lawfully admitted into as refugee granted asylum. Work various subject matter experts build reports run analysis for program tactical related questions Give voice data, talk people, ensure they understand what it means meets their needs Act change agent, help adopt new more efficient methods operating within large sets Determine articulate enable incremental delivery needed functionality Perform valued projects Capture user feedback flexible contributor agile team including building consensus on designs participating code reviews Participate Strategic Development technical performance / launch system health monitoring","Minimum B.S. degree Electrical Engineering, Systems Computer Science, Physics, Mathematics, major requiring engineering core courses 5+ years experience aerospace/ control systems software Excellent written communication presentation skills Ability collaborate across teams balance priorities The ability quickly absorb information unfamiliar domain A self-driven nature seek requirements propose solutions minimal direction Technical expertise data visualization tools e.g. Tableau, PowerBI Strong analytic skill set high proficiency mining Must U.S. citizen national, permanent resident current Green Card holder , lawfully admitted refugee granted asylum. Work various subject matter experts build reports run analysis program tactical related questions Give voice data, talk people, ensure understand means meets needs Act change agent, help adopt new efficient methods operating within large sets Determine articulate enable incremental delivery needed functionality Perform valued projects Capture user feedback flexible contributor agile team including building consensus designs participating code reviews Participate Strategic Development technical performance / launch system health monitoring"
375,Data Engineer,Big Data Engineer,"Seattle, WA",Seattle,WA,"Job Description:

Ensure high throughput of development teams by identifying potential issues, removing impediments or guiding the team to remove impediments by collaborating with the appropriate resource
Manage sprint planning and execution which includes the management of project progress and provide status and visibility
Facilitate release planning and scheduling by providing empirical Scrum team statistics, identifying project dependencies, and creating velocity forecasts
Assist with internal and external communications to improve transparency and radiate information ensuring the teamâs progress and successes are highly visible to all stakeholders including the team itself (e.g. backlogs, burn down/up charts, etc.)
Develop pipelines using copy activity from different sources like FTP, Windows Blob Storage, SQL SERVER, COSMOS big data etc. and scheduling the pipelines as per requirement using azure data factory.
Requirements:

Required minimum Bachelorâs degree in Computer Science",     Required minimum Bachelorâs degree in Computer Science,Required minimum Bachelorâs degree in Computer Science,Required minimum Bachelorâs degree Computer Science
376,Data Engineer,Data Engineer,"Seattle, WA 98101",Seattle,WA,"Job Description:
Who We Are
Since 2001, All Star Directories has been empowering people to advance their careers and improve their lives through education. As an independent marketing and technology company, we build long-term relationships with higher education institutions across the nation while providing millions of students with the resources they need to help them reach their goals.

At All Star, we pride ourselves on our authenticity, accountability, and flexibility. Our team is made up of hardworking, collaborative, and highly motivated people who strive to stay ahead of industry trends. Here, we work in an Agile environment on challenging projects both big and small, with each day offering new opportunities for growth.

All of our hard work is rewarded with competitive salaries, incredible benefits, and serious fun. After all, as an employee-owned company, we care about the people who work here just as much as the customers we serve. Joining the All Star team means generous PTO, ping pong tournaments, in-office massages, and a fully stocked fridge. (Oh, and your dog can join us too.) We also care deeply about our community, offering paid volunteer days off as well as toy and clothing drives throughout the year.
https://www.allstardirectories.com
Who You Are
We are looking for a passionate and experienced Data Engineer. The successful candidate will drive automation, personalization and data pipeline initiatives. You will own critical systems throughout our various platforms and be responsible for their development, new/valuable features, and administering that these systems perform correctly in production.

Required Experience:

Responsibilities
Collaborate with senior management, product management, data analytics, and web/app developers in the development of data products, pipelines, and infrastructure
Develop reliable and near real time data pipelines that make data easily consumable by end users and other systems that we use at All Star
Develop tools to monitor, debug, and analyze data pipeline health
Design and implement data schemas and models that can scale
Mentor technology team members to build the company's overall expertise
Administer all database automation and take corrective action when required
Essential Skills and Experience
Ability to communicate effectively with stakeholders to define requirements
Proficiency in writing SQL procedures and functions for administration and application support
Strong knowledge of operating systems, shell scripting, and python scripting
Good interpersonal skills along with effective communication (both written and verbal)
Demonstrated ability to solve complex systems and database environment issues
Experience with cloud platforms (AWS and AZURE)
Experience building data pipelines & ETL
Experience with command line
Experience with version control software (git) and best practices
Demonstrated experience leading database architecture development
Experience with big data technologies at scale
Strong understanding of PostgreSQL database fundamentals
Strong understanding of database security management
Experience with ETL tools
Experience with BI tools
Willingness to take ownership over wide range of databases and processes
Qualifications
Bachelorâs Degree in computer science or relevant information technology field
3-4 years of DBA experience
3-4 years of SQL replication experience and managing complex ETL processes
From: All Star Directories

Benefits:
We provide a full benefits package including:
Medical, Dental, and Vision
401(k) with company match
Commuter benefits (ORCA card or parking stipend)
Generous PTO and Volunteer Time Off
Eight paid holidays
Full kitchen with coffee and snacks
An amazing office one block from Pike Place Market with views of Elliot Bay."," Bachelorâs Degree in computer science or relevant information technology field 3-4 years of DBA experience 3-4 years of SQL replication experience and managing complex ETL processes  Ability to communicate effectively with stakeholders to define requirements Proficiency in writing SQL procedures and functions for administration and application support Strong knowledge of operating systems, shell scripting, and python scripting Good interpersonal skills along with effective communication  both written and verbal  Demonstrated ability to solve complex systems and database environment issues Experience with cloud platforms  AWS and AZURE  Experience building data pipelines & ETL Experience with command line Experience with version control software  git  and best practices Demonstrated experience leading database architecture development Experience with big data technologies at scale Strong understanding of PostgreSQL database fundamentals Strong understanding of database security management Experience with ETL tools Experience with BI tools Willingness to take ownership over wide range of databases and processes  Collaborate with senior management, product management, data analytics, and web/app developers in the development of data products, pipelines, and infrastructure Develop reliable and near real time data pipelines that make data easily consumable by end users and other systems that we use at All Star Develop tools to monitor, debug, and analyze data pipeline health Design and implement data schemas and models that can scale Mentor technology team members to build the company's overall expertise Administer all database automation and take corrective action when required  ","Bachelorâs Degree in computer science or relevant information technology field 3-4 years of DBA experience SQL replication and managing complex ETL processes Ability to communicate effectively with stakeholders define requirements Proficiency writing procedures functions for administration application support Strong knowledge operating systems, shell scripting, python scripting Good interpersonal skills along effective communication both written verbal Demonstrated ability solve systems database environment issues Experience cloud platforms AWS AZURE building data pipelines & command line version control software git best practices leading architecture development big technologies at scale understanding PostgreSQL fundamentals security management tools BI Willingness take ownership over wide range databases Collaborate senior management, product analytics, web/app developers the products, pipelines, infrastructure Develop reliable near real time that make easily consumable by end users other we use All Star monitor, debug, analyze pipeline health Design implement schemas models can Mentor team members build company's overall expertise Administer all automation corrective action when required","Bachelorâs Degree computer science relevant information technology field 3-4 years DBA experience SQL replication managing complex ETL processes Ability communicate effectively stakeholders define requirements Proficiency writing procedures functions administration application support Strong knowledge operating systems, shell scripting, python scripting Good interpersonal skills along effective communication written verbal Demonstrated ability solve systems database environment issues Experience cloud platforms AWS AZURE building data pipelines & command line version control software git best practices leading architecture development big technologies scale understanding PostgreSQL fundamentals security management tools BI Willingness take ownership wide range databases Collaborate senior management, product analytics, web/app developers products, pipelines, infrastructure Develop reliable near real time make easily consumable end users use All Star monitor, debug, analyze pipeline health Design implement schemas models Mentor team members build company's overall expertise Administer automation corrective action required"
377,Data Engineer,Big Data Engineer,"Renton, WA",Renton,WA,"Job Details
Job Code
JPSC-6693
Posted Date
01/12/18
Experience
8 Years
Primary Skills
Oracle,FIORI,sql server,Hive,MapReduce,HDFS,Oozie,â¢ 5-years in SQL,JDBC â¢ 5-years in Hadoop,YARN â¢ 5-years in Sqoop,Parquet
Required Documents
Resume
Overview
Role: Big Data Engineer
Location: Renton, Washington
Duration: 6 Months
Top Three Skills:
5-years in SQL, SQL Server, Oracle, JDBC
5-years in Hadoop, HDFS, MapReduce, YARN
5-years in Sqoop, Oozie, Parquet, Hive, Impala, Spark, HBase, HUE

Job Description:
Client is looking for a Big Data, Data Engineer.
This is deployed on the Microsoft Azure platform using core Cloudera technologies such as Cloudera Director 2.1, CDH 5.7, and Cloudera Manager along with Apache Hive, Yarn, HBase, and Spark.
Cloudera based data lake
CDH 5.7.3 on Azure cloud platform
Moving data from large SQL Server based tables into lake is biggest challenge
Concerns around Incremental data load strategy
Performance and Scalability issues in order to meet SLA with business teams
Limited internal bandwidth and skill set on No SQL database (Hbase)
Data Source - Epic (Clarity DB)
Mostly all structure data sets to ingest to lake
Large volume of data in some tables
 Please Fill up following details and send me back ASAP if youâre interested in this Position.

Full Name:
Email id:
Contact Information:
Current Location:
Visa status (Need Visa copy):
Visa Validity:
Availability:
Preferred interview timings:
Are you ready for F2F Interview:
Willingness to relocate across US:
Reason for looking new project :
Year of Graduation & Degree & university Name:
Date of Birth:
Skype ID:
Last 4 Digits of SSN:
References:
Details
Reference-1
Reference-2

Full Name

Company

Designation

Contact/Email

Syed Raza
585 532 7200 Ext 9002
Syed.j@avanitechsolutions.com"," 5-years in SQL, SQL Server, Oracle, JDBC 5-years in Hadoop, HDFS, MapReduce, YARN 5-years in Sqoop, Oozie, Parquet, Hive, Impala, Spark, HBase, HUE    ","5-years in SQL, SQL Server, Oracle, JDBC Hadoop, HDFS, MapReduce, YARN Sqoop, Oozie, Parquet, Hive, Impala, Spark, HBase, HUE","5-years SQL, SQL Server, Oracle, JDBC Hadoop, HDFS, MapReduce, YARN Sqoop, Oozie, Parquet, Hive, Impala, Spark, HBase, HUE"
378,Data Engineer,Data Engineer,"Issaquah, WA",Issaquah,WA,"Job Details
Level
Experienced
Job Location
Issaquah - Issaquah, WA
Position Type
Full Time
Education Level
Graduate Degree
Who We Are
Weâre Talking Rain, and weâre so much more than water. We build flavorful brands, like Sparkling Ice and Talking Rain Sparkling Waters, creating connections with every sip!
Want to get to know us better? Click to learn about us and our careers. You can also follow us on LinkedIn, Instagram, Twitter and Facebook.
When it comes to success, we know weâre only as strong as our team, so we empower our Rain Makers to forge the wayâfinding opportunity at the intersection of what they love, what we need, and where they thrive.
What to Expect
Data is more than stats to our Business Transformation teamâitâs a story waiting to be told. They translate the details into actionable information and strategy, helping us drive forward.

Youâll form partnerships across our business to help define and drive our strategy, reducing complexity within our operations and enhancing the effectiveness of our marketing activities.
Serve as a subject matter expert in Data Science, developing strong relationships with partners across Talking Rain as you conduct and support white-boarding sessions, workshops, design sessions, and project meetings.
Deliver solutions leveraging the emerging machine learning (ML) methods and technologies:
Big Data and streaming analytics
Exploratory data analysis (EDA) & cleansing
Feature engineering
Model selection, model evaluation, and cross-validation
Hyperparameter tuning, containerization, and deployment at all scales
Own productionalization and ongoing performance tuning for all models.
Ensure weâre ahead of the curve, staying abreast of the ever-shifting retail landscape and how we can best leverage syndicated data.
Develop custom data models and algorithms to generate predictive insights.
Ensure food safety, quality, and SQF practices are followed at all times, notifying immediate manager of any food safety and/or quality issues.
Complete other responsibilities as assigned.
Reports to: Senior Manager, Business Transformation
Direct Reports: N/A
What You Bring
Advanced degree in a highly quantitative field (e.g., MS or PhD in Data Science, Computer Science, Mathematics, or Physics).
Minimum of five (5) years of experience building and leading the development and productionalization of applied machine learning solutions and data products, ideally in consumer packaged goods (CPG).
Experience with traditional predictive ML models, such as decision trees, ensemble learning & random forests, KNN, support vector machines, and recommender systems.
Advanced knowledge of Python and/or R and their Data Science frameworks and libraries strongly preferred, such as PySpark, MLlib, SciKit Learn, MLFlow, Delta Lake, Koalas, and the Databricks Unified Analytics Platform.
Working knowledge of SQL, Apache Spark, Kafka, Pandas, and NumPy for the use of data extraction, cleansing, and analysis.
Experience using data visualization tools such as D3.js, Tableau, Qlik, or Power BI is a strong plus.
Experience leveraging Azure or AWS cognitive service APIs is a plus.
Highly refined communication skills, with the ability to speak both to technical engineers and executive stakeholders.
Integrity is at the core of who you are. You earn peopleâs trust with your ability to maintain confidentiality, particularly when handling sensitive information and situations.
You drive, not ride. Youâre stellar at prioritization, time-management, and troubleshooting.
Your curious mindset enables you to interpret complex data, developing and communicating value-add solutions.
Youâre approachable, actively listening and adapting your style to the audience.
Youâre flexible. You pivot on a dime and embrace the change.
To help ensure a safe, positive work environment, this role may be subject to random drug testing (including alcohol, cannabinoids, cocaine, methamphetamine, opiates, phencyclidine).","   Serve as a subject matter expert in Data Science, developing strong relationships with partners across Talking Rain as you conduct and support white-boarding sessions, workshops, design sessions, and project meetings. Deliver solutions leveraging the emerging machine learning  ML  methods and technologies  Big Data and streaming analytics Exploratory data analysis  EDA  & cleansing Feature engineering Model selection, model evaluation, and cross-validation Hyperparameter tuning, containerization, and deployment at all scales Own productionalization and ongoing performance tuning for all models. Ensure weâre ahead of the curve, staying abreast of the ever-shifting retail landscape and how we can best leverage syndicated data. Develop custom data models and algorithms to generate predictive insights. Ensure food safety, quality, and SQF practices are followed at all times, notifying immediate manager of any food safety and/or quality issues. Complete other responsibilities as assigned.  ","Serve as a subject matter expert in Data Science, developing strong relationships with partners across Talking Rain you conduct and support white-boarding sessions, workshops, design project meetings. Deliver solutions leveraging the emerging machine learning ML methods technologies Big streaming analytics Exploratory data analysis EDA & cleansing Feature engineering Model selection, model evaluation, cross-validation Hyperparameter tuning, containerization, deployment at all scales Own productionalization ongoing performance tuning for models. Ensure weâre ahead of curve, staying abreast ever-shifting retail landscape how we can best leverage syndicated data. Develop custom models algorithms to generate predictive insights. food safety, quality, SQF practices are followed times, notifying immediate manager any safety and/or quality issues. Complete other responsibilities assigned.","Serve subject matter expert Data Science, developing strong relationships partners across Talking Rain conduct support white-boarding sessions, workshops, design project meetings. Deliver solutions leveraging emerging machine learning ML methods technologies Big streaming analytics Exploratory data analysis EDA & cleansing Feature engineering Model selection, model evaluation, cross-validation Hyperparameter tuning, containerization, deployment scales Own productionalization ongoing performance tuning models. Ensure weâre ahead curve, staying abreast ever-shifting retail landscape best leverage syndicated data. Develop custom models algorithms generate predictive insights. food safety, quality, SQF practices followed times, notifying immediate manager safety and/or quality issues. Complete responsibilities assigned."
379,Data Engineer,Data Engineer,"Seattle, WA",Seattle,WA,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ","Expertise in at least one of the following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing more languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting other customer-facing role","Expertise least one following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role"
380,Data Engineer,Data Engineering Manager,"Seattle, WA",Seattle,WA,"Join SADA as a Data Engineering Manager!

Your Mission

As a Data Engineering Manager at SADA, you will build and lead a growing Data Engineering team as we deliver robust data solutions for our clients on Google Cloud Platform (GCP). You will be responsible for managing a blended team of data engineers and data scientists, so a broad background in Big Data, data warehouse modernization, analytics, disaster recovery, data science, and machine learning is highly advantageous.

The diversity of customers that SADA works with ensures a steady flow of challenging data work. Be prepared to tackle real-world data problems that our customers find too difficult or time-consuming to solve themselves. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of data domain areas. Management here at SADA also means developing people and being a leader.

In this role, you will:

Be comfortable working with customer executives to align business outcomes with technical vision and goals.
Guide the day-to-day activities of a geographically distributed team, including hiring world-class talent, reviewing work and setting goals.
Provide technical and professional leadership and mentorship on a diverse range of subject matter areas, such as Big Data pipelines and data warehouses to statistics and machine learning.
Develop and codify best practices for your team that can be replicated across multiple customer engagements.
Partner with your team to develop services and offerings that scale and are repeatable.
Participate in key technical and design discussions with technical leads as a hands-on manager.
Partner with other practice leads, architects, project managers, executives and sales personnel to develop statements of work, and then oversee execution by your team with high levels of agility and quality.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our employees know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing data practice area with vision and passion. You will be measured by your teamâs performance on customer engagements, how well your team achieves internal organizational goals, how well you collaborate with and support your team and peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the management growth track.

Expectations


Required Travel - 15-25% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Experience leading, managing and hiring a team of talented engineers
Expertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Expertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)
Natural language processing (e.g., conversational chatbots)
Document understanding
Image classification
Marketing analytics
IoT systems
Experience writing software in one or more languages such as Python or Java/Scala
Experience in technical consulting or customer-facing role
Excellent critical thinking, problem-solving and analytical skills

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience in a large scale, high-volume data warehouse environment
Experience operationalizing machine learning models on large datasets
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Experience leading, managing and hiring a team of talented engineers Expertise in at least one of the following engineering domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Expertise in at least one of the following data domains    Predictive analytics  e.g., recommendation systems, predictive maintenance  Natural language processing  e.g., conversational chatbots  Document understanding Image classification Marketing analytics IoT systems Experience writing software in one or more languages such as Python or Java/Scala Experience in technical consulting or customer-facing role Excellent critical thinking, problem-solving and analytical skills     ","Experience leading, managing and hiring a team of talented engineers Expertise in at least one the following engineering domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. domains Predictive analytics e.g., recommendation systems, predictive maintenance Natural language conversational chatbots Document understanding Image classification Marketing IoT systems writing more languages Python Java/Scala consulting customer-facing role Excellent critical thinking, problem-solving analytical skills","Experience leading, managing hiring team talented engineers Expertise least one following engineering domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. domains Predictive analytics e.g., recommendation systems, predictive maintenance Natural language conversational chatbots Document understanding Image classification Marketing IoT systems writing languages Python Java/Scala consulting customer-facing role Excellent critical thinking, problem-solving analytical skills"
381,Data Engineer,Senior Data Engineer,"Seattle, WA",Seattle,WA,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Hihg
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Mastery in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or customer-facing role     ","Mastery in at least one of the following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing more languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role","Mastery least one following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role"
382,Data Engineer,CORE - Data Engineer,"Seattle, WA 98115",Seattle,WA,"Lead Data Engineer

Greater Seattle Area
About the role. . .
In order to continue and accelerate our growth, we are looking for a Lead Data Engineer with Cloud Solutions background to add to our Seattle, Washington-based team.
Lead Engineer is responsible for building a large-scale data pipeline in cloud platform. This may involve in automation of manual processes to cloud environment. Candidate would direct the initiatives for creation of data sets. Delivering client value and ensuring high client satisfaction.
Core responsibilities for this position include, but are not limited to the following:
Extracts data from various databases; perform exploratory data analysis, cleanses, massages, and aggregates data
Employs scaling & automation to data preparation techniques - Introduces incremental improvements to data analysis, visualization, and presentation techniques to communicate discoveries
Researches relevant emerging empirical methods and quantitative tools
Possesses in-depth business knowledge in order to initiate and drive discussions with business partners to identify business issues needing analytic solutions
Leads innovative packaging and presentation of insights to business and broader analytics community
Develops processes to automate and scale insights operationalization
Develops and drives multiple cross-departmental projects
Establishes brand and team as subject matter experts in advanced analytics across departments.
Mentors data scientists in pioneering techniques and business acumen
Required Qualifications:
Cloud solution implementation experience with Azure Data Lake and Spark preferred
Minimum 8 years hands-on experience with SQL
At least one year of experience in scripting languages such as Python
Demonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform
Big data processing techniques, preferred
Can work independently in ambiguous environment
About Logic20/20. . .
Logic20/20 is one of Seattleâs fastest growing full service consulting firms. Our core competency is creating simplicity and efficiency in complex solutions. Although we make it look like magic, we succeed by combining methodical and structured approaches with our substantial experience to design elegant solutions for even the most intricate challenges. Our rapid growth is in response to our ability to deliver consistently for our clients, which is directly related to the quality of the people we hire.
The past four years, weâve been in the top 10 âBest Companies to Work Forâ â¦.. why? Our team members are highly self-motivated, comfortable conceiving strategies on the fly, and enjoy working both individually and as part of a team. Our environment is very high-energy and demanding, and individuals with remarkable enthusiasm and a can-do attitude are joining our team. We have lots of fun, focus on our employees and our clients, and work to bring our best to every opportunity."," Cloud solution implementation experience with Azure Data Lake and Spark preferred Minimum 8 years hands-on experience with SQL At least one year of experience in scripting languages such as Python Demonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform Big data processing techniques, preferred Can work independently in ambiguous environment    ","Cloud solution implementation experience with Azure Data Lake and Spark preferred Minimum 8 years hands-on SQL At least one year of in scripting languages such as Python Demonstrated a cloud-based -computing environment AWS, Azure, or Google Platform Big data processing techniques, Can work independently ambiguous","Cloud solution implementation experience Azure Data Lake Spark preferred Minimum 8 years hands-on SQL At least one year scripting languages Python Demonstrated cloud-based -computing environment AWS, Azure, Google Platform Big data processing techniques, Can work independently ambiguous"
383,Data Engineer,Senior Data Engineer,"Redmond, WA",Redmond,WA,"Microsoft Managed Desktop (MMD) is an important new MS offering. As an engineering organization we take a significant role in the deployment and management of the corporate desktop computing platforms for our customers. We leverage all of Microsoft 365, Enterprise cloud management and security offerings.

It has never been a more exciting time for us to have this offering in the market. MMD is leading with a cloud-first approach and using devices and integrated software offerings to provide great experience for our business customers. Weâre working as a startup team in a dynamic engineering organization, building and leveraging key features into the broader Microsoft cloud technology stack like:
Windows 10 (Windows as a Service, Windows Defender Suite + Advanced Threat Protection, Autopilot, imageless deployment, etc.)
Office 365
Cloud Identity and management infrastructure (Azure Active Directory & Intune)

MMD is a new end-to-end offering in the market revolutionizing the experience of end users using Windows for their work. You will get an amazing opportunity to extend your knowledge of several important Microsoft products and technical areas listed above.
Responsibilities
Be part of a small, agile team working with experienced engineers that behaves more like a start-up than an established team.
Leverage Microsoft BI Suites to provide actionable insights into customer acquisition, and other key business performance metrics
Engineer a modern data pipeline to collect, organize, and process data
Produce clean, reusable code that is unit tested, code reviewed, and adheres to code standards
Lead and facilitate a close development partnership with our Multi-tenant Operations team
Drive the design and creation of solutions that allow MMD to offer a high level of service â reliable, available, and scalable
Ideate and propose new product innovations to meet the needs of our customers.
Qualifications
Required Qualifications:
Bachelorâs degree in Computer Science, Engineering, or closely related field
8+ years of experience designing and developing software
Strong scripting skills to perform data/file manipulation
5+years of experience with data aggregation platforms based on technologies such as SQL, Azure Data Lake, Hadoop, Cosmos, CosmosDB, HDInsights etc.
Solid understanding and proven skills in raw and processed stream design, relational database design and dimensional models
Solid understanding of event processing including publish/subscribe mechanisms
Demonstrated ability to create and ship high quality code by using engineering best practices.
Hands on experience in big data components.
Experience with data warehousing and datamart design and implementation
Experience with data security and compliance (PII, PHI, GDPR etc.)
Strong understanding and inner workings of metadata management, data lineage, and data governance
Strong experience in structured, unstructured, semi-structured data techniques and processes

Preferred Qualifications:
Comfortable learning and growing in a fast paced, start-up environment.
Proven track record of strong customer advocate and creative problem solver
Ambitious, self-motivated, proactive, and results-oriented
Experience using scalable data ingestion and transformation systems in batch and near real time environments (Î»-lambda architecture)
Experience with SQL (T-SQL), relational modeling, and big data tools such as Hive, U-SQL (Scope) or Spark (experience with Spark is a plus)
Experience with ETL, data modeling, and working with Business Intelligence systems.
Experience with machine learning and predictive analytics is a plus.
Azure Data Factory or Integration Services Experiences is a plus.

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work."," Bachelorâs degree in Computer Science, Engineering, or closely related field 8+ years of experience designing and developing software Strong scripting skills to perform data/file manipulation 5+years of experience with data aggregation platforms based on technologies such as SQL, Azure Data Lake, Hadoop, Cosmos, CosmosDB, HDInsights etc. Solid understanding and proven skills in raw and processed stream design, relational database design and dimensional models Solid understanding of event processing including publish/subscribe mechanisms Demonstrated ability to create and ship high quality code by using engineering best practices. Hands on experience in big data components. Experience with data warehousing and datamart design and implementation Experience with data security and compliance  PII, PHI, GDPR etc.  Strong understanding and inner workings of metadata management, data lineage, and data governance Strong experience in structured, unstructured, semi-structured data techniques and processes   Be part of a small, agile team working with experienced engineers that behaves more like a start-up than an established team. Leverage Microsoft BI Suites to provide actionable insights into customer acquisition, and other key business performance metrics Engineer a modern data pipeline to collect, organize, and process data Produce clean, reusable code that is unit tested, code reviewed, and adheres to code standards Lead and facilitate a close development partnership with our Multi-tenant Operations team Drive the design and creation of solutions that allow MMD to offer a high level of service â reliable, available, and scalable Ideate and propose new product innovations to meet the needs of our customers.  ","Bachelorâs degree in Computer Science, Engineering, or closely related field 8+ years of experience designing and developing software Strong scripting skills to perform data/file manipulation 5+years with data aggregation platforms based on technologies such as SQL, Azure Data Lake, Hadoop, Cosmos, CosmosDB, HDInsights etc. Solid understanding proven raw processed stream design, relational database design dimensional models event processing including publish/subscribe mechanisms Demonstrated ability create ship high quality code by using engineering best practices. Hands big components. Experience warehousing datamart implementation security compliance PII, PHI, GDPR inner workings metadata management, lineage, governance structured, unstructured, semi-structured techniques processes Be part a small, agile team working experienced engineers that behaves more like start-up than an established team. Leverage Microsoft BI Suites provide actionable insights into customer acquisition, other key business performance metrics Engineer modern pipeline collect, organize, process Produce clean, reusable is unit tested, reviewed, adheres standards Lead facilitate close development partnership our Multi-tenant Operations Drive the creation solutions allow MMD offer level service â reliable, available, scalable Ideate propose new product innovations meet needs customers.","Bachelorâs degree Computer Science, Engineering, closely related field 8+ years experience designing developing software Strong scripting skills perform data/file manipulation 5+years data aggregation platforms based technologies SQL, Azure Data Lake, Hadoop, Cosmos, CosmosDB, HDInsights etc. Solid understanding proven raw processed stream design, relational database design dimensional models event processing including publish/subscribe mechanisms Demonstrated ability create ship high quality code using engineering best practices. Hands big components. Experience warehousing datamart implementation security compliance PII, PHI, GDPR inner workings metadata management, lineage, governance structured, unstructured, semi-structured techniques processes Be part small, agile team working experienced engineers behaves like start-up established team. Leverage Microsoft BI Suites provide actionable insights customer acquisition, key business performance metrics Engineer modern pipeline collect, organize, process Produce clean, reusable unit tested, reviewed, adheres standards Lead facilitate close development partnership Multi-tenant Operations Drive creation solutions allow MMD offer level service â reliable, available, scalable Ideate propose new product innovations meet needs customers."
384,Data Engineer,Data Engineer in Applied Machine Learning Ã¢â¬â Intermediate,"Seattle, WA",Seattle,WA,"Organization and Job ID
Job ID: 309746
Directorate: National Security Directorate
Division: Computing and Analytics Division
Group: Data Sciences and Analytics
Job Description
Do you want to create a legacy of meaningful research for the greater good? Do you want to lead and contribute to work in support of an organization that addresses some of todayâs most challenging problems that face our Nation? Then join us in the Data Sciences and Analytics Group at the Pacific Northwest National Laboratory (PNNL)!
For more than 50 years, PNNL has advanced the frontiers of science and engineering in the service of our nation and the world in the areas of energy, the environment and national security. PNNL is committed to advancing the state-of-the-art in artificial intelligence through applied machine learning and deep learning to support scientific discovery and our sponsorsâ missions. Help us advance this frontier and protect our nation!
Data Engineering at the Pacific Northwest National Laboratory (PNNL) addresses critical national and global issues by developing data processing/storage systems utilizing cloud, parallel and distributed architectures. We provide capabilities in data modeling and the design, development and deployment of relational, data warehousing, non-relational, and streaming systems. Also focuses on high-performance extract-transform-load processes and big data services that form the foundation of our data science research.
Job Description:
The Data Engineer should have working knowledge in several of the following related-skillsets:

Acquiring/integrating data within a distributed environment (e.g., AWS, Azure, OpenStack, Hadoop)

Data pipeline development (e.g. Spark, NiFi, Kafka, AWS Glue, Kinesis)

Data cleansing and ETL (e.g. Python, SQL)

Scalable/massively parallel databases (e.g. Hive, Redshift, Impala, Athena)

NoSQL databases and cache/indexing services (e.g. Elasticsearch, HBase, MongoDB, Redis).

Interest, curiosity and technical depth to support the development and advancement of a variety of applied problems specific to the national security community.

Discipline, principal job duties/expectations, and qualitative and quantitative measures of performance that exceed the Functional Descriptor:
Technical knowledge in developing and deploying applications in multiple environments â cloud, container services, clusters.

Contribute to the technical content of proposals and technical products such as journal and conference publications technical presentations, and software releases will be expected. Must possess excellent verbal and written communication skills.

The hiring level will be determined based on the education, experience and skill set of the successful candidate based on the following:
Level 2

Design and implement technical approaches to well-defined tasks supporting larger projects.

Mentoring junior staff and students

Work effectively in a dynamic team environment with high expectations for quality.

Level 3

Lead medium sized projects or large tasks.

Key role in defining technical approach and setting technical direction.

Provides solutions to an extensive range of complex and/or ambiguous problems.

Minimum Qualifications
Bachelor's degree in computer science or closely related field with strong software development and machine learning skills with 2 years' experience or a Master's degree with 0-2 years' experience, or a Ph.D with 0 yearsâ experience is required.
Preferred Qualifications
Bachelor's degree in computer science or closely related field with strong software development and machine learning skills with 5 years of experience, MS/MA with 3 years of experience, or PhD with 1 year of experience.
Equal Employment Opportunity
Battelle Memorial Institute (BMI) at Pacific Northwest National Laboratory (PNNL) is an Affirmative Action/Equal Opportunity Employer and supports diversity in the workplace. All employment decisions are made without regard to race, color, religion, sex, national origin, age, disability, veteran status, marital or family status, sexual orientation, gender identity, or genetic information. All BMI staff must be able to demonstrate the legal right to work in the United States. BMI is an E-Verify employer. Learn more at jobs.pnnl.gov.
Other Information
This position requires the ability to obtain and maintain a federal security clearance.
Requirements:
U.S. Citizenship

Background Investigation: Applicants selected will be subject to a Federal background investigation and must meet eligibility requirements for access to classified matter in accordance 10 CFR 710, Appendix B.

Drug Testing: All Security Clearance (L or Q) positions will be considered by the Department of Energy to be Testing Designated Positions which means that they are subject to applicant, random, and for cause drug testing. In addition, applicants must be able to demonstrate non-use of illegal drugs, including marijuana, for the 12 consecutive months preceding completion of the requisite Questionnaire for National Security Positions (QNSP).

Note: Applicants will be considered ineligible for security clearance processing by the U.S. Department of Energy until non-use of illegal drugs, including marijuana, for 12 consecutive months can be demonstrated.
Directorate: National Security Dir
Job Category: Computation and Information Sciences
Group: Data Sciences & Analytics
Opening Date: 2019-09-03
Closing Date: 2019-10-18","Bachelor's degree in computer science or closely related field with strong software development and machine learning skills with 2 years' experience or a Master's degree with 0-2 years' experience, or a Ph.D with 0 yearsâ experience is required.     ","Bachelor's degree in computer science or closely related field with strong software development and machine learning skills 2 years' experience a Master's 0-2 experience, Ph.D 0 yearsâ is required.","Bachelor's degree computer science closely related field strong software development machine learning skills 2 years' experience Master's 0-2 experience, Ph.D 0 yearsâ required."
385,Data Engineer,Senior Data Engineer,"Seattle, WA 98103",Seattle,WA,"Our mission is to protect life.
Weâre out to make the world a safer place by solving big problems and taking on the public safety challenges of our time. From our company's inception building the TASER device to a full suite of hardware and software solutions, we are focused on providing police agencies with the state-of-the-art devices and services they need to successfully serve and protect us. In the next few years, we're going to eliminate the burden of paperwork in policing, so officers can increase the time they spend building relationships and serving in their communities. Weâll put video at the heart of the police record so our justice system can get to the truth faster. And we won't stop innovating until the bullet is rendered obsolete.

Itâs a big mission, but itâs one weâll pursue relentlessly every single day.

Your Impact

You are a Senior Full Stack Software Engineer with experience building large-scale software applications. At Axon, youâll create and maintain a data architecture that is the connective tissue between Axon products and public safety systems. Successfully completing this work means that you are:
Protecting life by surfacing key information to ensure Officer and Jail SafetyEnabling Crime Analysts to unlock that missing piece of data to solve crimeAllowing justice to be swift and accurate by giving Prosecutors and Courts accurate informationAllowing Investigators and Forensic Technicians easy access to crime scene evidence

When you are successful, you will equip public safety professionals at all levels with the information they need to protect life and truth.

Your Day to Day
Help build one of the largest cloud solutions on the planet. What you build will accelerate product innovation and help scale our platform to meet the ever-expanding needs of our growing customer base.
Partner with internal teams and agencies to make law enforcement data highly accessible and actionable.
Develop the core platform capabilities that support Axon's product development and evolution - at scale.
Solve challenging problems of scale, latency, reliability, and availability. Our team will draw from your experience in these areas and support your continued growth.
Write performant, maintainable code that is easy to read and well-documented.
Basic Qualifications
Bachelor CS degree and 7+ years of experience in software engineering.
Track record of developing and maintaining reliable, highly available, secure, high throughput web-scale data systems (e.g. Social, AdTech, MarTech, Heathcare, FinTech, etc.).
Experience with big data pipelines and processing (e.g. MapReduce, Hadoop, Big Query, Hive, Tez, Spark, etc.).
Experience with realtime streaming event logs (e.g. Kafka, GCP Cloud Pub/Sub, SNS/SQS).
You influence your peers, advise senior leaders, coach and mentor junior team members.
You facilitate cross-team collaboration among engineers and contribute to the broader community of senior engineers.
Must pass a Criminal Justice Information Services (CJIS) background check and maintain confidential and highly sensitive information.
Preferred Qualifications
Experience with Azure cloud components and .NET.
Experience creating and maintaining containerized web applications and serverless components.
Familiarity with build and CI tools/processes like Kubernetes, TeamCity, Azure DevOps, etc.
ETL Development for Business Intelligence.
Experience using statistical programming languages (e.g. Python, R) and to deliver results for real-world problems.
Compensation and Benefits
Competitive salary and 401K with employer match
Discretionary paid time off
Robust parental leave policy
An award-winning office/working environment
Ride along with real police officers in real life situations, see them use technology, get inspired
And more...
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."," Bachelor CS degree and 7+ years of experience in software engineering. Track record of developing and maintaining reliable, highly available, secure, high throughput web-scale data systems  e.g. Social, AdTech, MarTech, Heathcare, FinTech, etc. . Experience with big data pipelines and processing  e.g. MapReduce, Hadoop, Big Query, Hive, Tez, Spark, etc. . Experience with realtime streaming event logs  e.g. Kafka, GCP Cloud Pub/Sub, SNS/SQS . You influence your peers, advise senior leaders, coach and mentor junior team members. You facilitate cross-team collaboration among engineers and contribute to the broader community of senior engineers. Must pass a Criminal Justice Information Services  CJIS  background check and maintain confidential and highly sensitive information.    ","Bachelor CS degree and 7+ years of experience in software engineering. Track record developing maintaining reliable, highly available, secure, high throughput web-scale data systems e.g. Social, AdTech, MarTech, Heathcare, FinTech, etc. . Experience with big pipelines processing MapReduce, Hadoop, Big Query, Hive, Tez, Spark, realtime streaming event logs Kafka, GCP Cloud Pub/Sub, SNS/SQS You influence your peers, advise senior leaders, coach mentor junior team members. facilitate cross-team collaboration among engineers contribute to the broader community engineers. Must pass a Criminal Justice Information Services CJIS background check maintain confidential sensitive information.","Bachelor CS degree 7+ years experience software engineering. Track record developing maintaining reliable, highly available, secure, high throughput web-scale data systems e.g. Social, AdTech, MarTech, Heathcare, FinTech, etc. . Experience big pipelines processing MapReduce, Hadoop, Big Query, Hive, Tez, Spark, realtime streaming event logs Kafka, GCP Cloud Pub/Sub, SNS/SQS You influence peers, advise senior leaders, coach mentor junior team members. facilitate cross-team collaboration among engineers contribute broader community engineers. Must pass Criminal Justice Information Services CJIS background check maintain confidential sensitive information."
386,Data Engineer,Data Engineer,"Bellevue, WA 98004",Bellevue,WA,"Overview
Responsible for unlocking the value in our ever growing data by creating new and improved techniques and solutions for data collection, management, usage and reporting.
Responsibilities
Core Accountabilities:
Collect, transform, analyze, and refine operational and customer data.
Build-out data structures designed to efficiently answer business questions.
Assist in evolving data structures from a MSSQL Server footprint into a data lake environment.
Develop, implement and tune current ETL processes.
Assist with ad hoc report generation and analysis for merchandise and customers.
Create pipelines from internal and external data sources to AWS using custom python scripts.
Assist in managing our Tableau ecosystem and provide technical support.
Qualifications
Education/Experience Required:
Bachelorâs degree required
3-5 years of experience in database technologies
Minimum 3 years of experience in Data Warehousing
Experience with AWS tools (S3/EC2/Athena/Redshift Spectrum/IAM)
Experience with Python
Familiarity with Tableau or other data visualization tools.
Experienced with Linux administration and scripting
Deep experience with data modeling, data pipelines, SQL, AWS, and distributed compute platforms
Experience building effective relationships with a broad range of partners
Ability to communicate well with partners, both technical and non-technical

Physical Requirements:
The physical demands described here are representative of those that are required by an associate to successfully perform the essential functions of this job.
While performing the duties of this job, the associate is regularly required to talk or hear. The associate is frequently required to sit; stand; walk; use hands to finger, handle or feel; as well as reach with hands and arms.
The associate must frequently lift and/or move up to 15 pounds and occasionally lift and/or move up to 35 pounds. Specific vision abilities required by this job include close vision, distance vision, depth perception and ability to adjust focus.
Ability to work in open environment with fluctuating temperatures and standard lighting
Ability to work on computer and mobile phone for multiple hours; with frequent interruptions
Required to travel in elevator or stairwells to attend meetings and engage with associates on multiple floors throughout building
Hotel, Airplane, and Car Travel Required

Position Type/Expected Hours of Work:
This is a full-time position. As an International Retailer, occasional evening and/or weekend work may be required during periods of high volume. This role operates in a professional office environment and routinely uses standard office equipment.

Other Considerations:
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the associate for this job. Duties, responsibilities and activities may change at any time with or without notice. Reasonable accommodations may be made to qualified individuals with disabilities to enable them to perform the essential functions of the role."," Bachelorâs degree required 3-5 years of experience in database technologies Minimum 3 years of experience in Data Warehousing Experience with AWS tools  S3/EC2/Athena/Redshift Spectrum/IAM  Experience with Python Familiarity with Tableau or other data visualization tools. Experienced with Linux administration and scripting Deep experience with data modeling, data pipelines, SQL, AWS, and distributed compute platforms Experience building effective relationships with a broad range of partners Ability to communicate well with partners, both technical and non-technical   Collect, transform, analyze, and refine operational and customer data. Build-out data structures designed to efficiently answer business questions. Assist in evolving data structures from a MSSQL Server footprint into a data lake environment. Develop, implement and tune current ETL processes. Assist with ad hoc report generation and analysis for merchandise and customers. Create pipelines from internal and external data sources to AWS using custom python scripts. Assist in managing our Tableau ecosystem and provide technical support.  Bachelorâs degree required 3-5 years of experience in database technologies Minimum 3 years of experience in Data Warehousing Experience with AWS tools  S3/EC2/Athena/Redshift Spectrum/IAM  Experience with Python Familiarity with Tableau or other data visualization tools. Experienced with Linux administration and scripting Deep experience with data modeling, data pipelines, SQL, AWS, and distributed compute platforms Experience building effective relationships with a broad range of partners Ability to communicate well with partners, both technical and non-technical  While performing the duties of this job, the associate is regularly required to talk or hear. The associate is frequently required to sit; stand; walk; use hands to finger, handle or feel; as well as reach with hands and arms. The associate must frequently lift and/or move up to 15 pounds and occasionally lift and/or move up to 35 pounds. Specific vision abilities required by this job include close vision, distance vision, depth perception and ability to adjust focus. Ability to work in open environment with fluctuating temperatures and standard lighting Ability to work on computer and mobile phone for multiple hours; with frequent interruptions Required to travel in elevator or stairwells to attend meetings and engage with associates on multiple floors throughout building Hotel, Airplane, and Car Travel Required","Bachelorâs degree required 3-5 years of experience in database technologies Minimum 3 Data Warehousing Experience with AWS tools S3/EC2/Athena/Redshift Spectrum/IAM Python Familiarity Tableau or other data visualization tools. Experienced Linux administration and scripting Deep modeling, pipelines, SQL, AWS, distributed compute platforms building effective relationships a broad range partners Ability to communicate well partners, both technical non-technical Collect, transform, analyze, refine operational customer data. Build-out structures designed efficiently answer business questions. Assist evolving from MSSQL Server footprint into lake environment. Develop, implement tune current ETL processes. ad hoc report generation analysis for merchandise customers. Create pipelines internal external sources using custom python scripts. managing our ecosystem provide support. While performing the duties this job, associate is regularly talk hear. The frequently sit; stand; walk; use hands finger, handle feel; as reach arms. must lift and/or move up 15 pounds occasionally 35 pounds. Specific vision abilities by job include close vision, distance depth perception ability adjust focus. work open environment fluctuating temperatures standard lighting on computer mobile phone multiple hours; frequent interruptions Required travel elevator stairwells attend meetings engage associates floors throughout Hotel, Airplane, Car Travel","Bachelorâs degree required 3-5 years experience database technologies Minimum 3 Data Warehousing Experience AWS tools S3/EC2/Athena/Redshift Spectrum/IAM Python Familiarity Tableau data visualization tools. Experienced Linux administration scripting Deep modeling, pipelines, SQL, AWS, distributed compute platforms building effective relationships broad range partners Ability communicate well partners, technical non-technical Collect, transform, analyze, refine operational customer data. Build-out structures designed efficiently answer business questions. Assist evolving MSSQL Server footprint lake environment. Develop, implement tune current ETL processes. ad hoc report generation analysis merchandise customers. Create pipelines internal external sources using custom python scripts. managing ecosystem provide support. While performing duties job, associate regularly talk hear. The frequently sit; stand; walk; use hands finger, handle feel; reach arms. must lift and/or move 15 pounds occasionally 35 pounds. Specific vision abilities job include close vision, distance depth perception ability adjust focus. work open environment fluctuating temperatures standard lighting computer mobile phone multiple hours; frequent interruptions Required travel elevator stairwells attend meetings engage associates floors throughout Hotel, Airplane, Car Travel"
387,Data Engineer,Senior Data Engineer,"Bellevue, WA",Bellevue,WA,"Position Summary
About Discovery
Discovery, Inc. is the global leader in real life entertainment. We serve passionate fans with content that inspires, informs, and entertains, providing leadership across deeply loved and trusted brands, such as Discovery Channel, TLC, Animal Planet, HGTV, Food Network, and Travel Channel. Available in 220 countries and territories and 50 languages, Discovery reaches viewers on all screens and services, from free-to-air and pay-TV channels, to digital products and streaming services, to social and mobile-first content and formats. Discovery delivers over 8,000 hours of original programming each year.

About Us
Discovery's Digital group is a well-funded start-up within Discovery, Inc. We are fast, nimble, and have fun developing new, innovative, and immersive digital products and content for iconic brands. We are working at the crossroads of technology, entertainment, and every day utility. As content creators across the digital ecosystem, we continuously leverage our technology to create immersive viewing and interactive experiences. We tell engaging stories to millions of viewers across the Internet every day, and bring new interactive experiences to life to not only entertain, but improve the lives of our customers.

Position Summary
We are hiring a Senior Data Engineer in Bellevue, Washington to help build, scale, and maintain the data infrastructure needed to transform data into actionable insights and science-driven capabilities across our food, home, and lifestyle products. The Senior Data Engineer will be an experienced engineer and leader on the Data Science and Analytics team. They will help advise and create a scalable data ecosystem that encompasses the many data elements produced across our direct-to-consumer products and apps.

This role will build new data sets, scale existing data sets, and create innovative data solutions to support consumer products across mobile, tablets, web, connected TV, voice, and emerging technologies. They will use big data platforms, data warehousing, and business intelligence technology to enable data science, machine learning, and self-service visualization analytics to drive these insights and product capabilities. This senior engineer will ensure the ongoing performance of reliable and efficient data systems using AWS and other data technologies.

An ideal candidate will be a driven, passionate advocate for building and maintaining highly scalable data systems, and using that data to support analytics and solutions that produce great experiences for our customers. This person will make data-driven decisions, have an insatiable curiosity, and obsess about their customers. They will have a strong point of view but remain open-minded should evidence lead in another direction. They are a senior member of the team whom others enjoy working with because they are reliable, innovative, and care for their team members. They build trust with people from all areas of the business and have a proven track record. This senior engineer takes end-to-end ownership and consistently delivers results in a fast-paced environment.
Responsibilities
1. Partner with product, marketing, engineering, and analytics stakeholders to understand what data structures and systems are needed to support key business goals and processes.
2. Work with engineering and partner teams to identify scalable, accessible data sources for analysis and operational use.
3. Design, build, maintain, and improve a scalable data architecture and infrastructure to deliver customer, product, and marketing insights.
4. Design and build new/expanded data sets used to drive insights and models.
5. Partner closely with data analytics and scientists to deliver actionable insights, analytics, and science-driven models (e.g. using machine learning or other modeling techniques).
6. Prepare redundant and scalable systems that efficiently use resources and elegantly handle disruptions.
7. Ensure data integrity and accuracy across all owned data systems and sources.
8. Partner with central data warehousing team to ensure all relevant data is accessible enterprise-wide and meets common standards.
9. Make data engineering techniques approachable and understandable to non-data engineers or scientists.
10. Support the hiring and development of other data engineers, scientists, analysts, and other technical professionals, helping them enhance their skills, while supporting a fun and engaging culture.
Requirements
Minimum of 7+ years of proven technical experience in data engineering, data warehousing, and analytics.Expertise in structuring, cleansing, and preparing large, complex datasets for analysis and modeling.Expertise in building robust and scalable data pipelines using structured and unstructured data as well as batch data and real time streaming data services.Expertise with SQL and other data querying techniques (Scala, Shell).Experience with Apache Spark and Presto.Expertise in data warehousing solutions and techniques.Experience with Amazon Web Services (Redshift, S3, EC2, EMR, etc.) is required.Experience with Adobe Analytics is a plus.Experience with statistical programming languages, such as Python or R is a plus.Demonstrated ability to work across product, engineering, and analytics teams to evaluate new ideas, discuss technical concepts, create scalable designs, and make tradeoffs to remove roadblocks.Strong written and verbal communication skills. Can communicate the results of your work clearly to your audience.Masters in quantitative or technical field (statistics, mathematics, computer sciences, etc.) is a plus.Must have the legal right to work in the United States
Bellevue, Washington, WA, Seattle","  Minimum of 7+ years of proven technical experience in data engineering, data warehousing, and analytics.Expertise in structuring, cleansing, and preparing large, complex datasets for analysis and modeling.Expertise in building robust and scalable data pipelines using structured and unstructured data as well as batch data and real time streaming data services.Expertise with SQL and other data querying techniques  Scala, Shell .Experience with Apache Spark and Presto.Expertise in data warehousing solutions and techniques.Experience with Amazon Web Services  Redshift, S3, EC2, EMR, etc.  is required.Experience with Adobe Analytics is a plus.Experience with statistical programming languages, such as Python or R is a plus.Demonstrated ability to work across product, engineering, and analytics teams to evaluate new ideas, discuss technical concepts, create scalable designs, and make tradeoffs to remove roadblocks.Strong written and verbal communication skills. Can communicate the results of your work clearly to your audience.Masters in quantitative or technical field  statistics, mathematics, computer sciences, etc.  is a plus.Must have the legal right to work in the United States  Minimum of 7+ years of proven technical experience in data engineering, data warehousing, and analytics.Expertise in structuring, cleansing, and preparing large, complex datasets for analysis and modeling.Expertise in building robust and scalable data pipelines using structured and unstructured data as well as batch data and real time streaming data services.Expertise with SQL and other data querying techniques  Scala, Shell .Experience with Apache Spark and Presto.Expertise in data warehousing solutions and techniques.Experience with Amazon Web Services  Redshift, S3, EC2, EMR, etc.  is required.Experience with Adobe Analytics is a plus.Experience with statistical programming languages, such as Python or R is a plus.Demonstrated ability to work across product, engineering, and analytics teams to evaluate new ideas, discuss technical concepts, create scalable designs, and make tradeoffs to remove roadblocks.Strong written and verbal communication skills. Can communicate the results of your work clearly to your audience.Masters in quantitative or technical field  statistics, mathematics, computer sciences, etc.  is a plus.Must have the legal right to work in the United States","Minimum of 7+ years proven technical experience in data engineering, warehousing, and analytics.Expertise structuring, cleansing, preparing large, complex datasets for analysis modeling.Expertise building robust scalable pipelines using structured unstructured as well batch real time streaming services.Expertise with SQL other querying techniques Scala, Shell .Experience Apache Spark Presto.Expertise warehousing solutions techniques.Experience Amazon Web Services Redshift, S3, EC2, EMR, etc. is required.Experience Adobe Analytics a plus.Experience statistical programming languages, such Python or R plus.Demonstrated ability to work across product, analytics teams evaluate new ideas, discuss concepts, create designs, make tradeoffs remove roadblocks.Strong written verbal communication skills. Can communicate the results your clearly audience.Masters quantitative field statistics, mathematics, computer sciences, plus.Must have legal right United States","Minimum 7+ years proven technical experience data engineering, warehousing, analytics.Expertise structuring, cleansing, preparing large, complex datasets analysis modeling.Expertise building robust scalable pipelines using structured unstructured well batch real time streaming services.Expertise SQL querying techniques Scala, Shell .Experience Apache Spark Presto.Expertise warehousing solutions techniques.Experience Amazon Web Services Redshift, S3, EC2, EMR, etc. required.Experience Adobe Analytics plus.Experience statistical programming languages, Python R plus.Demonstrated ability work across product, analytics teams evaluate new ideas, discuss concepts, create designs, make tradeoffs remove roadblocks.Strong written verbal communication skills. Can communicate results clearly audience.Masters quantitative field statistics, mathematics, computer sciences, plus.Must legal right United States"
388,Data Engineer,Principal Data Engineer,"Renton, WA 98057",Renton,WA,"Providence St. Joseph Health is calling a Principal Data Engineer to one of our following locations: Renton, WA (preferred), Spokane, WA, Richland, WA, Everett, WA, Olympia, WA, Anchorage, AK, Missoula, MT, Portland, OR, Beaverton, OR, Anaheim, CA, Burbank, CA or Lubbock, TX. This position is 1 year-long in duration with full medical, dental, vision and vacation benefits. We are open to the possibility of working remotely within our 7 state region of operation: AK, WA, OR, MT, CA, NM and TX.
We are seeking a Principal Data Engineer to design and build modern data-centric software applications to support clinical and operational processes across all parts of the health system. These applications leverage cloud computing, big data, mobile, data science, and modern software development methodologies and frameworks. Data Engineers build data pipelines, enrichment processes, provisioning layers, APIs and user interfaces to meet the requirements of key initiatives. The Principal Data Engineer will take point on development of best practices and standards across the engineering team and participate in research and development of new technologies. A Principal Data Engineer should be able to and emphasize mentoring less experienced Data Engineers and training the team, as needed, to develop a robust skillset among the entire team. Strongly encourages and places a priority on collaboration with meticulous source control and documentation. An emphasis on simple solutions to complex problems through the use of modern and emerging methods and tools is critical. This position will works closely with the Product, Platform, and Architecture teams to deliver on joint efforts.
In this position you will have the following responsibilities:
Design, build and deliver quantitative applications that improve operations and generate value

Participate in DevOps, Agile, and continuous integration frameworks

Stay abreast of emerging technologies, open source projects, and best practices in the field

Data warehousing, big data, enterprise search, business intelligence, analytics, modern and mobile applications

Build processes that are fault-tolerant, self-healing, reliable, resilient and secure

Work effectively and in real-time with other developers, product managers, and customers to deliver on collective goals

Actively participate in code reviews, support the overall code base, and support the establishment of standard processes and frameworks. Take a lead role in the development of standard practices and enforce following standard processes.

Qualifications:
Required qualifications for this position include:
Bachelorâs Degree in computer science, engineering, mathematics, MIS or similar field.

10 years in technology roles.

Must have experience with the following technologies:

C#

ASP.net

T-SQL

HTML/CSS

JavaScript

Nodejs

Demonstrated analytical skills

Demonstrated problem solving skills

Promotes information sharing

Ability to work within tight timeframes and meet strict deadlines.

Possesses strong technical Aptitude.

Preferred qualifications for this position include:
Masterâs Degree.

Cloud computing, Linux, Hadoop, MapReduce, Spark, Hbase, Kudu and NoSQL platforms in general; Apache Solr and Lucene

Java, Scala, C#, Python, shell scripting and/or similar languages

Relational database platforms, database design, and SQL

APIs, JSON, REST and other relevant W3C open standards

Modern application development frameworks

Familiarity with commercial or open source ETL tools

About the department you will serve.
Providence Strategic and Management Services provides a variety of functional and system support services for all eight regions of Providence Health & Services from Alaska to California. We are focused on supporting our Mission by delivering a robust foundation of services and sharing of specialized expertise.
We offer a full comprehensive range of benefits - see our website for details
http://www.providenceiscalling.jobs/rewards-benefits/
Our Mission
As expressions of Godâs healing love, witnessed through the ministry of Jesus, we are steadfast in serving all, especially those who are poor and vulnerable.
About Us
Providence Health & Services is a not-for-profit Catholic network of hospitals, care centers, health plans, physicians, clinics, home health care and services guided by a Mission of caring the Sisters of Providence began over 160 years ago. Providence is proud to be an Equal Opportunity Employer. Providence does not discriminate on the basis of race, color, gender, disability, veteran, military status, religion, age, creed, national origin, sexual identity or expression, sexual orientation, marital status, genetic information, or any other basis prohibited by local, state, or federal law.
Schedule: Full-time
Shift: Day
Job Category: Information Technology
Location: Alaska-Anchorage
Other Location(s): Oregon-Portland, Oregon-Beaverton, Montana-Missoula, Washington-Everett, Washington-Renton, Washington-Richland, Washington-Spokane, California-Anaheim
Req ID: 234029","Bachelorâs Degree in computer science, engineering, mathematics, MIS or similar field.  10 years in technology roles.  Must have experience with the following technologies   C   ASP.net  T-SQL  HTML/CSS  JavaScript  Nodejs  Demonstrated analytical skills  Demonstrated problem solving skills  Promotes information sharing  Ability to work within tight timeframes and meet strict deadlines.  Possesses strong technical Aptitude.     ","Bachelorâs Degree in computer science, engineering, mathematics, MIS or similar field. 10 years technology roles. Must have experience with the following technologies C ASP.net T-SQL HTML/CSS JavaScript Nodejs Demonstrated analytical skills problem solving Promotes information sharing Ability to work within tight timeframes and meet strict deadlines. Possesses strong technical Aptitude.","Bachelorâs Degree computer science, engineering, mathematics, MIS similar field. 10 years technology roles. Must experience following technologies C ASP.net T-SQL HTML/CSS JavaScript Nodejs Demonstrated analytical skills problem solving Promotes information sharing Ability work within tight timeframes meet strict deadlines. Possesses strong technical Aptitude."
389,Data Engineer,SAP Concur - Data Engineer,"Bellevue, WA 98004",Bellevue,WA,"Requisition ID: 230562
Work Area: Software-Design and Development
Expected Travel: 0 - 10%
Career Status: Professional
Employment Type: Regular Full Time

COMPANY DESCRIPTION
SAP started in 1972 as a team of five colleagues with a desire to do something new. Together, they changed enterprise software and reinvented how business was done. Today, as a market leader in enterprise application software, we remain true to our roots. Thatâs why we engineer solutions to fuel innovation, foster equality and spread opportunity for our employees and customers across borders and cultures.
SAP values the entrepreneurial spirit, fostering creativity and building lasting relationships with our employees. We know that a diverse and inclusive workforce keeps us competitive and provides opportunities for all. We believe that together we can transform industries, grow economics, lift up societies and sustain our environment. Because itâs the best-run businesses that make the world run better and improve peopleâs lives.
We are looking for a talented Data Engineer to help build/enhance the next generation enterprise data warehouse to support internal reporting and analytics. You will own many large datasets, implement new data pipelines that feed into or from critical data systems at SAP Concur, come help us utilize years of Travel and Expense data to build a data ecosystem that powers product analytics.

In the first 12 months you will:

Integrate internal product data and other general portfolios into the SAP Concur BI AWS data lake and build models using AWS tools.
Dig into SAP Concurâs data, to perform data discovery and source analysis to assess the quality and structure for various data sources.
Build BI solutions to support the SAP Concur AWS Enterprise Data Warehouse (EDW), and deliver through an agile BI Sprint process.
Contribute to the data management initiatives to ensure data integrity, quality, and common definitions.
Build cloud analytic solutions that are secure and compliant with best practices in data security and privacy
Support management reporting efforts from a data perspective to drive key business decisions.

Position Requirements:

4+ years of experience in design, develop, and maintain ETL solutions that support building SAP Concurâs AWS Enterprise Data Warehouse environment (EDW) in the cloud.
Experience with AWS code (glue, EMR, python, pyspark and redshift) to support dimensions and data warehouse tables
Knowledge of scripting languages (Python, Java, Shell, Unix, etc)
Experience with troubleshooting and maintaining ETL jobs for production and non-production support needs
Experience with performance tune ETL processing to meet business requirements and performance expectations
Demonstrated strength in data modeling, ETL development, and data warehousing
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Excellent verbal/written communication & data presentation skills, including experience communicating to both business and technical teams
Ability to work effectively with both technical and non-technical staff
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Must be comfortable with ambiguity and fast change with an ability to adapt quickly and easily. Ability to analyze complex problems and move them to resolution.
Be aware of, and comply with, all corporate policies.

Value Competencies:
Displays passion for & responsibility to the customerDisplays leadership through innovation in everything you doDisplays a passion for what you do and a drive to improveDisplays a relentless commitment to winDisplays personal & corporate integrity

WHAT YOU GET FROM US
Success is what you make it. At SAP, we help you make it your own.
A career at SAP can open many doors for you. If youâre searching for a company thatâs dedicated to your ideas and individual growth, recognizes you for your unique contributions, fills you with a strong sense of purpose, and provides a fun, flexible and inclusive work environment â apply now.
SAP'S DIVERSITY COMMITMENT
To harness the power of innovation, SAP invests in the development of its diverse employees. We aspire to leverage the qualities and appreciate the unique competencies that each person brings to the company.
SAP is committed to the principles of Equal Employment Opportunity and to providing reasonable accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment with SAP and are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to Recruiting Operations Team. (Americas:Careers.NorthAmerica@sap.com or Careers.LatinAmerica@sap.com, APJ: Careers.APJ@sap.com, EMEA: Careers@sap.com). Requests for reasonable accommodation will be considered on a case-by-case basis. Successful candidates might be required to undergo a background verification with an external vendor.
EOE AA M/F/Vet/Disability:
Qualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, gender, sexual orientation, gender identity, protected veteran status or disability.
Successful candidates might be required to undergo a background verification with an external vendor.
Additional Locations:","     4+ years of experience in design, develop, and maintain ETL solutions that support building SAP Concurâs AWS Enterprise Data Warehouse environment  EDW  in the cloud. Experience with AWS code  glue, EMR, python, pyspark and redshift  to support dimensions and data warehouse tables Knowledge of scripting languages  Python, Java, Shell, Unix, etc  Experience with troubleshooting and maintaining ETL jobs for production and non-production support needs Experience with performance tune ETL processing to meet business requirements and performance expectations Demonstrated strength in data modeling, ETL development, and data warehousing Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations Excellent verbal/written communication & data presentation skills, including experience communicating to both business and technical teams Ability to work effectively with both technical and non-technical staff Knowledge of data management fundamentals and data storage principles Knowledge of distributed systems as it pertains to data storage and computing Must be comfortable with ambiguity and fast change with an ability to adapt quickly and easily. Ability to analyze complex problems and move them to resolution. Be aware of, and comply with, all corporate policies.","4+ years of experience in design, develop, and maintain ETL solutions that support building SAP Concurâs AWS Enterprise Data Warehouse environment EDW the cloud. Experience with code glue, EMR, python, pyspark redshift to dimensions data warehouse tables Knowledge scripting languages Python, Java, Shell, Unix, etc troubleshooting maintaining jobs for production non-production needs performance tune processing meet business requirements expectations Demonstrated strength modeling, development, warehousing Proven success communicating users, other technical teams, senior management collect requirements, describe modeling decisions engineering strategy software best practices across development lifecycle, including agile methodologies, coding standards, reviews, source management, build processes, testing, operations Excellent verbal/written communication & presentation skills, both teams Ability work effectively non-technical staff fundamentals storage principles distributed systems as it pertains computing Must be comfortable ambiguity fast change an ability adapt quickly easily. analyze complex problems move them resolution. Be aware of, comply with, all corporate policies.","4+ years experience design, develop, maintain ETL solutions support building SAP Concurâs AWS Enterprise Data Warehouse environment EDW cloud. Experience code glue, EMR, python, pyspark redshift dimensions data warehouse tables Knowledge scripting languages Python, Java, Shell, Unix, etc troubleshooting maintaining jobs production non-production needs performance tune processing meet business requirements expectations Demonstrated strength modeling, development, warehousing Proven success communicating users, technical teams, senior management collect requirements, describe modeling decisions engineering strategy software best practices across development lifecycle, including agile methodologies, coding standards, reviews, source management, build processes, testing, operations Excellent verbal/written communication & presentation skills, teams Ability work effectively non-technical staff fundamentals storage principles distributed systems pertains computing Must comfortable ambiguity fast change ability adapt quickly easily. analyze complex problems move resolution. Be aware of, comply with, corporate policies."
390,Data Engineer,Big Data Engineer,"Seattle, WA",Seattle,WA,"Senior Software Engineer with total 5+ yearsâ and 1-2 yrs+ experience in Apache Spark with Scala. Candidate should be able to create data pipelines; handling big data across multiple data sources.
Design, Development, Test, Deploy and maintain as big data systems for enterprise products
Coding, participating in Code Reviews, Enhancement discussion, maintenance of existing pipelines & systems, testing and bug-fix activities carried out on an on-going basis
Nice to have experience in Cloud environment, work experience on Azure is a plus. Knowledge in Azure Data Factory will be a plus
Candidate will deliver sprint work as defined by product backlog and prioritized by engineering leadership
Experience with Azure Data Services: Azure SQL, BLOB, ADF (Azure Data Factory) and Cosmos DB is desired but not necessary
Qualifications and other skills:
BS degree in CS or related engineering field
Excellent communication and collaboration skills
Passion for quality with strong customer empathy and focus
Strong intellectual curiosity and passion about learning new technologies
Preferred skills
Apache Kafka or Apache Spark
Data manipulation of large amounts of unstructured data
Location: Seattle, WA", BS degree in CS or related engineering field Excellent communication and collaboration skills Passion for quality with strong customer empathy and focus Strong intellectual curiosity and passion about learning new technologies    ,BS degree in CS or related engineering field Excellent communication and collaboration skills Passion for quality with strong customer empathy focus Strong intellectual curiosity passion about learning new technologies,BS degree CS related engineering field Excellent communication collaboration skills Passion quality strong customer empathy focus Strong intellectual curiosity passion learning new technologies
391,Data Engineer,Data Engineer,"Seattle, WA",Seattle,WA,"The Splunk Data Engineer is responsible for supporting the efforts of the data science team and itâs overall mission of providing a platform for the ingestion, management, storage, and analysis of unstructured and semi-structured machine generated data. This includes supporting the deployment and management of the underlying big data infrastructure including Splunk, HUNK, Hadoop/MapR.

Primary Duties and Responsibilities:Assist in the deployment and management of the Splunk, HUNK, Hadoop/MapR infrastructureDeploy various configurations in the lab for testing and evaluation by the data science teamDeploy and Manage the staging and production Splunk and MapR environmentsWork with the operations team as needed for support and maintenance issues related to Splunk and MapRProvide level 2 on call support as needed for Splunk, HUNK and MapR environmentsTroubleshoot production issues related to data ingestion and other big data related issues as needed
Qualifications
 2+ years experience with data engineering concepts
 2+ years experience deploying and maintaining Splunk environments
 Working knowledge of event logging and key performance indicators
 Analytical and problem solving skills
 Very strong troubleshooting skills
 Solid written and verbal communication skills
 Ability to work directly with customers and vendors in a congenial manner
 Willingness to be a team player
 Self-motivated with the ability to work independently with minimal supervision
 Proficient with common business software (Microsoft Office, Adobe Acrobat, etc.)
 Command line proficiency in server management with a wide variety of UNIX environments
Optional Desired Qualifications:
 Hadoop/MapR Certification
 Hadoop/MapR ecosystem experience
 Working knowledge of telephony and related concepts
 Splunk administration certification or training
 Experience with Splunk ITSI
Education and Certifications:
 B.S. in Computer Science (or similar)
We have been delivering industry-leading solutions for the payments, financial and telecommunications industries since 1990. We are the preferred supplier of networking, integrated data and voice services to many leading organizations in the global payments and financial communities, as well as a provider of extensive telecommunications network solutions to service providers.
We are a privately held company with a healthy balance sheet, secure assets and a loyal customer base that includes some of the largest global blue-chip companies in the world. Many of the worldâs leading companies continue to count on us as their primary provider of a range of networking and communication services, enabling them to expand regionally, nationally and globally. We provide services to customers in over 60 countries throughout the world.
We manage some of the largest real-time community networks in the world, enabling industry participants to simply and securely interact and transact with other businesses, to access the data and applications they need, over managed and secure communications platforms. Our existing footprint supports millions of connections and provide access to critical databases. Our network securely blends private and public networking to enable customers to utilize a single connection for ""one-to-many"" and ""many-to-many""connections over a global platform.
Since our launch we have helped our customers and communities of interest, requiring secure and reliable communications solutions, to evolve from legacy to leading-edge technologies. Today the company provides a full range of services from dedicated connections to managed IP network solutions, providing local support and global reach to medium and large enterprises and service providers.
Application Instructions
Applicants are encouraged to submit an electronic resume when applying for our positions. Job postings are open until filled, unless otherwise specified.", Hadoop/MapR Certification    B.S. in Computer Science  or similar  ,Hadoop/MapR Certification B.S. in Computer Science or similar,Hadoop/MapR Certification B.S. Computer Science similar
392,Data Engineer,Senior Data Engineer,"Seattle, WA",Seattle,WA,"This is an onsite position, i.e. not working remotely. The office is located in downtown Seattle.
Are you energized by the idea of disrupting an established industry with cutting-edge technologies? Are you motivated by the opportunity to shape the next milestone of a growing company?
AI and Machine learning (ML) is ripe for broad market adoption. At Kavout, we are the pioneer in AI for investing. As a FinTech company, Kavoutâs mission is to empower institutions and investors with augmented intelligence to generate alpha, manage wealth and do more with less.
The company was founded in 2016 and is headquartered in Seattle.
We are looking for a Data Engineer with a solid background in Python, Google Cloud Platform, AWS. In this role you will be exposed to different challenging tasks from data engineering to analytics, and be part of a team to build the next generation investing platform for capital markets with AI and machine learning.
Responsibilities
Build large-scale batch and real-time data pipelines with data processing frameworks by leveraging Google Cloud Platform, AWS, and Python data science tools.
Help drive optimization, testing and tooling to improve data quality.
Collaborate with other software engineers, ML experts and business stakeholders. And the opportunity to learn and lead every single day.
Work in multi-functional and agile teams to continuously experiment, iterate and deliver on new product objectives to meet customers needs.
Who you are
You know how to work with high volume heterogeneous data, preferably with distributed systems such as GCP, AWS, Hadoop, BigTable, Redis, MongoDB etc.
You are knowledgeable about data modeling, data access, data analysis, and data storage techniques.
You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
The ability to rapidly learn and understand complex data systems.
Comfortable dealing with ambiguity and working independently.
You understand the value of partnership within teams.
This is an onsite role, i.e. not a remote position. The office is at downtown Seattle.
Benefits
We offer a collaborative working environment. And we encourage accountability and integrity.
Competitive salary
Health insurance coverage
Paid time-off
Holiday pay
Please submit your resume to contact@kavout.co","   Build large-scale batch and real-time data pipelines with data processing frameworks by leveraging Google Cloud Platform, AWS, and Python data science tools. Help drive optimization, testing and tooling to improve data quality. Collaborate with other software engineers, ML experts and business stakeholders. And the opportunity to learn and lead every single day. Work in multi-functional and agile teams to continuously experiment, iterate and deliver on new product objectives to meet customers needs.  ","Build large-scale batch and real-time data pipelines with processing frameworks by leveraging Google Cloud Platform, AWS, Python science tools. Help drive optimization, testing tooling to improve quality. Collaborate other software engineers, ML experts business stakeholders. And the opportunity learn lead every single day. Work in multi-functional agile teams continuously experiment, iterate deliver on new product objectives meet customers needs.","Build large-scale batch real-time data pipelines processing frameworks leveraging Google Cloud Platform, AWS, Python science tools. Help drive optimization, testing tooling improve quality. Collaborate software engineers, ML experts business stakeholders. And opportunity learn lead every single day. Work multi-functional agile teams continuously experiment, iterate deliver new product objectives meet customers needs."
393,Data Engineer,"Data Engineer II (Woot LLC, Seattle, WA)","Seattle, WA",Seattle,WA,"We are looking for a lethally talented DE to join our Data Team to further build and automate our reporting, high value action (HVA) recommending, and business prioritizing system to enhance team efficiency and effectiveness. The ideal candidate will be passionate about contributing to the team growth through his/her expertise in utilizing big data technology to answer business questions and identify growth opportunities. This person will build new business intelligence solutions as an owner end-to-end, at the same time, collaborate with data engineers and data scientists to automate recommendation and prioritization system.

To be successful in this role, you should have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards and using visualization tools, always applying analytical rigor to solve business problems. You should have strong business and communication skills and be able to work closely with product managers and business units to define key business questions and build data sets and models that answer them. Our DEs are expected to have a detailed understanding of our business and never lose sight of the broader problems that we are trying to solve.

Responsibilities:
Design, develop and maintain scalable, automated, user-friendly systems, reports, dashboards, etc. that will support our analytical and business needs
Write SQL code to retrieve and analyze data from database tables (ex. Redshift, MySQL, DBs), and learn and understand a broad range of Amazonâs data resources and know how, when, and which to use and which not to use
Develop queries and visualizations for ad-hoc requests and projects, as well as ongoing reporting
Create pipelines for automated Use analytical and statistical rigor to solve complex problems and drive business decisions.
Develop Machine Learning models
Write scripts in Python for data processing
Basic Qualifications:
Bachelorâs degree in computer science, mathematics, statistics, economics, or other quantitative field
3+ years of relevant work experience in a role requiring application of analytic skills to integrate data into operational/business planning or advanced degree
Strong experience with ETL development, data modeling, data warehousing, MySQL, and databases in a business environment with large-scale, complex datasets
Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management as required.
Experience in gathering requirements and formulating business metrics for reporting.
Experienced working in a fast-paced, high-tech environment and comfortable navigating conflicting priorities and ambiguous problems
Strong track record in converting data analysis into tangible and significant real-world changes.
Strong grasp of quantitative data analysis and statistics.
Python scripting experience
AWS or Azure production development and managment
The ability to exclaim Woot!
Preferred Qualifications:
MBA or Masterâs degree in Computer Science, Engineering, Statistics, Mathematics or related field
Expert in writing and tuning SQL scripts
Experience working in very large data warehouse environments
3+ years of experience in a data engineer or BIE role with a technology company Â§ Experience conducting large scale data analysis to support business decision making
Strong verbal/written communication and data presentation skills, including an ability to effectively communicate with both business and technical teams
Be self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data.
Strong verbal/written communication & data presentation skills, including an ability to effectively communicate with both business and technical teams.
Strong dashboarding skills with Tableau/Microstrategy/Looker, etc.
Understand the meaning of Moofi
Woot, An Amazon Company, is an Equal Opportunity/Affirmative Action Employer - Female/Minority/Disability/Veteran.","Bachelorâs degree in computer science, mathematics, statistics, economics, or other quantitative field 3+ years of relevant work experience in a role requiring application of analytic skills to integrate data into operational/business planning or advanced degree Strong experience with ETL development, data modeling, data warehousing, MySQL, and databases in a business environment with large-scale, complex datasets Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management as required. Experience in gathering requirements and formulating business metrics for reporting. Experienced working in a fast-paced, high-tech environment and comfortable navigating conflicting priorities and ambiguous problems Strong track record in converting data analysis into tangible and significant real-world changes. Strong grasp of quantitative data analysis and statistics. Python scripting experience AWS or Azure production development and managment The ability to exclaim Woot!   Design, develop and maintain scalable, automated, user-friendly systems, reports, dashboards, etc. that will support our analytical and business needs Write SQL code to retrieve and analyze data from database tables  ex. Redshift, MySQL, DBs , and learn and understand a broad range of Amazonâs data resources and know how, when, and which to use and which not to use Develop queries and visualizations for ad-hoc requests and projects, as well as ongoing reporting Create pipelines for automated Use analytical and statistical rigor to solve complex problems and drive business decisions. Develop Machine Learning models Write scripts in Python for data processing   ","Bachelorâs degree in computer science, mathematics, statistics, economics, or other quantitative field 3+ years of relevant work experience a role requiring application analytic skills to integrate data into operational/business planning advanced Strong with ETL development, modeling, warehousing, MySQL, and databases business environment large-scale, complex datasets Advanced ability draw insights from clearly communicate them the stakeholders senior management as required. Experience gathering requirements formulating metrics for reporting. Experienced working fast-paced, high-tech comfortable navigating conflicting priorities ambiguous problems track record converting analysis tangible significant real-world changes. grasp statistics. Python scripting AWS Azure production development managment The exclaim Woot! Design, develop maintain scalable, automated, user-friendly systems, reports, dashboards, etc. that will support our analytical needs Write SQL code retrieve analyze database tables ex. Redshift, DBs , learn understand broad range Amazonâs resources know how, when, which use not Develop queries visualizations ad-hoc requests projects, well ongoing reporting Create pipelines automated Use statistical rigor solve drive decisions. Machine Learning models scripts processing","Bachelorâs degree computer science, mathematics, statistics, economics, quantitative field 3+ years relevant work experience role requiring application analytic skills integrate data operational/business planning advanced Strong ETL development, modeling, warehousing, MySQL, databases business environment large-scale, complex datasets Advanced ability draw insights clearly communicate stakeholders senior management required. Experience gathering requirements formulating metrics reporting. Experienced working fast-paced, high-tech comfortable navigating conflicting priorities ambiguous problems track record converting analysis tangible significant real-world changes. grasp statistics. Python scripting AWS Azure production development managment The exclaim Woot! Design, develop maintain scalable, automated, user-friendly systems, reports, dashboards, etc. support analytical needs Write SQL code retrieve analyze database tables ex. Redshift, DBs , learn understand broad range Amazonâs resources know how, when, use Develop queries visualizations ad-hoc requests projects, well ongoing reporting Create pipelines automated Use statistical rigor solve drive decisions. Machine Learning models scripts processing"
394,Data Engineer,Big Data Engineer,"Seattle, WA 98127",Seattle,WA,"We are looking for strong Big Data Engineers and Data Analysts. This person will be handling petabytes of consumer data for analytics. Excellent salary and benefits.


Locations â Connecticut, Los Angeles, Seattle and New York.


Basic Qualifications:

Have 5+ years of experience developing with a mix of languages (Scala, Python, SQL, etc.) and frameworks to implement data ingest, processing, and serving technologies. Experience with real-time and very large scalable online systems are preferred.

Cloud experience is must (AWS-S3, Snowflake, Redshift, Big Query etc.)
Experience with open source such as Hadoop, Spark, Kafka, Druid, Pilosa and Yarn/Kubernetes.
Experience in SQL, ETL Tools is required
Are passionate about data, technology, & creative innovation.
Experience in working with Data Scientists to operationalize machine learning models.","Cloud experience is must  AWS-S3, Snowflake, Redshift, Big Query etc.  Experience with open source such as Hadoop, Spark, Kafka, Druid, Pilosa and Yarn/Kubernetes. Experience in SQL, ETL Tools is required Are passionate about data, technology, & creative innovation. Experience in working with Data Scientists to operationalize machine learning models.    ","Cloud experience is must AWS-S3, Snowflake, Redshift, Big Query etc. Experience with open source such as Hadoop, Spark, Kafka, Druid, Pilosa and Yarn/Kubernetes. in SQL, ETL Tools required Are passionate about data, technology, & creative innovation. working Data Scientists to operationalize machine learning models.","Cloud experience must AWS-S3, Snowflake, Redshift, Big Query etc. Experience open source Hadoop, Spark, Kafka, Druid, Pilosa Yarn/Kubernetes. SQL, ETL Tools required Are passionate data, technology, & creative innovation. working Data Scientists operationalize machine learning models."
395,Data Engineer,Senior Data Engineer,"Redmond, WA",Redmond,WA,"We live in the Age of Data. And we LOVE Data! Data and insights from data power an increasing range of applications, transforming not just the technology industry but society at large. Individuals and businesses need a powerful, elastic, and highly available Data Platform that can help them derive insights from large volumes of data. The Database Systems organization is delivering such a platform for a range of Relational Database workloads, from online transaction processing workloads, to data warehousing solutions, from on premise enterprise systems, to on-demand cloud services. Uniquely in the industry, we deal with the full breadth of environments, from server-based to cloud-based systems, delivering features that work across these environments, providing differentiated value to our customers.
We are a full stack team, tackling the breadth of technology from distributed systems, availability, scalability, security, query processing, storage, operating systems, networking, management tools, web development, and most other fields in Computer Science. As an organization, we are also proud of our world class team, our deep investments in growing and retaining talent, and the diversity of skills, experiences, interests and personalities that makes our team strong. Along with the positive impact to society from our technology, our team also prides itself on direct involvement in various social causes, particularly related to broadening access to technology to all sections of society. To help our business to succeed in its ambitions, the Database Systems team is building up Business Analytics capabilities that allow us to track and improve customer experience, provide metrics around growth of existing and new offerings we are constantly developing, and want to apply Machine Learning techniques to answer various business questions. We are creating and maintaining data pipelines and Data Warehouses that allow us to analyze very large data sets that are emitted from our cluster telemetry, and relevant data from across the company. We wonât work in isolation but will leverage analytical work that has been done across the company.
Responsibilities
We are looking for engineers who are passionate about data and want to work in a multi-disciplinary team of Software Engineers, Data Engineers and Data Scientists to solve real-world business problems. You will work on either data from diverse structured and unstructured data sources, and various formats including tabular, text and time series or can contribute to our core data pipeline. In addition, we have deep investments in on-prem telemetry that we need to update/modernize. This business-critical telemetry stack enables business insights for our existing and new features on Windows and Linux. We expect all team members to contribute broadly, with an agile and growth mindset, so your ability to step into multiple areas will be a key attribute that we are looking for.
Qualifications
Basic Qualifications:
5+ years of relevant experience regarding designing and implementing software systemsSoftware engineering skill in one or more high level languages (C#, C++, Java, Python)BS, MS, or PhD in computer science (or equivalent)

Preferred, but not required:
Common ML and analysis tools (R, SAS, SPSS, MatLab)Experience with large scale applied machine learning techniques is a plus but not requiredExperience with SQL or equivalent query language.

AZDAT #ENGGJOBS
#AZDATABASE

Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.
Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.","5+ years of relevant experience regarding designing and implementing software systemsSoftware engineering skill in one or more high level languages  C , C++, Java, Python   5+ years of relevant experience regarding designing and implementing software systemsSoftware engineering skill in one or more high level languages  C , C++, Java, Python   ","5+ years of relevant experience regarding designing and implementing software systemsSoftware engineering skill in one or more high level languages C , C++, Java, Python","5+ years relevant experience regarding designing implementing software systemsSoftware engineering skill one high level languages C , C++, Java, Python"
396,Data Engineer,Software Engineer 2 (Data Engineer),"Redmond, WA",Redmond,WA,"While youâve heard about Microsoftâs Digital Transformation and how itâs leading our industry to the cloud, have you ever thought of becoming a key member of the team that powers the core Infrastructure services for this strategic effort in our company?
Core Platform Engineering is part of the Core Services Engineering (CSE) team - our goal is to boldly pursue big ideas that power transformational advancements at Microsoft, while helping Microsoft employees work smarter, faster, and more securely every day. We are technology leaders with deep technical experts, focused on digital transformation and enabling our stakeholders.

As part of CPE, the Enterprise Infrastructure Service (EIS) team has a very broad Infrastructure portfolio which includes:

Providing network connectivity services (Core routing & WAN, wireless, wired, VPN, firewalls, load balancers, DNS, etc.) for 580 sites globally, including Microsoft's large corporate campuses.
Managing 90 contact centers, ensuring that 36 million calls to Microsoft are delivered, and driving new capabilities, like speech to text translations for these calls.
Moving line-of-business workloads to Azure and partnering with the Azure team on the features and functionalities required for a modern enterprise, with a focus on customer experience and security.
Providing shared lab services for more than 40 labs and 4 legacy datacenters, while driving these workloads to Azure.
Playing a significant part in driving the strategic vision and implementation of new and existing prioritized investments across Microsoft to increase our capabilities to function as modern enterprise and continue our digital transformation.
We have exciting opportunities for you to innovate, influence, transform, inspire and grow within our organization and we encourage you to apply to learn more!

Are you someone with a passion for data, analytics, insights and technology? Do you want to be part of a team lighting up actionable insights to help the organization make business decisions and improve customer experience? Do you want to be at the forefront of driving company-wide impact using Big Data?

The Customer Engagement Solutions (CES) Data Engineering (DE) team is transforming the customer support infrastructure and executing against modern engineering and customer first strategy.

The Data Engineering team builds and operates an extensive data platform that associates telemetry and customer contact data. By connecting data across different product/service telemetry and contact routing systems, the DE platform provides Microsoft with visibility and insights into our customer and support agent interaction. Our scope is currently expanding to include, data ingestion and management, live site and scorecard reporting, deep data analysis and application development for entire telecom contact support space within Microsoft.
We are hiring a Data Engineer with a combination of programming and data skills to lead the evolution and development of our internal tools, reporting and analytics infrastructure. This position is a technical role with operational accountability.

Come be a core member of this exciting space and join an amazing, growing team!
Responsibilities
Lead the development of our data pipeline and reporting infrastructure including developing solutions for data collection, management and usage.
Help strategize and extend this system to handle Data Science models in platforms like Azure ML and/or DataBricks
Working on a system that speaks natively to various data platforms, enabling individual users to rapidly explore their data and author insightful visualizations.
Partner with our extended team of PMs, service engineers, support delivery managers and engineers to ensure reporting requirements, delivery plans, engineering execution, risks and issues and support scenarios are well-understood and communicated.
Become a SME on multiple business processes and how related solutions are expressed in our services and technology and mentor other engineers.
Qualifications
BS or MS Degree in Computer Science or Data Science.
Experience developing with cloud-based technologies, including relational databases, data warehouse, big data (i.e. Hadoop, Spark), orchestration/data pipeline tools.
Experience with telemetry and data mining eg: Azure Data Lake, Kusto, Hadoop and related big data systems
Software Programming experience with C#
Knowledge of an analysis tools such as R and Python
Proficiency in Power BI
Experience with Agile software development using the scrum methodology.
Preferred, not required:
Experience in Machine Learning systems
#CSEO

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work."," BS or MS Degree in Computer Science or Data Science. Experience developing with cloud-based technologies, including relational databases, data warehouse, big data  i.e. Hadoop, Spark , orchestration/data pipeline tools. Experience with telemetry and data mining eg  Azure Data Lake, Kusto, Hadoop and related big data systems Software Programming experience with C  Knowledge of an analysis tools such as R and Python Proficiency in Power BI Experience with Agile software development using the scrum methodology.   Lead the development of our data pipeline and reporting infrastructure including developing solutions for data collection, management and usage. Help strategize and extend this system to handle Data Science models in platforms like Azure ML and/or DataBricks Working on a system that speaks natively to various data platforms, enabling individual users to rapidly explore their data and author insightful visualizations. Partner with our extended team of PMs, service engineers, support delivery managers and engineers to ensure reporting requirements, delivery plans, engineering execution, risks and issues and support scenarios are well-understood and communicated. Become a SME on multiple business processes and how related solutions are expressed in our services and technology and mentor other engineers.  ","BS or MS Degree in Computer Science Data Science. Experience developing with cloud-based technologies, including relational databases, data warehouse, big i.e. Hadoop, Spark , orchestration/data pipeline tools. telemetry and mining eg Azure Lake, Kusto, Hadoop related systems Software Programming experience C Knowledge of an analysis tools such as R Python Proficiency Power BI Agile software development using the scrum methodology. Lead our reporting infrastructure solutions for collection, management usage. Help strategize extend this system to handle models platforms like ML and/or DataBricks Working on a that speaks natively various platforms, enabling individual users rapidly explore their author insightful visualizations. Partner extended team PMs, service engineers, support delivery managers engineers ensure requirements, plans, engineering execution, risks issues scenarios are well-understood communicated. Become SME multiple business processes how expressed services technology mentor other engineers.","BS MS Degree Computer Science Data Science. Experience developing cloud-based technologies, including relational databases, data warehouse, big i.e. Hadoop, Spark , orchestration/data pipeline tools. telemetry mining eg Azure Lake, Kusto, Hadoop related systems Software Programming experience C Knowledge analysis tools R Python Proficiency Power BI Agile software development using scrum methodology. Lead reporting infrastructure solutions collection, management usage. Help strategize extend system handle models platforms like ML and/or DataBricks Working speaks natively various platforms, enabling individual users rapidly explore author insightful visualizations. Partner extended team PMs, service engineers, support delivery managers engineers ensure requirements, plans, engineering execution, risks issues scenarios well-understood communicated. Become SME multiple business processes expressed services technology mentor engineers."
397,Data Engineer,Data Engineer,"Bellevue, WA",Bellevue,WA,"Who is Blueprint?

Blueprint Technologies is a group of solution minded thinkers changing the face of Technology in Bellevue, WA. We follow a Mission, Vision, and Core Values that allow us to function as a collaborative unit.

What are our Solutions?

Blueprint is a technology solutions firm that connects strategy, product and delivery. We help companies digitally transform. We have a special focus in cloud and infrastructure, data platform and engineering, data science and analytics, organizational modernization and customer experience optimization.

Why you want to be a part of Blueprint?

We are innovators. Motivators. Thought provokers. And coffee drinkers. Our collective backgrounds bring diverse perspectives that enable us to consistently think differently. Our people are our solutions. We want you to bring your biggest and best ideas to help positively impact our culture, clients and the community around us. We believe in the importance of a healthy and happy team, which is why our benefits include full medical, dental and vision coverage, as well as paid time off, 401k, paid volunteer hours and tuition reimbursement.

Blueprint is looking for Data Engineer to join us as we build cutting-edge technology solutions!

Qualifications:

At least 5-years of experience as a software development or data engineer
At least 3-years of experience with SQL, Python and/or other data collection tools & reporting
Experience with Pyspark or Scala is necessity (Databricks or Spark).
Advanced knowledge and skills with Azure, or similar cloud platforms.
Excellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)
Excellent organization skills and able to multi-task and detailed oriented
Excellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc.

Nice to have


Experience working with Business Intelligence BI tools such as PowerBI
Bachelor's Degree

**We are not able to sponsor Visa's at this time or do Corp-to-Corp arrangements. Must be able work on a W2 basis please!"," At least 5-years of experience as a software development or data engineer At least 3-years of experience with SQL, Python and/or other data collection tools & reporting Experience with Pyspark or Scala is necessity  Databricks or Spark . Advanced knowledge and skills with Azure, or similar cloud platforms. Excellent collaboration skills to work on a team as well as independently  be self-reliant and resourceful  Excellent organization skills and able to multi-task and detailed oriented Excellent verbal and written communication skills  must be able to write clear and concise emails for any audience, etc.     ","At least 5-years of experience as a software development or data engineer 3-years with SQL, Python and/or other collection tools & reporting Experience Pyspark Scala is necessity Databricks Spark . Advanced knowledge and skills Azure, similar cloud platforms. Excellent collaboration to work on team well independently be self-reliant resourceful organization able multi-task detailed oriented verbal written communication must write clear concise emails for any audience, etc.","At least 5-years experience software development data engineer 3-years SQL, Python and/or collection tools & reporting Experience Pyspark Scala necessity Databricks Spark . Advanced knowledge skills Azure, similar cloud platforms. Excellent collaboration work team well independently self-reliant resourceful organization able multi-task detailed oriented verbal written communication must write clear concise emails audience, etc."
398,Data Engineer,Senior Data Engineer,"Bellevue, WA",Bellevue,WA,"Who is Blueprint?

Blueprint Technologies is a group of solution minded thinkers changing the face of Technology in Bellevue, WA. We follow a Mission, Vision, and Core Values that allow us to function as a collaborative unit.

What are our Solutions?

Blueprint is a technology solutions firm that connects strategy, product and delivery. We help companies digitally transform. We have a special focus in cloud and infrastructure, data platform and engineering, data science and analytics, organizational modernization and customer experience optimization.

Why you want to be a part of Blueprint?

We are innovators. Motivators. Thought provokers. And coffee drinkers. Our collective backgrounds bring diverse perspectives that enable us to consistently think differently. Our people are our solutions. We want you to bring your biggest and best ideas to help positively impact our culture, clients and the community around us. We believe in the importance of a healthy and happy team, which is why our benefits include full medical, dental and vision coverage, as well as paid time off, 401k, paid volunteer hours and tuition reimbursement.

Blueprint is looking for Senior Data Engineer to join us to join our team in Bellevue, WA!!

Basic Qualifications:

8+ years of professional experience as a Data Engineer/BI Developer
4+ years of professional experience implementing and using Power BI
Expertise in Microsoft Business Intelligence Stack (SSRS, SSAS, SSIS) and T-SQL using SQL Server , SSIS
Well versed experience with SQL, Python and/or other data collection tools & reporting
Extensive experience with PySpark or Scala is necessity (Databricks or Spark).
Advanced knowledge and skills with Azure, or similar cloud platforms.
Experience required with DAX/MDX
Excellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)
Excellent organization skills and able to multi-task and detailed oriented
Excellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc.

FLSA - Job Classification: Exempt, Full Time Position.

**We are not able to sponsor Visa's at this time or do Corp-to-Corp arrangements. Must be able work on a W2 basis please!"," 8+ years of professional experience as a Data Engineer/BI Developer 4+ years of professional experience implementing and using Power BI Expertise in Microsoft Business Intelligence Stack  SSRS, SSAS, SSIS  and T-SQL using SQL Server , SSIS Well versed experience with SQL, Python and/or other data collection tools & reporting Extensive experience with PySpark or Scala is necessity  Databricks or Spark . Advanced knowledge and skills with Azure, or similar cloud platforms. Experience required with DAX/MDX Excellent collaboration skills to work on a team as well as independently  be self-reliant and resourceful  Excellent organization skills and able to multi-task and detailed oriented Excellent verbal and written communication skills  must be able to write clear and concise emails for any audience, etc.     ","8+ years of professional experience as a Data Engineer/BI Developer 4+ implementing and using Power BI Expertise in Microsoft Business Intelligence Stack SSRS, SSAS, SSIS T-SQL SQL Server , Well versed with SQL, Python and/or other data collection tools & reporting Extensive PySpark or Scala is necessity Databricks Spark . Advanced knowledge skills Azure, similar cloud platforms. Experience required DAX/MDX Excellent collaboration to work on team well independently be self-reliant resourceful organization able multi-task detailed oriented verbal written communication must write clear concise emails for any audience, etc.","8+ years professional experience Data Engineer/BI Developer 4+ implementing using Power BI Expertise Microsoft Business Intelligence Stack SSRS, SSAS, SSIS T-SQL SQL Server , Well versed SQL, Python and/or data collection tools & reporting Extensive PySpark Scala necessity Databricks Spark . Advanced knowledge skills Azure, similar cloud platforms. Experience required DAX/MDX Excellent collaboration work team well independently self-reliant resourceful organization able multi-task detailed oriented verbal written communication must write clear concise emails audience, etc."
399,Data Engineer,"Data Engineer, Runner Performance Lab","Seattle, WA",Seattle,WA,"Who We Are:
Brooks is a team of passionate people united by a desire to do meaningful work, lead healthy lives and make a difference. We share a focused mission: to inspire everyone to run and be active. Thatâs it. No distractionsâitâs all about the run. Through science, creativity, service, authenticity and connection, we obsess over delivering the best running gear on the planet. We do it our way, with our unique spirit, with a goal of being more relevant to runners than any other brand, day after day and mile after mile. We are determined to innovate, challenging ourselves to lead thought at every turn. Inside these walls and on the roads, tracks and trails, we live and breathe Run Happy, celebrating the positive impact running has on our lives and others. We inject it into all we do because it makes everything better, smarter, more fun and more memorable. Our company culture defines us, bonds us together and creates the conditions for success. It is lived daily as a behavioral expression of our collective set of brand values: Connect with People, Innovate for our Customer, Compete as a Team, Build Trust, Have Fun & Bring Passion, and Be Active. If youâre on our team, it means youâre part of creating something extraordinary. Youâre part of Brooks.

We are looking for a passionate Data Engineer on the Run Research team to help us build and create the future of the run. From optimizing performance to assessing injury risk to improving the experience on the run, youâll help design & build the components, frameworks and libraries to support and scale our analytics programs that will enable our teams to create amazing products and elevate experiences for our runners. In this role, you will work cross functionally, collaborating with the research teams, our product assessment teams and our product creation teams to develop data & analytics capabilities that will allow us to leverage data to inform how we help runners achieve their path to a better self. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection. The right candidate is excited and passionate about optimizing the Run Research Labâs data architecture to support our next generation of products and data initiatives.

Job Responsibilities
Create and maintain optimal data pipeline architecture for the Run Research Lab
 Lead and drive the re-structuring of the current data architecture, development and implementation of new data management projects & capabilities, data applications and data cleansing.
 Collaborate with appropriate data owners and key stakeholders including Research, Assessment, Run Sights and Product Creation to identify and map data from the source environment to the target data environment
Clean, prepare and optimize data at scale for ingestion and consumption including interfaces between Brooks and third-party systems to enable real time data consumption and preparation for analysis
 Identify data quality gaps and work with data owners to develop solutions and close gaps. Participate in on-going service delivery, including documentation and ownership of relevant change control requests (including evaluation, test, implementation, and verification).
Write code or use specialized development tools to create product features, enhance and/or customize software components
 Anticipate, identify and solve issues concerning data management in the Lab to improve data quality.
Troubleshoot data issues and perform root cause analysis to proactively resolve product and operational issues
 Build continuous integration, test-driven development and production deployment frameworks. Drive collaborative reviews of design, code, test plans and data set implementation in support of maintaining data engineering standards. Test developed programs and integration of data from various sources.
Liaise with enterprise data teams to ensure that development adheres to organizational architecture guidelines.
Participate in key architectural and technical decisions as they apply to the Run Research Lab
Coordinate and conduct application testing (new support packages, releases, functionality and customizing) in close cooperation with the technology team.
Engage system owners to filter, size and prioritize business requests and drive towards appropriate decision points.
Establish consistent technical architecture & contribute to development policies, standards and conventions
Maintain expert knowledge of development tools, technologies and related delivery methods.
Requirements
Bachelorâs degree in computer science, statistics or applicable engineering fields with a focus in biomechanics and a research environment a plus.
3+ yearsâ experience with data management tools and industry standard relational database systems preferably in the lab based setting.
 An expert in database technologies (SQL, Big Data frameworks (Hadoop, Spark), advanced data modelling, cloud platforms (AWS, Azure) as well as real-time (Kafka) and batch data integration frameworks
Significant experience in writing programs to analyze biomechanical data is strongly preferred (matlab, visual 3D, Labview, ATL, Python, Jave, C/C++)
Advanced knowledge and experience in use of biomechanics systems for analyzing running/walking gait (3D mocap systems (Motion Analysis, Vicon, Qualysis), Visual 3D, plantar pressure systems (Novel)
Experience in algorithms, especially in the field of AI and machine learning
Experienced in Agile/ Scrum methodologies and collaboration with cross functional teams
Strong project management and analytical skills
Ability to work cross functionally in a fast paced, dynamic environment
Curious and open minded; always open for a challenge, inventive, creative. Ability to challenge the status quo â always looking at improving our products and processes while also displaying a willingness to dive into the details.
Unwavering demonstration of Brooksâ corporate values: Serve People, Lead Thought, Compete as a Team, Have Integrity, Be Active, Have Fun!
A passion for the running enthusiast and active lifestyle
Travel 5% of the time


At Brooks, we celebrate diversity & equity. We are committed to creating an inclusive environment, and encourage people of all backgrounds, perspectives, experiences, and skills to apply. Brooks is proud to be an equal employment opportunity employer. All employment decisions are made without regard to race, religion, color, national origin, gender, gender identity, the presence of a sensory, physical or mental disability, medical condition, military status, marital status, pregnancy or child birth, sexual orientation, age, genetic information, status as a victim of domestic violence, sexual assault or stalking, political ideology, or any other non-merit based factors.","   Create and maintain optimal data pipeline architecture for the Run Research Lab  Lead and drive the re-structuring of the current data architecture, development and implementation of new data management projects & capabilities, data applications and data cleansing.  Collaborate with appropriate data owners and key stakeholders including Research, Assessment, Run Sights and Product Creation to identify and map data from the source environment to the target data environment Clean, prepare and optimize data at scale for ingestion and consumption including interfaces between Brooks and third-party systems to enable real time data consumption and preparation for analysis  Identify data quality gaps and work with data owners to develop solutions and close gaps. Participate in on-going service delivery, including documentation and ownership of relevant change control requests  including evaluation, test, implementation, and verification . Write code or use specialized development tools to create product features, enhance and/or customize software components  Anticipate, identify and solve issues concerning data management in the Lab to improve data quality. Troubleshoot data issues and perform root cause analysis to proactively resolve product and operational issues  Build continuous integration, test-driven development and production deployment frameworks. Drive collaborative reviews of design, code, test plans and data set implementation in support of maintaining data engineering standards. Test developed programs and integration of data from various sources. Liaise with enterprise data teams to ensure that development adheres to organizational architecture guidelines. Participate in key architectural and technical decisions as they apply to the Run Research Lab Coordinate and conduct application testing  new support packages, releases, functionality and customizing  in close cooperation with the technology team. Engage system owners to filter, size and prioritize business requests and drive towards appropriate decision points. Establish consistent technical architecture & contribute to development policies, standards and conventions Maintain expert knowledge of development tools, technologies and related delivery methods.   Bachelorâs degree in computer science, statistics or applicable engineering fields with a focus in biomechanics and a research environment a plus. 3+ yearsâ experience with data management tools and industry standard relational database systems preferably in the lab based setting.  An expert in database technologies  SQL, Big Data frameworks  Hadoop, Spark , advanced data modelling, cloud platforms  AWS, Azure  as well as real-time  Kafka  and batch data integration frameworks Significant experience in writing programs to analyze biomechanical data is strongly preferred  matlab, visual 3D, Labview, ATL, Python, Jave, C/C++  Advanced knowledge and experience in use of biomechanics systems for analyzing running/walking gait  3D mocap systems  Motion Analysis, Vicon, Qualysis , Visual 3D, plantar pressure systems  Novel  Experience in algorithms, especially in the field of AI and machine learning Experienced in Agile/ Scrum methodologies and collaboration with cross functional teams Strong project management and analytical skills Ability to work cross functionally in a fast paced, dynamic environment Curious and open minded; always open for a challenge, inventive, creative. Ability to challenge the status quo â always looking at improving our products and processes while also displaying a willingness to dive into the details. Unwavering demonstration of Brooksâ corporate values  Serve People, Lead Thought, Compete as a Team, Have Integrity, Be Active, Have Fun! A passion for the running enthusiast and active lifestyle Travel 5% of the time","Create and maintain optimal data pipeline architecture for the Run Research Lab Lead drive re-structuring of current architecture, development implementation new management projects & capabilities, applications cleansing. Collaborate with appropriate owners key stakeholders including Research, Assessment, Sights Product Creation to identify map from source environment target Clean, prepare optimize at scale ingestion consumption interfaces between Brooks third-party systems enable real time preparation analysis Identify quality gaps work develop solutions close gaps. Participate in on-going service delivery, documentation ownership relevant change control requests evaluation, test, implementation, verification . Write code or use specialized tools create product features, enhance and/or customize software components Anticipate, solve issues concerning improve quality. Troubleshoot perform root cause proactively resolve operational Build continuous integration, test-driven production deployment frameworks. Drive collaborative reviews design, code, test plans set support maintaining engineering standards. Test developed programs integration various sources. Liaise enterprise teams ensure that adheres organizational guidelines. architectural technical decisions as they apply Coordinate conduct application testing packages, releases, functionality customizing cooperation technology team. Engage system filter, size prioritize business towards decision points. Establish consistent contribute policies, standards conventions Maintain expert knowledge tools, technologies related delivery methods. Bachelorâs degree computer science, statistics applicable fields a focus biomechanics research plus. 3+ yearsâ experience industry standard relational database preferably lab based setting. An SQL, Big Data frameworks Hadoop, Spark , advanced modelling, cloud platforms AWS, Azure well real-time Kafka batch Significant writing analyze biomechanical is strongly preferred matlab, visual 3D, Labview, ATL, Python, Jave, C/C++ Advanced analyzing running/walking gait 3D mocap Motion Analysis, Vicon, Qualysis Visual plantar pressure Novel Experience algorithms, especially field AI machine learning Experienced Agile/ Scrum methodologies collaboration cross functional Strong project analytical skills Ability functionally fast paced, dynamic Curious open minded; always challenge, inventive, creative. challenge status quo â looking improving our products processes while also displaying willingness dive into details. Unwavering demonstration Brooksâ corporate values Serve People, Thought, Compete Team, Have Integrity, Be Active, Fun! A passion running enthusiast active lifestyle Travel 5%","Create maintain optimal data pipeline architecture Run Research Lab Lead drive re-structuring current architecture, development implementation new management projects & capabilities, applications cleansing. Collaborate appropriate owners key stakeholders including Research, Assessment, Sights Product Creation identify map source environment target Clean, prepare optimize scale ingestion consumption interfaces Brooks third-party systems enable real time preparation analysis Identify quality gaps work develop solutions close gaps. Participate on-going service delivery, documentation ownership relevant change control requests evaluation, test, implementation, verification . Write code use specialized tools create product features, enhance and/or customize software components Anticipate, solve issues concerning improve quality. Troubleshoot perform root cause proactively resolve operational Build continuous integration, test-driven production deployment frameworks. Drive collaborative reviews design, code, test plans set support maintaining engineering standards. Test developed programs integration various sources. Liaise enterprise teams ensure adheres organizational guidelines. architectural technical decisions apply Coordinate conduct application testing packages, releases, functionality customizing cooperation technology team. Engage system filter, size prioritize business towards decision points. Establish consistent contribute policies, standards conventions Maintain expert knowledge tools, technologies related delivery methods. Bachelorâs degree computer science, statistics applicable fields focus biomechanics research plus. 3+ yearsâ experience industry standard relational database preferably lab based setting. An SQL, Big Data frameworks Hadoop, Spark , advanced modelling, cloud platforms AWS, Azure well real-time Kafka batch Significant writing analyze biomechanical strongly preferred matlab, visual 3D, Labview, ATL, Python, Jave, C/C++ Advanced analyzing running/walking gait 3D mocap Motion Analysis, Vicon, Qualysis Visual plantar pressure Novel Experience algorithms, especially field AI machine learning Experienced Agile/ Scrum methodologies collaboration cross functional Strong project analytical skills Ability functionally fast paced, dynamic Curious open minded; always challenge, inventive, creative. challenge status quo â looking improving products processes also displaying willingness dive details. Unwavering demonstration Brooksâ corporate values Serve People, Thought, Compete Team, Have Integrity, Be Active, Fun! A passion running enthusiast active lifestyle Travel 5%"
400,Data Engineer,Senior Data Services Engineer,"Santa Monica, CA 90405",Santa Monica,CA,"Retention Science is looking for an experienced data engineer who is passionate about writing clean, well-tested code. You will be working on our data services squad - the team that handles data engineering pipelines, and creates scalable / high-throughput backend services, primarily in Scala / Spark / Python. You will be building out robust micro-services that ingest and process huge data sets, work closely with our data science team to ensure that our machine learning models scale and support our product. Ultimately, this is all used to power our predictions and optimize and automate marketing campaigns for our clients. You will work with real-time / streaming technologies and build tools that help our clients visualize and interact with their data (millions of users and billions of data points.)

You'll get to work across our micro-services and distributed systems. You should love working with Python, although youâll also have the opportunity to work on other services, frameworks, languages (eg. AWS/lambda, etc). In addition, you will gain exposure into our machine learning stack (Spark framework) that our data scientists have developed to predict and personalize at scale.

In addition to having meaningful responsibilities and improving your engineering chops, you will also receive comprehensive exposure to all aspects of our business. The code and ideas that you contribute will have a tangible impact on the cumulative work of the team as a whole. You thrive in a small, dynamic team environment and want to make an impact in a fast-paced start-up environment.

Qualifications:
3+ years experience data engineering development using scala/python
Experience designing and implementing large, scalable services
Experience with automating & deploying micro-services
Experience working with third-party APIs and creating robust APIs
Know when and where to optimize and cache
Comfortable working with large data sets and distributed systems (datastores, frameworks)
Experience with streaming / realtime technologies (Kinesis, Kafka, Spark streaming etc)
Deep understanding of MySQL, Postgres, Redshift, relational databases etc.
Understand when to use NOSQL databases / other distributed ""big data"" technologies
Experience with AWS technologies
Startup work experience a major plus!
About Us

ReSci's mission is to make artificial intelligence accessible and usable for brands.

Our values:

Inspire with passion

Persevere with determination

Collaborate with unity

Grow without bounds

Create with impact

Lead with character

Based out of Santa Monica, CA, our team consists of serial entrepreneurs who have all made Retention Science a leader in AI marketing. Our SaaS platform, âCortexâ helps online businesses target, engage, and retain customers. The Cortex marketing platform uses machine-learning algorithms to predict customer behavior by analyzing massive sets of demographic, social, and behavioral data to generate 1-to-1 retention campaigns personalized to each customer. Cortex makes 3.5+ billion predictions per day and processes 5k+ events per second.

Our founders have been recognized as the Ernst & Young Entrepreneurs of the Year, and our company was awarded Top 10 Big Data Startup of the Year by CRN, one of Fast Company's nnovation Agents, Top 10 Software Company in Southern California from SocalTech, and identified by Inc. Magazine as one of the most innovative and fastest growing startups. Retention Science has also been featured in Forbes, the Wall Street Journal, TechCrunch, Bloomberg, and Reuters, among other notable publications. In addition, the most prestigious startup accelerator in LA (part of the TechStar Network), as well as many reputable angel investors and Venture Capital firms have provided their support and backing for our business.

We're passionate about what we do and we put our people first! We are a close-knit family whose members drink too much coffee, work hard, and never cease to brainstorm creative new ways to improve our solutions. We foster a dynamic and exciting start-up environment that is conducive for innovative thought; join us if you are interested in working with our world-class team!","3+ years experience data engineering development using scala/python Experience designing and implementing large, scalable services Experience with automating & deploying micro-services Experience working with third-party APIs and creating robust APIs Know when and where to optimize and cache Comfortable working with large data sets and distributed systems  datastores, frameworks  Experience with streaming / realtime technologies  Kinesis, Kafka, Spark streaming etc  Deep understanding of MySQL, Postgres, Redshift, relational databases etc. Understand when to use NOSQL databases / other distributed ""big data"" technologies Experience with AWS technologies Startup work experience a major plus!    ","3+ years experience data engineering development using scala/python Experience designing and implementing large, scalable services with automating & deploying micro-services working third-party APIs creating robust Know when where to optimize cache Comfortable large sets distributed systems datastores, frameworks streaming / realtime technologies Kinesis, Kafka, Spark etc Deep understanding of MySQL, Postgres, Redshift, relational databases etc. Understand use NOSQL other ""big data"" AWS Startup work a major plus!","3+ years experience data engineering development using scala/python Experience designing implementing large, scalable services automating & deploying micro-services working third-party APIs creating robust Know optimize cache Comfortable large sets distributed systems datastores, frameworks streaming / realtime technologies Kinesis, Kafka, Spark etc Deep understanding MySQL, Postgres, Redshift, relational databases etc. Understand use NOSQL ""big data"" AWS Startup work major plus!"
401,Data Engineer,Principal Data Engineer,"Glendale, CA",Glendale,CA,"The Walt Disney Studios is comprised of large team of creative professionals who produce and acquire live-action and animated motion pictures, direct-to-video content, television and serial properties, musical recordings and live stage plays that tell memorable, life changing stories. Few media and entertainment companies can rival the brands we have the honor of bringing to the world.

Responsibilities :
Architect and lead the implementation of distributed systems solutions to data engineering problems.
Partner with management in hiring and team building
Analyze business needs, and design and build high-quality solutions
Balance resources, requirements, and complexity
Collaborate with customer representatives, product and project management teams
Evangelize technologies, solutions, and best practices
Contribute new ideas to a larger community of high-caliber professionals


Basic Qualifications :
We are looking for a seasoned, dynamic engineer who wants to shape the way that a major, fortune 100 entertainment company engages with data analytics. You have shipped millions of lines of code. You have lead teams, in a technical (and maybe managerial) role. You think naturally in terms of tradeoffs between flexibility and simplicity, between usability and security, between timeline, budget, and quality.

We are looking for the following in a candidate:
An educational background that has facilitated your technology career-surprise us!
10+ years of professional software experience
Passion for building high-quality software and systems that deliver business value
Passion about system design, with interests and experience across the architecture from virtual machine and language internals to distributed systems performance and analysis
Fluency with the fundamentals of algorithms and data structures
You should have some mastery of:
Spark, Hive, Databricks, and other big data technologies
SQL and related tools Docker and Kubernetes
The AWS environment and toolset
A range of JVM/CLR languages, as well as scripting/systems languages like Python
Distributed systems and API design
Testing practices (unit, integration, performance)
Revision control with git


Preferred Qualifications:
It would be great if you knew anything about:
Kafka and other streaming technologies
Dependency and build tools for continuous integration and deployment
Elastic Search, Kibana, or other aggregation and search tools
Teradata, Informatica, Snowflake, and other data warehouse solutions
Tableau, MicroStrategy, d3, and other data visualization tools
It would be cool if you have experience with or interest in:
Data science, statistics, machine learning and other applied mathematics
Data lake implementation and support
GraphQL
DynamoDB or RDS","An educational background that has facilitated your technology career-surprise us! 10+ years of professional software experience Passion for building high-quality software and systems that deliver business value Passion about system design, with interests and experience across the architecture from virtual machine and language internals to distributed systems performance and analysis Fluency with the fundamentals of algorithms and data structures   Architect and lead the implementation of distributed systems solutions to data engineering problems. Partner with management in hiring and team building Analyze business needs, and design and build high-quality solutions Balance resources, requirements, and complexity Collaborate with customer representatives, product and project management teams Evangelize technologies, solutions, and best practices Contribute new ideas to a larger community of high-caliber professionals   ","An educational background that has facilitated your technology career-surprise us! 10+ years of professional software experience Passion for building high-quality and systems deliver business value about system design, with interests across the architecture from virtual machine language internals to distributed performance analysis Fluency fundamentals algorithms data structures Architect lead implementation solutions engineering problems. Partner management in hiring team Analyze needs, design build Balance resources, requirements, complexity Collaborate customer representatives, product project teams Evangelize technologies, solutions, best practices Contribute new ideas a larger community high-caliber professionals","An educational background facilitated technology career-surprise us! 10+ years professional software experience Passion building high-quality systems deliver business value system design, interests across architecture virtual machine language internals distributed performance analysis Fluency fundamentals algorithms data structures Architect lead implementation solutions engineering problems. Partner management hiring team Analyze needs, design build Balance resources, requirements, complexity Collaborate customer representatives, product project teams Evangelize technologies, solutions, best practices Contribute new ideas larger community high-caliber professionals"
402,Data Engineer,Data Center Engineer,"Santa Monica, CA 90401",Santa Monica,CA,"The data center engineer, recognized as a versatile technical expert, is responsible for the operations of RANDâs three Data Centers and over 50 communication rooms.
The Data Center engineer will design, install, maintain, and retire data center hardware (switches, storage, servers) and their connections. The engineer is responsible to regularly update equipment firmware, operating systems, and patch equipment as recommended by the various vendors. The engineer is expected to perform various networking, storage, and server functions.

The data center engineer is responsible for preparing regular reports on the state of data center equipment to Management, specifically the maintenance state, location, and lifecycle of hardware. Additionally, the engineer is responsible for monitoring data center resources to proactively identify issues.

The data center engineer works in close partnership with the Information Technology Asset Manager, other members on the Network Operations group and with vendors on data center equipment lifecycle, upgrades, and best practice configurations.

Qualifications

Deep technical understanding of the operation and management of Ciscoâs Unified Computing System, HP servers, and Dell servers.
Deep technical understanding of the operation and management of network attached storage and fiber interconnects.
Extensive experience in conducting equipment firmware and operating system upgrades.
Integration process of equipment in Data Center and all user Intermediate Distribution Frame (IDF) closets with cable management, patch panel, and appropriate preventative maintenance experience.
Experience with DCIM tool and maintaining diagrams, drawing schematics, and documentation of data center equipment to include serial numbers, asset tags, to location (row, rack, etc.).
High degree of initiative and dependability. Experience with supporting data center services 24/7, responding to service interruptions at any time.
Ability to work with little supervision, proactively identify and respond to issues before they impact service to the RAND staff.
Experience with Information Technology Service Management (ITSM) practices including Change Management, Problem Management, and Incident Management.
Highly accountable with a significant focus on customer service and high network availability.
Excellent technical written, diagramming, and oral communication skills with the ability to effectively communicate with information technology professionals as well as management.

Preferred Skills:

Experience with performing system backups, scheduling tape rotations, and using Veeam backup solution
Understanding of the operation and management of Ciscoâs Nexus switches and patching network devices
Experience and understanding of UPS systems, HVAC systems, and other physical infrastructure associated with temperature control and power systems


Education Requirements

Bachelorâs degree in Computer Science, or a related field or equivalent work experience is acceptable. Strong consideration will be given to related professional certifications.


Experience

Minimum 8 years of Data Center and network hardware experience.

Security Clearance

U.S. Citizenship is required to obtain a security clearance. This position requires the ability to obtain and maintain a security clearance.


Location

Santa Monica


Positions Open

One","  Experience with performing system backups, scheduling tape rotations, and using Veeam backup solution Understanding of the operation and management of Ciscoâs Nexus switches and patching network devices Experience and understanding of UPS systems, HVAC systems, and other physical infrastructure associated with temperature control and power systems   ","Experience with performing system backups, scheduling tape rotations, and using Veeam backup solution Understanding of the operation management Ciscoâs Nexus switches patching network devices understanding UPS systems, HVAC other physical infrastructure associated temperature control power systems","Experience performing system backups, scheduling tape rotations, using Veeam backup solution Understanding operation management Ciscoâs Nexus switches patching network devices understanding UPS systems, HVAC physical infrastructure associated temperature control power systems"
403,Data Engineer,Database Engineer Intern,"Los Angeles, CA 90064",Los Angeles,CA,"Riot Games was founded in 2006 by Brandon Beck and Marc Merrill with the intent to change the way video games are made and supported for players. In 2009, Riot released its debut title League of Legends and over 100 million people now play the game every month. Whether you're in Rio, Seoul, or Moscow, you can find an excited and engaged community of League players. Delivering content to a global audience of millions of players whilst also building new games affords Riot's Engineering discipline with a mountain of exciting and technically complex challenges. That's where you come in.

As a Database Engineer at Riot, you'll have a variety of database and infrastructure tools in your arsenal, and a curiosity to research and evaluate new technologies that will deliver the most player value. You'll come equipped with knowledge of MySQL and Linux, as well as the ability to script in a language of your choice (Golang is a plus!). You have a natural tendency to automate tasks whenever possible, including disaster recovery, backup testing, deployments, monitoring, and clustering. You'll also partner with other DBEs and engineers across Riot in order to understand their data needs, troubleshoot high priority errors, and collaborate on the architecture and implementation of existing database solutions.

---------------------------------------

As a Database Engineer Intern, you may:
---------------------------------------


Work with the LDBE in supporting and maintaining over 1800 databases.
Automation through various technologies
Experience in-depth performance tuning for databases
Learn how to resolve live incidents related to databases.

--------------------------

Internship Qualifications:
--------------------------


Enrolled in a college or university
Returning for at least one more term following the internship
Available to work full-time hours for 10-12 weeks at the indicated office during the summer
Authorized to work in the U.S.

------------------------

Required Qualifications:
------------------------


Comfortable programming in Python or Java
Knowledge of data structures and algorithms
Experience working with Linux operating systems
Experience with SQL

-----------------------

Desired Qualifications:
-----------------------


Basic Networking Knowledge
Basic OS and file system knowledge
Basic database systems knowledge (MySQL, AWS Aurora, AWS RDS)

For this role, you'll find success through craft expertise, a collaborative spirit, and decision-making that prioritizes your fellow Rioters, who are the customers of your work. Being a dedicated fan of games is not necessary for this position!

----------

Our Perks:
----------

Riot Internships are paid, and we offer relocation and housing support. You can expect fully subsidized meals and the ability to participate in our Play Fund, so you can broaden and deepen your knowledge of our players and community through games. The University Programs team also invites you to intern focused events so you can connect with your fellow interns and Rioters to make the most out of your experience.

Applications Close October 31st 11:59pm PT
------------------------------------------

===

It's our policy to provide equal employment opportunity for all applicants and members of Riot Games, Inc. Riot Games makes reasonable accommodations for handicapped and disabled Rioters and does not unlawfully discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, handicap, veteran status, marital status, criminal history, or any other category protected by applicable federal and state law, including the City of Los Angeles' Fair Chance Initiative for Hiring Ordinance relating to an applicant's criminal history (LAMC 189.00)", Enrolled in a college or university Returning for at least one more term following the internship Available to work full-time hours for 10-12 weeks at the indicated office during the summer Authorized to work in the U.S.     ,Enrolled in a college or university Returning for at least one more term following the internship Available to work full-time hours 10-12 weeks indicated office during summer Authorized U.S.,Enrolled college university Returning least one term following internship Available work full-time hours 10-12 weeks indicated office summer Authorized U.S.
404,Data Engineer,Lead Database Engineer (Data Modeler),"Pasadena, CA 91107",Pasadena,CA,"Job Type:
Regular
Department:
Data Engineering
City:
Pasadena
State/Territory:
California
FLSA:
Exempt
Job Summary
Green Dot is seeking a Database Engineer to work on multiple applications and data development projects that help millions of people manage, access and spend their money every day.

You should have hands-on experience in a multitude of domains including but not limited to: 24x7x365 enterprise and cloud database infrastructure design and management, security, business continuity, disaster recovery, database design, database development, database tuning, enterprise data storage and backup, service delivery, incident tracking, change management, and general database administration.

Job Responsibilities

Design strategies for database systems as well as set standards for operations, programming and security.
Develop physical modeling standards and strategies to support OLTP and Data Warehouse domains.
Ensures physical database features and capabilities are incorporated into data model designs to optimize performance.
Designs deliverables that are defined with appropriate database platform and security context.
Perform problem-solving of application issues and production errors, including high level critical production issues that require immediate attention.
Design and code a high volume of SQL Queries, stored procedures, and SSIS packages
Monitor and perform performance tuning on stored procedures and ETL jobs
Monitor and analyze SQL Server production metrics
Job Requirements

Strong knowledge and experience in data modeling, relational database design, optimization for performance, T-SQL, stored procedures, SSIS packages
Extensive knowledge of SQL Server including the writing of complex stored procedures, functions and SSIS packages as well as basic knowledge of database administration
Strong understanding and experience of database development methodologies (Agile and Scrum)
Expert level knowledge of SQL administration, engineering, and monitoring tools
Demonstrated expertise in performance tuning including both query and server optimization
Expertise in understanding complex business needs, analyzing, designing and developing solutions
Advanced knowledge of relational database design, testing and implementation
Advanced communication and professional skills and the ability to establish relationships across business units
Ability to participate in an on-call support rotation and provide non-standard work hour support
Extensive SQL Server performance tuning and troubleshooting experience
Hands-on development experience in SQL Server 2012/2016
Experience with SSIS
Financial technology domain knowledge would be a plus
Experience with .NET, MVC, C# HTML5 CSS3 AJAX jQuery IIS and Java script would be a plus
Computer Science Degree Highly Desired
Green Dot Corporation is committed to achieving a diverse workforce and is proud to be an equal opportunity employer without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any category protected by law.


We do not accept unsolicited resumes from employment agencies. No fee of any kind will be paid in the event we hire a candidate whose resume is submitted by an employment agency to this career site or directly to any of our employees. Such resume shall be deemed the sole property of Green Dot Corporation. Employment agencies that have fee agreements with us and have been directed by our designated HR personnel to recruit for a particular position may submit resumes to such designated HR personnel or as otherwise directed by such personnel.","   Design strategies for database systems as well as set standards for operations, programming and security. Develop physical modeling standards and strategies to support OLTP and Data Warehouse domains. Ensures physical database features and capabilities are incorporated into data model designs to optimize performance. Designs deliverables that are defined with appropriate database platform and security context. Perform problem-solving of application issues and production errors, including high level critical production issues that require immediate attention. Design and code a high volume of SQL Queries, stored procedures, and SSIS packages Monitor and perform performance tuning on stored procedures and ETL jobs Monitor and analyze SQL Server production metrics    Strong knowledge and experience in data modeling, relational database design, optimization for performance, T-SQL, stored procedures, SSIS packages Extensive knowledge of SQL Server including the writing of complex stored procedures, functions and SSIS packages as well as basic knowledge of database administration Strong understanding and experience of database development methodologies  Agile and Scrum  Expert level knowledge of SQL administration, engineering, and monitoring tools Demonstrated expertise in performance tuning including both query and server optimization Expertise in understanding complex business needs, analyzing, designing and developing solutions Advanced knowledge of relational database design, testing and implementation Advanced communication and professional skills and the ability to establish relationships across business units Ability to participate in an on-call support rotation and provide non-standard work hour support Extensive SQL Server performance tuning and troubleshooting experience Hands-on development experience in SQL Server 2012/2016 Experience with SSIS Financial technology domain knowledge would be a plus Experience with .NET, MVC, C  HTML5 CSS3 AJAX jQuery IIS and Java script would be a plus Computer Science Degree Highly Desired ","Design strategies for database systems as well set standards operations, programming and security. Develop physical modeling to support OLTP Data Warehouse domains. Ensures features capabilities are incorporated into data model designs optimize performance. Designs deliverables that defined with appropriate platform security context. Perform problem-solving of application issues production errors, including high level critical require immediate attention. code a volume SQL Queries, stored procedures, SSIS packages Monitor perform performance tuning on procedures ETL jobs analyze Server metrics Strong knowledge experience in modeling, relational design, optimization performance, T-SQL, Extensive the writing complex functions basic administration understanding development methodologies Agile Scrum Expert administration, engineering, monitoring tools Demonstrated expertise both query server Expertise business needs, analyzing, designing developing solutions Advanced testing implementation communication professional skills ability establish relationships across units Ability participate an on-call rotation provide non-standard work hour troubleshooting Hands-on 2012/2016 Experience Financial technology domain would be plus .NET, MVC, C HTML5 CSS3 AJAX jQuery IIS Java script Computer Science Degree Highly Desired","Design strategies database systems well set standards operations, programming security. Develop physical modeling support OLTP Data Warehouse domains. Ensures features capabilities incorporated data model designs optimize performance. Designs deliverables defined appropriate platform security context. Perform problem-solving application issues production errors, including high level critical require immediate attention. code volume SQL Queries, stored procedures, SSIS packages Monitor perform performance tuning procedures ETL jobs analyze Server metrics Strong knowledge experience modeling, relational design, optimization performance, T-SQL, Extensive writing complex functions basic administration understanding development methodologies Agile Scrum Expert administration, engineering, monitoring tools Demonstrated expertise query server Expertise business needs, analyzing, designing developing solutions Advanced testing implementation communication professional skills ability establish relationships across units Ability participate on-call rotation provide non-standard work hour troubleshooting Hands-on 2012/2016 Experience Financial technology domain would plus .NET, MVC, C HTML5 CSS3 AJAX jQuery IIS Java script Computer Science Degree Highly Desired"
405,Data Engineer,Sr. Data Engineer,"Glendale, CA",Glendale,CA,"As a Senior Data Engineer: You will create custom batch-oriented and real-time streaming data pipelines, creating new data workloads/applications and also migrating applications from on-premise ecosystems to cloud data lakes. Implement, troubleshoot, and optimize distributed solutions based on modern big data technologies like Hive, Hadoop, Spark, Elastic Search, Kafka, etc. to solve large scale processing problems in a cloud cluster environment. Ensure proper data governance policies are followed by implementing defining project specific quality checks, classification, etc. Apply technical expertise to challenging programming and design problems. Strong communication, interpersonal and teaming skills, including the ability to work effectively in agile / scrum environment. Demonstrated interest and ability to quickly learn new technologies.

Responsibilities :
Proficient understanding of distributed computing principles
Experience developing data storage processing and streaming flows using technologies like Spark, HDFS, Yarn and Hadoop
Experience with various messaging systems, such as Kafka or RabbitMQ
Good knowledge of Big Data querying tools, such as Pig or Hive
Good understanding of Lambda Architecture, along with its advantages and drawbacks
Experience with integration of data from multiple data sources
Ability to solve any ongoing issues with operating the cluster
Experience with scripting languages and automation of processes
Proven track record of development accomplishments in highly collaborative environment
Translate complex functional and technical requirements into detailed design
Be passionate about solving customer problems and develop solutions that result in a passionate customer/community following

Basic Qualifications :
Experience working with cloud as infrastructure (Google/AWS/Azure) or other cloud platform based on IaaS and PaaS Solutions
2+ years working with and demonstrating a solid understanding of open source data management components and services including Hadoop, Spark, Flume, Kafka and Hive
Experience with Google BigQuery, App Engine, Dataflow
Proficiency in SQL, R, Python, Java, Java Script and Web Services, and experience in visualization tools like Tableau, Looker, MicroStrategy and PowerBI
NoSQL DB (Dynamo DB/Mongo DB knowledge is helpful)
Experience with Code Management/CICD tools like GITHub, Jenkins is useful
Prior experience with automation is helpful
Prior experience working in a Linux/Unix environment is helpful
Prior experience working in an enterprise technology environment is helpful
Understanding of AI/ML services: TensorFlow, Keras, Rekognition, and/or PyTorch
Strong written and verbal communication skills
You will be a rock-star if you have:
Passion for learning new technologies and comfort with âfail-fastâ mentality, and sharing that passion with your peers
Innovation mindset, driving cultural change with the creative use of data and technology
Demonstrated ability to think strategically, and partner with business and technology teams to define solutions
Experience participating on projects in Agile mode: Scrum, Kaizen and Kanban
Experience with Big Data and open source ecosystems: HortonWorks, Nifi, Spark, Flume, Kafka and Hive
Understanding of WDP&Râs technical infrastructure and server environments
Innovation mindset, driving cultural change with the creative use of data and technology

Required Education :
Bachelorâs degree in Computer Science or related field or equivalent work experience","Experience working with cloud as infrastructure  Google/AWS/Azure  or other cloud platform based on IaaS and PaaS Solutions 2+ years working with and demonstrating a solid understanding of open source data management components and services including Hadoop, Spark, Flume, Kafka and Hive Experience with Google BigQuery, App Engine, Dataflow Proficiency in SQL, R, Python, Java, Java Script and Web Services, and experience in visualization tools like Tableau, Looker, MicroStrategy and PowerBI NoSQL DB  Dynamo DB/Mongo DB knowledge is helpful  Experience with Code Management/CICD tools like GITHub, Jenkins is useful Prior experience with automation is helpful Prior experience working in a Linux/Unix environment is helpful Prior experience working in an enterprise technology environment is helpful Understanding of AI/ML services  TensorFlow, Keras, Rekognition, and/or PyTorch Strong written and verbal communication skills  Proficient understanding of distributed computing principles Experience developing data storage processing and streaming flows using technologies like Spark, HDFS, Yarn and Hadoop Experience with various messaging systems, such as Kafka or RabbitMQ Good knowledge of Big Data querying tools, such as Pig or Hive Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with integration of data from multiple data sources Ability to solve any ongoing issues with operating the cluster Experience with scripting languages and automation of processes Proven track record of development accomplishments in highly collaborative environment Translate complex functional and technical requirements into detailed design Be passionate about solving customer problems and develop solutions that result in a passionate customer/community following  ","Experience working with cloud as infrastructure Google/AWS/Azure or other platform based on IaaS and PaaS Solutions 2+ years demonstrating a solid understanding of open source data management components services including Hadoop, Spark, Flume, Kafka Hive Google BigQuery, App Engine, Dataflow Proficiency in SQL, R, Python, Java, Java Script Web Services, experience visualization tools like Tableau, Looker, MicroStrategy PowerBI NoSQL DB Dynamo DB/Mongo knowledge is helpful Code Management/CICD GITHub, Jenkins useful Prior automation Linux/Unix environment an enterprise technology Understanding AI/ML TensorFlow, Keras, Rekognition, and/or PyTorch Strong written verbal communication skills Proficient distributed computing principles developing storage processing streaming flows using technologies HDFS, Yarn Hadoop various messaging systems, such RabbitMQ Good Big Data querying tools, Pig Lambda Architecture, along its advantages drawbacks integration from multiple sources Ability to solve any ongoing issues operating the cluster scripting languages processes Proven track record development accomplishments highly collaborative Translate complex functional technical requirements into detailed design Be passionate about solving customer problems develop solutions that result customer/community following","Experience working cloud infrastructure Google/AWS/Azure platform based IaaS PaaS Solutions 2+ years demonstrating solid understanding open source data management components services including Hadoop, Spark, Flume, Kafka Hive Google BigQuery, App Engine, Dataflow Proficiency SQL, R, Python, Java, Java Script Web Services, experience visualization tools like Tableau, Looker, MicroStrategy PowerBI NoSQL DB Dynamo DB/Mongo knowledge helpful Code Management/CICD GITHub, Jenkins useful Prior automation Linux/Unix environment enterprise technology Understanding AI/ML TensorFlow, Keras, Rekognition, and/or PyTorch Strong written verbal communication skills Proficient distributed computing principles developing storage processing streaming flows using technologies HDFS, Yarn Hadoop various messaging systems, RabbitMQ Good Big Data querying tools, Pig Lambda Architecture, along advantages drawbacks integration multiple sources Ability solve ongoing issues operating cluster scripting languages processes Proven track record development accomplishments highly collaborative Translate complex functional technical requirements detailed design Be passionate solving customer problems develop solutions result customer/community following"
406,Data Engineer,Data Engineer- Python,"Los Angeles, CA 90013",Los Angeles,CA,"As a Senior Consultant, you will focus on managing the information supply chain from acquisition to ingestion, storage and the provisioning of data to points of impact by modernizing and enabling new capabilities. Information value is enhanced through enterprise-scale applications that enable visualization, consumption and monetization of both structured and unstructured data. Big data is becoming one of the most important technology trends that has the potential for dramatically changing the way organizations use information to enhance the customer experience and transform their business models.
Work you'll do

Senior Consultants work within an engagement team. Key responsibilities will include:
 Function as integrators between business needs and technology solutions, helping to create technology solutions to meet clientsâ business needs.
 Identifying business requirements, requirements management, functional design, prototyping, process design (including scenario design, flow mapping), testing, training, defining support procedures and supporting implementations.

The Team

Analytics & Cognitive

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.


The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.


Analytics & Cognitive will work with our clients to:
 Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
 Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
 Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements


Qualifications

Required:

 5+ years of experience in core JAVA and SQL
 3+ years of experience in Python& Unix Shell Scripting
 3+ years of experience in building scalable and high performance data pipelines using Apache Hadoop, Map Reduce, Pig & Hive
 Experience with bigdata cross platform compatible file formats like Apache Avro & Apache Parquet
 Experience in Apache Spark is a plus
 1+ years of experience with data lake implementations, core modernizations and data ingestion

 1 or more years of hands on experience designing and implementing data ingestion techniques for real time and batch processes for video, voice, weblog, sensor, machine and social media data into Hadoop ecosystems and HDFS clusters.
 2+ years of experience leading workstreams or small teams
 Willingness for weekly client-based travel, up to 80-100% (Monday â Thursday/Friday)
 Bachelorâs Degree or equivalent professional experience

 Preferred:

AWS Certification, Hadoop Certification or Spark Certification
Experience with Cloud using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
Experience with data integration products like Informatica Power Center Big Data Edition (BDE), IBM BigInsights, Talend etc.
Experience designing and implementing reporting and visualization for unstructured and structured data sets
Experience in designing and implementing scalable, distributed systems leveraging cloud computing technologies like AWS EC2, AWS Elastic Map Reduce and Microsoft Azure
Experience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
Knowledge of data, master data and metadata related standards, processes and technology
Experience working with multi-Terabyte data sets
Experience with Data Integration on traditional and Hadoop environments
Ability to work independently, manage small engagements or parts of large engagements.
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint).
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment.
Eagerness to mentor junior staff.
An advanced degree in the area of specialization is preferred.

How youâll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe thereâs always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.


Benefits

At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitteâs culture

Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.


Corporate citizenship

Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitteâs impact on the world.


Recruiter tips

We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youâre applying to. Check out recruiting tips from Deloitte professionals.

#LI:PTY
#IND:PTY", 5+ years of experience in core JAVA and SQL    ,5+ years of experience in core JAVA and SQL,5+ years experience core JAVA SQL
407,Data Engineer,Data Integration Engineer,"Los Angeles, CA",Los Angeles,CA,"ABOUT NuORDER:
NuORDER is a venture-backed, B2B eCommerce technology company revolutionizing the way the $130 billion wholesale business is done. Since 2011, weâve offered a platform to empower brands and retailers to conduct their business in a smarter, modern, more efficient way and 100% online. Weâre digitizing the buying process and currently partnering with over 1,200 brands and 500,000 retailers within different verticals to provide a central place to browse different products and catalogs, access up-to-the-minute sales and inventory data, and place orders any time of day, even via mobile.

THE TEAM:
This position, located in Los Angeles, is part of the Data Integration team within the Services department, which owns the onboarding, enhancement, and support of data integrations between the NuOrder platform and our clientsâ ERP, PLM, and other database systems. Our customers are top brands and retailers, and we help them integrate their internal systems with our Wholesale eCommerce platform to fully automate their order and sales pipeline. The Data Integration team is responsible for the data infrastructure that drives the partnership with customers and ensures their success through broad adoption of our solutions and best practices.

POSITION SUMMARY:
As a Data Integration Engineer, you will be responsible for scoping and delivering integrations and customization solutions that connect the NuOrder platform to data sources and workflows within third party applications. Your efforts will be critical to driving the long-term partnership between NuOrder and our customers.
You will drive the successful deployment of projects through development of solution infrastructure and the completion of integration jobs. You will be primarily responsible for solution designs and completing implementations. You will be expected to contribute to several implementations simultaneously, whether to setup new customers or to complete enhancement and expansion projects for existing customers. You will work directly with our clients and internal project teams to analyze and map data files, design solutions that address client needs, and implement specific business rule configurations.
Essential Functions
Develop scalable and extensible approaches to data integration between the NuOrder platform and third-party applications
Provide technical expertise on design and architecture decisions as it relates to integration tactics and strategy
Actively deliver multiple customer and partner projects simultaneously and resolve obstacles to completion
Design and execute manual and automated tests, analyze the results, and deduce next steps
Understand and interpret the business objectives and requirements in order to identify, access, and integrate the customerâs data sources
Get involved in the pre-sales cycle to understand specific client requirements and contribute to statement of work proposals by producing technical requirements and cost estimates
Collaborate with Customer Success, Product Management, and Engineering to identify and prioritize new platform features based on market needs
Qualifications
Client services or programming experience with responsibility for scoping and delivering technical projects
Experience with full-stack web application development using modern Javascript frameworks (NodeJS)
Demonstrated experience building, testing, and troubleshooting web applications built around web services and RESTful APIs
Knowledge in database architecture, system environments, SQL, and other data integration tools
Experience with UNIX / Linux command line
Bonus points for experience with any of the following: Go, Java, Node.js, React, Bigtable, MongoDB, PostgreSQL, ElasticSearch, AWS, Docker
Experience working in Agile development environments
Project management expertise with an eye for detail and organized thought process
Good speaker with the ability to build rapport with key stakeholders inside and outside the organization
Experience in a fast-paced, high growth environment where decision making and swift execution command a high premium
Education/Certifications
Work Experience: 3+ years of experience with client services or engineering, preferably in a B2B SaaS company
Education: An undergraduate degree or equivalent work experience
Skills: Proficiency in web services, APIs, HTTP, JavaScript, ETL, SQL, Linux/Unix. Comfortable in customer facing positions.
Why NuORDER?
Weâre creative, innovative, and helping businesses become more efficient. Ensuring that each member of our team feels fulfilled and on track to become the very best employee they can be is important - and we encourage our people to discover new ways of achieving specific goals. We fully believe that each and every individual part of our organization provides value, a new perspective and progress to NuORDERâs growth and success. Come join us so we can build together!

BENEFITS :
NuORDER offers a competitive salary and benefits package complete with medical, dental & vision insurance, a matching 401k program, flexible PTO & a wide array of holidays. We also offer paid maternal and paternal leave.

As a NuORDER employee, you can expect a variety of learning and development opportunities and a plethora of snacks, coffee, and sparkling water in our fully stocked kitchen! In addition, itâs not unusual for us to have Bagel Fridays, free lunches, game nights, and happy hours to bond as a team!"," Client services or programming experience with responsibility for scoping and delivering technical projects Experience with full-stack web application development using modern Javascript frameworks  NodeJS  Demonstrated experience building, testing, and troubleshooting web applications built around web services and RESTful APIs Knowledge in database architecture, system environments, SQL, and other data integration tools Experience with UNIX / Linux command line Bonus points for experience with any of the following  Go, Java, Node.js, React, Bigtable, MongoDB, PostgreSQL, ElasticSearch, AWS, Docker Experience working in Agile development environments Project management expertise with an eye for detail and organized thought process Good speaker with the ability to build rapport with key stakeholders inside and outside the organization Experience in a fast-paced, high growth environment where decision making and swift execution command a high premium    Work Experience  3+ years of experience with client services or engineering, preferably in a B2B SaaS company Education  An undergraduate degree or equivalent work experience Skills  Proficiency in web services, APIs, HTTP, JavaScript, ETL, SQL, Linux/Unix. Comfortable in customer facing positions. ","Client services or programming experience with responsibility for scoping and delivering technical projects Experience full-stack web application development using modern Javascript frameworks NodeJS Demonstrated building, testing, troubleshooting applications built around RESTful APIs Knowledge in database architecture, system environments, SQL, other data integration tools UNIX / Linux command line Bonus points any of the following Go, Java, Node.js, React, Bigtable, MongoDB, PostgreSQL, ElasticSearch, AWS, Docker working Agile environments Project management expertise an eye detail organized thought process Good speaker ability to build rapport key stakeholders inside outside organization a fast-paced, high growth environment where decision making swift execution premium Work 3+ years client engineering, preferably B2B SaaS company Education An undergraduate degree equivalent work Skills Proficiency services, APIs, HTTP, JavaScript, ETL, Linux/Unix. Comfortable customer facing positions.","Client services programming experience responsibility scoping delivering technical projects Experience full-stack web application development using modern Javascript frameworks NodeJS Demonstrated building, testing, troubleshooting applications built around RESTful APIs Knowledge database architecture, system environments, SQL, data integration tools UNIX / Linux command line Bonus points following Go, Java, Node.js, React, Bigtable, MongoDB, PostgreSQL, ElasticSearch, AWS, Docker working Agile environments Project management expertise eye detail organized thought process Good speaker ability build rapport key stakeholders inside outside organization fast-paced, high growth environment decision making swift execution premium Work 3+ years client engineering, preferably B2B SaaS company Education An undergraduate degree equivalent work Skills Proficiency services, APIs, HTTP, JavaScript, ETL, Linux/Unix. Comfortable customer facing positions."
408,Data Engineer,"Senior Software Engineer, Data","Santa Monica, CA",Santa Monica,CA,"Who is Bird
At Bird, weâre on a mission to make cities more livable by reducing traffic and carbon emissions with an affordable, convenient, and eco-friendly transportation option. We planted roots in Los Angeles, California. Now, our fleet of electric scooters is available to riders in cities around the world for short trips or the âlast mileâ of their journey. In our first year, Bird launched in over 100 cities and provided over 10 million rides. And weâre only just getting started.


Responsibilities
Build APIs and design documents
Work closely with various business partners (firmware team, UI/UX)
Architecture design
Large scale data processing to understand user behavior and help define new product features
Requirements
Bachelor's degree in Computer Science or related technical discipline
7+ years of industry experience
Preferred Qualifications
Experience in Java, Kotlin, JVM based languages (Scala, Clojure, Groovy, JRuby)
SQL experience (Postgresql is a plus)
Distributed Computing experience
Excellent understanding of computer science fundamentals, data structures, and algorithms
Expertise required in object-oriented design methodology and large scale application development in an object oriented language
Understanding of distributed systems and service oriented architecture
Large scale data processing experience
Functional programming experience
RabbitMQ
Technology we're working with
Flink
Hadoop
Kafka
Redis
Culture at Bird
We're an ambitious, smart, and open-minded group. Our employees are passionate about our mission, and eager to complete their work at the highest level. The office itself is up tempo and supportive, because we care about each other. People first, people.

Perks up
We want people to succeed at Bird, so we give our teams plenty of time off to relax and recharge. We also offer a generous employer-paid healthcare coverage, on-demand doctor visits, childcare support, a pre-tax commuter account for mass transit or parking, a wellness stipend, and more.

Bird is the world
Bird stands for a culture of inclusion. We celebrate different backgrounds, experiences, and perspectives âencouraging everyone to bring their authentic selves to work. We have a diverse environment that empowers our team to feel comfortable when they voice their opinions. For these reasons and more Bird is a proud equal employment opportunity employer. We welcome everyone regardless of their race, color, religion, sex, national origin, age, disability, veteran status, or genetics, and we are dedicated to providing an inclusive, open, and diverse work environment.

Sound like a place youâd like to work? Sweet. Letâs chat."," Experience in Java, Kotlin, JVM based languages  Scala, Clojure, Groovy, JRuby  SQL experience  Postgresql is a plus  Distributed Computing experience Excellent understanding of computer science fundamentals, data structures, and algorithms Expertise required in object-oriented design methodology and large scale application development in an object oriented language Understanding of distributed systems and service oriented architecture Large scale data processing experience Functional programming experience RabbitMQ    Build APIs and design documents Work closely with various business partners  firmware team, UI/UX  Architecture design Large scale data processing to understand user behavior and help define new product features   Bachelor's degree in Computer Science or related technical discipline 7+ years of industry experience","Experience in Java, Kotlin, JVM based languages Scala, Clojure, Groovy, JRuby SQL experience Postgresql is a plus Distributed Computing Excellent understanding of computer science fundamentals, data structures, and algorithms Expertise required object-oriented design methodology large scale application development an object oriented language Understanding distributed systems service architecture Large processing Functional programming RabbitMQ Build APIs documents Work closely with various business partners firmware team, UI/UX Architecture to understand user behavior help define new product features Bachelor's degree Computer Science or related technical discipline 7+ years industry","Experience Java, Kotlin, JVM based languages Scala, Clojure, Groovy, JRuby SQL experience Postgresql plus Distributed Computing Excellent understanding computer science fundamentals, data structures, algorithms Expertise required object-oriented design methodology large scale application development object oriented language Understanding distributed systems service architecture Large processing Functional programming RabbitMQ Build APIs documents Work closely various business partners firmware team, UI/UX Architecture understand user behavior help define new product features Bachelor's degree Computer Science related technical discipline 7+ years industry"
409,Data Engineer,PRINCIPAL SOFTWARE ENGINEER (IoT/BIG DATA),"Los Angeles County, CA",Los Angeles County,CA,"Basic Function
Performs as a Technical Project Leader of complex system development or enhancement projects and/or multiple application support teams, which may include various Metro Information Technology personnel, technical contractors, consultants or other internal or external entities.
Example Of Duties
Leads technical personnel in support of related systems
Oversees technical personnel to accurately and reliably produce required reports and documents
Leads the evaluation, configuration, implementation, and integration of open source software
Oversees related processes; ensures tasks are implemented in a timely and accurate manner
Oversees the software development and enhancement, and provides production support for business application systems on behalf of client departments
Sets guidelines and exercises leadership skills to establish and meet target dates on large and highly-complex projects involving multiple systems
Acts as technical team leader and maintains responsibility for analysis, design and implementation of projects as assigned, including complex and highly specialized applications
Conducts the most complex programming, system analysis and design of highly-complex systems using Information System Development Methodology in a multi-platform infrastructure and environment
Consults and advises user departments and other Information Technology Services support teams on system requirements and enhancements and supports large computer systems for various departments
Recommends approval of system designs and interacts with user managers and department users in a senior leadership capacity
Resolves highly difficult problems in large highly complex applications systems using highly proactive problem management, diagnosis and resolution
Identifies diagnoses and resolves system problems, at the most complex levels, involving programming logic, interoperability between several technical components, and client interface
Researches, identifies, and refers complex technical problems related to platform management, database administration, network connectivity and desktop support to the appropriate ITS operating unit for resolution
Spearheads the preparation of project reports such as technical overviews, client recommendations, alternatives to business requirements, feasibility studies, project proposals, and requests for proposals for the purchase of new software and hardware
Leads consultants in the analysis, design, implementation and integration of application business systems
Develops software and hardware specifications and standards
Contributes to ensuring that the EEO policies and programs of Metro are carried out
Requirements For Employment

A combination of education and/or experience that provides the required knowledge, skills, and abilities to perform the essential functions of the position. Additional experience, as outlined below, may be substituted for required education on a year-for-year basis. A typical combination includes:
Bachelor's degree - Computer Science, Business Information System Technology, Computer Engineering, or related field
4 years of Senior Software Engineer-level experience in designing, implementing and maintaining an enterprise wide system
Preferred Qualifications

Preferred Qualifications (PQs) are used to identify relevant knowledge, skills, and abilities (KSAs) as determined by business necessity. These criteria are considered preferred qualifications and are not intended to serve as minimum requirements for the position. PQs will help support selection decisions throughout the recruitment. In addition, applicants who possess these PQs will not automatically be selected.

The following are the preferred qualifications:
Experience analyzing Big Data
Experience utilizing Unix/Linux systems for installation and support
Experience researching a collection of transit data to identify discrepancies
Experience working in the public sector
Knowledge:

Complex business processes, as well as areas outside of immediate responsibility
State and Federal Tax Filling regulations and Metro/Union negotiated agreements
Theories, principles, and practices related to information systems and supported business areas
Project management techniques and tools
Applicable local, state, and federal laws, rules, and regulations governing information systems for a public agency
Data communications involved in LANs, WANs, and gateways
Advanced capabilities of computers such as and microcomputers, including specified software products/platforms such as UNIX Script, AIX, Microsoft Networking, Microsoft Proxy, IIS, multiple Linux distributions and Windows Operating Systems, Apache and other Web tools
Mathematics used in complex business applications
Business process and system process modeling and problem solving techniques
Knowledge of change control systems and other Information Technology best practices
Project management methodology and techniques
Principals of supervision
Abilities:

Lead the development and implementation of complex information systems using software engineering techniques
Resolve highly difficult problems regarding highly complex programming and systems logic
Perform highly-complex systems implementation and integration tasks
Perform complex programming in various languages such as: PL/SQL, HTML, XML, , PHP, SQL Server, MySQL, jQuery, Node.js, AngularJS, , JAVA Script, Java Developer, Acrobat Professional, M/S SQL Server, VB, SQL, Visual Studio (2 or greater), C/C+/C++, or other specified languages
Provide technical project management across multiple projects and concurrent tasks
Develop and implement web-based computer applications
Analyze and evaluate system changes to determine feasibility
Analyze situations, identify problems, and recommend and implement solutions in a cost effective manner
Clearly define system integration and interface requirements
Identify and resolve conflicting project and/or client requirements
Understand, interpret, and apply laws, business rules, regulations, policies, procedures, contracts, budgets, and labor/management agreements to support business processes
Prepare reports, proposals, presentations and correspondence in a professional manner
Exercise judgment and creativity in making decisions
Interact professionally with various levels of employees and outside representatives
Communicate effectively orally and in writing
Meet tight time constraints and frequent deadlines
Operate a personal computer, computer terminal, and general office equipment
Multi-task on multiple concurrent projects
Identify and lead the implementation of new and creative solutions that are compatible with current and future technologies
Leads, organizes, technically supervises and assumes primary responsibility for one or more project teams in performing system analysis, design and programming, trouble-shooting, modifications, maintenance, implementation and integration of one or more systems comprising large and /or highly-complex application areas and processes
Read, write, speak, and understand English
Selection Procedure
Applicants who best meet job-related qualifications will be invited to participate in the examination process that may consist of any combination of written, performance, or oral appraisal to further evaluate job-related experience, knowledge, skills and abilities.",Experience analyzing Big Data Experience utilizing Unix/Linux systems for installation and support Experience researching a collection of transit data to identify discrepancies Experience working in the public sector    ,Experience analyzing Big Data utilizing Unix/Linux systems for installation and support researching a collection of transit data to identify discrepancies working in the public sector,Experience analyzing Big Data utilizing Unix/Linux systems installation support researching collection transit data identify discrepancies working public sector
410,Data Engineer,Data Engineer,"Long Beach, CA 90807",Long Beach,CA,"If youâre a problem-solver who loves working with data, Laserfiche has a great opportunity for you! As a Data Engineer on the Operations team, youâll build, maintain, monitor and improve enterprise-wide data pipelines and analytics platforms. Youâll have the opportunity to build the foundation upon which our data is structured, as well as create reports and tools to help stakeholders make critical decisions regarding our key corporate initiatives.

Learn and grow your career via a variety of career paths, including a technical engineering track, a project management track, and a hybrid of the two. Youâll have the support of a collaborative team and the freedom to explore best-practice approaches to develop solutions on your own. To succeed, youâll need to learn quickly and think creatively.

If youâre up to the challenge, we want to talk to you! Both experienced engineers and recent college graduates are encouraged to apply.

Responsibilities Include:
Build, maintain, monitor and improve data pipelines and analytics platforms at scale

Define and calculate metrics to be analyzed
Develop tools for electronic data collection
Query, merge and extract data across sources
Maintain data refresh and update pipelines
Data cleaning/manipulation

Clean datasets for analysis
Improve data quality through data inaccuracy profiling and recommend process improvements
Reporting & Visualization

Develop data reports, visualizations and/or interactive BI reports
Develop, implement and automate reporting by working with stakeholders in their design, planning and implementation while ensuring consistency
Statistical Modeling & Machine Learning

Create summary statistics
Define, calculate and validate algorithms
Conduct analysis (descriptive, correlational, inferential and predictive)
What Youâll Need:

BA/BS required in computer science, engineering or related technical field
Excellent analytical and problem-solving skills
Strong coding proficiency
Knowledge of statistics a plus
Experience with deriving meaningful insights and analysis from structured and unstructured datasets
Strong attention to detail
Self-starter mentality and team-player attitude
Resourcefulness and creativity in finding solutions
Click here to learn more about Life at Laserfiche","   Define and calculate metrics to be analyzed Develop tools for electronic data collection Query, merge and extract data across sources Maintain data refresh and update pipelines  ","Define and calculate metrics to be analyzed Develop tools for electronic data collection Query, merge extract across sources Maintain refresh update pipelines","Define calculate metrics analyzed Develop tools electronic data collection Query, merge extract across sources Maintain refresh update pipelines"
411,Data Engineer,Sr. Database Engineer,"Los Angeles, CA",Los Angeles,CA,"Description
This role will be responsible for supporting multiple applications and data transfers between internal and client facing applications. The successful candidate will have hands-on experience in a multitude of domains; including, but not limited to; 24 x 7 x 365 enterprise and cloud database infrastructure design and management, security, business continuity, disaster recovery, database design, database development, database tuning, enterprise data storage and backup, service delivery, incident tracking, change management, and general database administration.
Responsibilities
Design strategies for database systems as well as set standards for operations, programming and security.
Definition and governance of all structured and unstructured physical data models, data entities, supporting artifacts and metadata repositories.
Develops physical modeling standards and strategies.
Executes design sessions to gather requirements, review, approve, and communicate design artifacts with stakeholders.
Creates and manages physical models to support Enterprise Data Warehouse domains that include Operational Data Stores, Staging, Integrated EDW, Data Mart, Business, Reference/Master Data and Extract Transform Load (ETL) environments.
Ensures physical database features and capabilities are incorporated into data model designs to optimize performance.
Designs deliverables that are defined with appropriate database platform and security context.
Design and manage models using various Data Modeling tools.
Maintains optimal Enterprise Architecture alignment with Solution Architecture, Technical Leads, Business Analysis, Infrastructure, and PMO roles.
Design, Implementation and maintenance of database solutions, management of data access, and resolving database performance and capacity issues.
Develop, implement, administrate, and maintain policies and procedures for ensuring the security and integrity of our databases.
Perform problem-solving of application issues and production errors, including high level critical production issues that require immediate attention.
Design and code a high volume of SQL Queries, stored procedures, and SSIS packages
Implement processes and procedures for the development and release of products/projects that facilitate high quality and rapid deployment
Provide technical documentation as needed
Monitor and perform performance tuning on stored procedures and ETL jobs
Monitor and analyze SQL Server production metrics
Support complex service impacting issues with little to no supervision
Participate in an on-call support rotation and provide non-standard work hour support
Trouble shooting and resolving data issues
Perform other duties as assigned.
Qualifications
Extensive knowledge of SQL Server including the writing of complex stored procedures, functions and SSIS packages as well as basic knowledge of database administration
Strong understanding and experience of database development methodologies (Agile and Scrum)
Expert level knowledge of SQL administration, engineering, and monitoring tools
Demonstrated expertise in performance tuning including both query and server optimization
Strong knowledge and experience in data modeling, relational database design, optimization for performance, T-SQL, stored procedures, SSIS packages
Expertise in understanding complex business needs, analyzing, designing and developing solutions
Advanced knowledge of relational database design, testing and implementation
Advanced communication and professional skills and the ability to establish relationships across business units
Ability to participate in an on-call support rotation and provide non-standard work hour support
Extensive SQL Server performance tuning and troubleshooting experience
Hands-on development experience in SQL Server 2012/2016
Experience with SSIS
Financial technology domain knowledge would be a plus
Experience with .NET, MVC, C# HTML5 CSS3 AJAX jQuery IIS and Java script would be a plus
Computer Science Degree Highly Desired
Job Type: Full-time
Salary: $120,000.00 to $135,000.00 /year
Experience:
CSS: 3 years (Preferred)
.Net: 3 years (Preferred)
AJAX: 3 years (Preferred)
Enterprise Software: 3 years (Preferred)
SQL: 3 years (Preferred)
Tsql Scripting: 3 years (Preferred)"," Extensive knowledge of SQL Server including the writing of complex stored procedures, functions and SSIS packages as well as basic knowledge of database administration Strong understanding and experience of database development methodologies  Agile and Scrum  Expert level knowledge of SQL administration, engineering, and monitoring tools Demonstrated expertise in performance tuning including both query and server optimization Strong knowledge and experience in data modeling, relational database design, optimization for performance, T-SQL, stored procedures, SSIS packages Expertise in understanding complex business needs, analyzing, designing and developing solutions Advanced knowledge of relational database design, testing and implementation Advanced communication and professional skills and the ability to establish relationships across business units Ability to participate in an on-call support rotation and provide non-standard work hour support Extensive SQL Server performance tuning and troubleshooting experience Hands-on development experience in SQL Server 2012/2016 Experience with SSIS Financial technology domain knowledge would be a plus Experience with .NET, MVC, C  HTML5 CSS3 AJAX jQuery IIS and Java script would be a plus Computer Science Degree Highly Desired   Design strategies for database systems as well as set standards for operations, programming and security. Definition and governance of all structured and unstructured physical data models, data entities, supporting artifacts and metadata repositories. Develops physical modeling standards and strategies. Executes design sessions to gather requirements, review, approve, and communicate design artifacts with stakeholders. Creates and manages physical models to support Enterprise Data Warehouse domains that include Operational Data Stores, Staging, Integrated EDW, Data Mart, Business, Reference/Master Data and Extract Transform Load  ETL  environments. Ensures physical database features and capabilities are incorporated into data model designs to optimize performance. Designs deliverables that are defined with appropriate database platform and security context. Design and manage models using various Data Modeling tools. Maintains optimal Enterprise Architecture alignment with Solution Architecture, Technical Leads, Business Analysis, Infrastructure, and PMO roles. Design, Implementation and maintenance of database solutions, management of data access, and resolving database performance and capacity issues. Develop, implement, administrate, and maintain policies and procedures for ensuring the security and integrity of our databases. Perform problem-solving of application issues and production errors, including high level critical production issues that require immediate attention. Design and code a high volume of SQL Queries, stored procedures, and SSIS packages Implement processes and procedures for the development and release of products/projects that facilitate high quality and rapid deployment Provide technical documentation as needed Monitor and perform performance tuning on stored procedures and ETL jobs Monitor and analyze SQL Server production metrics Support complex service impacting issues with little to no supervision Participate in an on-call support rotation and provide non-standard work hour support Trouble shooting and resolving data issues Perform other duties as assigned.  ","Extensive knowledge of SQL Server including the writing complex stored procedures, functions and SSIS packages as well basic database administration Strong understanding experience development methodologies Agile Scrum Expert level administration, engineering, monitoring tools Demonstrated expertise in performance tuning both query server optimization data modeling, relational design, for performance, T-SQL, Expertise business needs, analyzing, designing developing solutions Advanced testing implementation communication professional skills ability to establish relationships across units Ability participate an on-call support rotation provide non-standard work hour troubleshooting Hands-on 2012/2016 Experience with Financial technology domain would be a plus .NET, MVC, C HTML5 CSS3 AJAX jQuery IIS Java script Computer Science Degree Highly Desired Design strategies systems set standards operations, programming security. Definition governance all structured unstructured physical models, entities, supporting artifacts metadata repositories. Develops modeling strategies. Executes design sessions gather requirements, review, approve, communicate stakeholders. Creates manages models Enterprise Data Warehouse domains that include Operational Stores, Staging, Integrated EDW, Mart, Business, Reference/Master Extract Transform Load ETL environments. Ensures features capabilities are incorporated into model designs optimize performance. Designs deliverables defined appropriate platform security context. manage using various Modeling tools. Maintains optimal Architecture alignment Solution Architecture, Technical Leads, Business Analysis, Infrastructure, PMO roles. Design, Implementation maintenance solutions, management access, resolving capacity issues. Develop, implement, administrate, maintain policies procedures ensuring integrity our databases. Perform problem-solving application issues production errors, high critical require immediate attention. code volume Queries, Implement processes release products/projects facilitate quality rapid deployment Provide technical documentation needed Monitor perform on jobs analyze metrics Support service impacting little no supervision Participate Trouble shooting other duties assigned.","Extensive knowledge SQL Server including writing complex stored procedures, functions SSIS packages well basic database administration Strong understanding experience development methodologies Agile Scrum Expert level administration, engineering, monitoring tools Demonstrated expertise performance tuning query server optimization data modeling, relational design, performance, T-SQL, Expertise business needs, analyzing, designing developing solutions Advanced testing implementation communication professional skills ability establish relationships across units Ability participate on-call support rotation provide non-standard work hour troubleshooting Hands-on 2012/2016 Experience Financial technology domain would plus .NET, MVC, C HTML5 CSS3 AJAX jQuery IIS Java script Computer Science Degree Highly Desired Design strategies systems set standards operations, programming security. Definition governance structured unstructured physical models, entities, supporting artifacts metadata repositories. Develops modeling strategies. Executes design sessions gather requirements, review, approve, communicate stakeholders. Creates manages models Enterprise Data Warehouse domains include Operational Stores, Staging, Integrated EDW, Mart, Business, Reference/Master Extract Transform Load ETL environments. Ensures features capabilities incorporated model designs optimize performance. Designs deliverables defined appropriate platform security context. manage using various Modeling tools. Maintains optimal Architecture alignment Solution Architecture, Technical Leads, Business Analysis, Infrastructure, PMO roles. Design, Implementation maintenance solutions, management access, resolving capacity issues. Develop, implement, administrate, maintain policies procedures ensuring integrity databases. Perform problem-solving application issues production errors, high critical require immediate attention. code volume Queries, Implement processes release products/projects facilitate quality rapid deployment Provide technical documentation needed Monitor perform jobs analyze metrics Support service impacting little supervision Participate Trouble shooting duties assigned."
412,Data Engineer,Senior Data Engineer,"Burbank, CA",Burbank,CA,"Movies Anywhere is the next generation of in-home entertainment, providing an unparalleled digital entertainment experience. Leveraging cutting edge technology, unique partnerships, and a talented team, Movies Anywhere is an exclusive, cross-platform, cloud-based movie service that enables consumers to seamlessly discover, grow, access, and enjoy their personal digital movie collection across a variety of studios, retailers, and platforms all in one convenient app and/or website.

Movies Anywhere seeks a Staff Data Engineer to join a team of seasoned, dedicated technologists solving a range of interesting problems in innovative ways in an exciting and dynamic industry. We are looking for a self-starting engineer who wants to shape the next generation of video consumption applications. Weâre a casual shop that values passion, community involvement and code that stands out. If you are interested, weâd love to hear from you.

The Data Engineer will work in a small team of multi-disciplined technologists developing insights to drive the Business, Marketing and Finance decisions for our next-generation video delivery and consumption platform. We expect you to be up to date on the happening in the data community, passionate about what you do, and connected to the open source community. You will participate in overall system design, developing multi-tiered data solutions emphasizing reuse and good design patterns.

Responsibilities :
Design, build and implement Hadoop/Spark batch jobs
Build and optimize performance of Spark, Kafka, ELK, and whatever else makes sense for real-time pipelines
Design and architect high quality data-lake, data-warehouse, and data-marts data models
Enable and implement Data Science workflows and advanced machine learning algorithms
Build and optimize performance of ElasticSearch cluster and relevance
Build and maintain data pipelines orchestration
Develop and cultivate expertise in current and new technologies and tools
Collaborate with other software engineers and cross-functional teams
Share new ideas with a larger community of highly experienced technologists
Ability to prioritize tasks, requirements, and complexity
Mentor junior data engineers on best practices

Basic Qualifications :
Real passion for coding (If you have a Github profile, thatâs awesome! We would love to check it out!)
Understanding of distributed systems and distributed computation
Working knowledge in at least 2 of: Scala, Java, Python, or Go-Lang
Working knowledge of data Apache Spark ecosystem technologies like Spark, Kafka, Hive, Presto, Oozie, Pig, Hue, Zeppelin
Demonstrated working knowledge of data modeling, data-warehouse, data-mart and data-lake
Unit, Integration, and Load testing
Developing REST APIs
Git
Maven, SBT, and/or Gradle
Unix/Linux
Docker containers building and deployment
Working experience of AWS
Excellent communication and collaboration skills

Preferred Qualifications:
Knowledge of Machine Learning Frameworks (MLib,Tensorflow, etc)
GraphQL Knowledge
Kubernetes knowledge
Apache Spark GraphX
R
GitLab CI/CD
Akka Streams

Required Education :
BS in Computer Science or related field with 7+ years of experience","Real passion for coding  If you have a Github profile, thatâs awesome! We would love to check it out!  Understanding of distributed systems and distributed computation Working knowledge in at least 2 of  Scala, Java, Python, or Go-Lang Working knowledge of data Apache Spark ecosystem technologies like Spark, Kafka, Hive, Presto, Oozie, Pig, Hue, Zeppelin Demonstrated working knowledge of data modeling, data-warehouse, data-mart and data-lake Unit, Integration, and Load testing Developing REST APIs Git Maven, SBT, and/or Gradle Unix/Linux Docker containers building and deployment Working experience of AWS Excellent communication and collaboration skills  Design, build and implement Hadoop/Spark batch jobs Build and optimize performance of Spark, Kafka, ELK, and whatever else makes sense for real-time pipelines Design and architect high quality data-lake, data-warehouse, and data-marts data models Enable and implement Data Science workflows and advanced machine learning algorithms Build and optimize performance of ElasticSearch cluster and relevance Build and maintain data pipelines orchestration Develop and cultivate expertise in current and new technologies and tools Collaborate with other software engineers and cross-functional teams Share new ideas with a larger community of highly experienced technologists Ability to prioritize tasks, requirements, and complexity Mentor junior data engineers on best practices BS in Computer Science or related field with 7+ years of experience ","Real passion for coding If you have a Github profile, thatâs awesome! We would love to check it out! Understanding of distributed systems and computation Working knowledge in at least 2 Scala, Java, Python, or Go-Lang data Apache Spark ecosystem technologies like Spark, Kafka, Hive, Presto, Oozie, Pig, Hue, Zeppelin Demonstrated working modeling, data-warehouse, data-mart data-lake Unit, Integration, Load testing Developing REST APIs Git Maven, SBT, and/or Gradle Unix/Linux Docker containers building deployment experience AWS Excellent communication collaboration skills Design, build implement Hadoop/Spark batch jobs Build optimize performance ELK, whatever else makes sense real-time pipelines Design architect high quality data-lake, data-marts models Enable Data Science workflows advanced machine learning algorithms ElasticSearch cluster relevance maintain orchestration Develop cultivate expertise current new tools Collaborate with other software engineers cross-functional teams Share ideas larger community highly experienced technologists Ability prioritize tasks, requirements, complexity Mentor junior on best practices BS Computer related field 7+ years","Real passion coding If Github profile, thatâs awesome! We would love check out! Understanding distributed systems computation Working knowledge least 2 Scala, Java, Python, Go-Lang data Apache Spark ecosystem technologies like Spark, Kafka, Hive, Presto, Oozie, Pig, Hue, Zeppelin Demonstrated working modeling, data-warehouse, data-mart data-lake Unit, Integration, Load testing Developing REST APIs Git Maven, SBT, and/or Gradle Unix/Linux Docker containers building deployment experience AWS Excellent communication collaboration skills Design, build implement Hadoop/Spark batch jobs Build optimize performance ELK, whatever else makes sense real-time pipelines Design architect high quality data-lake, data-marts models Enable Data Science workflows advanced machine learning algorithms ElasticSearch cluster relevance maintain orchestration Develop cultivate expertise current new tools Collaborate software engineers cross-functional teams Share ideas larger community highly experienced technologists Ability prioritize tasks, requirements, complexity Mentor junior best practices BS Computer related field 7+ years"
413,Data Engineer,Data & Control Systems Engineer (Stennis),"Los Angeles, CA",Los Angeles,CA,"Team and Role Overview

Relativity seeks a highly talented Data & Control Systems Engineer to join a rapidly growing space startup. This position will help build the core of our propulsion capability via the development and operation of a world-class test facility for components, engines, and complete rocket vehicle stages. A high degree of autonomy is required.


The Mission/Outcomes and Objectives

Our mission is to change the peopleâs conception of the way rockets are built and flown. As the Data & Control Systems Engineer, you will have a big role in ensuring the proper testing of our engines, stages and launch vehicle. This will include the development of a test infrastructure that can be used across the organization as well as the development of test racks to support testing needs.

Your main objectives in this role are:
Develop the test infrastructure to supporting engine and stage testing at Stennis.
Develop test racks to support these testing activities.
Develop test software to support these testing activities.

This position is based at our Mississippi test site but requires travel to our Los Angeles HQ as needed.


Candidate Profile

Relativity Space is changing the way rockets are built-this means that you will have a hand in the design of advanced propulsion-test data and control systems for the next generation of rockets. On a day to day, you will be selecting DAQ/PLC hardware and develop wiring schematics, implementing filtering strategies and sensor selection. This means that you should come from a fast paced company where you dealt with integrating large scale systems-this could be from automotive, aerospace, rocketry, etc.


Minimum Required Skills and Competencies

An undergraduate or graduate degree (BS/MS/PhD) in STEM majors (science, technology, engineering, and math) comparable and 4+ yearsâ relevant experience.
Experience designing, fabricating, and testing complex data & control systems
Ability to develop and review complex electrical schematics.
Knowledge of noise reduction, grounding, filtering, and other techniques necessary to ensure quality data.
Able to work in a fast-paced and intense startup environment.


Preferred Skills and Competencies

Knowledge of common fluid test instrumentation and components, such as pressure transducers, thermocouples, flow meters, accelerometers, strain gauges, load cells, and solenoid/pneumatic/other valves.
Proficiency with hands-on electrical skills such as soldering, insulating, splicing, and assembling harnesses.
Hands-on experience working with pressurized fluid systems.


This position must meet Export Control compliance requirements, therefore a United States Person as defined by 22 C.F.R. Â§ 120.15 is required.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","  An undergraduate or graduate degree  BS/MS/PhD  in STEM majors  science, technology, engineering, and math  comparable and 4+ yearsâ relevant experience. Experience designing, fabricating, and testing complex data & control systems Ability to develop and review complex electrical schematics. Knowledge of noise reduction, grounding, filtering, and other techniques necessary to ensure quality data. Able to work in a fast-paced and intense startup environment.    ","An undergraduate or graduate degree BS/MS/PhD in STEM majors science, technology, engineering, and math comparable 4+ yearsâ relevant experience. Experience designing, fabricating, testing complex data & control systems Ability to develop review electrical schematics. Knowledge of noise reduction, grounding, filtering, other techniques necessary ensure quality data. Able work a fast-paced intense startup environment.","An undergraduate graduate degree BS/MS/PhD STEM majors science, technology, engineering, math comparable 4+ yearsâ relevant experience. Experience designing, fabricating, testing complex data & control systems Ability develop review electrical schematics. Knowledge noise reduction, grounding, filtering, techniques necessary ensure quality data. Able work fast-paced intense startup environment."
414,Data Engineer,Experienced Satellite Command & Data Handling Engineer,"El Segundo, CA",El Segundo,CA,"At Boeing, we are all innovators on a mission to connect, protect, explore and inspire. From the seabed to outer space, youâll learn and grow, contributing to work that shapes the world. Find your future with us.
Boeing Defense, Space, and Security is seeking an Experienced Satellite Command & Data Handling Engineer (Level 3) to develop and manage Command and Data Handling systems for our satellite programs. Typical assignments will include requirements definition and flow down, systems design and analysis, systems integration, test, and verification. Functioning in this role will provide you with the opportunity to work with senior management and technical experts across multiple disciplines including subsystem engineering, software architecture, Communications Security (COMSEC), software validation, electronic hardware testing, spacecraft level testing, and end to end system engineering. This position is located in El Segundo, CA.
**This position offers relocation based on candidate eligibility. Basic relocation is available for internal candidates.**
Primary Responsibilities:
Gathers, defines and documents system level requirements to support flight control and mission requirements definition.
Performs and documents analysis and design of Command and Data Handling systems to validate or assess the system design or models.
Resolves technical issues and ensuring the spacecraft and system are flight worthy prior to delivery.
Performing trade studies to meet program requirements.
Performing analyses to evaluate and optimize total system performance to meet customer operation requirements.
Develops solutions and disposition of issues to assure customer satisfaction.
Designs trade studies to meet program requirements.
This position requires an active U.S. Security Clearance with a SSBI (Single Scope Background Investigation), for which the U.S Government requires U.S. Citizenship. A U.S. Security Clearance that has been active in the past 24 months is considered active.
Typical Education & Experience:
Education/experience typically acquired through advanced technical education from an accredited course of study in engineering, computer science, mathematics, physics or chemistry (e.g. Bachelor) and typically 5 or more years' related work experience or an equivalent combination of technical education and experience (e.g. PhD, Master+3 years' related work experience). In the USA, ABET accreditation is the preferred, although not required, accreditation standard.
Basic Qualifications (Required Skills/Experience):
Experience with Electrical Engineering.
Preferred Qualifications (Desired Skills/Experience):
Experience with Systems Engineering.
Experience with CCSDS and IP (Internet Protocol).
Experience working with 1553, SpaceWire, and CAN.
Experience with COMSEC/CRYPTO/TRANSEC.
Experience working with MATLAB and VBA tool development.
Boeing is a Drug Free Workplace where post offer applicants and employees are subject to testing for marijuana, cocaine, opioids, amphetamines, PCP, and alcohol when criteria is met as outlined in our policies."," Experience with Electrical Engineering.  Experience with Electrical Engineering.  Gathers, defines and documents system level requirements to support flight control and mission requirements definition. Performs and documents analysis and design of Command and Data Handling systems to validate or assess the system design or models. Resolves technical issues and ensuring the spacecraft and system are flight worthy prior to delivery. Performing trade studies to meet program requirements. Performing analyses to evaluate and optimize total system performance to meet customer operation requirements. Develops solutions and disposition of issues to assure customer satisfaction. Designs trade studies to meet program requirements.  Experience with Electrical Engineering. ","Experience with Electrical Engineering. Gathers, defines and documents system level requirements to support flight control mission definition. Performs analysis design of Command Data Handling systems validate or assess the models. Resolves technical issues ensuring spacecraft are worthy prior delivery. Performing trade studies meet program requirements. analyses evaluate optimize total performance customer operation Develops solutions disposition assure satisfaction. Designs","Experience Electrical Engineering. Gathers, defines documents system level requirements support flight control mission definition. Performs analysis design Command Data Handling systems validate assess models. Resolves technical issues ensuring spacecraft worthy prior delivery. Performing trade studies meet program requirements. analyses evaluate optimize total performance customer operation Develops solutions disposition assure satisfaction. Designs"
415,Data Engineer,Data Center Facilities Engineer IV,"El Segundo, CA",El Segundo,CA,"Data Center Facilities Engineer IV
Equinix is one of the fastest growing data center companies, growing connectivity between clients worldwide. Thatâs why we're always looking for creative and forward-thinking people who can help us achieve our goal of global interconnection. With 200 data centers in over 24 countries spanning across 5 continents, we are home to the Cloud, supporting over 1000 Cloud and IT services companies that are directly engaged in technological innovation and development. We are passionate about further evolving the specific areas of software development, software and network architecture, network operations and complex cloud and application solutions.
At Equinix, we make the internet work faster, better, and more reliably. We hire talented people like you who thrive on solving problems and give them opportunities to hone new skills, try new approaches, and grow in new directions. Our culture is at the heart of our success and itâs our authentic, humble, gritty people who create The Magic of Equinix. We share a real passion for winning and put the customer at the center of everything we do.
Are you ready to join the one company where all the worldâs major technology trends are converging to change the way we work and live?
In this role, you will complete repairs, corrective maintenance, and routine installations. You can resolve incidents as required.
Responsibilities
You will perform site inspections and monitor the building and IBX alarms
Performs preventative maintenance on site infrastructure (e.g. maintenance of primary infrastructures), or leads vendors
You undertake repairs and corrective maintenance
You operate critical infrastructure under the supervision of more senior technical staff
Completion of site logs and data gathering issuing for basic permits, such as MOPs and scripts
You respond to all on-site incidents and acts as required
You complete routine work requests and circuit installations
You provide assistance during critical maintenance activities
You are able to effectively collaborate within the department and provide recommendations to peers for general maintenance activities
You carry out basic infrastructure projects
Qualifications
1+ years experience preferred
High School Diploma
You perform all essential job functions, including walking, standing, bending, stooping, climbing, lifting and manual dexterity, with or without reasonable accommodation
You are available to work days/nights/weekends/holidays, if needed and/or required
You can lift heavy equipment/items up to 50 pounds
Equinix is an equal opportunity employer. All applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, or status as a qualified individual with disability.","1+ years experience preferred High School Diploma You perform all essential job functions, including walking, standing, bending, stooping, climbing, lifting and manual dexterity, with or without reasonable accommodation You are available to work days/nights/weekends/holidays, if needed and/or required You can lift heavy equipment/items up to 50 pounds   You will perform site inspections and monitor the building and IBX alarms Performs preventative maintenance on site infrastructure  e.g. maintenance of primary infrastructures , or leads vendors You undertake repairs and corrective maintenance You operate critical infrastructure under the supervision of more senior technical staff Completion of site logs and data gathering issuing for basic permits, such as MOPs and scripts You respond to all on-site incidents and acts as required You complete routine work requests and circuit installations You provide assistance during critical maintenance activities You are able to effectively collaborate within the department and provide recommendations to peers for general maintenance activities You carry out basic infrastructure projects   ","1+ years experience preferred High School Diploma You perform all essential job functions, including walking, standing, bending, stooping, climbing, lifting and manual dexterity, with or without reasonable accommodation are available to work days/nights/weekends/holidays, if needed and/or required can lift heavy equipment/items up 50 pounds will site inspections monitor the building IBX alarms Performs preventative maintenance on infrastructure e.g. of primary infrastructures , leads vendors undertake repairs corrective operate critical under supervision more senior technical staff Completion logs data gathering issuing for basic permits, such as MOPs scripts respond on-site incidents acts complete routine requests circuit installations provide assistance during activities able effectively collaborate within department recommendations peers general carry out projects","1+ years experience preferred High School Diploma You perform essential job functions, including walking, standing, bending, stooping, climbing, lifting manual dexterity, without reasonable accommodation available work days/nights/weekends/holidays, needed and/or required lift heavy equipment/items 50 pounds site inspections monitor building IBX alarms Performs preventative maintenance infrastructure e.g. primary infrastructures , leads vendors undertake repairs corrective operate critical supervision senior technical staff Completion logs data gathering issuing basic permits, MOPs scripts respond on-site incidents acts complete routine requests circuit installations provide assistance activities able effectively collaborate within department recommendations peers general carry projects"
416,Data Engineer,Data/Backend Engineer,"Los Angeles, CA",Los Angeles,CA,"Vertical Mass is searching for a world class Data / Backend Engineer to join our team. This role is vital to designing and building back-end systems to support our clients' data needs. We are looking for a passionate engineer to build simple, efficient, and scalable solutions for complex problems. In this role you will:
Design, implement, and maintain reliable systems to retrieve, normalize, and store consumer data
Design, implement, and manage scalable, cloud-hosted solutions to store data in support of our underlying systems and client needs
Assist with all parts of our internal analytics system (ETL, Redshift, Airflow)
Work with marketing, finance, and sales to help define and implement reporting solutions for clients
Provide technical support and advice to analytics and data visualization teams
Develop a comprehensive system of automated audit procedures to identify and alert on discrepancies / integrity issues
Create, document, and maintain various ETL processes
Write code to interface with 3rd party platform services
Technical Requirements
The successful candidate needs to be able to clear roadblocks, reduce complexity, and increase performance while reducing risks. We are looking for:
Deep knowledge of the AWS stack and infrastructure
Athena
Redshift/Lambda
ECS/Docker/ECR
S3
RDS
Kinesis Firehose
Application Load Balancing
Cloudwatch and monitoring
Experience in high volume data processing via batch and streaming
Experience with Data Warehouse design and query issues
Experience with the following languages/frameworks
Python
Go
Shell Scripting
PostgreSQL
Apache Airflow (good to have)
Aerospike (good to have)
Terraform
Education and Experience
BS in Computer Science or Related Field (MS preferred)
Excellent technical, written, and verbal interpersonal communication skills
5+ years of backend engineering experience building products from ideation to launch
Benefits
Vertical Mass offers a competitive benefits package including health, 401k, and 10 days paid time off. We also offer flexible work arrangements with occasional remote work possible.
About Vertical Mass
Vertical Mass is the world's leading data platform and co-op for Sports, Music, Pop Culture, and Entertainment audiences.
More than 350 clients like Google, P&G, and Fox Television leverage the Vertical Mass data management platform to collect data on their consumers, understand those audiences via our business insights products, and monetize those audiences through smarter marketing. Our platform processes 500M monthly unique users, making it the richest known data set of entertainment and sports audiences.
Vertical Mass is headquartered in LA and is backed by Greycroft, Formation 8, Sierra Wasatch, SV Angel, Live Nation/Ticketmaster, several sports team owners and other top strategic investors.

vzTw4QHVcN","   BS in Computer Science or Related Field  MS preferred  Excellent technical, written, and verbal interpersonal communication skills 5+ years of backend engineering experience building products from ideation to launch  Deep knowledge of the AWS stack and infrastructure Athena Redshift/Lambda ECS/Docker/ECR S3 RDS Kinesis Firehose Application Load Balancing Cloudwatch and monitoring Experience in high volume data processing via batch and streaming Experience with Data Warehouse design and query issues Experience with the following languages/frameworks Python Go Shell Scripting PostgreSQL Apache Airflow  good to have  Aerospike  good to have  Terraform ","BS in Computer Science or Related Field MS preferred Excellent technical, written, and verbal interpersonal communication skills 5+ years of backend engineering experience building products from ideation to launch Deep knowledge the AWS stack infrastructure Athena Redshift/Lambda ECS/Docker/ECR S3 RDS Kinesis Firehose Application Load Balancing Cloudwatch monitoring Experience high volume data processing via batch streaming with Data Warehouse design query issues following languages/frameworks Python Go Shell Scripting PostgreSQL Apache Airflow good have Aerospike Terraform","BS Computer Science Related Field MS preferred Excellent technical, written, verbal interpersonal communication skills 5+ years backend engineering experience building products ideation launch Deep knowledge AWS stack infrastructure Athena Redshift/Lambda ECS/Docker/ECR S3 RDS Kinesis Firehose Application Load Balancing Cloudwatch monitoring Experience high volume data processing via batch streaming Data Warehouse design query issues following languages/frameworks Python Go Shell Scripting PostgreSQL Apache Airflow good Aerospike Terraform"
417,Data Engineer,Pr. Yield & Data Engineer,"Northridge, CA 91325",Northridge,CA,"Careers that Change Lives
As a member of the Medtronic Diabetes â Continuous Glucose Monitoring Sensor R&D team this Principal Engineer will assist with development, validation and transfer to manufacturing of Medtronicâs micro-fabricated glucose bio-sensors. This position is in the Process Development group of Sensor R&D and focuses on process data analytics, enabling process control, and transfer to manufacturing. The Principal Engineer will have a heavy emphasis on developing predictive models relating process or design inputs to test/performance outputs. This will also include incorporating input parameter population distributions into these relationship models and evaluating output population distributions along with statistical confidence levels of these predictions.

The engineer will be expected to perform independently on large scale projects while balancing leadership responsibilities of said projects. There is a strong technical leadership path available to successful candidates and the Principal Engineer will have the opportunity to grow his/her scope of technical and project leadership over time as well as coach junior team members. He/she will also have the opportunity to collaborate in cross-functional development teams as a subject matter expert with manufacturing model development and yield prediction. There will be communication to upper management on a regular basis through presentations and reports, and the individual must have the ability to effectively convey project updates to internal and external stakeholders (vendors, manufacturing, etc.).

A Day in the Life

 Utilize technical skills associated withâ¦
o Model development including, for example, Machine Learning
o DRM/DFSS, Six Sigma, Lean and other appropriate statistical improvement techniques Utilize Best-Known-Statistical/Modeling-Methods to build parameter models predicting process & part variability Evaluate effectiveness of and recommend alternatives for processes control to achieve operating excellence Provide detailed engineering analysis and documentation to support model deployment and transfer for use in manufacturing volume ramp Act as a key interface with Manufacturing/Production teams Drive projects to successful completion using Project Management skills Promote a positive culture that focuses on continuous improvement & operational excellence

Responsibilities may include the following and other duties may be assigned:
 Provides technical and sustaining engineering support of a wafer fabrication area in a semiconductor company or a product manufacturing area of a medical or pharmaceutical company. Establishes operating equipment specifications, improves manufacturing techniques and production yields, and introduces new process equipment to improve production efficiencies. Interacts with product design and development personnel to ensure that processes and designs are compatible.
Must Have: Minimum Requirements
IN ORDER TO BE CONSIDERED FOR THIS POSITION, THE FOLLOWING BASIC QUALIFICATIONS MUST BE EVIDENT ON YOUR RESUME
Education & Years of Experience Required:
Bachelors of Science in Engineering, Chemistry, Physics or Sciences with 7+ years of professional experience; or Masters of Science in Engineering, Chemistry, Physics or Sciences with 5+ years of professional experience.

Specialized Knowledge or Skills Required: Experience with statistical analysis of data and variability (Ex. Monte Carlo simulation, Bayesian network, Minitab, JMP, excel, etc.) Exposure to microfabrication processes, microelectromechanical systems (MEMS), or similar work experience. Experience with technical documentation (Ex. laboratory notebooks, publications, engineering reports, IQ/OQ/PQâs, Standard Operating Procedures, test protocols, etc.)

Nice to have: Demonstrated experience developing/building models relating process or design inputs to test/performance outputs (highly desired qualification) Demonstrated experience incorporating input parameter variability into these relationship models and evaluating statistical confidence levels for output variability predictions (highly desired qualification) Experience with Monte Carlo, Bayesian Networks or other statistical modeling techniques Experience with model development through Machine Learning, Neural Networks, etc. Experience with yield prediction, yield monitoring and Statistical Process Control (SPC) Exposure to a microfabrication/wafer/MEMS manufacturing environment Experience with wafer test/analysis systems and associated data Experience with gage R&R studies, demonstrating process capability (Ppk & Cpk), and Qualification procedures (IQ/OQ/PQ) Experience with DRM, Lean and/or Six Sigma (ideally Black Belt certified or equivalent experience with demonstrated work in advanced statistical analysis techniques) Experience with MATLAB Prior project management experience Experience with medical devices and FDA regulations Demonstrated ability to manage multiple tasks & projects and work independently & within a team-based environment Demonstrated written and verbal communication skills


About Medtronic

Together, we can change healthcare worldwide. At Medtronic, we push the limits of what technology, therapies and services can do to help alleviate pain, restore health and extend life. We challenge ourselves and each other to make tomorrow better than yesterday. It is what makes this an exciting and rewarding place to be.

We want to accelerate and advance our ability to create meaningful innovations - but we will only succeed with the right people on our team. Letâs work together to address universal healthcare needs and improve patientsâ lives. Help us shape the future.

Physical Job Requirements

The physical demands described within the Responsibilities section of this job description are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. For Office Roles: While performing the duties of this job, the employee is regularly required to be independently mobile. The employee is also required to interact with a computer, and communicate with peers and co-workers. Contact your manager or local HR to understand the Work Conditions and Physical requirements that may be specific to each role. (ADA-United States of America)","     Experience with statistical analysis of data and variability  Ex. Monte Carlo simulation, Bayesian network, Minitab, JMP, excel, etc.  Exposure to microfabrication processes, microelectromechanical systems  MEMS , or similar work experience. Experience with technical documentation  Ex. laboratory notebooks, publications, engineering reports, IQ/OQ/PQâs, Standard Operating Procedures, test protocols, etc. ","Experience with statistical analysis of data and variability Ex. Monte Carlo simulation, Bayesian network, Minitab, JMP, excel, etc. Exposure to microfabrication processes, microelectromechanical systems MEMS , or similar work experience. technical documentation laboratory notebooks, publications, engineering reports, IQ/OQ/PQâs, Standard Operating Procedures, test protocols,","Experience statistical analysis data variability Ex. Monte Carlo simulation, Bayesian network, Minitab, JMP, excel, etc. Exposure microfabrication processes, microelectromechanical systems MEMS , similar work experience. technical documentation laboratory notebooks, publications, engineering reports, IQ/OQ/PQâs, Standard Operating Procedures, test protocols,"
418,Data Engineer,Data Engineer (All Levels),"Los Angeles, CA",Los Angeles,CA,"About NEXT:
NEXT was founded with a vision to implement solutions and bring change to a fragmented industry. As a FreightTech pioneer, we are designing technology-driven solutions to transform the $800B U.S. shipping & trucking industry. Our platform connects shippers with freight capacity for every step of a shipmentâs journey.

Headquartered in âSilicon Beachâ El Segundo, CA - NEXT is a company driven by a commitment to provide world-class service to shippers and truckers alike. Weâre on a mission to solve a trillion dollar puzzle and make freight painless.

Armed with experienced professionals from Google, Amazon, Salesforce and Twilio, NEXT is seeking change agents who are excited to make a meaningful impact on an industry driving the U.S. economy. We have been recognized as one of Built in LAâs Best Small Companies to Work For and 50 Startups to Watch. NEXT is venture-backed by leaders such as Brookfield Ventures, GLP and Sequoia Capital.

Are you NEXT?

About The Role:
We are passionate entrepreneurs, creative thinkers, and decision makers who are transforming the freight transportation industry with modern technology. We create delightful product experiences that enable freight to move effortlessly, solving complex problems for our shippers and carriers. We collaborate with some of the best designers, engineers, and business partners to achieve our goals.

As a member of NEXTâs Data Engineering Team, you'll have a meaningful and lasting impact on our data science & analytics practice. You should have a passion for the craft of software development and a track record of delivering exceptional technical solutions in a fast-paced high growth software company. You thrive in a startup where every individual has a significant impact on the technology, is empowered to make decisions and get things done.
What You'll Do
Build large scale fault tolerant data collection and processing pipelines from the ground up
Develop software services that leverage data and machine learning to support NEXTs data science practice, helping improve customer engagement within our flagship products
Work with the platform engineering team to collaborate on instrumentation and event stream implementation within NEXTs core platform architecture.
Identify and leverage third-party data feeds to improve our overall data offerings
What You'll Have
Data engineering or related software engineering experience in a fast-paced start-up environment.
Track record of delivering ETLs, machine learning pipelines, and data products within a cloud-based microservices or event streaming architecture.
Experience developing solutions in a continuous delivery eco-system.
Strong verbal and written communication skills and able to communicate effectively to technical and non-technical team members
Motivated by a sense of urgency and ownership
Preferred Qualifications:
Graduate degree in computer science.
Active involvement in the open source community.
Experience in transportation, logistics, and supply chain industries.
Experience in venture-backed startups or other hyper-growth environments.


NEXT is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires accommodation, please let us know by emailing candidateaccommodations@nexttrucking.com.

To all recruitment agencies: NEXT does not accept agency resumes. Please do not forward resumes to our jobs alias, NEXT employees or any other organization location. NEXT is not responsible for any fees related to unsolicited resumes."," Graduate degree in computer science. Active involvement in the open source community. Experience in transportation, logistics, and supply chain industries. Experience in venture-backed startups or other hyper-growth environments.    ","Graduate degree in computer science. Active involvement the open source community. Experience transportation, logistics, and supply chain industries. venture-backed startups or other hyper-growth environments.","Graduate degree computer science. Active involvement open source community. Experience transportation, logistics, supply chain industries. venture-backed startups hyper-growth environments."
419,Data Engineer,Senior Data Engineer,"Santa Monica, CA",Santa Monica,CA,"About Us:
PlayQ is a rapidly growing global entertainment and technology company delivering high-quality mobile titles and innovative game development solutions to a worldwide audience. Our games have been downloaded more than 60 million times across the globe, with millions of users playing every day!

Our dedicated teams, based in downtown Santa Monica, CA, work together to craft the clever, visually stunning, and unforgettable experiences that our players love. Our emphasis on individual leadership means each team member has the opportunity to make a big impact, while our commitment to creative freedom gives them the ability to create whatever they can imagine.

It's this mindset that has led us to develop our own IP, infuse games with rich storytelling, build our own development tools, and solve the deepest technical challenges - all in the name of disrupting the mobile gaming landscape.

Job Overview:
PlayQ is looking for a highly motivated and skilled Senior Data Engineer to help us unlock the full potential of our analytical capabilities. You'll lead the data engineering process for our rapidly growing datasets and have the freedom to innovate as you work closely with our data and engineering teams to develop new ways to store and analyze business-critical data from multiple sources. This role will allow you to make the most of your versatility, leadership qualities, and enthusiasm for new challenges as we continue to scale our platform and infrastructure.

Our ideal candidate will have a breadth of experience managing big data and cloud infrastructure with the ability to tie engineering initiatives to business impact. If you're excited by the opportunity to own our data engineering strategy then we'd love to hear from you!

Responsibilities:

Build and manage efficient and reliable real-time data pipelines from disparate data sources
Design, develop and launch data ingestion and storage systems with high availability and reliability that can scale
Drive the advancement of data infrastructure by developing and implementing underlying logic and structure for how data is set up, cleaned and stored
Take an integral role in designing and implementing a data lake strategy
Build and manage a universal semantic layer over the data lake
Architect, launch and manage automated extraction & transformation processes
Build scalable data aggregation layer from queues and batches of data for data visualization
Collaborate with development teams on design, architecture, and expansion of infrastructure

Requirements:

BS from an accredited university in Computer Science, Engineering, Math or related field
8+ years of experience in building data pipelines, data architecture, data modeling & data governance
Proficient working with SQL/NoSQL databases and MPP/columnar data warehouse solutions (Redshift, BigQuery, Snowflake etc.)
Experience with AWS environments: Redshift, EC2, Data Pipeline, S3, RDS, Glue, Spectrum, Dynamodb, Lambda
Proficient working with Python, bash or other scripting languages
Must have experience working with large data sets

Bonus Points:

Experience working in the mobile gaming industry

Perks:

Competitive compensation and equity options
Comprehensive medical, dental, vision, life and long term disability insurance
Flexible time off
401K plan with company match
Stocked kitchen with free snacks and beverages of your choice
Catered weekly team lunches
Brand new penthouse office space equipped with outdoor patios offering beachfront views
Monthly team outings and volunteer opportunities
Help build and support awesome GAMES. For a living! Who doesn't love games?

Interested? Please get in touch!","   Build and manage efficient and reliable real-time data pipelines from disparate data sources Design, develop and launch data ingestion and storage systems with high availability and reliability that can scale Drive the advancement of data infrastructure by developing and implementing underlying logic and structure for how data is set up, cleaned and stored Take an integral role in designing and implementing a data lake strategy Build and manage a universal semantic layer over the data lake Architect, launch and manage automated extraction & transformation processes Build scalable data aggregation layer from queues and batches of data for data visualization Collaborate with development teams on design, architecture, and expansion of infrastructure    BS from an accredited university in Computer Science, Engineering, Math or related field 8+ years of experience in building data pipelines, data architecture, data modeling & data governance Proficient working with SQL/NoSQL databases and MPP/columnar data warehouse solutions  Redshift, BigQuery, Snowflake etc.  Experience with AWS environments  Redshift, EC2, Data Pipeline, S3, RDS, Glue, Spectrum, Dynamodb, Lambda Proficient working with Python, bash or other scripting languages Must have experience working with large data sets ","Build and manage efficient reliable real-time data pipelines from disparate sources Design, develop launch ingestion storage systems with high availability reliability that can scale Drive the advancement of infrastructure by developing implementing underlying logic structure for how is set up, cleaned stored Take an integral role in designing a lake strategy universal semantic layer over Architect, automated extraction & transformation processes scalable aggregation queues batches visualization Collaborate development teams on design, architecture, expansion BS accredited university Computer Science, Engineering, Math or related field 8+ years experience building pipelines, modeling governance Proficient working SQL/NoSQL databases MPP/columnar warehouse solutions Redshift, BigQuery, Snowflake etc. Experience AWS environments EC2, Data Pipeline, S3, RDS, Glue, Spectrum, Dynamodb, Lambda Python, bash other scripting languages Must have large sets","Build manage efficient reliable real-time data pipelines disparate sources Design, develop launch ingestion storage systems high availability reliability scale Drive advancement infrastructure developing implementing underlying logic structure set up, cleaned stored Take integral role designing lake strategy universal semantic layer Architect, automated extraction & transformation processes scalable aggregation queues batches visualization Collaborate development teams design, architecture, expansion BS accredited university Computer Science, Engineering, Math related field 8+ years experience building pipelines, modeling governance Proficient working SQL/NoSQL databases MPP/columnar warehouse solutions Redshift, BigQuery, Snowflake etc. Experience AWS environments EC2, Data Pipeline, S3, RDS, Glue, Spectrum, Dynamodb, Lambda Python, bash scripting languages Must large sets"
420,Data Engineer,Data Engineer,"Los Angeles, CA 90045",Los Angeles,CA,"Who are we?
SDL is the global leader and innovator in language and content management technology and services. We enable companies to create meaningful digital journeys and form important emotional connections with their global customers by powering the creation, translation and delivery of relevant, personalized content to make understanding possible. We are a dynamic, collaborative and globally diverse team, working together to completely transform how the world communicates and connects with one another through content. If you like making an impact, helping others, taking a fresh look at processes and tools, furthering an open an innovative culture and tend to generally look at even the smallest task in the context of how it impacts the overall organization â we should talk!
What is this role?
At SDL Research, we have over a decade of experience in designing, building and deploying large-scale cutting-edge software applications. We offer the opportunity of research and software development in a dynamic setting where you can have an enormous impact.
As a Data Engineer at SDL Research, you will be part of a strong R&D team working on problems at the intersection of Artificial Intelligence and Software Engineering. You will build advanced data pipelines and algorithmic systems that enable development of large-scale state-of-the art Natural Language Processing capabilities, powering the next generation of SDL Research products and platforms. You have strong analytical and problem-solving skills, advanced knowledge of at least one programming language, excellent communication skills and the desire to build innovative complex large-scale systems.
Job Responsibilities:
Build data pipelines to feed Machine Learning models for language analysis and translation
Work closely with Machine Learning Scientist to explore new data sources and create new capabilities and models
Establish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation
Collaborate to create robust large-scale, production-ready applications which leverage distributed computing and dynamic provisioning
Write efficient, scalable code
Build SDL Research next-generation products and platforms
Minimum qualifications:
BS degree in Computer Science or related field
2+ years of experience in software development, data engineering, data science, or related field
Experience with Big Data technologies (HBase, Hadoop, Spark, Hive, etc.)
Knowledge of data structures, algorithms, and software design
Development experience on Linux and Windows
Preferred qualifications:
4+ years of experience as a Data Engineer or related job in a company with large, complex data sources
Solid scripting skills (shell scripting and Python/Perl)
Exposure to statistical modelling, neural networks, or natural language processing
Experience working with large-scale workflows
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Strong team player with excellent communication, documentation and problem-solving skills
Our Company:
SDL Research has a global footprint with a network of 57 offices in 38 countries, yet we manage to keep a small company feel. We research and design large-scale cutting-edge software applications in Machine Translation, Machine Learning and Natural Language Processing.
With a strong infrastructure, a rich product ecosystem, a large customer base, state-of-the-art machine learning, big data platforms and technology enabling global communication, SDL Research drives the future of customer experience.
We are looking for talented, collaborative and creative people whose expertise and knowledge of algorithms, data structures and software development can help build SDL's Research next-generation products and platforms.
So come and join our team today and begin your new journey with us!
Benefits:
Amazing benefits. (Seriously.)
Infinite training, professional development and personal growth opportunities
The rare opportunity to impact how organizations communicate globally. Thereâs a reason we work with 90 of the top 100 brands.
Smart, engaged co-workers, a culture of innovation and opportunity.
Great work life balance
SDL is an Equal Opportunity / Affirmative Action Employer. Qualified applicants will be evaluated for employment without regard to race, color, religion, sex, national origin, veteran, and disability status. For more information about EEO/AAP legislation please visit http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf.
Should you require assistance in completing your application, please contact usrecruitment@sdl.com for accommodation.","  Build data pipelines to feed Machine Learning models for language analysis and translation Work closely with Machine Learning Scientist to explore new data sources and create new capabilities and models Establish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation Collaborate to create robust large-scale, production-ready applications which leverage distributed computing and dynamic provisioning Write efficient, scalable code Build SDL Research next-generation products and platforms   ","Build data pipelines to feed Machine Learning models for language analysis and translation Work closely with Scientist explore new sources create capabilities Establish scalable, efficient, automated processes large scale analyses, model development, validation implementation Collaborate robust large-scale, production-ready applications which leverage distributed computing dynamic provisioning Write scalable code SDL Research next-generation products platforms","Build data pipelines feed Machine Learning models language analysis translation Work closely Scientist explore new sources create capabilities Establish scalable, efficient, automated processes large scale analyses, model development, validation implementation Collaborate robust large-scale, production-ready applications leverage distributed computing dynamic provisioning Write scalable code SDL Research next-generation products platforms"
421,Data Engineer,Data Engineer,"Santa Monica, CA",Santa Monica,CA,"Weâre looking for a Data Platform Engineer to design and implement core components of our data platform and scale it to serve millions of users across the world. In this role you will lead these vital initiatives.

We expect you to work collaboratively with the data science and analytics teams to ship scalable software solutions. Your work takes into consideration the holistic user experience from mobile to data science driven loan decision. You value the craftsmanship of clean and efficient code as well as solving problems using the most appropriate technology. You help set the standards and drive your teammatesâ productivity.

Responsibilities:
Develop and deploy platforms, services, abstractions, and frameworks that enable Data Science to iterate and launch data models into production
Design and implement database and storage solutions that fit the needs for both data transaction and reporting
Mentor engineers, data scientists, and analysts on best practices and code efficiency
Requirements:

Strong Computer Science fundamentals
4+ years experience working on backend software using modern languages and frameworks (Java, Scala, Python)
Strong database experience (MySQL, PostgreSQL, NoSQL)
Strong familiarity AWS (EC2, S3, RDS, ELB)
Experience with Hadoop data and underlying file systems (HDFS, HBase, Cassandra, Hive)
Working knowledge in API development for mobile/web use
Preferred Skills & Experience:

Expert proficiency in Agile development process
Advanced degree in Computer Science and/or Math
Excellent ability to prioritize and communication in a fast pace environment
Bonus: Experience in predictive analysis and machine learning
We strongly believe that inclusion fosters innovation and weâre proud to have a diverse team with a wide variety of backgrounds and experiences. We focus on hiring talented people regardless of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

About Tala:

Tala is the leading mobile technology and data science company committed to financial inclusion globally. Millions of people have borrowed through Tala's smartphone app, which provides instant, personalized credit to underserved customers in East Africa, Southeast Asia, and Latin America. Tala is backed by leading venture and impact investors including PayPal, Revolution Growth, IVP, and Lowercase Capital. Tala is headquartered in Santa Monica with additional offices in Nairobi, Manila, Dar Es Salaam, Mexico City, and Bangalore.","  Expert proficiency in Agile development process Advanced degree in Computer Science and/or Math Excellent ability to prioritize and communication in a fast pace environment Bonus  Experience in predictive analysis and machine learning  Develop and deploy platforms, services, abstractions, and frameworks that enable Data Science to iterate and launch data models into production Design and implement database and storage solutions that fit the needs for both data transaction and reporting Mentor engineers, data scientists, and analysts on best practices and code efficiency    Strong Computer Science fundamentals 4+ years experience working on backend software using modern languages and frameworks  Java, Scala, Python  Strong database experience  MySQL, PostgreSQL, NoSQL  Strong familiarity AWS  EC2, S3, RDS, ELB  Experience with Hadoop data and underlying file systems  HDFS, HBase, Cassandra, Hive  Working knowledge in API development for mobile/web use ","Expert proficiency in Agile development process Advanced degree Computer Science and/or Math Excellent ability to prioritize and communication a fast pace environment Bonus Experience predictive analysis machine learning Develop deploy platforms, services, abstractions, frameworks that enable Data iterate launch data models into production Design implement database storage solutions fit the needs for both transaction reporting Mentor engineers, scientists, analysts on best practices code efficiency Strong fundamentals 4+ years experience working backend software using modern languages Java, Scala, Python MySQL, PostgreSQL, NoSQL familiarity AWS EC2, S3, RDS, ELB with Hadoop underlying file systems HDFS, HBase, Cassandra, Hive Working knowledge API mobile/web use","Expert proficiency Agile development process Advanced degree Computer Science and/or Math Excellent ability prioritize communication fast pace environment Bonus Experience predictive analysis machine learning Develop deploy platforms, services, abstractions, frameworks enable Data iterate launch data models production Design implement database storage solutions fit needs transaction reporting Mentor engineers, scientists, analysts best practices code efficiency Strong fundamentals 4+ years experience working backend software using modern languages Java, Scala, Python MySQL, PostgreSQL, NoSQL familiarity AWS EC2, S3, RDS, ELB Hadoop underlying file systems HDFS, HBase, Cassandra, Hive Working knowledge API mobile/web use"
422,Data Engineer,Field Engineer | Data Acquisition Group,"Los Angeles, CA",Los Angeles,CA,"Target Start Date: Fall 2019
Reports To: Director of Engineering | Data Acquisition Group
Locations: Los Angeles, CA

Carbon Lighthouse is looking for driven and smart Field Engineers to join our Data Acquisition Group, the group responsible for collecting and delivering site information that enables us to provide our clean energy service in buildings.

Weâre looking for people to innovate our technology and energy service, and help us grow our national presence. As a Field Engineer you will directly influence how we operate, focusing on improvements like: gathering accurate data more quickly, making field work more efficient and effective, improving feasibility checks, and applying new strategies to maximize energy savings potential. Your initial career path will be primarily field-focused with many growth opportunities for specialization as our company expands.

You will have installation, commissioning/testing, controls, or other field work experience and strong mechanical aptitude, and be excited by complex, open-ended, and real-world engineering challenges. You genuinely enjoy working on collaborative teams, and want to be involved in large scale projects that require long term effort. You enjoy real world field implementation work and project coordination, and you think beyond existing processes in search of better methods.

Your role will be based out of San Francisco, Los Angeles, or New York City, and will require traveling locally and nationally on a regular basis.

About Carbon Lighthouse:
Carbon Lighthouse is on a mission to stop climate change by making it easy and profitable for building owners to eliminate carbon emissions caused by wasted energy. The companyâs unique approach to Efficiency Production goes deep into buildings to uncover and continuously correct hidden inefficiencies that add up to meaningful financial value and carbon elimination that lasts. Since 2010, commercial real estate, educational, hospitality and industrial customers nationwide have chosen Carbon Lighthouse to enhance building comfort, increase net operating income and achieve their sustainability goals.

We are a team that highly value question asking, getting it done, integrity, and teamwork. We appreciate a fulfilling work-life balance, prize transparency and communication, hold ourselves to high standards of performance and professionalism, strive for dynamism and innovation, and support our team membersâ professional development.

Responsibilities:

Frequent site travel (up to 75%) to mechanical and industrial spaces to assess feasibility of solutions and manage contractors
Lead and participate in field-based sensor deployment, functional testing, equipment measurements, and other field data collection needs
Manage gear and sensor inventory, calibration, conditions, and needs
Work with a variety of Building Automation Systems (BAS/BMS) to assess functionality and limitations, set up and download trends, and identify sequences of operation
Support simultaneous complex tasks and projects across multiple client sites
Support Engineers in acquiring data, developing proposals, and implementing solutions
Apply real world judgment in assessing risks and uncertainties in the field
Cultivate effective relationships with contractors, facility engineers, and clients
Develop new field processes and tools that reduce time, improve contractor relationships, or increase energy savings in delivered solutions
Work in a cross-functional team environment where asking questions and challenging assumptions is necessary due to complex and multi-disciplinary problems
Stay current with established safety procedures. Exemplify CL safety culture both internally and externally.

Required Qualifications:

Dedication to Carbon Lighthouseâs environmental mission
Bachelors in Engineering/Science or 2 year technical school
2+ years work experience or applicable lab/prototyping experience
Demonstrable field work and testing experience, such as: controls & programming, construction, environmental sampling, installation & commissioning, operations & maintenance, or similar
Mechanical aptitude and ability to work with hands and tools
Excellent question asker and communicator, and enjoys working on teams
Excellent organization, attention to detail, and time management skills
Demonstrable experience solving complex and real-world problems:
Able to adapt solutions to changing field conditions and constraints
Able to create a solution, fail, and iterate
Exercise initiative and make sound decisions with imperfect information
Ability to travel to local and national sites (~5-20 days per month)
Ability to work in industrial and mechanical spaces, with exposure to heat, noise, heights, and confined spaces
Ability to lift 40 pounds, and carry 20 pounds up a ladder
Must be able to work in the US, this role is not eligible for visa sponsorship

Bonus Qualifications:

HVAC systems and/or Controls software/hardware background
Experience working with mechanical, electrical, or controls drawings/schematics
Experience with mechanical workspace health and safety practices
EPA Certification (HVAC) or Apprenticeship experience
Strong working knowledge of Microsoft Office
Technical sales of any kind

Compensation and Benefits:

Salary + equity
Medical, dental, vision, and disability insurance
Generous vacation policy
Fully paid maternity and paternity leave benefits
Subsidized public transit and bike to work benefits
401(k)

Carbon Lighthouse is an equal employment opportunity employer and considers qualified applicants without regard to gender, sexual orientation, gender identity, race, veteran or disability status.

If youâre excited about our environmental mission and this looks like a fit, you should apply! Please fill out the brief application form and be prepared to submit three references at a later date."," Dedication to Carbon Lighthouseâs environmental mission Bachelors in Engineering/Science or 2 year technical school 2+ years work experience or applicable lab/prototyping experience Demonstrable field work and testing experience, such as  controls & programming, construction, environmental sampling, installation & commissioning, operations & maintenance, or similar Mechanical aptitude and ability to work with hands and tools Excellent question asker and communicator, and enjoys working on teams Excellent organization, attention to detail, and time management skills Demonstrable experience solving complex and real-world problems  Able to adapt solutions to changing field conditions and constraints Able to create a solution, fail, and iterate Exercise initiative and make sound decisions with imperfect information Ability to travel to local and national sites  ~5-20 days per month  Ability to work in industrial and mechanical spaces, with exposure to heat, noise, heights, and confined spaces Ability to lift 40 pounds, and carry 20 pounds up a ladder Must be able to work in the US, this role is not eligible for visa sponsorship    Frequent site travel  up to 75%  to mechanical and industrial spaces to assess feasibility of solutions and manage contractors Lead and participate in field-based sensor deployment, functional testing, equipment measurements, and other field data collection needs Manage gear and sensor inventory, calibration, conditions, and needs Work with a variety of Building Automation Systems  BAS/BMS  to assess functionality and limitations, set up and download trends, and identify sequences of operation Support simultaneous complex tasks and projects across multiple client sites Support Engineers in acquiring data, developing proposals, and implementing solutions Apply real world judgment in assessing risks and uncertainties in the field Cultivate effective relationships with contractors, facility engineers, and clients Develop new field processes and tools that reduce time, improve contractor relationships, or increase energy savings in delivered solutions Work in a cross-functional team environment where asking questions and challenging assumptions is necessary due to complex and multi-disciplinary problems Stay current with established safety procedures. Exemplify CL safety culture both internally and externally.   ","Dedication to Carbon Lighthouseâs environmental mission Bachelors in Engineering/Science or 2 year technical school 2+ years work experience applicable lab/prototyping Demonstrable field and testing experience, such as controls & programming, construction, sampling, installation commissioning, operations maintenance, similar Mechanical aptitude ability with hands tools Excellent question asker communicator, enjoys working on teams organization, attention detail, time management skills solving complex real-world problems Able adapt solutions changing conditions constraints create a solution, fail, iterate Exercise initiative make sound decisions imperfect information Ability travel local national sites ~5-20 days per month industrial mechanical spaces, exposure heat, noise, heights, confined spaces lift 40 pounds, carry 20 pounds up ladder Must be able the US, this role is not eligible for visa sponsorship Frequent site 75% assess feasibility of manage contractors Lead participate field-based sensor deployment, functional testing, equipment measurements, other data collection needs Manage gear inventory, calibration, conditions, Work variety Building Automation Systems BAS/BMS functionality limitations, set download trends, identify sequences operation Support simultaneous tasks projects across multiple client Engineers acquiring data, developing proposals, implementing Apply real world judgment assessing risks uncertainties Cultivate effective relationships contractors, facility engineers, clients Develop new processes that reduce time, improve contractor relationships, increase energy savings delivered cross-functional team environment where asking questions challenging assumptions necessary due multi-disciplinary Stay current established safety procedures. Exemplify CL culture both internally externally.","Dedication Carbon Lighthouseâs environmental mission Bachelors Engineering/Science 2 year technical school 2+ years work experience applicable lab/prototyping Demonstrable field testing experience, controls & programming, construction, sampling, installation commissioning, operations maintenance, similar Mechanical aptitude ability hands tools Excellent question asker communicator, enjoys working teams organization, attention detail, time management skills solving complex real-world problems Able adapt solutions changing conditions constraints create solution, fail, iterate Exercise initiative make sound decisions imperfect information Ability travel local national sites ~5-20 days per month industrial mechanical spaces, exposure heat, noise, heights, confined spaces lift 40 pounds, carry 20 pounds ladder Must able US, role eligible visa sponsorship Frequent site 75% assess feasibility manage contractors Lead participate field-based sensor deployment, functional testing, equipment measurements, data collection needs Manage gear inventory, calibration, conditions, Work variety Building Automation Systems BAS/BMS functionality limitations, set download trends, identify sequences operation Support simultaneous tasks projects across multiple client Engineers acquiring data, developing proposals, implementing Apply real world judgment assessing risks uncertainties Cultivate effective relationships contractors, facility engineers, clients Develop new processes reduce time, improve contractor relationships, increase energy savings delivered cross-functional team environment asking questions challenging assumptions necessary due multi-disciplinary Stay current established safety procedures. Exemplify CL culture internally externally."
423,Data Engineer,Senior Data Engineer,"Santa Monica, CA",Santa Monica,CA,"We are a 200+ person team funded by top-tier firms, based in Santa Monica, CA with a satellite office in San Francisco. We are a low-key but tight-knit group of hard-working people working to fix America's broken healthcare system. Americans spend over $400 billion per year on prescriptions and too many people simply can't afford the medications they desperately need. Join GoodRx and help us solve meaningful problems that help us make peopleâs lives better every day.

About the Role
GoodRx is looking for extremely smart and curious Senior Data Engineers, who are deft at working with a wide variety of languages, such as Python and SQL, a variety of raw data formats, such as JSON and CSV, in a fast-paced and friendly environment. Previous work with Python-based ETL solutions, such as Luigi or Airflow, is a huge plus.

Responsibilities:

Analyze, design, develop, test and implement data warehouse and business intelligence solutions with emphasis on data quality
Design highly scalable ETL processes with complex data transformations
Gather and document business requirements and translate into technical architecture/design
Ability to understand and document data flows in and between different systems and map data from a data source to target tables in a data warehouse
Work closely with other engineers to enhance infrastructure, improve reliability and efficiency
Make smart engineering and product decisions based on data analysis and collaboration
Act as an in-house data expert and make recommendations regarding standards for code quality and timeliness

Skills & Qualifications:

Degree in Computer Science or a related field or 5+ years of professional experience in developing ETL and data warehouse solutions
In depth knowledge of how to write and optimize SQL statements
Deep familiarity with distributed processing (Map Reduce, MPP, etc.)
3+ years experience with schema design (logical and physical)
Strong experience with data integration tool sets
Experience with cloud solutions (AWS, Redshift, Snowflake, other) is a must
Strong programming (Java/C# or related) and scripting skills (Python/Javascript) is a plus
Experience with workflow management tools (Airflow, Luigi etc) helpful
Ability to quickly learn complex domains
Strong attention to detail with excellent analytical, problem-solving, and communication skills

About GoodRx
GoodRx is Americaâs leading prescription price transparency platform. More than 8 million people use the GoodRx website and our mobile apps each month. GoodRx helps consumers save up to 80% on their medications by delivering prices and available discounts at nearly every pharmacy in the U.S. Thousands of physicians and employees use GoodRx, and its services have been positively reviewed by Good Morning America, the American Heart Association, The New York Times, ABC News, AARP, Forbes and many others."," Degree in Computer Science or a related field or 5+ years of professional experience in developing ETL and data warehouse solutions In depth knowledge of how to write and optimize SQL statements Deep familiarity with distributed processing  Map Reduce, MPP, etc.  3+ years experience with schema design  logical and physical  Strong experience with data integration tool sets Experience with cloud solutions  AWS, Redshift, Snowflake, other  is a must Strong programming  Java/C  or related  and scripting skills  Python/Javascript  is a plus Experience with workflow management tools  Airflow, Luigi etc  helpful Ability to quickly learn complex domains Strong attention to detail with excellent analytical, problem-solving, and communication skills   Degree in Computer Science or a related field or 5+ years of professional experience in developing ETL and data warehouse solutions In depth knowledge of how to write and optimize SQL statements Deep familiarity with distributed processing  Map Reduce, MPP, etc.  3+ years experience with schema design  logical and physical  Strong experience with data integration tool sets Experience with cloud solutions  AWS, Redshift, Snowflake, other  is a must Strong programming  Java/C  or related  and scripting skills  Python/Javascript  is a plus Experience with workflow management tools  Airflow, Luigi etc  helpful Ability to quickly learn complex domains Strong attention to detail with excellent analytical, problem-solving, and communication skills   Analyze, design, develop, test and implement data warehouse and business intelligence solutions with emphasis on data quality Design highly scalable ETL processes with complex data transformations Gather and document business requirements and translate into technical architecture/design Ability to understand and document data flows in and between different systems and map data from a data source to target tables in a data warehouse Work closely with other engineers to enhance infrastructure, improve reliability and efficiency Make smart engineering and product decisions based on data analysis and collaboration Act as an in-house data expert and make recommendations regarding standards for code quality and timeliness   ","Degree in Computer Science or a related field 5+ years of professional experience developing ETL and data warehouse solutions In depth knowledge how to write optimize SQL statements Deep familiarity with distributed processing Map Reduce, MPP, etc. 3+ schema design logical physical Strong integration tool sets Experience cloud AWS, Redshift, Snowflake, other is must programming Java/C scripting skills Python/Javascript plus workflow management tools Airflow, Luigi etc helpful Ability quickly learn complex domains attention detail excellent analytical, problem-solving, communication Analyze, design, develop, test implement business intelligence emphasis on quality Design highly scalable processes transformations Gather document requirements translate into technical architecture/design understand flows between different systems map from source target tables Work closely engineers enhance infrastructure, improve reliability efficiency Make smart engineering product decisions based analysis collaboration Act as an in-house expert make recommendations regarding standards for code timeliness","Degree Computer Science related field 5+ years professional experience developing ETL data warehouse solutions In depth knowledge write optimize SQL statements Deep familiarity distributed processing Map Reduce, MPP, etc. 3+ schema design logical physical Strong integration tool sets Experience cloud AWS, Redshift, Snowflake, must programming Java/C scripting skills Python/Javascript plus workflow management tools Airflow, Luigi etc helpful Ability quickly learn complex domains attention detail excellent analytical, problem-solving, communication Analyze, design, develop, test implement business intelligence emphasis quality Design highly scalable processes transformations Gather document requirements translate technical architecture/design understand flows different systems map source target tables Work closely engineers enhance infrastructure, improve reliability efficiency Make smart engineering product decisions based analysis collaboration Act in-house expert make recommendations regarding standards code timeliness"
424,Data Engineer,"Sr Engineer, Data Management","Long Beach, CA 90802",Long Beach,CA,"Job Description Job Summary

Responsible for performing all Middleware Platform operations. Building the SQL Server and AlwaysOn cluster.

Knowledge/Skills/Abilities:

Monitors and takes correction action on Middleware technologies. â¢ Responsible for operations activities of Database, ERP, BI, ETL, Data Warehouse. â¢ Respond, prioritize, and escalate incidents or service requests as necessary according to EIS processes and procedures. â¢ Develop and implement database operational procedures â¢ Perform the loading of production data and any other issues regarding data loads. â¢ Continue to provide a pre-determined and agreed level of services to support the minimum business requirements following an interruption to the Platform operations. â¢ Support application development functions. â¢ Maintain service level agreements, policies and procedures. â¢ Assist with application support functions, including coordinating vendor assisted support. â¢ Participate in problem management task forces and perform root cause troubleshooting. â¢ Implement monitoring solutions as defined by Senior Staff Member â¢ Implement backup solutions as defined by Senior Staff Members â¢ Ensure adequate capacity is available at all times to meet the requirements of the operations and business through capacity planning and management. â¢ Maintains existing databases, ERP, BI, ETL, Data Warehouse based on specifications. â¢ Establishes documentation procedures and standards and backups. â¢ Manages security of Middleware technologies structures. â¢ Responsible for triage of performance and tuning of Middleware technologies. â¢ Provides on-call support 24/7. â¢ Experience in data virtulization using Delphix. â¢ Experience in ETL using Attunity, Talend, Informatica.

Qualifications Job Qualifications Required Education:

Bachelorsâ Degree in related technology field or equivalent experience.

Preferred Education:

Mastersâ Degree in related technology field or equivalent experience.

Required Experience:

5+ years progressive experience in SQL Server. â¢ Delivery systems experience supporting managed care service.
Good knowledge of the following Middleware Technologies: SQL Server, Oracle, ETL, ERP, BI, Datawarehouse. â¢ Windows/SQL Cluster. â¢ SQL AlwaysOn.

Preferred Experience:

Managed Care. â¢ Data Virtualization using Delphix. â¢ ETL using Attunity, Talend, Informatica.

To all current Molina employees: If you are interested in applying for this position, please apply through the intranet job listing.

Molina Healthcare offers a competitive benefits and compensation package. Molina Healthcare is an Equal Opportunity Employer (EOE) M/F/D/V."," Bachelorsâ Degree in related technology field or equivalent experience.   Monitors and takes correction action on Middleware technologies. â¢ Responsible for operations activities of Database, ERP, BI, ETL, Data Warehouse. â¢ Respond, prioritize, and escalate incidents or service requests as necessary according to EIS processes and procedures. â¢ Develop and implement database operational procedures â¢ Perform the loading of production data and any other issues regarding data loads. â¢ Continue to provide a pre-determined and agreed level of services to support the minimum business requirements following an interruption to the Platform operations. â¢ Support application development functions. â¢ Maintain service level agreements, policies and procedures. â¢ Assist with application support functions, including coordinating vendor assisted support. â¢ Participate in problem management task forces and perform root cause troubleshooting. â¢ Implement monitoring solutions as defined by Senior Staff Member â¢ Implement backup solutions as defined by Senior Staff Members â¢ Ensure adequate capacity is available at all times to meet the requirements of the operations and business through capacity planning and management. â¢ Maintains existing databases, ERP, BI, ETL, Data Warehouse based on specifications. â¢ Establishes documentation procedures and standards and backups. â¢ Manages security of Middleware technologies structures. â¢ Responsible for triage of performance and tuning of Middleware technologies. â¢ Provides on-call support 24/7. â¢ Experience in data virtulization using Delphix. â¢ Experience in ETL using Attunity, Talend, Informatica.    Bachelorsâ Degree in related technology field or equivalent experience.  ","Bachelorsâ Degree in related technology field or equivalent experience. Monitors and takes correction action on Middleware technologies. â¢ Responsible for operations activities of Database, ERP, BI, ETL, Data Warehouse. Respond, prioritize, escalate incidents service requests as necessary according to EIS processes procedures. Develop implement database operational procedures Perform the loading production data any other issues regarding loads. Continue provide a pre-determined agreed level services support minimum business requirements following an interruption Platform operations. Support application development functions. Maintain agreements, policies Assist with functions, including coordinating vendor assisted support. Participate problem management task forces perform root cause troubleshooting. Implement monitoring solutions defined by Senior Staff Member backup Members Ensure adequate capacity is available at all times meet through planning management. Maintains existing databases, Warehouse based specifications. Establishes documentation standards backups. Manages security technologies structures. triage performance tuning Provides on-call 24/7. Experience virtulization using Delphix. ETL Attunity, Talend, Informatica.","Bachelorsâ Degree related technology field equivalent experience. Monitors takes correction action Middleware technologies. â¢ Responsible operations activities Database, ERP, BI, ETL, Data Warehouse. Respond, prioritize, escalate incidents service requests necessary according EIS processes procedures. Develop implement database operational procedures Perform loading production data issues regarding loads. Continue provide pre-determined agreed level services support minimum business requirements following interruption Platform operations. Support application development functions. Maintain agreements, policies Assist functions, including coordinating vendor assisted support. Participate problem management task forces perform root cause troubleshooting. Implement monitoring solutions defined Senior Staff Member backup Members Ensure adequate capacity available times meet planning management. Maintains existing databases, Warehouse based specifications. Establishes documentation standards backups. Manages security technologies structures. triage performance tuning Provides on-call 24/7. Experience virtulization using Delphix. ETL Attunity, Talend, Informatica."
425,Data Engineer,Data Analysis Engineer,"Los Angeles, CA 90067",Los Angeles,CA,"At Factual, we're looking for analytical technologists who love data, learning, and problem solving. Join us as we work to organize and optimize the world's location information. Our team is focused on cleaning, structuring, and delivering our dataset of over 100 million places to bring contextual awareness to digital devices in the physical world. To make our goals a reality, we end up taking on unusual problems with small, focused teams made up of highly motivated people.

Our Data team is seeking a Data Analysis Engineer to help us ensure we deliver the highest quality Global Places product in the market. In this role, you will investigate and solve complex data quality and delivery problems. You will acquire new data sources and contribute heavily to our data processing software. You will author specifications for new tools and help manage technical projects. In short, you'll work on everything from experimental data science to implementing production level code to technical product management.

At Factual, we cultivate multidisciplinary engineering teams. Everyone is comfortable writing code and analyzing data, but we expect each individual contributor to bring an additional unique skill or expertise to the table. You should be a fast-working, independent, highly-focused individual who pays strong attention to detail, shows great leadership and organizational skills, and gets things done.

About you:

You hold at least a bachelor's degree in a quantitative field with coursework in statistics (e.g. Math, Linguistics, Physics, Chemistry, Engineering)
You have at least 1-3 years of industry experience
You have significant experience working with data (e.g. via internships or personal projects)
You excel at data analysis, ETL, and curation
You have proven experience thrive working independently on ill-defined problems
You pride yourself on your written and verbal communication skills
You feel at home on the command line and with text processing
You enjoy learning new technologies and skills in data processing and analysis

Baseline Skills (these are required):

Proficient with Unix commands and comfortable working on the command line
Adept using a scripting language like Python or Ruby to process text files
Experience with relational and non-relational databases

Specialized Skills (you need expertise in at least one of the following):


Implementing machine learning pipelines
Data Querying and manipulation in Spark, Hadoop, and Hive
Web scraping

Cover letters are greatly appreciated!",  Proficient with Unix commands and comfortable working on the command line Adept using a scripting language like Python or Ruby to process text files Experience with relational and non-relational databases    ,Proficient with Unix commands and comfortable working on the command line Adept using a scripting language like Python or Ruby to process text files Experience relational non-relational databases,Proficient Unix commands comfortable working command line Adept using scripting language like Python Ruby process text files Experience relational non-relational databases
426,Data Engineer,Senior Data Engineer,"Glendale, CA",Glendale,CA,"DISQO is a next generation consumer insights platform. We provide the highest quality consumer data to the world's largest market research agencies, analytics companies, and brands. We operate one of the world's largest true consumer insights panels. This data helps our clients understand user behavior, build better experiences, and make better decisions. We utilize cutting-edge technology and innovative, out-of-the-box strategies to collect and analyze insights which help shape the products and services of tomorrow.

This is a great opportunity to join a fun, exciting & highly motivated team and upgrade your skills while creating real impact. We use a modern tech stack and cloud infrastructure. We are not only looking for work experience, but rather the willingness to step up to challenges and the ability to learn quickly.

We believe the best software is written and managed by small teams that know how to make the impossible possible. We use agile software development techniques and modern tools to focus our efforts on solving our business goals. We use OKRs to track everything we do. We deliver early and often. We obsess over our code, architecture and infrastructure. And we believe that these practices lead to higher quality products.

Responsibilities:

Leverage your software development and data engineering skills to impact our business by taking ownership of key projects requiring coding and data pipelines
Collaborate with product managers, software engineers and data engineers to design, implement, and deliver successful data solutions
Define technical requirements and implementation details for data solutions
Design, build and optimize performant databases, data models, integrations and ETL pipelines in RDBMS and NoSQL environments
Maintain detailed documentation of your work and changes to support data quality and governance
Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers
Be an active participant and advocate of agile/scrum practices to ensure health and process improvements for your team

Requirements:

5+ years of experience designing and delivering large scale, 24/7, mission-critical data pipelines and features using modern big data architectures
3+ years of Scala
3+ years of Python
3+ years of Spark
2+ years of experience with AWS data ecosystem (Redshift, EMR, Glue, Athena, ...)
Deep knowledge in various ETL/ELT tools and concepts, data modeling, SQL, query performance optimization
Experience with building stream processing applications using Kinesis or Kafka
Experience with workflow management tools (Airflow, Oozie, Azkaban, Luigi, etc.)
Comfortable working in Linux environment
Ability to thrive in an agile, entrepreneurial start-up environment

Benefits & Perks:

100% covered Medical/Dental/Vision for employee
Equity
Paid vacation
Flexible work hours
Catered lunches 3x's a week
Stocked pantry
Happy Hours
Weekly Yoga
Discounted Gym Membership
Commuter Program
Quarterly Offsite's

DISQO is an equal opportunity employer


Recruiting firms that submit resumes to DISQO without first entering into a written contract will not be entitled to any compensation on candidates referred by that firm.

","   Leverage your software development and data engineering skills to impact our business by taking ownership of key projects requiring coding and data pipelines Collaborate with product managers, software engineers and data engineers to design, implement, and deliver successful data solutions Define technical requirements and implementation details for data solutions Design, build and optimize performant databases, data models, integrations and ETL pipelines in RDBMS and NoSQL environments Maintain detailed documentation of your work and changes to support data quality and governance Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers Be an active participant and advocate of agile/scrum practices to ensure health and process improvements for your team    5+ years of experience designing and delivering large scale, 24/7, mission-critical data pipelines and features using modern big data architectures 3+ years of Scala 3+ years of Python 3+ years of Spark 2+ years of experience with AWS data ecosystem  Redshift, EMR, Glue, Athena, ...  Deep knowledge in various ETL/ELT tools and concepts, data modeling, SQL, query performance optimization Experience with building stream processing applications using Kinesis or Kafka Experience with workflow management tools  Airflow, Oozie, Azkaban, Luigi, etc.  Comfortable working in Linux environment Ability to thrive in an agile, entrepreneurial start-up environment ","Leverage your software development and data engineering skills to impact our business by taking ownership of key projects requiring coding pipelines Collaborate with product managers, engineers design, implement, deliver successful solutions Define technical requirements implementation details for Design, build optimize performant databases, models, integrations ETL in RDBMS NoSQL environments Maintain detailed documentation work changes support quality governance Ensure high operational efficiency meet SLAs commitment the customers Be an active participant advocate agile/scrum practices ensure health process improvements team 5+ years experience designing delivering large scale, 24/7, mission-critical features using modern big architectures 3+ Scala Python Spark 2+ AWS ecosystem Redshift, EMR, Glue, Athena, ... Deep knowledge various ETL/ELT tools concepts, modeling, SQL, query performance optimization Experience building stream processing applications Kinesis or Kafka workflow management Airflow, Oozie, Azkaban, Luigi, etc. Comfortable working Linux environment Ability thrive agile, entrepreneurial start-up","Leverage software development data engineering skills impact business taking ownership key projects requiring coding pipelines Collaborate product managers, engineers design, implement, deliver successful solutions Define technical requirements implementation details Design, build optimize performant databases, models, integrations ETL RDBMS NoSQL environments Maintain detailed documentation work changes support quality governance Ensure high operational efficiency meet SLAs commitment customers Be active participant advocate agile/scrum practices ensure health process improvements team 5+ years experience designing delivering large scale, 24/7, mission-critical features using modern big architectures 3+ Scala Python Spark 2+ AWS ecosystem Redshift, EMR, Glue, Athena, ... Deep knowledge various ETL/ELT tools concepts, modeling, SQL, query performance optimization Experience building stream processing applications Kinesis Kafka workflow management Airflow, Oozie, Azkaban, Luigi, etc. Comfortable working Linux environment Ability thrive agile, entrepreneurial start-up"
427,Data Engineer,Database Engineer,"Woodland Hills, CA",Woodland Hills,CA,"As a Database Engineer at BlackLine, you will play a vital role in the performance, delivery, stability, and security of the databases we use, while continually driving forward improvements and optimizations at the database layer. As a member of the database team, you will be involved in the planning, development, and maintenance of the database, including troubleshooting issues, collaborating with other teams to define and build new features, optimize existing features, and collaborate in order to drive growth, improve controls and processes, and reduce overhead and complexity. In this role, we are looking for people who are team players, passionate about their areas of expertise, and constantly striving to learn and improve, not just in the sense of their own skills, but also in growing with peers whom they work with day-to-day. If you are someone who strives for excellence in all that they do, including helping those in your team, and someone who wants to ultimately deliver the best value and success you can, then we want to talk to you.

Responsibilities:
Participate in cross-functional teams and build relationships across the organization
Build high-performance, massively-scalable, always-available Cloud-based systems.
Ensure data integrity and quality in database systems.
Maintain standard policies for database development activities.
Provide database solutions based on technical documents and business requirements.
Provide technical assistance to resolve all database issues related to performance, capacity and access.
Analyze issues holistically, from the application tier through the database, down to the storage.
Database Engineer will ensure that all deadlines are met and that quality is of the highest level throughout the Software Development Life Cycle
Qualifications:
3+ years experience SQL 2008, 2012, 2014, 2017.
2 or more years working in high-transaction environments is required
Working knowledge of different index types and how they are used: columnstore, full-text, filtered, indexes with include columns
Basic Execution Plan understanding
SSIS/SSRS experience is a strong plus
Experience with performance tuning and optimization, using native monitoring and troubleshooting tools and techniques, including complex queries as well as procedure and indexing strategies
Excellent written and verbal communication
Adaptable team-player with a focus on results and value delivery
Able to organize and plan work independently
Participates in various technology POCs
Understanding of OLTP vs OLAP environments
Working knowledge of relational database internals (locking, consistency, serialization, recovery paths)
NoSQL is a plus
MCTS, MCITP, and/or MVP certifications are a plus","3+ years experience SQL 2008, 2012, 2014, 2017. 2 or more years working in high-transaction environments is required Working knowledge of different index types and how they are used  columnstore, full-text, filtered, indexes with include columns Basic Execution Plan understanding SSIS/SSRS experience is a strong plus Experience with performance tuning and optimization, using native monitoring and troubleshooting tools and techniques, including complex queries as well as procedure and indexing strategies Excellent written and verbal communication Adaptable team-player with a focus on results and value delivery Able to organize and plan work independently Participates in various technology POCs Understanding of OLTP vs OLAP environments Working knowledge of relational database internals  locking, consistency, serialization, recovery paths  NoSQL is a plus MCTS, MCITP, and/or MVP certifications are a plus    ","3+ years experience SQL 2008, 2012, 2014, 2017. 2 or more working in high-transaction environments is required Working knowledge of different index types and how they are used columnstore, full-text, filtered, indexes with include columns Basic Execution Plan understanding SSIS/SSRS a strong plus Experience performance tuning optimization, using native monitoring troubleshooting tools techniques, including complex queries as well procedure indexing strategies Excellent written verbal communication Adaptable team-player focus on results value delivery Able to organize plan work independently Participates various technology POCs Understanding OLTP vs OLAP relational database internals locking, consistency, serialization, recovery paths NoSQL MCTS, MCITP, and/or MVP certifications","3+ years experience SQL 2008, 2012, 2014, 2017. 2 working high-transaction environments required Working knowledge different index types used columnstore, full-text, filtered, indexes include columns Basic Execution Plan understanding SSIS/SSRS strong plus Experience performance tuning optimization, using native monitoring troubleshooting tools techniques, including complex queries well procedure indexing strategies Excellent written verbal communication Adaptable team-player focus results value delivery Able organize plan work independently Participates various technology POCs Understanding OLTP vs OLAP relational database internals locking, consistency, serialization, recovery paths NoSQL MCTS, MCITP, and/or MVP certifications"
428,Data Engineer,Senior Data Engineer,"Los Angeles, CA",Los Angeles,CA,"About Us

At VideoAmp, we are looking for a Sr. Data Engineer who will be responsible for supporting our engineer team. You are passionate about all things data and tech, and have extensive knowledge of the digital space. You are accomplished in the areas of data engineering and you are ready to support the strategic objectives of a highly functional organization. This role would be based in our Boston, MA office.

Responsibilities:

You will be involved in the design, development, deployment, and testing processes for new products. Our platform provides disruptive insights and optimizes the efficacy of our client's ad campaigns.
You will be a core member of our engineering team and will have a major impact on new initiatives. We're mindful of how the time and space complexities of the small parts greatly affect our services at scale and we need your help to build responsive systems.
You will help build, test, benchmark, and tune our distributed systems to ensure that our product is scalable, responsive, robust, cost-effective, and correct.

Qualifications:
You'll need 5 years of hands-on coding experience - Python or Scala preferred:


Experience working with Spark, Hadoop, or other big data processing platform in high-volume environments
A solid and demonstrable understanding of ETL workflows and data warehousing
Experience with high-performance, low-latency, distributed systems
SQL fluency and an understanding of relational data models
Experience optimizing queries and developing stored procedure
Building out Spark apps to support new product development
Building data models to support new functionality
Build data pipelines to analyze, scrub, and integrate first and third party data
Productionalizing data science models
Coordinating data models with other engineering teams
Tuning and optimizing Spark apps to get the most out of the quickly evolving platform

Perks:

Competitive compensation
Comprehensive health benefits
Meaningful equity
401k
Unlimited vacation with a stipend for travel and accommodations of $2,000/year
Unlimited in-office gym use with personal trainer
Childcare stipend
Plenty of snacks and beverages
Personal and professional development
Epic growth

"," Experience working with Spark, Hadoop, or other big data processing platform in high-volume environments A solid and demonstrable understanding of ETL workflows and data warehousing Experience with high-performance, low-latency, distributed systems SQL fluency and an understanding of relational data models Experience optimizing queries and developing stored procedure Building out Spark apps to support new product development Building data models to support new functionality Build data pipelines to analyze, scrub, and integrate first and third party data Productionalizing data science models Coordinating data models with other engineering teams Tuning and optimizing Spark apps to get the most out of the quickly evolving platform    You will be involved in the design, development, deployment, and testing processes for new products. Our platform provides disruptive insights and optimizes the efficacy of our client's ad campaigns. You will be a core member of our engineering team and will have a major impact on new initiatives. We're mindful of how the time and space complexities of the small parts greatly affect our services at scale and we need your help to build responsive systems. You will help build, test, benchmark, and tune our distributed systems to ensure that our product is scalable, responsive, robust, cost-effective, and correct.   ","Experience working with Spark, Hadoop, or other big data processing platform in high-volume environments A solid and demonstrable understanding of ETL workflows warehousing high-performance, low-latency, distributed systems SQL fluency an relational models optimizing queries developing stored procedure Building out Spark apps to support new product development functionality Build pipelines analyze, scrub, integrate first third party Productionalizing science Coordinating engineering teams Tuning get the most quickly evolving You will be involved design, development, deployment, testing processes for products. Our provides disruptive insights optimizes efficacy our client's ad campaigns. a core member team have major impact on initiatives. We're mindful how time space complexities small parts greatly affect services at scale we need your help build responsive systems. build, test, benchmark, tune ensure that is scalable, responsive, robust, cost-effective, correct.","Experience working Spark, Hadoop, big data processing platform high-volume environments A solid demonstrable understanding ETL workflows warehousing high-performance, low-latency, distributed systems SQL fluency relational models optimizing queries developing stored procedure Building Spark apps support new product development functionality Build pipelines analyze, scrub, integrate first third party Productionalizing science Coordinating engineering teams Tuning get quickly evolving You involved design, development, deployment, testing processes products. Our provides disruptive insights optimizes efficacy client's ad campaigns. core member team major impact initiatives. We're mindful time space complexities small parts greatly affect services scale need help build responsive systems. build, test, benchmark, tune ensure scalable, responsive, robust, cost-effective, correct."
429,Data Engineer,Data Engineer : 19-04614,"Santa Monica, CA 90404",Santa Monica,CA,"Primary Skills: ETL, SQL/T-SQL, Hive/Pig, Java/Python
Position Type: Contract
Duration: 12+ Months

Responsibilities:
Collaborate with data product managers, data architects, and data engineers to design, implement and deliver successful data solutions
Define technical requirements and implementation details for the underlying data warehouse and data marts.
Develop and optimize performant database, data model, integration and ETL in RDBMS and NoSQL environments.
Maintain detailed documentation of your work and changes to support data quality and governance.
Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers.
Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team.
Skills required:
Proven experience with at least one major RDBMS (SQL Server, MySQL or Oracle)
Solid experience with data integration toolsets and writing and maintaining ETL jobs
Expert level experience with SQL/TSQL and ability to create and tune queries
Strong programming (Java/C# or related) and scripting skills (Python/JS) helpful
Agile and Scrum experience.
Experience with MapReduce or other Hadoop implementations â associated understanding of Hive or Pig to query and process data in Hadoop, a big plus.
 To follow up with any questions, please contact Vipul at 408-512-2357

Akraya is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US. We offer comprehensive benefits including Health Insurance (medical, dental, and vision), Cafeteria Plan (HSA, FSA, and dependent care), 401(k) (enrollment subject to eligibility), and Sick Pay (varies based on city and state laws).

If this position is not quite what you're looking for, visit akraya.com and submit a copy of your resume. We will get to work finding you a job that is a better fit at one of our many amazing clients.

Akraya is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Akraya is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation."," Collaborate with data product managers, data architects, and data engineers to design, implement and deliver successful data solutions Define technical requirements and implementation details for the underlying data warehouse and data marts. Develop and optimize performant database, data model, integration and ETL in RDBMS and NoSQL environments. Maintain detailed documentation of your work and changes to support data quality and governance. Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers. Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team.  Collaborate with data product managers, data architects, and data engineers to design, implement and deliver successful data solutions Define technical requirements and implementation details for the underlying data warehouse and data marts. Develop and optimize performant database, data model, integration and ETL in RDBMS and NoSQL environments. Maintain detailed documentation of your work and changes to support data quality and governance. Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers. Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team.   ","Collaborate with data product managers, architects, and engineers to design, implement deliver successful solutions Define technical requirements implementation details for the underlying warehouse marts. Develop optimize performant database, model, integration ETL in RDBMS NoSQL environments. Maintain detailed documentation of your work changes support quality governance. Ensure high operational efficiency meet SLAs commitment customers. Be an active participant advocate agile/scrum practice ensure health process improvements team.","Collaborate data product managers, architects, engineers design, implement deliver successful solutions Define technical requirements implementation details underlying warehouse marts. Develop optimize performant database, model, integration ETL RDBMS NoSQL environments. Maintain detailed documentation work changes support quality governance. Ensure high operational efficiency meet SLAs commitment customers. Be active participant advocate agile/scrum practice ensure health process improvements team."
430,Data Engineer,Senior Data Engineer,"Los Angeles, CA",Los Angeles,CA,"TWO NIL is a growth consultancy that uses a holistic approach to create profitable go to market campaigns â at scale. We provide clients with vertically integrated, unbundled solutions across their acquisition marketing needs, from strategy and planning to execution, forecasting, and optimization. Our clients represent best of breed brands across a diverse range of verticals and geographies.

As a Senior Data Engineer, you'll help us build out our core product by designing and implementing high complexity ETL pipelines using PySpark and AWS Glue. You'll be responsible for delivering quality, scalable, and reusable software, and providing input on software architecture, data storage, API and user interface design. You'll be responsible for the design and implementation of serverless microservices using AWS Lamba and API Gateway.

Responsibilities:

Work closely with Business Intelligence and Data Science teams to understand the data needs, define, scope and plan new product features
Design, build, deploy new data models and complex ETL pipelines into production with PySpark and AWS Glue
Design and implement serverless microservices with AWS Lambda, API Gateway and Fargate
Be accountable for operational efficiency and be proactive in monitoring data pipelines
Make smart platform and engineering decisions based on data analysis and collaboration
Deliver quality scalable, efficient and reusable software
Ensure unit test coverage and promote code quality
Research new tools and technologies applicable to new and existing use cases
Identify areas to improve architecture, application design and scalability
Provide input on software architecture, data storage, API and user interface design
Mentor more junior team members

Qualifications:

Degree in Computer Science
6 + year's building enterprise software applications
5+ years' experience in custom ETL design, implementation, and maintenance
3+ years experience with Spark/PySpark or equivalent distributed processing systems
3 + years experience with AWS Services (Athena, Glue, Redshift, DynamoDB, ECS, S3) and python.
Experience with Service Oriented Architecture and Microservices
Experience in the complete Software Development Life Cycle (SDLC)
Experience with Agile Software Development (SCRUM)
Good understanding on relational databases (Postgres, MySql, etc.)
Good understanding on NoSQL databases (DynamoDB, MongoDB, etc.)
Familiarity with container orchestration services (ECS, Kubernetes, etc.)
Familiarity with serverless compute platforms (Lambda, Cloud Functions, etc.)
Drive to analyze data to identify deliverables, anomalies, and gaps
Accountable, Curious and Organized
Experience in data analytics tools such as Tableau a good to have
Strong communication skills to collaborate with various teams

As a TWO NIL employee, you will enjoy:

Competitive compensation package
Unlimited paid time off policy
Flexible working hours
Benefits (Health, Dental, Vision, Life Insurance, 401k, Flexible Spending Account and more)
Fitness reimbursement
Catered lunches and stocked kitchen with fresh fruit, snacks, premium coffee & tea, and cold brew coffee
Ongoing learning and classes for employees
Team events and outings

"," Degree in Computer Science 6 + year's building enterprise software applications 5+ years' experience in custom ETL design, implementation, and maintenance 3+ years experience with Spark/PySpark or equivalent distributed processing systems 3 + years experience with AWS Services  Athena, Glue, Redshift, DynamoDB, ECS, S3  and python. Experience with Service Oriented Architecture and Microservices Experience in the complete Software Development Life Cycle  SDLC  Experience with Agile Software Development  SCRUM  Good understanding on relational databases  Postgres, MySql, etc.  Good understanding on NoSQL databases  DynamoDB, MongoDB, etc.  Familiarity with container orchestration services  ECS, Kubernetes, etc.  Familiarity with serverless compute platforms  Lambda, Cloud Functions, etc.  Drive to analyze data to identify deliverables, anomalies, and gaps Accountable, Curious and Organized Experience in data analytics tools such as Tableau a good to have Strong communication skills to collaborate with various teams    Work closely with Business Intelligence and Data Science teams to understand the data needs, define, scope and plan new product features Design, build, deploy new data models and complex ETL pipelines into production with PySpark and AWS Glue Design and implement serverless microservices with AWS Lambda, API Gateway and Fargate Be accountable for operational efficiency and be proactive in monitoring data pipelines Make smart platform and engineering decisions based on data analysis and collaboration Deliver quality scalable, efficient and reusable software Ensure unit test coverage and promote code quality Research new tools and technologies applicable to new and existing use cases Identify areas to improve architecture, application design and scalability Provide input on software architecture, data storage, API and user interface design Mentor more junior team members   ","Degree in Computer Science 6 + year's building enterprise software applications 5+ years' experience custom ETL design, implementation, and maintenance 3+ years with Spark/PySpark or equivalent distributed processing systems 3 AWS Services Athena, Glue, Redshift, DynamoDB, ECS, S3 python. Experience Service Oriented Architecture Microservices the complete Software Development Life Cycle SDLC Agile SCRUM Good understanding on relational databases Postgres, MySql, etc. NoSQL MongoDB, Familiarity container orchestration services Kubernetes, serverless compute platforms Lambda, Cloud Functions, Drive to analyze data identify deliverables, anomalies, gaps Accountable, Curious Organized analytics tools such as Tableau a good have Strong communication skills collaborate various teams Work closely Business Intelligence Data understand needs, define, scope plan new product features Design, build, deploy models complex pipelines into production PySpark Glue Design implement microservices API Gateway Fargate Be accountable for operational efficiency be proactive monitoring Make smart platform engineering decisions based analysis collaboration Deliver quality scalable, efficient reusable Ensure unit test coverage promote code Research technologies applicable existing use cases Identify areas improve architecture, application design scalability Provide input storage, user interface Mentor more junior team members","Degree Computer Science 6 + year's building enterprise software applications 5+ years' experience custom ETL design, implementation, maintenance 3+ years Spark/PySpark equivalent distributed processing systems 3 AWS Services Athena, Glue, Redshift, DynamoDB, ECS, S3 python. Experience Service Oriented Architecture Microservices complete Software Development Life Cycle SDLC Agile SCRUM Good understanding relational databases Postgres, MySql, etc. NoSQL MongoDB, Familiarity container orchestration services Kubernetes, serverless compute platforms Lambda, Cloud Functions, Drive analyze data identify deliverables, anomalies, gaps Accountable, Curious Organized analytics tools Tableau good Strong communication skills collaborate various teams Work closely Business Intelligence Data understand needs, define, scope plan new product features Design, build, deploy models complex pipelines production PySpark Glue Design implement microservices API Gateway Fargate Be accountable operational efficiency proactive monitoring Make smart platform engineering decisions based analysis collaboration Deliver quality scalable, efficient reusable Ensure unit test coverage promote code Research technologies applicable existing use cases Identify areas improve architecture, application design scalability Provide input storage, user interface Mentor junior team members"
431,Data Engineer,Data Engineer,"Los Angeles, CA",Los Angeles,CA,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with the first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and the ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADAâs values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."," Expertise in at least one of the following domain areas    Data warehouse modernization  building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software  such as Beam, Airflow, Hadoop, Spark, Hive . Data migration  migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime. Backup, restore & disaster recovery  building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. Experience writing software in one or more languages such as Python, Java, Scala, or Go Experience building production-grade data solutions  relational and NoSQL  Experience with systems monitoring/alerting, capacity planning and performance tuning Experience in technical consulting or other customer-facing role     ","Expertise in at least one of the following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming processing software such as Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores to reliable scalable cloud-based stores, strategies for near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing more languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting other customer-facing role","Expertise least one following domain areas Data warehouse modernization building complete data solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, reporting/analytic tools. Must hands-on experience working batch streaming processing software Beam, Airflow, Hadoop, Spark, Hive . migration migrating stores reliable scalable cloud-based stores, strategies near zero-downtime. Backup, restore & disaster recovery production-grade backup restore, solutions. Up petabytes scale. Experience writing languages Python, Java, Scala, Go solutions relational NoSQL systems monitoring/alerting, capacity planning performance tuning consulting customer-facing role"
432,Data Engineer,Science Data Processing Engineer,"Pasadena, CA",Pasadena,CA,"Raytheon Pasadena is looking for a Science Data Processing Engineer to support the combined needs of two exciting projects for our JPL/NASA customer. The role involves two primary activities for two projects: (1) monitor and maintain local data archives and (2) control of data production activities. The projects at JPL serve the local project science staff, related NASA discipline science teams, and the NASA Goddard Earth Science Data and Information Services Center (GES DISC) at GSFC. While the two projects have two separate management teams and utilized their own unique processing environments, there is a large degree of synergy and cooperation between the two projects. Key work activities are outlined below:

Data Production Activities: Maintain, develop and run operational data processing scripts, monitor processing activities, assess success or failure of jobs, and rerun jobs as needed. Report conditions and results to Project Engineers, the development and Science Teams, and the interested data community at large.

Monitor and Maintain Data Archives: Order all necessary data from external sources required for local data production and analysis based on Science Team guidance. Review data ingest to identify gaps and reorder data as needed. Monitor data archives to ensure that they are performing up to specification. Monitor data status tools to ensure they are working according to specification. Investigate any problems with the archive at the database level using SQL commands.

Job responsibilities include, but are not limited to:

Perform routine daily and ad-hoc special processing for both the projects
Use Zabbix to monitor servers and processing
Monitor the state of data ingest and the local archive on a daily basis
Report system status to the local development team, the discipline Science Teams and the external user community on a daily basis
Maintain and develop new tools to enhance operator capability and enhance user experience with our local data archive
Responsible for full 'shut-down' and 'start-up' of operational components (file manager, push/pull, crawler, workflow manager, etc.)
Stage locally generated data products for archive at the GES DISC
Continually monitor operational system health

Education and Experience:
Bachelorâs degree in STEM with typically a minimum of 4 years of related experience

Required Skills:

Capability to fully understand the operational missions of both projects and their science team.
Linux proficiency, including use of screen
Use of server monitoring tools (Zabbix)
Extensive experience with scripting language and capability to create/edit scripts to (1) control job flow and (2) monitor data archives.
Knowledge of Oracle database query language, SQL
Knowledge of Docker containers
Use of JIRA for submitting and receiving processing requests
Excellent written and verbal communication skills

Desirable Skills:

Extensive experience and demonstrated versatility in Python, Perl and Bash.
Extensive experience and demonstrated versatility JAVA, C
Full understanding of file formats: HDF 4, HDF 5, NetCDF
Knowledge of Makefile system

Location
The candidate will be required to work in Pasadena CA

Debbie Guzman

(909) 345-7222

dguzman@geologics.com","  Capability to fully understand the operational missions of both projects and their science team. Linux proficiency, including use of screen Use of server monitoring tools  Zabbix  Extensive experience with scripting language and capability to create/edit scripts to  1  control job flow and  2  monitor data archives. Knowledge of Oracle database query language, SQL Knowledge of Docker containers Use of JIRA for submitting and receiving processing requests Excellent written and verbal communication skills    Capability to fully understand the operational missions of both projects and their science team. Linux proficiency, including use of screen Use of server monitoring tools  Zabbix  Extensive experience with scripting language and capability to create/edit scripts to  1  control job flow and  2  monitor data archives. Knowledge of Oracle database query language, SQL Knowledge of Docker containers Use of JIRA for submitting and receiving processing requests Excellent written and verbal communication skills  ","Capability to fully understand the operational missions of both projects and their science team. Linux proficiency, including use screen Use server monitoring tools Zabbix Extensive experience with scripting language capability create/edit scripts 1 control job flow 2 monitor data archives. Knowledge Oracle database query language, SQL Docker containers JIRA for submitting receiving processing requests Excellent written verbal communication skills","Capability fully understand operational missions projects science team. Linux proficiency, including use screen Use server monitoring tools Zabbix Extensive experience scripting language capability create/edit scripts 1 control job flow 2 monitor data archives. Knowledge Oracle database query language, SQL Docker containers JIRA submitting receiving processing requests Excellent written verbal communication skills"
433,Data Engineer,Data Engineer,"Glendale, CA",Glendale,CA,"Summary
We are looking for a savvy data engineer to join our growing team of data experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams and product lines. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our companyâs data architecture to support our next generation of products and data initiatives.

Responsibilities
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and as well as AWS and Azure-based data technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the executive, operations, and business development teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.


Qualifications
Bachelorâs degree in computer science, software/computer engineering, applied mathematics, or physics statistics.
5+ years of experience in a Data Engineer role.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing âbig dataâ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Develop set processes for data mining, data modeling, and data production.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable âbig dataâ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Ensure that all systems meet the business/company requirements as well as industry practices.
Integrate up-and-coming data management and software engineering technologies into existing data structures.
Recommend different ways to constantly improve data reliability and quality.


Experience
Experience with big data tools: AWS EMR, Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, especially, Microsoft SQL Server, IBM DB2, Apache Derby, and PostgreSQL.
Experience with data pipeline and workflow management tools.
Experience with AWS and Azure cloud services, including AWS EMR
Experience with GraphQL
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Java, .Net, Python, Scala, etc.


About us
PSI Services has 70 years of experience with providing solutions to federal and state agencies, corporations, professional associations and certifying bodies worldwide. We offer a comprehensive solutions approach from test development to delivery to results processing which includes pre-hire employment selection, managerial assessments, licensing and certification tests, license management services and professional services.

PSI, a leader in the assessment industry, provides various solutions through its multiple business channels including talent assessments (of job-seeking candidates as well as employee development programs), licensure testing services (for government regulatory agencies), certification credentialing services (for professional associations), license management services and biometric identification authentication services. PSI offers clients various solutions to measure, assess and identify the skills, abilities, traits and identities of individuals seeking jobs, licensure, or certification credentials.

Benefits
PSI offers a competitive and comprehensive benefits package inclusive of:
Medical, Dental, Vision, Life, and Short and Long-Term Disability Insurance
Flexible Spending Accounts
401k plan with company match
Generous PTO and Holiday Pay


We are an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status.","Bachelorâs degree in computer science, software/computer engineering, applied mathematics, or physics statistics. 5+ years of experience in a Data Engineer role. Advanced working SQL knowledge and experience working with relational databases, query authoring  SQL  as well as working familiarity with a variety of databases. Experience building and optimizing âbig dataâ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. Develop set processes for data mining, data modeling, and data production. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable âbig dataâ data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. Ensure that all systems meet the business/company requirements as well as industry practices. Integrate up-and-coming data management and software engineering technologies into existing data structures. Recommend different ways to constantly improve data reliability and quality.   Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements  automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and as well as AWS and Azure-based data technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the executive, operations, and business development teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems.   ","Bachelorâs degree in computer science, software/computer engineering, applied mathematics, or physics statistics. 5+ years of experience a Data Engineer role. Advanced working SQL knowledge and with relational databases, query authoring as well familiarity variety databases. Experience building optimizing âbig dataâ data pipelines, architectures sets. performing root cause analysis on internal external processes to answer specific business questions identify opportunities for improvement. Strong analytic skills related unstructured datasets. Build supporting transformation, structures, metadata, dependency workload management. Develop set mining, modeling, production. A successful history manipulating, processing extracting value from large disconnected Working message queuing, stream processing, highly scalable stores. project management organizational skills. cross-functional teams dynamic environment. Ensure that all systems meet the business/company requirements industry practices. Integrate up-and-coming software engineering technologies into existing structures. Recommend different ways constantly improve reliability quality. Create maintain optimal pipeline architecture, Assemble large, complex sets functional / non-functional requirements. Identify, design, implement process improvements automating manual processes, delivery, re-designing infrastructure greater scalability, etc. required extraction, loading wide sources using AWS Azure-based technologies. analytics tools utilize provide actionable insights customer acquisition, operational efficiency other key performance metrics. Work stakeholders including executive, operations, development assist data-related technical issues support their needs. Keep our separated secure across national boundaries through multiple centers regions. scientist team members them product an innovative leader. experts strive functionality systems.","Bachelorâs degree computer science, software/computer engineering, applied mathematics, physics statistics. 5+ years experience Data Engineer role. Advanced working SQL knowledge relational databases, query authoring well familiarity variety databases. Experience building optimizing âbig dataâ data pipelines, architectures sets. performing root cause analysis internal external processes answer specific business questions identify opportunities improvement. Strong analytic skills related unstructured datasets. Build supporting transformation, structures, metadata, dependency workload management. Develop set mining, modeling, production. A successful history manipulating, processing extracting value large disconnected Working message queuing, stream processing, highly scalable stores. project management organizational skills. cross-functional teams dynamic environment. Ensure systems meet business/company requirements industry practices. Integrate up-and-coming software engineering technologies existing structures. Recommend different ways constantly improve reliability quality. Create maintain optimal pipeline architecture, Assemble large, complex sets functional / non-functional requirements. Identify, design, implement process improvements automating manual processes, delivery, re-designing infrastructure greater scalability, etc. required extraction, loading wide sources using AWS Azure-based technologies. analytics tools utilize provide actionable insights customer acquisition, operational efficiency key performance metrics. Work stakeholders including executive, operations, development assist data-related technical issues support needs. Keep separated secure across national boundaries multiple centers regions. scientist team members product innovative leader. experts strive functionality systems."
434,Data Engineer,Senior Data Engineer - Riot Data Products,"Los Angeles, CA 90064",Los Angeles,CA,"Riot Games was founded in 2006 by Brandon Beck and Marc Merrill with the intent to change the way video games are made and supported for players. In 2009, Riot released its debut title League of Legends to worldwide acclaim. The game has since gone on to become the most played PC game in the world and an important driver of the explosive growth of Esports. Players are the foundation of our community and it's for them we continue to evolve and improve the League of Legends experience.

Riot Data Products (RDP) is an engineering team whose mission is to build solutions that equip Rioters with data technology that helps them to enhance the experience of players and Rioters. Our products range from data pipelines deployed globally that used to publish a million plus events per second into our data ecosystem all the way to single page web apps that guide essential processes inside of Riot and influences our ability to draw insights about Riot itself, our people, teams, and organization.

Experts in data have a broad range of skills which cover aspects of software engineering, systems engineering and mix it with a deep appreciation for data processing through SQL, Python, and other languages. As a Senior Data Engineer on RDP, your experience with data pipelines will cover the full stack from working with Gameplay Engineers on sourcing data to consolidating data from 18 regions worldwide using tools like Google BigQuery, Kinesis, and Kafka to modelling Data Marts for use by Analysts and Data Scientists for business analytics, player facing features, and AI capabilities.

-----------------

Responsibilities:
-----------------


Automate and maintain batch pipelines that collect and process data to improve the player experience, drive game understanding, and report business metrics.
Design and build tools that provide confidence in our data quality-- from data QA processes for publishing new events, to systems for automatic detection of unexpected changes in our data's volume and distributional properties.
Partner with software engineers to build data-driven feedback loops into our game server, client, and backend services by defining data models and contributing to telemetry implementations.
Work with data customers (product owners, game designers, analysts, and data scientists) to identify important questions and then provide the services, datasets, and tools that empower them to find the answers.
Help other developers select data technologies that provide efficient insights with a high return on investment.
Be the face of data best-practices to other developers around the product by advocating for data considerations early in the product life cycle and educating teammates on how to get the most out of our ecosystem.
Mentor and coach data and software engineers and collaborate with other technologists to increase the Data Discipline's capabilities and influence across the product.

-----------------------

Desired Qualifications:
-----------------------


Bachelor's degree in Computer Science or comparable field
8+ years experience in Python and SQL
8+ years experience in Java, Scala, or similar OO experience
Experience with data analysis, processing, and validation
5+ years experience with Spark, Hadoop, or Databricks
Professional experience with open source ETL frameworks such as Airflow, Luigi, or similar
Knowledge within a diverse set of public cloud technologies: AWS RDS, S3, EC2, Lambda, Google Cloud Big Query, Google Cloud Bigtable, etc.
For this role, you'll find success through craft expertise, a collaborative spirit, and decision-making that prioritizes your fellow Rioters, who are the customers of your work. Being a dedicated fan of games is not necessary for this position!

----------

Our Perks:
----------

We offer medical, dental, and vision plans that cover you, your spouse/domestic partner, and children. Life insurance, parental leave, plus short-term and long-term disability coverage are also available. Riot will support your retirement benefits with a company match, and double down on your donations of time and money to non-profit charitable organizations. Balance between work and personal life is encouraged with open paid time off, and a play fund so you can broaden and deepen your personal relationship with games.

===

It's our policy to provide equal employment opportunity for all applicants and members of Riot Games, Inc. Riot Games makes reasonable accommodations for handicapped and disabled Rioters and does not unlawfully discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, handicap, veteran status, marital status, criminal history, or any other category protected by applicable federal and state law, including the City of Los Angeles' Fair Chance Initiative for Hiring Ordinance relating to an applicant's criminal history (LAMC 189.00)."," Bachelor's degree in Computer Science or comparable field 8+ years experience in Python and SQL 8+ years experience in Java, Scala, or similar OO experience Experience with data analysis, processing, and validation 5+ years experience with Spark, Hadoop, or Databricks Professional experience with open source ETL frameworks such as Airflow, Luigi, or similar Knowledge within a diverse set of public cloud technologies  AWS RDS, S3, EC2, Lambda, Google Cloud Big Query, Google Cloud Bigtable, etc. For this role, you'll find success through craft expertise, a collaborative spirit, and decision-making that prioritizes your fellow Rioters, who are the customers of your work. Being a dedicated fan of games is not necessary for this position!    Automate and maintain batch pipelines that collect and process data to improve the player experience, drive game understanding, and report business metrics. Design and build tools that provide confidence in our data quality-- from data QA processes for publishing new events, to systems for automatic detection of unexpected changes in our data's volume and distributional properties. Partner with software engineers to build data-driven feedback loops into our game server, client, and backend services by defining data models and contributing to telemetry implementations. Work with data customers  product owners, game designers, analysts, and data scientists  to identify important questions and then provide the services, datasets, and tools that empower them to find the answers. Help other developers select data technologies that provide efficient insights with a high return on investment. Be the face of data best-practices to other developers around the product by advocating for data considerations early in the product life cycle and educating teammates on how to get the most out of our ecosystem. Mentor and coach data and software engineers and collaborate with other technologists to increase the Data Discipline's capabilities and influence across the product.   ","Bachelor's degree in Computer Science or comparable field 8+ years experience Python and SQL Java, Scala, similar OO Experience with data analysis, processing, validation 5+ Spark, Hadoop, Databricks Professional open source ETL frameworks such as Airflow, Luigi, Knowledge within a diverse set of public cloud technologies AWS RDS, S3, EC2, Lambda, Google Cloud Big Query, Bigtable, etc. For this role, you'll find success through craft expertise, collaborative spirit, decision-making that prioritizes your fellow Rioters, who are the customers work. Being dedicated fan games is not necessary for position! Automate maintain batch pipelines collect process to improve player experience, drive game understanding, report business metrics. Design build tools provide confidence our quality-- from QA processes publishing new events, systems automatic detection unexpected changes data's volume distributional properties. Partner software engineers data-driven feedback loops into server, client, backend services by defining models contributing telemetry implementations. Work product owners, designers, analysts, scientists identify important questions then services, datasets, empower them answers. Help other developers select efficient insights high return on investment. Be face best-practices around advocating considerations early life cycle educating teammates how get most out ecosystem. Mentor coach collaborate technologists increase Data Discipline's capabilities influence across product.","Bachelor's degree Computer Science comparable field 8+ years experience Python SQL Java, Scala, similar OO Experience data analysis, processing, validation 5+ Spark, Hadoop, Databricks Professional open source ETL frameworks Airflow, Luigi, Knowledge within diverse set public cloud technologies AWS RDS, S3, EC2, Lambda, Google Cloud Big Query, Bigtable, etc. For role, find success craft expertise, collaborative spirit, decision-making prioritizes fellow Rioters, customers work. Being dedicated fan games necessary position! Automate maintain batch pipelines collect process improve player experience, drive game understanding, report business metrics. Design build tools provide confidence quality-- QA processes publishing new events, systems automatic detection unexpected changes data's volume distributional properties. Partner software engineers data-driven feedback loops server, client, backend services defining models contributing telemetry implementations. Work product owners, designers, analysts, scientists identify important questions services, datasets, empower answers. Help developers select efficient insights high return investment. Be face best-practices around advocating considerations early life cycle educating teammates get ecosystem. Mentor coach collaborate technologists increase Data Discipline's capabilities influence across product."
435,Data Engineer,Google Data Engineer,"Los Angeles, CA",Los Angeles,CA,"Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet todayâs high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps â Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP  DevOps on an GCP platform. Multi-cloud experience a plus.   Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","Minimum of 3 years previous Consulting or client service delivery experience on Google GCP DevOps an platform. Multi-cloud a plus. Proven ability to build, manage and foster team-oriented environment work creatively analytically in problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","Minimum 3 years previous Consulting client service delivery experience Google GCP DevOps platform. Multi-cloud plus. Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
436,Data Engineer,Data Engineer (Cassandra),"Pasadena, CA 91107",Pasadena,CA,"Job Type:
Regular
Department:
Data Engineering
City:
Pasadena
State/Territory:
California
FLSA:
Exempt
Job Summary
In this role, you will be responsible for supporting multiple applications and data transfers between internal and client facing applications.
The successful candidate will have hands-on experience in a multitude of domains such as:

24x7x365 enterprise and cloud database infrastructure design and management
Database security, business continuity, and disaster recovery
Database design, development, tuning, and general database administration
Job Responsibilities

Design, maintenance, and/or administration with Apache Cassandra, including data modeling and CQL scripting.
Work with DataStax Enterprise Graph, including Gremlin query language.
Develop data models, and plan and write high performing queries.
Help analyze data access patterns and identify hotspots and bottlenecks.
Plan, coordinate, and administer database/DSE systems, including base definition, structure, documentation, requirements, operational guidelines and protection.
You'll be doing some administrator tasks for Cassandra, including configuration of Cassandra cluster with mutli-data centers.
Your tasks will also include performance tuning for memory, troubleshooting, and node rebuild/backup
Job Requirements

5+ years of related experience
Past DB production support experience is helpful
Prior experience in the following areas is strongly preferred:
Adding /removing a datacenter
Adding/removing nodes
Replication, database schema, database administration and handling user/application security using Cassandra required managing storage capacity of systems
Experience with AWS and/or Azure is required
Unix/Shell/Java scripting and DB automation experience is required
Experience with Microsoft .NET Framework (C#, .NET Core) is a plus.
Knowledge of both Windows and Linux Operating Systems is a plus
Ability to quickly learn new technologies and business processes/functions is helpful
Strong analytical skills to determine effective approaches to business solutions is important
Understanding of database best practices and the ability to promote those best practices in the organization is very important
Bachelorâs degree in Computer Science or related technical field is required
Experience supporting mission critical, customer facing applications is required
Green Dot Corporation is committed to achieving a diverse workforce and is proud to be an equal opportunity employer without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any category protected by law.


We do not accept unsolicited resumes from employment agencies. No fee of any kind will be paid in the event we hire a candidate whose resume is submitted by an employment agency to this career site or directly to any of our employees. Such resume shall be deemed the sole property of Green Dot Corporation. Employment agencies that have fee agreements with us and have been directed by our designated HR personnel to recruit for a particular position may submit resumes to such designated HR personnel or as otherwise directed by such personnel.","   Design, maintenance, and/or administration with Apache Cassandra, including data modeling and CQL scripting. Work with DataStax Enterprise Graph, including Gremlin query language. Develop data models, and plan and write high performing queries. Help analyze data access patterns and identify hotspots and bottlenecks. Plan, coordinate, and administer database/DSE systems, including base definition, structure, documentation, requirements, operational guidelines and protection. You'll be doing some administrator tasks for Cassandra, including configuration of Cassandra cluster with mutli-data centers. Your tasks will also include performance tuning for memory, troubleshooting, and node rebuild/backup    5+ years of related experience Past DB production support experience is helpful Prior experience in the following areas is strongly preferred  Adding /removing a datacenter Adding/removing nodes Replication, database schema, database administration and handling user/application security using Cassandra required managing storage capacity of systems Experience with AWS and/or Azure is required Unix/Shell/Java scripting and DB automation experience is required Experience with Microsoft .NET Framework  C , .NET Core  is a plus. Knowledge of both Windows and Linux Operating Systems is a plus Ability to quickly learn new technologies and business processes/functions is helpful Strong analytical skills to determine effective approaches to business solutions is important Understanding of database best practices and the ability to promote those best practices in the organization is very important Bachelorâs degree in Computer Science or related technical field is required Experience supporting mission critical, customer facing applications is required ","Design, maintenance, and/or administration with Apache Cassandra, including data modeling and CQL scripting. Work DataStax Enterprise Graph, Gremlin query language. Develop models, plan write high performing queries. Help analyze access patterns identify hotspots bottlenecks. Plan, coordinate, administer database/DSE systems, base definition, structure, documentation, requirements, operational guidelines protection. You'll be doing some administrator tasks for configuration of Cassandra cluster mutli-data centers. Your will also include performance tuning memory, troubleshooting, node rebuild/backup 5+ years related experience Past DB production support is helpful Prior in the following areas strongly preferred Adding /removing a datacenter Adding/removing nodes Replication, database schema, handling user/application security using required managing storage capacity systems Experience AWS Azure Unix/Shell/Java scripting automation Microsoft .NET Framework C , Core plus. Knowledge both Windows Linux Operating Systems plus Ability to quickly learn new technologies business processes/functions Strong analytical skills determine effective approaches solutions important Understanding best practices ability promote those organization very Bachelorâs degree Computer Science or technical field supporting mission critical, customer facing applications","Design, maintenance, and/or administration Apache Cassandra, including data modeling CQL scripting. Work DataStax Enterprise Graph, Gremlin query language. Develop models, plan write high performing queries. Help analyze access patterns identify hotspots bottlenecks. Plan, coordinate, administer database/DSE systems, base definition, structure, documentation, requirements, operational guidelines protection. You'll administrator tasks configuration Cassandra cluster mutli-data centers. Your also include performance tuning memory, troubleshooting, node rebuild/backup 5+ years related experience Past DB production support helpful Prior following areas strongly preferred Adding /removing datacenter Adding/removing nodes Replication, database schema, handling user/application security using required managing storage capacity systems Experience AWS Azure Unix/Shell/Java scripting automation Microsoft .NET Framework C , Core plus. Knowledge Windows Linux Operating Systems plus Ability quickly learn new technologies business processes/functions Strong analytical skills determine effective approaches solutions important Understanding best practices ability promote organization Bachelorâs degree Computer Science technical field supporting mission critical, customer facing applications"
437,Data Engineer,Data Management Engineer - Neighbors,"Santa Monica, CA",Santa Monica,CA,"Ring is looking for a Software Development Engineer who will help us create the next generation of apps, and services. In this role, you will work as a part of the Neighbors team working closely to empower a cross-functional team of engineers, marketers, designers, and product managers to design, test, learn, and iterate using the data that you will make actionable.

Ring launched the Neighbors app as a public safety communication tool that allows users to view and share real-time crime and safety alerts with their neighbors. Anyone with an iOS or Android device can download Neighbors and use the app to: view neighborhood activity; share crime and safety-related videos, photos and text-based posts; and receive real-time safety alerts from your neighbors, local law enforcement and the Ring team. As a Software Development Engineer, you will play a pivotal role in shaping the definition, vision, design, roadmap and development of the Neighbors product building backend services to serve our customers.
Responsibilities
Work with engineering and business stakeholders to understand data requirements
Help shape the growth of in-house data resources to support business Intelligence, analytics, and data science
Define, design, model, implement, and operate large, evolving, structured and unstructured datasets
Take action with quality, performance, scalability, and maintainability in mind
Interact and integrate with internal and external teams and systems to extract, transform, and load data from a wide variety of sources
Implement secure and auditable data infrastructure as required
Help execute the roll out of modern business intelligence and analytics tools and visualizations
Assist with ad hoc data investigations and analysis
Train users on data capabilities and best practices
Basic Qualifications
Bachelorâs degree in Computer Science or related field.
3+ years experience working with AWS and Redshift
3+ years experience in Business Intelligence / Data Engineering / Data Warehousing roles working with multiple disparate, complex, large datasets
3+ years experience in one of Python, Java, Scala, or a similar programming language
5+ years experience working with SQL
Preferred Qualifications
Firm understanding of large scale data sets and SDLC best practices
Experience with Hadoop, Hive, Spark, and SQL on Hadoop related technologies
Experience with Airflow
Experience evaluating and deploying BI and analytics tools

About Ring

Ring's mission is to make neighborhoods safer by creating a Ring of Security around homes and communities with its suite of home security products and services. The Ring product line, along with the Ring Neighbors app, enable Ring to offer affordable, complete, proactive home and neighborhood security in a way no other company has before. In fact, two Newark, NJ neighborhoods saw an over 50 percent decrease in home break-ins after Ring Video Doorbells and Spotlight Cams were installed on 11% of homes in the communities from April-July 2018 when compared to the same time period in 2017. Ring is an Amazon company. For more information, visit www.ring.com. With Ring, youâre always home.

Ring LLC is proud to be an equal opportunity employer and provides equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, gender identity or genetics."," Bachelorâs degree in Computer Science or related field. 3+ years experience working with AWS and Redshift 3+ years experience in Business Intelligence / Data Engineering / Data Warehousing roles working with multiple disparate, complex, large datasets 3+ years experience in one of Python, Java, Scala, or a similar programming language 5+ years experience working with SQL   Work with engineering and business stakeholders to understand data requirements Help shape the growth of in-house data resources to support business Intelligence, analytics, and data science Define, design, model, implement, and operate large, evolving, structured and unstructured datasets Take action with quality, performance, scalability, and maintainability in mind Interact and integrate with internal and external teams and systems to extract, transform, and load data from a wide variety of sources Implement secure and auditable data infrastructure as required Help execute the roll out of modern business intelligence and analytics tools and visualizations Assist with ad hoc data investigations and analysis Train users on data capabilities and best practices  ","Bachelorâs degree in Computer Science or related field. 3+ years experience working with AWS and Redshift Business Intelligence / Data Engineering Warehousing roles multiple disparate, complex, large datasets one of Python, Java, Scala, a similar programming language 5+ SQL Work engineering business stakeholders to understand data requirements Help shape the growth in-house resources support Intelligence, analytics, science Define, design, model, implement, operate large, evolving, structured unstructured Take action quality, performance, scalability, maintainability mind Interact integrate internal external teams systems extract, transform, load from wide variety sources Implement secure auditable infrastructure as required execute roll out modern intelligence analytics tools visualizations Assist ad hoc investigations analysis Train users on capabilities best practices","Bachelorâs degree Computer Science related field. 3+ years experience working AWS Redshift Business Intelligence / Data Engineering Warehousing roles multiple disparate, complex, large datasets one Python, Java, Scala, similar programming language 5+ SQL Work engineering business stakeholders understand data requirements Help shape growth in-house resources support Intelligence, analytics, science Define, design, model, implement, operate large, evolving, structured unstructured Take action quality, performance, scalability, maintainability mind Interact integrate internal external teams systems extract, transform, load wide variety sources Implement secure auditable infrastructure required execute roll modern intelligence analytics tools visualizations Assist ad hoc investigations analysis Train users capabilities best practices"
438,Data Engineer,Senior Data Engineer,"Santa Monica, CA",Santa Monica,CA,"Ring.com is looking for an amazing Senior Data Engineer with the passion to drive the design, execution, and ongoing support of mission-critical data services enabling large-scale data collection, near real-time and offline analytics, distributed search, and machine learning for security, engineering, and business intelligence purposes.
Responsibilities
Work with engineering and business stakeholders to understand data requirements
Help shape the growth of transformative in-house big data resources and capabilities
Drive design, model, implement, and operate large, evolving, structured and unstructured datasets
Evaluate and implement efficient distributed storage and query techniques
Interact and integrate with internal and external teams and systems to extract, transform, and load data from a wide variety of sources
Implement secure and auditable data infrastructure as required
Train users on data capabilities and best practices
Basic Qualifications
Bachelorâs degree in Computer Science or a related field
5+ years of experience in Software Engineering / Data Engineering / Data Warehousing roles working in high traffic, fault tolerant, and highly available environments
5+ years experience with SQL skills
5+ years experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
2+ years of experience with Streaming platforms like Kafka or Kinesis
2+ years of experience with Elasticsearch, Logstash, Splunk or similar technologies
2+ years of experience with Scala, Golang, Python, Java, Ruby or a similar programming language
Preferred Qualifications
Experience implementing solutions to comply with GDPR and/or other Data Privacy regulations
Experience designing and implementing Data Governance programs
Experience with AWS
Experience integrating NoSQL alongside RDBMS such as Postgres or MySQL in a multi-tiered, globally replicated environment
Having planned and/or participated in terabyte-scale data migrations
About Ring

Ring's mission is to make neighborhoods safer by creating a Ring of Security around homes and communities with its suite of home security products and services. The Ring product line, along with the Ring Neighbors app, enable Ring to offer affordable, complete, proactive home and neighborhood security in a way no other company has before. In fact, two Newark, NJ neighborhoods saw an over 50 percent decrease in home break-ins after Ring Video Doorbells and Spotlight Cams were installed on 11% of homes in the communities from April-July 2018 when compared to the same time period in 2017. Ring is an Amazon company. For more information, visit www.ring.com. With Ring, youâre always home.

Ring LLC is proud to be an equal opportunity employer and provides equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, gender identity or genetics."," Bachelorâs degree in Computer Science or a related field 5+ years of experience in Software Engineering / Data Engineering / Data Warehousing roles working in high traffic, fault tolerant, and highly available environments 5+ years experience with SQL skills 5+ years experience with Big Data Technologies  Hadoop, Hive, Hbase, Pig, Spark, etc.  2+ years of experience with Streaming platforms like Kafka or Kinesis 2+ years of experience with Elasticsearch, Logstash, Splunk or similar technologies 2+ years of experience with Scala, Golang, Python, Java, Ruby or a similar programming language   Work with engineering and business stakeholders to understand data requirements Help shape the growth of transformative in-house big data resources and capabilities Drive design, model, implement, and operate large, evolving, structured and unstructured datasets Evaluate and implement efficient distributed storage and query techniques Interact and integrate with internal and external teams and systems to extract, transform, and load data from a wide variety of sources Implement secure and auditable data infrastructure as required Train users on data capabilities and best practices  ","Bachelorâs degree in Computer Science or a related field 5+ years of experience Software Engineering / Data Warehousing roles working high traffic, fault tolerant, and highly available environments with SQL skills Big Technologies Hadoop, Hive, Hbase, Pig, Spark, etc. 2+ Streaming platforms like Kafka Kinesis Elasticsearch, Logstash, Splunk similar technologies Scala, Golang, Python, Java, Ruby programming language Work engineering business stakeholders to understand data requirements Help shape the growth transformative in-house big resources capabilities Drive design, model, implement, operate large, evolving, structured unstructured datasets Evaluate implement efficient distributed storage query techniques Interact integrate internal external teams systems extract, transform, load from wide variety sources Implement secure auditable infrastructure as required Train users on best practices","Bachelorâs degree Computer Science related field 5+ years experience Software Engineering / Data Warehousing roles working high traffic, fault tolerant, highly available environments SQL skills Big Technologies Hadoop, Hive, Hbase, Pig, Spark, etc. 2+ Streaming platforms like Kafka Kinesis Elasticsearch, Logstash, Splunk similar technologies Scala, Golang, Python, Java, Ruby programming language Work engineering business stakeholders understand data requirements Help shape growth transformative in-house big resources capabilities Drive design, model, implement, operate large, evolving, structured unstructured datasets Evaluate implement efficient distributed storage query techniques Interact integrate internal external teams systems extract, transform, load wide variety sources Implement secure auditable infrastructure required Train users best practices"
439,Data Engineer,Senior Data Engineer,"Los Angeles, CA",Los Angeles,CA,"PlayStation isn't just the Best Place to Play âit's also the Best Place to Work. We've thrilled gamers since 1994, when we launched the original PlayStation. Today, we're recognized as a global leader in interactive and digital entertainment. The PlayStation brand falls under Sony Interactive Entertainment, a wholly-owned subsidiary of Sony Corporation.

--------------------------
Senior Data Engineer
--------------------------

Los Angeles, CA

Our group develops software for the PlayStation Network. This network provides online services for the PS4, Web, PS3, PSP, Vita, Tablets, mobile phones, PC, Bravia TVs and more. We build large internal enterprise applications which integrate and support services such as Movie/TV Show streaming, PSN gaming commerce and much more. Our environment is fast, agile and we use some of the latest tools out there.

The Content and Customer Service Data Engineering team is looking for creative, curious, energetic professionals who are passionate about solving complex problems with data in creative and innovative ways. Our mission is to deliver timely, scalable and high quality data solutions and be a force multiplier by unlocking the full potential behind our data.

-----------------

Responsibilities:
-----------------


Collaborate with product and engineering teams in multiple projects building forward thinking, innovative data solutions that up-level our features and get results in a data driven way.
Build highly scalable resilient data pipelines and models which produce high quality datasets.
Deliver visualizations that distill clear, actionable insights from large, complex datasets.
Improve our tooling by building generic data features such as data quality and anomaly detection and drive overall improvements in our data infrastructure.
Drive ideas and projects through deep understanding of our data and how it applies in the broader sense of the organization.

---------------

Qualifications:
---------------


4+ years of industry experience building highly scalable data pipelines (batch and/or streaming) utilizing Spark, Hive, Presto or other open source frameworks.architecture.
Python and shell scripting experience for automation and data manipulation.
Prior experience utilizing dashboarding tools such as Tableau, Superset or similar.
Experience translating ambiguous business needs to highly scalable data models and datasets.
Background in software engineering - able to write elegant, scalable and maintainable code.
Strong SQL skills

--------

Desired:
--------


Experience with Airflow, Superset or similar open source tools.
Previous experience with AWS or similar cloud environment.
Experience preparing large, complex datasets for Machine Learning pipelines a plus.
Prior experience working in an Agile environment.
Gamer or experience in the gaming industry a plus.

Sony is an Equal Opportunity Employer. Aersons will receive consideration for employment without regard to race, color, religion, gender, pregnancy, national origin, ancestry, citizenship, age, legally protected physical or mental disability, covered veteran status, status in the U.S. uniformed services, sexual orientation, gender identity, marital status, genetic information or membership in any other legally protected category.

We strive to create an inclusive environment, empower employees and embrace diversity. We encourage everyone to respond.

We sincerely appreciate the time and effort you spent in contacting us and we thank you for your interest in PlayStation.

#LI-REINE"," 4+ years of industry experience building highly scalable data pipelines  batch and/or streaming  utilizing Spark, Hive, Presto or other open source frameworks.architecture. Python and shell scripting experience for automation and data manipulation. Prior experience utilizing dashboarding tools such as Tableau, Superset or similar. Experience translating ambiguous business needs to highly scalable data models and datasets. Background in software engineering - able to write elegant, scalable and maintainable code. Strong SQL skills    Collaborate with product and engineering teams in multiple projects building forward thinking, innovative data solutions that up-level our features and get results in a data driven way. Build highly scalable resilient data pipelines and models which produce high quality datasets. Deliver visualizations that distill clear, actionable insights from large, complex datasets. Improve our tooling by building generic data features such as data quality and anomaly detection and drive overall improvements in our data infrastructure. Drive ideas and projects through deep understanding of our data and how it applies in the broader sense of the organization.   ","4+ years of industry experience building highly scalable data pipelines batch and/or streaming utilizing Spark, Hive, Presto or other open source frameworks.architecture. Python and shell scripting for automation manipulation. Prior dashboarding tools such as Tableau, Superset similar. Experience translating ambiguous business needs to models datasets. Background in software engineering - able write elegant, maintainable code. Strong SQL skills Collaborate with product teams multiple projects forward thinking, innovative solutions that up-level our features get results a driven way. Build resilient which produce high quality Deliver visualizations distill clear, actionable insights from large, complex Improve tooling by generic anomaly detection drive overall improvements infrastructure. Drive ideas through deep understanding how it applies the broader sense organization.","4+ years industry experience building highly scalable data pipelines batch and/or streaming utilizing Spark, Hive, Presto open source frameworks.architecture. Python shell scripting automation manipulation. Prior dashboarding tools Tableau, Superset similar. Experience translating ambiguous business needs models datasets. Background software engineering - able write elegant, maintainable code. Strong SQL skills Collaborate product teams multiple projects forward thinking, innovative solutions up-level features get results driven way. Build resilient produce high quality Deliver visualizations distill clear, actionable insights large, complex Improve tooling generic anomaly detection drive overall improvements infrastructure. Drive ideas deep understanding applies broader sense organization."
440,Data Engineer,Software Engineer â Big Data,"Glendale, CA",Glendale,CA,"Position Overview


We are looking for a Software Engineer with experience in all things Big Data. In this role, you will work closely with cross departmental resources to meet requirements imposed by both Data Scientists and other Software Engineers collecting, parsing, analyzing and visualizing large sets of data.

Responsibilities and Job Duties
Build production grade live data processing systems.
Implement custom data pipelines streaming large amounts of sensor and location data.
Ensure data is processed correctly and efficiently.
Attend daily stand-ups led by TPM.
Write clean testable code.
Minimum Qualifications
3-5 years of hands-on experience in âbig-dataâ technologies.
BS or MS in Computer Science or a related degree.
Hands on experience and strong proficiency with either Python, Java, or Scala.
Familiarity with software engineering practices for the full software development life cycle, including coding standards, code reviews, build processes, testing, and operations.
Familiarity of the fundamentals of distributed data processing, data modeling and ETL.
Ability to extract data from multiple data sources and load them into a centralized data warehouse to facilitate unified reporting.
Desired Experience:
Hands on experience in a Hadoop environment.
Hands on experience working with Spark or Storm.
Experience in determining testing strategy and execution of test cases.
About Beyond Limits

Beyond Limits is a pioneering Artificial Intelligence company with a technology legacy in Space Exploration. A spinoff of NASA's Jet Propulsion Lab and Caltech, Beyond Limits provides advanced intelligence solutions that go far beyond conventional AI. Our cognitive computing technology mimics human thought processes and provides autonomous reasoning to aid human-like decision-making.

Beyond Limits provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Beyond Limits complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

Beyond Limits expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Beyond Limitâs employees to perform their job duties may result in discipline up to and including discharge."," 3-5 years of hands-on experience in âbig-dataâ technologies. BS or MS in Computer Science or a related degree. Hands on experience and strong proficiency with either Python, Java, or Scala. Familiarity with software engineering practices for the full software development life cycle, including coding standards, code reviews, build processes, testing, and operations. Familiarity of the fundamentals of distributed data processing, data modeling and ETL. Ability to extract data from multiple data sources and load them into a centralized data warehouse to facilitate unified reporting.   Build production grade live data processing systems. Implement custom data pipelines streaming large amounts of sensor and location data. Ensure data is processed correctly and efficiently. Attend daily stand-ups led by TPM. Write clean testable code.  ","3-5 years of hands-on experience in âbig-dataâ technologies. BS or MS Computer Science a related degree. Hands on and strong proficiency with either Python, Java, Scala. Familiarity software engineering practices for the full development life cycle, including coding standards, code reviews, build processes, testing, operations. fundamentals distributed data processing, modeling ETL. Ability to extract from multiple sources load them into centralized warehouse facilitate unified reporting. Build production grade live processing systems. Implement custom pipelines streaming large amounts sensor location data. Ensure is processed correctly efficiently. Attend daily stand-ups led by TPM. Write clean testable code.","3-5 years hands-on experience âbig-dataâ technologies. BS MS Computer Science related degree. Hands strong proficiency either Python, Java, Scala. Familiarity software engineering practices full development life cycle, including coding standards, code reviews, build processes, testing, operations. fundamentals distributed data processing, modeling ETL. Ability extract multiple sources load centralized warehouse facilitate unified reporting. Build production grade live processing systems. Implement custom pipelines streaming large amounts sensor location data. Ensure processed correctly efficiently. Attend daily stand-ups led TPM. Write clean testable code."
441,Data Engineer,Data Services Engineer,"El Segundo, CA",El Segundo,CA,"Company Overview
Company Statement
Since 1964, Teledyne Controls has been working closely with civil and military aircraft operators worldwide. The company counts over 300 airlines among its valued clients, including the worldâs major carriers. We also works directly with the aircraft manufacturers, supplying products to the major OEMs, such as Airbus and Boeing. Fundamental to our success are our core values which includes dedication to every employee and clientâs success; Innovation that matters - for our company and for the world; Trust and personal responsibility in all relationships!

Our adaptable suite of products include Data Acquisition & Management Systems, Wireless Data Transfer Systems, Flight Data Analysis & Investigation Solutions, Data Loading Solutions, Aircraft Network Systems, Aircraft Manufacturer and Supplier. Combined together, these products provide comprehensive data management solutions that leverage aircraft data intelligence and create value for our customers.

Are you looking to be part of a dynamic and growing organization? Teledyne Controls is EveryWhereYouLook! and looking for the best available talent to support our growth.
Position Summary and Responsibilities
The Data Services Engineer will be responsible for development, deployment, and maintenance of Teledyneâs cloud based Flight Data Service platforms. The successful candidate will have previous knowledge of programming Teledyneâs AirFASE Flight Analysis Profiles and ACMS, and the ability to become proficient on the different products used by Teledyne Controls to provide Services.

Essential Duties and Responsibilities include the following. Other duties may be assigned.
Support deployment, integration and maintenance of Controlsâ software products within clientsâ networking environments.
Program simple applications (ACMS, AirFASE) using Teledyne Controls tools.
Support all in-service issues with deployed systems.
Work with tools such as SQL, Tableau, MatLab, etc. to extract meaningful information.
Resolve challenging, potentially high impact customer situations with a high level of tact and understanding.
Understand customer needs and identify possible new services opportunities.
Share best practices with fellow team members to enhance the quality and efficiency of the services deployment and support process.
Create reports using data provided by our Services Customers and analyzed by our software platforms.
Effectively interact with the engineering teams to provide solutions to complex technical issues.
Must be able to travel domestically and internationally to customer locations.
Qualifications
Education and/or Experience
Requires a Bachelor's Degree (BA, BS) in Computer Science, Computer Engineering, Aeronautical or other related Engineering discipline;
Minimum of to five (5) years of experience with custom software applications and data analysis.
Basic understanding of aviation and/or avionics system is a plus.
Thorough knowledge of Microsoft Office application tools: Outlook, Word, PowerPoint, Advanced knowledge of Microsoft Excel (Pivot tables).
Experience working with Tableau or any other Data Analysis/Visualization software is highly desired.
Programming skills are highly desired. Experience working with SQL databases software highly preferred.
Solid understanding of Tableau strongly preferred.
Knowledgeable experience in C++, web applications, HTML JQuery, CSS is a plus.
Requirements
In our efforts to maintain a safe and drug-free workplace, Teledyne Controls requires that candidates complete a satisfactory background check and pass a drug screen prior to employment.
Citizenship Requirements
Due to the type of work at the facility and certain access restrictions, successful applicants must be a ""U.S. Person"" (US citizens, US nationals, lawful permanent residents, asylees or refugees).
Teledyne is an Affirmative Action/Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race,color,religion,religious creed,gender,sexual orientation,gender identity,gender expression,transgender,pregnancy,marital status,national origin,ancestry,citizenship status,age,disability,protected Veteran Status,genetics or any other characteristic protected by applicable federal,state,or local law. ? If you need assistance or an accommodation while seeking employment,please email teledynerecruitment@teledyne.com or call (805)373-4545. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. Please note that only those inquiries concerning a request for reasonable accommodation will receive a response.","Requires a Bachelor's Degree  BA, BS  in Computer Science, Computer Engineering, Aeronautical or other related Engineering discipline; Minimum of to five  5  years of experience with custom software applications and data analysis. Basic understanding of aviation and/or avionics system is a plus. Thorough knowledge of Microsoft Office application tools  Outlook, Word, PowerPoint, Advanced knowledge of Microsoft Excel  Pivot tables . Experience working with Tableau or any other Data Analysis/Visualization software is highly desired. Programming skills are highly desired. Experience working with SQL databases software highly preferred. Solid understanding of Tableau strongly preferred. Knowledgeable experience in C++, web applications, HTML JQuery, CSS is a plus.    Support deployment, integration and maintenance of Controlsâ software products within clientsâ networking environments. Program simple applications  ACMS, AirFASE  using Teledyne Controls tools. Support all in-service issues with deployed systems. Work with tools such as SQL, Tableau, MatLab, etc. to extract meaningful information. Resolve challenging, potentially high impact customer situations with a high level of tact and understanding. Understand customer needs and identify possible new services opportunities. Share best practices with fellow team members to enhance the quality and efficiency of the services deployment and support process. Create reports using data provided by our Services Customers and analyzed by our software platforms. Effectively interact with the engineering teams to provide solutions to complex technical issues. Must be able to travel domestically and internationally to customer locations. Requires a Bachelor's Degree  BA, BS  in Computer Science, Computer Engineering, Aeronautical or other related Engineering discipline; Minimum of to five  5  years of experience with custom software applications and data analysis. Basic understanding of aviation and/or avionics system is a plus. Thorough knowledge of Microsoft Office application tools  Outlook, Word, PowerPoint, Advanced knowledge of Microsoft Excel  Pivot tables . Experience working with Tableau or any other Data Analysis/Visualization software is highly desired. Programming skills are highly desired. Experience working with SQL databases software highly preferred. Solid understanding of Tableau strongly preferred. Knowledgeable experience in C++, web applications, HTML JQuery, CSS is a plus.  ","Requires a Bachelor's Degree BA, BS in Computer Science, Engineering, Aeronautical or other related Engineering discipline; Minimum of to five 5 years experience with custom software applications and data analysis. Basic understanding aviation and/or avionics system is plus. Thorough knowledge Microsoft Office application tools Outlook, Word, PowerPoint, Advanced Excel Pivot tables . Experience working Tableau any Data Analysis/Visualization highly desired. Programming skills are SQL databases preferred. Solid strongly Knowledgeable C++, web applications, HTML JQuery, CSS Support deployment, integration maintenance Controlsâ products within clientsâ networking environments. Program simple ACMS, AirFASE using Teledyne Controls tools. all in-service issues deployed systems. Work such as SQL, Tableau, MatLab, etc. extract meaningful information. Resolve challenging, potentially high impact customer situations level tact understanding. Understand needs identify possible new services opportunities. Share best practices fellow team members enhance the quality efficiency deployment support process. Create reports provided by our Services Customers analyzed platforms. Effectively interact engineering teams provide solutions complex technical issues. Must be able travel domestically internationally locations.","Requires Bachelor's Degree BA, BS Computer Science, Engineering, Aeronautical related Engineering discipline; Minimum five 5 years experience custom software applications data analysis. Basic understanding aviation and/or avionics system plus. Thorough knowledge Microsoft Office application tools Outlook, Word, PowerPoint, Advanced Excel Pivot tables . Experience working Tableau Data Analysis/Visualization highly desired. Programming skills SQL databases preferred. Solid strongly Knowledgeable C++, web applications, HTML JQuery, CSS Support deployment, integration maintenance Controlsâ products within clientsâ networking environments. Program simple ACMS, AirFASE using Teledyne Controls tools. in-service issues deployed systems. Work SQL, Tableau, MatLab, etc. extract meaningful information. Resolve challenging, potentially high impact customer situations level tact understanding. Understand needs identify possible new services opportunities. Share best practices fellow team members enhance quality efficiency deployment support process. Create reports provided Services Customers analyzed platforms. Effectively interact engineering teams provide solutions complex technical issues. Must able travel domestically internationally locations."
442,Data Engineer,Data & Control Systems Engineer,"Los Angeles, CA",Los Angeles,CA,"Team and Role Overview

The structures test team is responsible for completing all development, acceptance, and qualification testing of the primary and secondary structure of the vehicle. This position will integrate with both the structures and avionics team to implement all of the control systems (hardware and software) for the structural test stands.


The Mission/Outcomes and Objectives

Our mission is to change peopleâs conception of the way rockets are built and flown. As the Data & Control Systems Engineer, you will be involved in every test we run. You will help to write our control software, select instrumentation, design wiring schematics, and troubleshoot field installations. As a part of the integrated software team, you will ensure that the structures test needs are represented and fulfilled.

Upon joining the team, your first priorities will be:

Design actuator control circuitry
Build out data acquisition capability
Drive requirements for internal software tools


Candidate Profile

In order to be successful in this role, you should be equally at home working from your desk or in the test yard. Some days you will be working on the test stands to diagnose and solve electrical problems, while other days you will be designing new systems for upcoming test campaigns. You should be nimble and scrappy, but able with a constant eye to safety and quality.

The test team frequently shifts priorities and redistributes work amongst our teammates. You should be flexible and able to work in a very dynamic environment. You should be able to work independently, but willing to jump in wherever help is needed. The ideal candidate will specialize in electrons, but have a basic understanding of mechanical systems. The most successful candidates have a hunger to continually learn new skills and are not afraid to give and receive constructive feedback.


Minimum Required Skills and Competencies

Undergraduate degree in STEM major (science, technology, engineering, and math) and at least 3 years of engineering experience
Willing to travel as required (up to 20%)
Demonstrated track record of completing hardware/software projects within time and budget constraints
Experience designing, fabricating, and testing complex data & control systems
Ability to develop and review complex electrical schematics.
Knowledge of noise reduction, grounding, filtering, and other techniques necessary to ensure quality data.
Demonstrated competency in at least 2 programming languages
Able to work in a fast-paced and intense startup environment.


Preferred Skills and Competencies

Experience in aerospace launch or test environment
Comfortable working at heights or enclosed spaces
Significant ""hands on"" experience
Familiarity with EtherCAT architecture
Demonstrated ability to tune and troubleshoot closed loop control systems
Knowledge of common fluid test instrumentation and components, such as pressure transducers, thermocouples, flow meters, accelerometers, strain gauges, load cells, and solenoid/pneumatic/other valves.


This position must meet Export Control compliance requirements, therefore a United States Person as defined by 22 C.F.R. Â§ 120.15 is required.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","  Undergraduate degree in STEM major  science, technology, engineering, and math  and at least 3 years of engineering experience Willing to travel as required  up to 20%  Demonstrated track record of completing hardware/software projects within time and budget constraints Experience designing, fabricating, and testing complex data & control systems Ability to develop and review complex electrical schematics. Knowledge of noise reduction, grounding, filtering, and other techniques necessary to ensure quality data. Demonstrated competency in at least 2 programming languages Able to work in a fast-paced and intense startup environment.    ","Undergraduate degree in STEM major science, technology, engineering, and math at least 3 years of engineering experience Willing to travel as required up 20% Demonstrated track record completing hardware/software projects within time budget constraints Experience designing, fabricating, testing complex data & control systems Ability develop review electrical schematics. Knowledge noise reduction, grounding, filtering, other techniques necessary ensure quality data. competency 2 programming languages Able work a fast-paced intense startup environment.","Undergraduate degree STEM major science, technology, engineering, math least 3 years engineering experience Willing travel required 20% Demonstrated track record completing hardware/software projects within time budget constraints Experience designing, fabricating, testing complex data & control systems Ability develop review electrical schematics. Knowledge noise reduction, grounding, filtering, techniques necessary ensure quality data. competency 2 programming languages Able work fast-paced intense startup environment."
443,Data Engineer,AWS Data Engineer,"El Segundo, CA 90245",El Segundo,CA,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet todayâs high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todayâs corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Â§ Certified AWS Developer - Associate
Â§ Certified AWS DevOps â Professional (Nice to have)
Â§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of Consulting or client service delivery experience on Amazon AWS  AWS At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C  , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services  VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c , node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline. DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C  , Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud    Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication  written and oral  and interpersonal skills Excellent leadership and management skills","At least 5 years of Consulting or client service delivery experience on Amazon AWS in developing data ingestion, processing and analytical pipelines for big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within the Native HadoopExperience with private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies such as Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science a related discipline. an platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability to build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management","At least 5 years Consulting client service delivery experience Amazon AWS developing data ingestion, processing analytical pipelines big data, relational databases, NoSQL warehouse solutionsExtensive providing practical direction within Native HadoopExperience private public cloud architectures, pros/cons, migration considerations.Minimum hands-on Big Data technologies Java, Node.js, C , Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie streaming Kafka, Kinesis, NiFI etc.Extensive implementing using services VPC/SG, AutoScaling, CloudFormation, LakeFormation, DMS, Nifi, CDC Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ hands programming languages c node.js, python, pyspark, spark, Unix shell/Perl scripting etc.Minimum RDBMS experienceExperience Hadoop File Formats compression techniquesExperience working DevOps tools GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors higher degree Computer Science related discipline. platform. Multi-cloud plus.Experience deploying ETL solutions like Talend, Informatica, MatillionStrong Spark, PySpark, scriptingIoT, event-driven, microservices, containers/Kubernetes Proven ability build, manage foster team-oriented environment work creatively analytically problem-solving Desire information systems Excellent communication written oral interpersonal skills leadership management"
444,Data Engineer,Azure Data Engineer,"Los Angeles, CA",Los Angeles,CA,"Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an âAppreciating Business Asset.â

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and thatâs why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on todayâs biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, âas isâ and âto beâ scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.","At least 5 years of consulting or client service delivery experience on Azure  DevOps on an Azure platform    Proven ability to build, manage and foster a team-oriented environment ","At least 5 years of consulting or client service delivery experience on Azure DevOps an platform Proven ability to build, manage and foster a team-oriented environment","At least 5 years consulting client service delivery experience Azure DevOps platform Proven ability build, manage foster team-oriented environment"
445,Data Engineer,DATA INFRASTRUCTURE ENGINEER,"Los Angeles, CA",Los Angeles,CA,"Location: Los Angeles, CA
Job Type: Full-Time, Salaried with Full Benefits
Role Description:

We are currently looking to hire a Data Infrastructure Engineer. You will work on building out a data infrastructure on AWS for storage and analysis of large and complex datasets which includes genomics data. You and the team will be responsible for the design, development, and maintenance of the companies new data platform and will focus on infrastructure, design, architecting, building, iterating and optimizing the data environment. You will work closely with bioinformaticians to implement this infrastructure. Genomics knowledge is not necessary, but nice to have. Excellent team and compensation package.

How you will have an impact:

Your work will contribute meaningfully to core systems, from design -> implementation and you will help solve complex technical problems across all services and at all levels.

Responsibilities:

Develop pipelines for continuous integration/deployment of data infrastructure and full-stack applications on AWS.
Building pipelines/systems with scalability, performance, and security in mind.
Minimum qualifications:

Experience with HPC environments via cfncluster or parallel cluster
Experience with AWS cloud services: Lambda, S3, Batch, Step functions
Experience with various database systems (SQL, noSQL, graph)
Excellent teamwork, time management, and organizational skills.
Preferred Qualifications:

5+ years of experience leading a companies data infrastructure
Desire to work in a startup environment
Demonstrated ability to design and implement algorithms and methods, especially in collaborative software and data analysis development environments (i.e. source control, code and analysis reviews, API use and definition).
Seniority Level
Senior
Industry
Biotechnology
Genetics
Information Services
Employment Type Full-time

Job Functions
Engineering
Benefits:

Medical, Vision, and Dental Plan available
Salary based on experience and options
How to apply:
Send your resume (or CV) and cover letter to purrfectjob@basepaws.com"," 5+ years of experience leading a companies data infrastructure Desire to work in a startup environment Demonstrated ability to design and implement algorithms and methods, especially in collaborative software and data analysis development environments  i.e. source control, code and analysis reviews, API use and definition .    Develop pipelines for continuous integration/deployment of data infrastructure and full-stack applications on AWS. Building pipelines/systems with scalability, performance, and security in mind.   ","5+ years of experience leading a companies data infrastructure Desire to work in startup environment Demonstrated ability design and implement algorithms methods, especially collaborative software analysis development environments i.e. source control, code reviews, API use definition . Develop pipelines for continuous integration/deployment full-stack applications on AWS. Building pipelines/systems with scalability, performance, security mind.","5+ years experience leading companies data infrastructure Desire work startup environment Demonstrated ability design implement algorithms methods, especially collaborative software analysis development environments i.e. source control, code reviews, API use definition . Develop pipelines continuous integration/deployment full-stack applications AWS. Building pipelines/systems scalability, performance, security mind."
446,Data Engineer,Data Platform Engineer,"Los Angeles, CA",Los Angeles,CA,"Summary
Our mission at Supplyframe is to deliver the best information, tools, and technology to electronic industry professionals. Through our network of products, we help engineer research, design, and develop their products, while sharing their creations and collaborating with other like-minded engineers around the world. We are a Search Engine and a digital ad network and our mission is to make hardware engineerâs life easier and better.
The Data Platform Engineer position focuses on expanding and enhancing our big data platform to delivering the right data to both our internal and external customers. As we expand our capabilities in the areas of data mining, machine learning, and big data analysis, this position will be key to help deliver value to our end users. The data platform engineer will have the opportunity to create new data products from our data lakes while working to enhance the clusterâs stability and flexibility. This position offers the opportunity to tackle complex problems while delivering real-world solutions.
Responsibilities
Develop and automate data pipelines using Hadoop/Spark to model large data sets
Performing algorithm development and implementation in production systems
Developing software in Java, Python, Scala or scripting programming language
Improve existing data frameworks within the data lake to handle anticipated growth and new objectives
Collaborate with business teams to create monetizable product
Expose and deliver aggregated/customer-centric data through reports, visualization products, and RESTful APIs
Manage space allocations / data partitioning across our data lakes
Increase the capabilities of our reporting/analytics platforms to support business insight for internal and external users
Maintain data integrity by enhancing our ability to remove content generated by undesirable actors such as bots, scrapers, and pen testers
Requirements
3+ years of Java experience working with unstructured data and perform raw text processing
2+ years of experience in data analysis and the ability to translate raw, technical data into actionable insight
1+ year of hands on experience using Hadoop/Spark
Comfortable with Unix shell or other scripting languages
Demonstrate clear understanding of web analytics and tracking
Comfortable working with open-source tools and have the self learning ability to get tools to work with little instructions
Understanding of software development best practices and revision control (git)
Benefits
Competitive salary and bonus program in an entrepreneurial environment
Top notch health, dental, and vision insurance
Stock options in a fast growing tech company
401k plan with matching contribution
Generous paid time off plan plus paid holidays
Frequent company sponsored lunches, happy hours, fun events, and plenty of snacks and drinks
Supplyframe is an equal opportunity employer
Interested?
Send an email to h4x@supplyframe.com with a cover letter, your rÃ©sumÃ©, and include the job title in the subject line.","  Develop and automate data pipelines using Hadoop/Spark to model large data sets Performing algorithm development and implementation in production systems Developing software in Java, Python, Scala or scripting programming language Improve existing data frameworks within the data lake to handle anticipated growth and new objectives Collaborate with business teams to create monetizable product Expose and deliver aggregated/customer-centric data through reports, visualization products, and RESTful APIs Manage space allocations / data partitioning across our data lakes Increase the capabilities of our reporting/analytics platforms to support business insight for internal and external users Maintain data integrity by enhancing our ability to remove content generated by undesirable actors such as bots, scrapers, and pen testers   3+ years of Java experience working with unstructured data and perform raw text processing 2+ years of experience in data analysis and the ability to translate raw, technical data into actionable insight 1+ year of hands on experience using Hadoop/Spark Comfortable with Unix shell or other scripting languages Demonstrate clear understanding of web analytics and tracking Comfortable working with open-source tools and have the self learning ability to get tools to work with little instructions Understanding of software development best practices and revision control  git  ","Develop and automate data pipelines using Hadoop/Spark to model large sets Performing algorithm development implementation in production systems Developing software Java, Python, Scala or scripting programming language Improve existing frameworks within the lake handle anticipated growth new objectives Collaborate with business teams create monetizable product Expose deliver aggregated/customer-centric through reports, visualization products, RESTful APIs Manage space allocations / partitioning across our lakes Increase capabilities of reporting/analytics platforms support insight for internal external users Maintain integrity by enhancing ability remove content generated undesirable actors such as bots, scrapers, pen testers 3+ years Java experience working unstructured perform raw text processing 2+ analysis translate raw, technical into actionable 1+ year hands on Comfortable Unix shell other languages Demonstrate clear understanding web analytics tracking open-source tools have self learning get work little instructions Understanding best practices revision control git","Develop automate data pipelines using Hadoop/Spark model large sets Performing algorithm development implementation production systems Developing software Java, Python, Scala scripting programming language Improve existing frameworks within lake handle anticipated growth new objectives Collaborate business teams create monetizable product Expose deliver aggregated/customer-centric reports, visualization products, RESTful APIs Manage space allocations / partitioning across lakes Increase capabilities reporting/analytics platforms support insight internal external users Maintain integrity enhancing ability remove content generated undesirable actors bots, scrapers, pen testers 3+ years Java experience working unstructured perform raw text processing 2+ analysis translate raw, technical actionable 1+ year hands Comfortable Unix shell languages Demonstrate clear understanding web analytics tracking open-source tools self learning get work little instructions Understanding best practices revision control git"
447,Data Engineer,Data Systems I&T Engineer,"Pasadena, CA 91107",Pasadena,CA,"Raytheon Pasadena is looking for a software systems engineer to work on exciting Ground Data Systems for multiple missions.

This position is ideal for an energetic and savvy multi-disciplined engineer who loves technology and problem solving, but also has teamwork, communication, people skills as part of their DNA. This is a position for someone who is customer-focused, reacts well to changes, can work independently or with teams and is able to multi-task on multiple products and projects.

Responsibilities include, but are not limited to:

Working as a member of a larger team, functioning as a Ground Data Systems I&T Engineer supporting mission infrastructure and architecture (including mission system software, hardware, networks, and facilities) to meets functional, performance, security, and interface requirements and integrates with external systems hardware, software, and users.
Under the guidance of the Mission Lead, will support all phases of the mission including design, development, test, and operations.
Implement, test, and support deploy the GDS designs and visions of GDS system-engineers and architects.
Participate in verification and validation, and to ensure that GDS subsystems integrate with each other through their interfaces, the end-to-end GDS provides all the functional capabilities to support all flight-project use-case scenarios, and the GDS operates reliably while meeting all performance and usability expectations.
Support user community and production environments, which can both present a myriad of technical challenges thus requiring our group members to have excellent problem-solving, root cause analysis, communication, interpersonal, and broad technical skills.

Education / Work Experience:
Bachelor's degree in Computer Science with a minimum of 6 years of software engineering experience

Required skills:
Extensive knowledge and work experience in system engineering lifecycle activities and product including Configuration Management, Continuous Integration, Software and/or System Testing, Software Test Automation, Software Deployment.
In-depth knowledge of software architecture practices and extensive experience in the integration and deployment of large scale and highly complex software systems.
Excellent coding and scripting skills in languages such as Java, shell, Python, Perl, or Ruby
Good knowledge of test automation practices, familiarity with test automation tools, practical experience developing test automation solutions, and software deployment methods
Experience authoring automated tests using frameworks and tools, such as RobotFramework, TestNG, Selenium, Watir, TestComplete
Working knowledge of software configuration management and issue tracking tools (such as GIT, SVN, AccuRev, CVS, JIRA) and/or test management tool (such as TestRail, qTest) with understanding of best practices
Strong analytical and troubleshooting skills with extensive problem-solving experience in a distributed Unix/Linux environment where extensive knowledge of Unix/Linux environment is required
Experience running test environments in AWS
Strong written and verbal communication skills with experience in writing technical documents and presenting at product reviews
Exceptional interpersonal skills and demonstrated solid technical leadership and teaming skills to effectively interface with key stakeholders and customers including scientists, project management, OPS Teams, Developers, and industry partners, including advising senior management.

Desired skills:
Advanced knowledge of applicable JPL policies and procedures, NASA policies and procedures, and government regulations.
Prior experience leading a test or production operations team
Prior experience developing and executing a strategy and roadmap in area such as test engineering or production operations support
Knowledgeable of test management practices, including identification and interpretation of test metrics
Demonstrated ability to specify and deploy the infrastructure, processes, and artifacts necessary to conduct and manage a strong, robust, and efficient test program with full traceability
Experience in using one or more continuous integration tools such as Bamboo, Hudson, Jenkins, GitLab
Experience with dockers container platform and puppet deployment
Experience with cybersecurity testing

Work Location:
The work will be performed in Pasadena CA

This position requires a U.S. Person who is eligible to obtain any required Export Authorization
145570
Raytheon is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, age, color, religion, creed, sex, sexual orientation, gender identity, national origin, disability, or protected Veteran status.","   Extensive knowledge and work experience in system engineering lifecycle activities and product including Configuration Management, Continuous Integration, Software and/or System Testing, Software Test Automation, Software Deployment. In-depth knowledge of software architecture practices and extensive experience in the integration and deployment of large scale and highly complex software systems. Excellent coding and scripting skills in languages such as Java, shell, Python, Perl, or Ruby Good knowledge of test automation practices, familiarity with test automation tools, practical experience developing test automation solutions, and software deployment methods Experience authoring automated tests using frameworks and tools, such as RobotFramework, TestNG, Selenium, Watir, TestComplete Working knowledge of software configuration management and issue tracking tools  such as GIT, SVN, AccuRev, CVS, JIRA  and/or test management tool  such as TestRail, qTest  with understanding of best practices Strong analytical and troubleshooting skills with extensive problem-solving experience in a distributed Unix/Linux environment where extensive knowledge of Unix/Linux environment is required Experience running test environments in AWS Strong written and verbal communication skills with experience in writing technical documents and presenting at product reviews Exceptional interpersonal skills and demonstrated solid technical leadership and teaming skills to effectively interface with key stakeholders and customers including scientists, project management, OPS Teams, Developers, and industry partners, including advising senior management.  ","Extensive knowledge and work experience in system engineering lifecycle activities product including Configuration Management, Continuous Integration, Software and/or System Testing, Test Automation, Deployment. In-depth of software architecture practices extensive the integration deployment large scale highly complex systems. Excellent coding scripting skills languages such as Java, shell, Python, Perl, or Ruby Good test automation practices, familiarity with tools, practical developing solutions, methods Experience authoring automated tests using frameworks RobotFramework, TestNG, Selenium, Watir, TestComplete Working configuration management issue tracking tools GIT, SVN, AccuRev, CVS, JIRA tool TestRail, qTest understanding best Strong analytical troubleshooting problem-solving a distributed Unix/Linux environment where is required running environments AWS written verbal communication writing technical documents presenting at reviews Exceptional interpersonal demonstrated solid leadership teaming to effectively interface key stakeholders customers scientists, project management, OPS Teams, Developers, industry partners, advising senior management.","Extensive knowledge work experience system engineering lifecycle activities product including Configuration Management, Continuous Integration, Software and/or System Testing, Test Automation, Deployment. In-depth software architecture practices extensive integration deployment large scale highly complex systems. Excellent coding scripting skills languages Java, shell, Python, Perl, Ruby Good test automation practices, familiarity tools, practical developing solutions, methods Experience authoring automated tests using frameworks RobotFramework, TestNG, Selenium, Watir, TestComplete Working configuration management issue tracking tools GIT, SVN, AccuRev, CVS, JIRA tool TestRail, qTest understanding best Strong analytical troubleshooting problem-solving distributed Unix/Linux environment required running environments AWS written verbal communication writing technical documents presenting reviews Exceptional interpersonal demonstrated solid leadership teaming effectively interface key stakeholders customers scientists, project management, OPS Teams, Developers, industry partners, advising senior management."
448,Data Engineer,Data Center Critical Infrastructure Engineer,"Los Angeles, CA 90017",Los Angeles,CA,"General Overview:
Design and manage all critical infrastructure components including power, cooling, security and fire systems by performing the following duties:

Essential Duties/Responsibilities:
Ã¢â¬Â¢ Capacity planning/budgeting
Ã¢â¬Â¢ Infrastructure upgrade planning/budgeting
Ã¢â¬Â¢ Administration of environmental monitoring, control systems and networks
Ã¢â¬Â¢ Vendor relationship management
Ã¢â¬Â¢ Infrastructure-related cost management
Ã¢â¬Â¢ Energy efficiency/conservation research
Ã¢â¬Â¢ Research, evaluate and implement new data center technologies
Ã¢â¬Â¢ Advanced co-location customer planning, implementation and support
Ã¢â¬Â¢ Design, review and maintain security policies as well as access control and surveillance systems
Ã¢â¬Â¢ Implement policies and procedures adhering to data center best practices
Ã¢â¬Â¢ Management of infrastructure-related change control
Ã¢â¬Â¢ Response to service-impacting events/emergencies relating to critical infrastructure
Ã¢â¬Â¢ 24 x 7 x 365 on-call availability for Tier 2 and 3 environmental escalations
Ã¢â¬Â¢ Generation of evaluations, proposals, reporting surrounding infrastructure performance for executive management
Ã¢â¬Â¢ Ensure team alignment with their local Data Center Manager and Operations staff at all times
Ã¢â¬Â¢ Ability to travel

Other Duties/Responsibilities:
Ã¢â¬Â¢ Perform routine walkthroughs of facility to ensure everything is operating properly
Ã¢â¬Â¢ Ensure that all security related forms/documentation are accurately completed
Ã¢â¬Â¢ Train all new employees regarding critical infrastructure and operations policies and procedures
Ã¢â¬Â¢ Support the installation, maintenance, operation and adjustments made to building controls systems, HVAC equipment, i.e. pumps, boilers, air handling units, air compressors, cooling towers, process gas systems, chillers, and chemical delivery systems. Expert/thorough knowledge of power systems, generators and UPSÃ¢â¬â¢s.
Develop and Maintain preventative maintenance schedule for all site equipment ensuring accurate technical/maintenance instructions and content. Understand and document all maintenance contracts including scope of work and length of contracts. Ensure no lapses in contracts and all PM occurs as scheduled.
Develop CEWA, schedule events, publish calendar, and manage all critical equipment maintenance events in strict accordance with CEWA program.
On call 24 hours a day 7 days a week.

Knowledge, Skills and Abilities Required:
Ã¢â¬Â¢ Excellent verbal and written communication skills
Ã¢â¬Â¢ Detail oriented
Ã¢â¬Â¢ Proficient in MS Office applications
Ã¢â¬Â¢ Proficient in AutoCAD
Ã¢â¬Â¢ Proficient in building automation systems (BAS/BMS)
Ã¢â¬Â¢ Flexible and adapts well to a constantly changing environment

Education and/or Experience:
Ã¢â¬Â¢ 5+ years working in a data center environment
Ã¢â¬Â¢ 2+ years managing projects with significant capital expenditures
Ã¢â¬Â¢ BachelorÃ¢â¬â¢s Degree in engineering field preferred

Working Conditions
Most work is done in an area where normal office noise is present.

- Disclaimer-
The above statements are neither intended to be an all-inclusive list of the duties and responsibilities of the job described, nor are they intended to be a listing of all of the skills and abilities required to do the job. Rather, they are intended only to describe the general nature of the job. This job description is not a contract of employment, either express or implied. Employment with INAP will be voluntarily entered into and your employment is considered at will. INAP reserves the right to alter the job description at any time without notice.","  Develop and Maintain preventative maintenance schedule for all site equipment ensuring accurate technical/maintenance instructions and content. Understand and document all maintenance contracts including scope of work and length of contracts. Ensure no lapses in contracts and all PM occurs as scheduled. Develop CEWA, schedule events, publish calendar, and manage all critical equipment maintenance events in strict accordance with CEWA program. On call 24 hours a day 7 days a week.  Knowledge, Skills and Abilities Required  Ã¢â¬Â¢ Excellent verbal and written communication skills Ã¢â¬Â¢ Detail oriented Ã¢â¬Â¢ Proficient in MS Office applications Ã¢â¬Â¢ Proficient in AutoCAD Ã¢â¬Â¢ Proficient in building automation systems  BAS/BMS  Ã¢â¬Â¢ Flexible and adapts well to a constantly changing environment  Education and/or Experience  Ã¢â¬Â¢ 5+ years working in a data center environment Ã¢â¬Â¢ 2+ years managing projects with significant capital expenditures Ã¢â¬Â¢ BachelorÃ¢â¬â¢s Degree in engineering field preferred  Working Conditions Most work is done in an area where normal office noise is present.  - Disclaimer- The above statements are neither intended to be an all-inclusive list of the duties and responsibilities of the job described, nor are they intended to be a listing of all of the skills and abilities required to do the job. Rather, they are intended only to describe the general nature of the job. This job description is not a contract of employment, either express or implied. Employment with INAP will be voluntarily entered into and your employment is considered at will. INAP reserves the right to alter the job description at any time without notice.  ","Develop and Maintain preventative maintenance schedule for all site equipment ensuring accurate technical/maintenance instructions content. Understand document contracts including scope of work length contracts. Ensure no lapses in PM occurs as scheduled. CEWA, events, publish calendar, manage critical events strict accordance with CEWA program. On call 24 hours a day 7 days week. Knowledge, Skills Abilities Required Ã¢â¬Â¢ Excellent verbal written communication skills Detail oriented Proficient MS Office applications AutoCAD building automation systems BAS/BMS Flexible adapts well to constantly changing environment Education and/or Experience 5+ years working data center 2+ managing projects significant capital expenditures BachelorÃ¢â¬â¢s Degree engineering field preferred Working Conditions Most is done an area where normal office noise present. - Disclaimer- The above statements are neither intended be all-inclusive list the duties responsibilities job described, nor they listing abilities required do job. Rather, only describe general nature This description not contract employment, either express or implied. Employment INAP will voluntarily entered into your employment considered at will. reserves right alter any time without notice.","Develop Maintain preventative maintenance schedule site equipment ensuring accurate technical/maintenance instructions content. Understand document contracts including scope work length contracts. Ensure lapses PM occurs scheduled. CEWA, events, publish calendar, manage critical events strict accordance CEWA program. On call 24 hours day 7 days week. Knowledge, Skills Abilities Required Ã¢â¬Â¢ Excellent verbal written communication skills Detail oriented Proficient MS Office applications AutoCAD building automation systems BAS/BMS Flexible adapts well constantly changing environment Education and/or Experience 5+ years working data center 2+ managing projects significant capital expenditures BachelorÃ¢â¬â¢s Degree engineering field preferred Working Conditions Most done area normal office noise present. - Disclaimer- The statements neither intended all-inclusive list duties responsibilities job described, listing abilities required job. Rather, describe general nature This description contract employment, either express implied. Employment INAP voluntarily entered employment considered will. reserves right alter time without notice."
449,Data Engineer,Data Engineer,"Los Angeles, CA 90049",Los Angeles,CA,"Job Summary
The J. Paul Getty Trust is looking for an enthusiastic Data Engineer, with the experience and passion to carry out the execution of technical projects to support, enrich and ensure the persistence of the institution's cultural heritage knowledge bases. Our aim is to provide a deeply connected and consistent experience for scholars, researchers, and enthusiasts as they explore the complex information held across the organization, and your participation is crucial for that to be successful.

You will report to the Enterprise Semantic Architect, and interact with software engineers, data engineers and content specialists in the cultural heritage programs. Your work will improve the quality, reliability, connectedness, and consistency of our data by engineering project-specific data pipelines and validation tools, configuring Linked Open Usable Data (LOUD) platforms such as Arches and assisting with the implementation of our, and the community's, overall data model. You will have a hands-on role with content specialists in the programs, and be responsible for working with them to understand data requirements and then implement those requirements in software.

The Getty is among the most prestigious cultural heritage organizations in the world, dedicated to furthering the study of the history of art. You will work on an amazing campus amongst fabulous art, architecture, and information systems, collaborating with world-class scientists, curators, librarians, archivists, and academics. We offer paid vacation, personal and sick leave plus every other Friday off, excellent benefits, and a very strong commitment to balancing work and personal life.
Major Job Responsibilities
With the Semantic Architect, work with technical and content stakeholders to understand data-oriented project requirements
With the Semantic Architect, design and document the institution's data model and resulting APIs
With other Data Engineers, ensure the accuracy of data transformation pipelines to migrate legacy datasets into Linked Open Usable Data (LOUD) within our ecosystem
With other Data Engineers, design, implement and ensure the accuracy of validation and related services for data models
Integrate external content services to enrich and reconcile our data
Work in an agile way, including supporting testing, continuous integration and deployment
Configure institutional LOUD data management instances built on the Arches Platform
Assist software engineering teams by translating stakeholder requirements into feature requests
Qualifications
Bachelor's degree in a related field or a combination of education and relevant experience
2-5 years software development experience
Knowledge, Skills and Abilities
Experience of data-oriented work within cultural heritage organizations, including transformation of JSON, CSV and/or XML formats
Attention to detail combined with a focus on usability
Excellent verbal and written communication skills, especially when interacting with non-technical stakeholders
Proficiency in Python, or willingness to translate experience in equivalent language
Proficiency in relational and document oriented databases
Familiarity with Linked Open Data standards and technologies
Familiarity with cultural heritage data standards
Familiarity with engineering tools such as git and docker
Familiarity with test driven and agile software development methodologies
Familiarity with machine learning techniques","Bachelor's degree in a related field or a combination of education and relevant experience 2-5 years software development experience Experience of data-oriented work within cultural heritage organizations, including transformation of JSON, CSV and/or XML formats Attention to detail combined with a focus on usability Excellent verbal and written communication skills, especially when interacting with non-technical stakeholders Proficiency in Python, or willingness to translate experience in equivalent language Proficiency in relational and document oriented databases Familiarity with Linked Open Data standards and technologies Familiarity with cultural heritage data standards Familiarity with engineering tools such as git and docker Familiarity with test driven and agile software development methodologies Familiarity with machine learning techniques With the Semantic Architect, work with technical and content stakeholders to understand data-oriented project requirements With the Semantic Architect, design and document the institution's data model and resulting APIs With other Data Engineers, ensure the accuracy of data transformation pipelines to migrate legacy datasets into Linked Open Usable Data  LOUD  within our ecosystem With other Data Engineers, design, implement and ensure the accuracy of validation and related services for data models Integrate external content services to enrich and reconcile our data Work in an agile way, including supporting testing, continuous integration and deployment Configure institutional LOUD data management instances built on the Arches Platform Assist software engineering teams by translating stakeholder requirements into feature requests  ","Bachelor's degree in a related field or combination of education and relevant experience 2-5 years software development Experience data-oriented work within cultural heritage organizations, including transformation JSON, CSV and/or XML formats Attention to detail combined with focus on usability Excellent verbal written communication skills, especially when interacting non-technical stakeholders Proficiency Python, willingness translate equivalent language relational document oriented databases Familiarity Linked Open Data standards technologies data engineering tools such as git docker test driven agile methodologies machine learning techniques With the Semantic Architect, technical content understand project requirements design institution's model resulting APIs other Engineers, ensure accuracy pipelines migrate legacy datasets into Usable LOUD our ecosystem design, implement validation services for models Integrate external enrich reconcile Work an way, supporting testing, continuous integration deployment Configure institutional management instances built Arches Platform Assist teams by translating stakeholder feature requests","Bachelor's degree related field combination education relevant experience 2-5 years software development Experience data-oriented work within cultural heritage organizations, including transformation JSON, CSV and/or XML formats Attention detail combined focus usability Excellent verbal written communication skills, especially interacting non-technical stakeholders Proficiency Python, willingness translate equivalent language relational document oriented databases Familiarity Linked Open Data standards technologies data engineering tools git docker test driven agile methodologies machine learning techniques With Semantic Architect, technical content understand project requirements design institution's model resulting APIs Engineers, ensure accuracy pipelines migrate legacy datasets Usable LOUD ecosystem design, implement validation services models Integrate external enrich reconcile Work way, supporting testing, continuous integration deployment Configure institutional management instances built Arches Platform Assist teams translating stakeholder feature requests"
450,Data Engineer,Data Engineer,"Burbank, CA 91501",Burbank,CA,"We are looking for a Data
Engineer with AWS, Snowflake, Python experience to join our AI & Analytics
team.

We are looking for

someone who has:
Â·
Worked closely with data
architect to design and implement data solutions.

Â·
Developed automated
pipelines for ingesting, cleansing and publishing data.

Â·
Developed extracts from
REST API sources in a recoverable manner.

Â·
Supported existing
solutions for Digital Application analytics.

Our Team:
Artificial intelligence
(AI) and the data it collects and analyzes will soon sit at the core of all
intelligent, human-centric businesses. By decoding customer needs, preferences,
and behaviors, our clients can understand exactly what services, products, and
experiences their consumers need. Within AI & Analytics, we work to design
the futureâa future in which trial-and-error business decisions have been
replaced by informed choices and data-supported strategies. By applying AI and
data science, we help leading companies prototype, refine, validate, and scale
their AI and analytics products and delivery models.

Requirements:

Bachelorâs Degree in Computer Science, Information Systems, or other related field as well as equivalent work experience.
Minimum 5 years of data engineering, ETL or software engineering experience with a focus on data extraction, transformation and publishing.
Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.

Experience in relevant technical languages and tools/technologies such as SQL, Python, REST APIs, Airflow, Spark, AWS.

Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Sep 24 2019

About Cognizant Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 193 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @Cognizant.
Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.","     Bachelorâs Degree in Computer Science, Information Systems, or other related field as well as equivalent work experience. Minimum 5 years of data engineering, ETL or software engineering experience with a focus on data extraction, transformation and publishing. Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards. ","Bachelorâs Degree in Computer Science, Information Systems, or other related field as well equivalent work experience. Minimum 5 years of data engineering, ETL software engineering experience with a focus on extraction, transformation and publishing. Experience creating maintaining automated pipelines, standards, best practices to maintain integrity security the data; ensure adherence developed standards.","Bachelorâs Degree Computer Science, Information Systems, related field well equivalent work experience. Minimum 5 years data engineering, ETL software engineering experience focus extraction, transformation publishing. Experience creating maintaining automated pipelines, standards, best practices maintain integrity security data; ensure adherence developed standards."
451,Data Engineer,Big Data Engineer,"Woodland Hills, CA",Woodland Hills,CA,"We are looking for Bigdata Developer. This is a client
facing role & the candidate will have regular interactions with various client
managers.

Responsibilities:
Own it â take full engineering responsibility from Dev to Test to
Deploy

Lead the architecture, design, and development of components and
services to enable Machine Learning at scale

Identify and recommend the most appropriate paradigms and
technology choices for batch and realtime ML scenarios

Write highly efficient Spark jobs to process TB of data

Collect heuristics and optimize Spark jobs on large Hadoop
clusters

Perform DevOps on AWS to ensure resiliency, availability,
security, and HA/DR

Partner with a x-functional team of Architects, Data scientists,
Data engineers, Analysts, PMs, and customers

Write clean, efficient code and deployment artifacts in Java or
Python or Chef

Teach and mentor other engineers on the team

Technical Skills SNo Primary Skill Proficiency Level * Rqrd./Dsrd. 1 SparkSQL PL2 Required 2 Tableau PL2 Required 3 Oracle PL2 Desired 4 PL/SQL PL2 Required


Proficiency Legends Proficiency Level Generic Reference PL1 The associate has basic awareness and comprehension of the skill and is in the process of acquiring this skill through various channels. PL2 The associate possesses working knowledge of the skill, and can actively and independently apply this skill in engagements and projects. PL3 The associate has comprehensive, in-depth and specialized knowledge of the skill. She / he has extensively demonstrated successful application of the skill in engagements or projects. PL4 The associate can function as a subject matter expert for this skill. The associate is capable of analyzing, evaluating and synthesizing solutions using the skill.

Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Jun 13 2019

About Cognizant Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 193 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @Cognizant.
Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.","   Proficiency Legends Proficiency Level Generic Reference PL1 The associate has basic awareness and comprehension of the skill and is in the process of acquiring this skill through various channels. PL2 The associate possesses working knowledge of the skill, and can actively and independently apply this skill in engagements and projects. PL3 The associate has comprehensive, in-depth and specialized knowledge of the skill. She / he has extensively demonstrated successful application of the skill in engagements or projects. PL4 The associate can function as a subject matter expert for this skill. The associate is capable of analyzing, evaluating and synthesizing solutions using the skill.   ","Proficiency Legends Level Generic Reference PL1 The associate has basic awareness and comprehension of the skill is in process acquiring this through various channels. PL2 possesses working knowledge skill, can actively independently apply engagements projects. PL3 comprehensive, in-depth specialized skill. She / he extensively demonstrated successful application or PL4 function as a subject matter expert for capable analyzing, evaluating synthesizing solutions using","Proficiency Legends Level Generic Reference PL1 The associate basic awareness comprehension skill process acquiring various channels. PL2 possesses working knowledge skill, actively independently apply engagements projects. PL3 comprehensive, in-depth specialized skill. She / extensively demonstrated successful application PL4 function subject matter expert capable analyzing, evaluating synthesizing solutions using"
452,Data Engineer,"Sr. Engineer, Data Management","Pasadena, CA 91107",Pasadena,CA,"Job Type:
Regular
Department:
Data Warehouse
City:
Pasadena
State/Territory:
California
FLSA:
Exempt
Job Responsibilities

Design, build and oversee the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establish and build processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develop technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Create and establish design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Review internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs.


Job Requirements

Bachelorâs degree or equivalent in Computer Science, Computer Engineering or related field, plus five (5) years of experience as a Software Engineer, Database Engineer, Systems Engineer or related occupation. Must have experience in building and maintaining large-scale data-intensive systems using advanced SQL, ELT/ETL tools and concepts (including SSIS, Talend, Pentaho), OLTP, OLAP and MDM, multi-dimensional data modeling, and query performance optimizations; Participating in all phases of the software development life cycle (SDLC); Analyzing source data using 3NF, DBA, and reporting and analytical tools; Working on scripting languages including Bash, Powershell, Python, and Ruby; Utilizing Microsoft BI Stack, .Net, C#, and schedulers and source control software including TFS; and Experience with Amazon cloud technologies including Amazon RedShift, S3, Elastic MapReduce, and Hive.
OR

Masterâs degree or equivalent in Computer Science, Computer Engineering or related field., plus three (3) years of experience as a Software Engineer, Database Engineer, Systems Engineer or related occupation. Must have experience in building and maintaining large-scale data-intensive systems using advanced SQL, ELT/ETL tools and concepts (including SSIS, Talend, Pentaho), OLTP, OLAP and MDM, multi-dimensional data modeling, and query performance optimizations; Participating in all phases of the software development life cycle (SDLC); Analyzing source data using 3NF, DBA, and reporting and analytical tools; Working on scripting languages including Bash, Powershell, Python, and Ruby; Utilizing Microsoft BI Stack, .Net, C#, and schedulers and source control software including TFS; and Experience with Amazon cloud technologies including Amazon RedShift, S3, Elastic MapReduce, and Hive.
Send Resumes to: Amber Taylorson, Green Dot Corporation, 3465 E. Foothill Blvd. Pasadena, CA 91107

Green Dot Corporation is committed to achieving a diverse workforce and is proud to be an equal opportunity employer without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any category protected by law.


We do not accept unsolicited resumes from employment agencies. No fee of any kind will be paid in the event we hire a candidate whose resume is submitted by an employment agency to this career site or directly to any of our employees. Such resume shall be deemed the sole property of Green Dot Corporation. Employment agencies that have fee agreements with us and have been directed by our designated HR personnel to recruit for a particular position may submit resumes to such designated HR personnel or as otherwise directed by such personnel.","   Design, build and oversee the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establish and build processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed  cloud  structures, local databases, and other applicable storage forms as required. Develop technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Create and establish design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Review internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs.    Bachelorâs degree or equivalent in Computer Science, Computer Engineering or related field, plus five  5  years of experience as a Software Engineer, Database Engineer, Systems Engineer or related occupation. Must have experience in building and maintaining large-scale data-intensive systems using advanced SQL, ELT/ETL tools and concepts  including SSIS, Talend, Pentaho , OLTP, OLAP and MDM, multi-dimensional data modeling, and query performance optimizations; Participating in all phases of the software development life cycle  SDLC ; Analyzing source data using 3NF, DBA, and reporting and analytical tools; Working on scripting languages including Bash, Powershell, Python, and Ruby; Utilizing Microsoft BI Stack, .Net, C , and schedulers and source control software including TFS; and Experience with Amazon cloud technologies including Amazon RedShift, S3, Elastic MapReduce, and Hive. ","Design, build and oversee the deployment operation of technology architecture, solutions software to capture, manage, store utilize structured unstructured data from internal external sources. Establish processes structures based on business technical requirements channel multiple inputs, route appropriately using any combination distributed cloud structures, local databases, other applicable storage forms as required. Develop tools programming that leverage artificial intelligence, machine learning big-data techniques cleanse, organize transform maintain, defend update integrity an automated basis. Create establish design standards assurance for software, systems applications development ensure compatibility operability connections, flows requirements. Review product operations activity suggests changes upgrades accommodate ongoing needs. Bachelorâs degree or equivalent in Computer Science, Engineering related field, plus five 5 years experience a Software Engineer, Database Systems Engineer occupation. Must have building maintaining large-scale data-intensive advanced SQL, ELT/ETL concepts including SSIS, Talend, Pentaho , OLTP, OLAP MDM, multi-dimensional modeling, query performance optimizations; Participating all phases life cycle SDLC ; Analyzing source 3NF, DBA, reporting analytical tools; Working scripting languages Bash, Powershell, Python, Ruby; Utilizing Microsoft BI Stack, .Net, C schedulers control TFS; Experience with Amazon technologies RedShift, S3, Elastic MapReduce, Hive.","Design, build oversee deployment operation technology architecture, solutions software capture, manage, store utilize structured unstructured data internal external sources. Establish processes structures based business technical requirements channel multiple inputs, route appropriately using combination distributed cloud structures, local databases, applicable storage forms required. Develop tools programming leverage artificial intelligence, machine learning big-data techniques cleanse, organize transform maintain, defend update integrity automated basis. Create establish design standards assurance software, systems applications development ensure compatibility operability connections, flows requirements. Review product operations activity suggests changes upgrades accommodate ongoing needs. Bachelorâs degree equivalent Computer Science, Engineering related field, plus five 5 years experience Software Engineer, Database Systems Engineer occupation. Must building maintaining large-scale data-intensive advanced SQL, ELT/ETL concepts including SSIS, Talend, Pentaho , OLTP, OLAP MDM, multi-dimensional modeling, query performance optimizations; Participating phases life cycle SDLC ; Analyzing source 3NF, DBA, reporting analytical tools; Working scripting languages Bash, Powershell, Python, Ruby; Utilizing Microsoft BI Stack, .Net, C schedulers control TFS; Experience Amazon technologies RedShift, S3, Elastic MapReduce, Hive."
453,Data Engineer,AWS Data Engineer - Python / SQL & Snowflake,"Burbank, CA",Burbank,CA,"At least 2 years of experience with Python
At least 3 years of experience with SQL (complex SQL)
Snowflake experience in at least one PoC
Database design experience
DWH experience for at least 2 years in AWS environment
Excellent problem solving experience
Strong Analytical capabilities
Spark experience is not required but good to have
Airflow experience is not required but good to have
Competent in the use of own technology area for client and Capgemini benefit
Demonstrates an interest in associated technologies
Undertakes analysis of user and business requirements, with particular regard to their impact on existing systems and environments, and produce an appropriate business and/or system design
Assists in the construction and implementation aspects of delivery
Has a (deep) understanding of the business and business processes
Excellent analytical and problem solving skills
Excellent verbal and written communication skills


Candidates should be flexible / willing to work across this delivery landscape which includes and not limited to Agile Applications Development, Support and Deployment.

Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.
Qualifications
(Includes Data Modeler, Data Miner.) Responsible for importing, cleaning, transforming, validating and modeling data with the purpose of understanding and drawing conclusions from data (may be presented in charts, graphs, and/or tables). Also, design and develop relational databases for collecting and storing data and build and design data input and data collection mechanisms.

Required Skills and Experience:
You are responsible for data related activities such as data extraction, profiling, cleansing, de-duplication, standardization, conversion, transformation and loading, data mining, warehousing, archiving and reporting. Responsible for all activities required to ensure optimum performance and data integrity of databases in production environments, in line with the requirements. Responsible for server based databases in development and test environments including database software installation, database creation, performance and capacity design, backup and recovery design, security design.

Qualifications: 3-7 years (2 years min relevant experience in the role) experience, Bachelorâs Degree.
Should be proficient in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Life cycle and Data Management.
Should have progressing skills in Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Click the following link for more information on your rights as an Applicant - http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law

About Capgemini
A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clientsâ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.
Visit us at www.capgemini.com. People matter, results count.","Qualifications  3-7 years  2 years min relevant experience in the role  experience, Bachelorâs Degree.    ","Qualifications 3-7 years 2 min relevant experience in the role experience, Bachelorâs Degree.","Qualifications 3-7 years 2 min relevant experience role experience, Bachelorâs Degree."
